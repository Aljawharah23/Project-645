[
    [
        "DRILL-93",
        "DRILL-91",
        "Support more file formats We should support scanners for:\r\n\r\nParquet\r\nORC\r\nRC\r\nCSV\r\nSequenceFile\r\n\r\nThis JIRA can be replaced by an exploded set of JIRA's or sub-tasks can be added if necessary.",
        "Support more file formats We should support scanners for:\r\n\r\nParquet\r\nORC\r\nRC\r\nCSV\r\nSequenceFile\r\n\r\nThis JIRA can be replaced by an exploded set of JIRA's or sub-tasks can be added if necessary."
    ],
    [
        "DRILL-165",
        "DRILL-106",
        "Directory Code cleanup: Move prototype to base directory and reclassify other directories as  needed We need to promote the prototype directory to the top level.  This isn't hard but requires that the maximum changes be committed or rebases will be painful.  We should do this sooner rather than later.",
        "Reorganize file structure of git repo The git repo has a number of old directories and dead ends that are not needed. We should pull the contents of the prototype directory up so they are easier to find for people exploring drill."
    ],
    [
        "DRILL-209",
        "DRILL-208",
        "Build is broken due to incorrect dependency versions of drill artifacts Some of the project poms refers uses specific version numbers (e.g. \"1.0-SNAPSHOT\") instead of $\\{project.version\\} to specify dependencies on other drill modules.",
        "Build fails Following the steps of the M1 README.MD, a fresh build fails. (Some cleanup activity first):\r\n\r\n//Hard clean git \r\nHarris-MacBook-Pro:git harri$ rm -rf incubator-drill/\r\n\r\n\r\n//Hard clean maven repository\r\nHarris-MacBook-Pro:git harri$ cd ..\r\nHarris-MacBook-Pro:~ harri$ rm -rf .m2\r\n\r\n\r\n//Checks\r\nHarris-MacBook-Pro:~ harri$ protoc --version\r\nlibprotoc 2.5.0\r\n\r\n\r\nHarris-MacBook-Pro:git harri$ java -version\r\njava version \"1.7.0_25\"\r\nJava(TM) SE Runtime Environment (build 1.7.0_25-b15)\r\nJava HotSpot(TM) 64-Bit Server VM (build 23.25-b01, mixed mode)\r\nHarris-MacBook-Pro:git harri$ \r\n\r\n\r\nHarris-MacBook-Pro:git harri$ mvn -v\r\nApache Maven 3.0.3 (r1075438; 2011-02-28 19:31:09+0200)\r\nMaven home: /usr/share/maven\r\nJava version: 1.7.0_25, vendor: Oracle Corporation\r\nJava home: /Library/Java/JavaVirtualMachines/jdk1.7.0_25.jdk/Contents/Home/jre\r\nDefault locale: en_US, platform encoding: UTF-8\r\nOS name: \"mac os x\", version: \"10.8.4\", arch: \"x86_64\", family: \"mac\"\r\nHarris-MacBook-Pro:git harri$ \r\n\r\n\r\n// Get the source\r\nHarris-MacBook-Pro:~ harri$ cd git\r\nHarris-MacBook-Pro:git harri$ git clone https://github.com/apache/incubator-drill.git\r\nCloning into 'incubator-drill'...\r\nremote: Counting objects: 8598, done.\r\nremote: Compressing objects: 100% (5001/5001), done.\r\nremote: Total 8598 (delta 3188), reused 7158 (delta 1914)\r\nReceiving objects: 100% (8598/8598), 3.65 MiB | 669 KiB/s, done.\r\nResolving deltas: 100% (3188/3188), done.\r\n\r\n//Build\r\nHarris-MacBook-Pro:git harri$ cd incubator-drill\r\nHarris-MacBook-Pro:incubator-drill harri$ mvn clean install\r\n\r\n\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Reactor Summary:\r\n[INFO] \r\n[INFO] Apache Drill Root POM ............................. SUCCESS [1:46.010s]\r\n[INFO] Common (Logical Plan, Base expressions) ........... SUCCESS [1:05.267s]\r\n[INFO] contrib/Parent Pom ................................ SUCCESS [0.119s]\r\n[INFO] contrib/hbase-storage-engine ...................... SUCCESS [0.155s]\r\n[INFO] contrib/sqlline ................................... FAILURE [21.821s]\r\n[INFO] exec/Parent Pom ................................... SKIPPED\r\n[INFO] exec/Netty Little Endian Buffers .................. SKIPPED\r\n[INFO] exec/Java Execution Engine ........................ SKIPPED\r\n[INFO] exec/Reference Interpreter ........................ SKIPPED\r\n[INFO] SQL Parser ........................................ SKIPPED\r\n[INFO] Packaging and Distribution Assembly ............... SKIPPED\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD FAILURE\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time: 3:16.181s\r\n[INFO] Finished at: Wed Sep 04 23:36:06 EEST 2013\r\n[INFO] Final Memory: 29M/165M\r\n[INFO] ------------------------------------------------------------------------\r\n[ERROR] Failed to execute goal on project sqlline: Could not resolve dependencies for project org.apache.drill:sqlline:jar:1.0.0-m2-SNAPSHOT: Could not find artifact org.apache.drill:sqlparser:jar:1.0-SNAPSHOT in sonatype-nexus-snapshots (https://oss.sonatype.org/content/repositories/snapshots) -> [Help 1]\r\n[ERROR] \r\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n[ERROR] \r\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException\r\n[ERROR] \r\n[ERROR] After correcting the problems, you can resume the build with the command\r\n[ERROR]   mvn <goals> -rf :sqlline\r\n\r\n\r\n// Just in case...\r\nHarris-MacBook-Pro:incubator-drill harri$ git checkout tags/drill-root-1.0.0-m1\r\n\r\nHarris-MacBook-Pro:incubator-drill harri$ mvn clean install\r\n[ERROR] Failed to execute goal on project sqlline: Could not resolve dependencies for project org.apache.drill:sqlline:jar:1.0.0-m1: Failure to find org.apache.drill:sqlparser:jar:1.0-SNAPSHOT in https://oss.sonatype.org/content/repositories/snapshots was cached in the local repository, resolution will not be reattempted until the update interval of sonatype-nexus-snapshots has elapsed or updates are forced -> [Help 1]\r\n[ERROR] \r\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n[ERROR] \r\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException\r\n[ERROR] \r\n[ERROR] After correcting the problems, you can resume the build with the command\r\n[ERROR]   mvn <goals> -rf :sqlline\r\nHarris-MacBook-Pro:incubator-drill harri$ \r\n\r\n\r\n"
    ],
    [
        "DRILL-399",
        "DRILL-397",
        "Specifying schema=dfs and not specifying schema at query does not work Repro steps:\r\n\r\nStart sqlline \r\nbin/sqlline -u jdbc:drill:schema=dfs -n admin -p admin\r\n\r\n0: jdbc:drill:schema=dfs> select * from `/region.parquet`;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"76bae1a1-3b86-4dc8-9cd4-6e42cb7fe900\"\r\nendpoint {\r\n  address: \"perfnode166.perf.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < ValidationException:[ org.eigenbase.util.EigenbaseContextException: From line 1, column 15 to line 1, column 31 ] < EigenbaseContextException:[ From line 1, column 15 to line 1, column 31 ] < SqlValidatorException:[ Table \\'/region.parquet\\' not found ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n\r\nThis works though.\r\n0: jdbc:drill:schema=dfs> select * from dfs.`region.parquet`;\r\n\r\nAlso, specifying cp as schema works.\r\n\r\n[root@perfnode166 drill]# bin/sqlline -u jdbc:drill:schema=cp -n admin -p admin\r\n\r\n0: jdbc:drill:schema=cp> select * from `employee.json`;\r\n\r\n",
        "need to be able to specify schema through sqlline Before the current merge, we can specify schema through sqlline like this:\r\n\r\n./sqlline -u jdbc:drill:schema=parquet-local -n admin -p admin\r\n\r\nWe need this feature be restored so we don't need to modify our existing queries to add storage engine type such as cp.`employee.json`."
    ],
    [
        "DRILL-406",
        "DRILL-365",
        "JSON integer range of valid values in drill different from native JSON JSON has a maximum integer value of 9007199254740992 (2^53). However Drill seems to follow maximum integer range for JSON. IE 2^31\r\n\r\n0: jdbc:drill:schema=dfs> select salary from dfs.`/user/root/employeeNestedObject.json`;\r\n+------------+\r\n|   salary   |\r\n+------------+\r\n| 2147483647 |\r\n+------------+\r\n1 row selected (0.079 seconds)\r\n\r\nHowever when data is above 2^31 (Integer range) it fails with the exception\r\n\r\n13:29:12.926 [WorkManager-3] ERROR o.a.d.e.s.easy.json.JSONRecordReader - Error reading next in Json reader\r\ncom.fasterxml.jackson.core.JsonParseException: Numeric value (2147483648) out of range of int\r\n at [Source: com.mapr.fs.MapRFsDataInputStream@6d9acc54; line: 6, column: 23]\r\n\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1369) ~[jackson-core-2.2.0.jar:2.2.0]\r\n\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:599) ~[jackson-core-2.2.0.jar:2.2.0]\r\n\tat com.fasterxml.jackson.core.base.ParserBase.convertNumberToInt(ParserBase.java:847) ~[jackson-core-2.2.0.jar:2.2.0]\r\n\tat com.fasterxml.jackson.core.base.ParserBase.getIntValue(ParserBase.java:643) ~[jackson-core-2.2.0.jar:2.2.0]\r\n\tat org.apache.drill.exec.schema.json.jackson.JacksonHelper.getValueFromFieldType(JacksonHelper.java:86) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.easy.json.JSONRecordReader$ReadType.recordData(JSONRecordReader.java:398) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.easy.json.JSONRecordReader$ReadType.readRecord(JSONRecordReader.java:304) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.easy.json.JSONRecordReader.next(JSONRecordReader.java:143) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:94) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.next(AbstractSingleRecordBatch.java:42) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.next(ScreenCreator.java:80) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:83) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n\tat java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n13:29:12.928 [WorkManager-3] DEBUG o.a.d.e.w.fragment.FragmentExecutor - Caught exception while running fragment\r\njava.lang.NullPointerException: null\r\n\tat org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.setupNewSchema(ProjectRecordBatch.java:131) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.next(AbstractSingleRecordBatch.java:53) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.next(ScreenCreator.java:80) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:83) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n\tat java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n13:29:12.929 [WorkManager-3] ERROR o.a.d.e.w.f.AbstractStatusReporter - Error 05a5adcf-4e7b-4a3c-a509-5ac75198856f: Failure while running fragment.\r\njava.lang.NullPointerException: null\r\n\tat org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.setupNewSchema(ProjectRecordBatch.java:131) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.next(AbstractSingleRecordBatch.java:53) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.next(ScreenCreator.java:80) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:83) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n\tat java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n\r\n",
        "Large number is taken in as int which overflows if it is too big 0: jdbc:drill:schema=json-cp> select * from \"customer.json\";\r\nwill fail with Numeric value (xxxxxxxxxxxxx) out of range of int\r\n\r\n"
    ],
    [
        "DRILL-428",
        "DRILL-419",
        "Reading from parquet file created from impala fails 0: jdbc:drill:> select * from dfs.home.`customer.impala.parquet`;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"f6d7ebbc-5618-4a8c-b34f-d62e57606394\"\r\nendpoint {\r\n  address: \"perfnode166.perf.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < UnsupportedOperationException:[ Error decoding [c_name] BINARY. PLAIN_DICTIONARY is dictionary based ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n",
        "parquet files created using dictionary encoding cannot be read in drill 0: jdbc:drill:> select * from dfs.home.`customer_drill.dict.parquet`;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"a01e5572-8587-4aa8-a352-ac64674f58c4\"\r\nendpoint {\r\n  address: \"perfnode166.perf.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < NullPointerException\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n\r\nFrom logs:\r\n11:24:07.681 [WorkManager-4] DEBUG o.a.d.e.w.fragment.FragmentExecutor - Caught exception while running fragment\r\njava.lang.NullPointerException: null\r\n        at org.apache.drill.exec.store.parquet.PageReadStatus.next(PageReadStatus.java:89) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.store.parquet.VarLenBinaryReader.readFields(VarLenBinaryReader.java:113) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.store.parquet.ParquetRecordReader.next(ParquetRecordReader.java:368) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:94) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.next(AbstractSingleRecordBatch.java:42) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.next(ScreenCreator.java:80) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:83) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n11:24:07.682 [WorkManager-4] ERROR o.a.d.e.w.f.AbstractStatusReporter - Error a01e5572-8587-4aa8-a352-ac64674f58c4: Failure while running fragment.\r\njava.lang.NullPointerException: null\r\n        at org.apache.drill.exec.store.parquet.PageReadStatus.next(PageReadStatus.java:89) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.store.parquet.VarLenBinaryReader.readFields(VarLenBinaryReader.java:113) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.store.parquet.ParquetRecordReader.next(ParquetRecordReader.java:368) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:94) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.next(AbstractSingleRecordBatch.java:42) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.next(ScreenCreator.java:80) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:83) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n11:24:07.683 [WorkManager-4] DEBUG o.a.d.exec.work.foreman.QueryManager - New fragment status was provided to Foreman of memory_use: 450020\r\nbatches_completed: 0\r\nrecords_completed: 0\r\nstate: FAILED\r\n"
    ],
    [
        "DRILL-453",
        "DRILL-405",
        "Query on a directory throws exception Following exception is thrown when issuing query against a directory.\r\nThe directory contained 2 files of similar schema. Queries against each single file works but query against doesn't work.\r\n0: jdbc:drill:> select * from dfs.`/home/mapr/apache-drill-1.0.0-m2-incubating-SNAPSHOT/sample-data/fb`;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"3dc30404-99b7-4ba1-ba38-c8afb7006fb0\"\r\nendpoint {\r\n  address: \"ubuntu\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < ValidationException:[ org.eigenbase.util.EigenbaseContextException: From line 1, column 15 to line 1, column 87 ] < EigenbaseContextException:[ From line 1, column 15 to line 1, column 87 ] < SqlValidatorException:[ Table \\'dfs./home/mapr/apache-drill-1.0.0-m2-incubating-SNAPSHOT/sample-data/fb\\' not found ]\"\r\n]Error: exception while executing query (state=,code=0)",
        "Not able to query directory containing json files Trying to query a directory which contains json files fails. The same test works for parquet files. Also if entire path upto the file is specified the json file can be queried successfully.\r\n\r\n0: jdbc:drill:schema=dfs> select * from dfs.`/user/root/testJsonDir`;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"afb20373-1010-4d2d-8a88-d89015db5801\"\r\nendpoint {\r\n  address: \"perfnode166.perf.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < ValidationException:[ org.eigenbase.util.EigenbaseContextException: From line 1, column 15 to line 1, column 42 ] < EigenbaseContextException:[ From line 1, column 15 to line 1, column 42 ] < SqlValidatorException:[ Table \\'dfs./user/root/testJsonDir\\' not found ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n\r\nLog file has\r\n\r\n13:16:57.398 [WorkManager-2] DEBUG o.a.d.e.w.fragment.FragmentExecutor - Fragment runner complete. 0:0\r\n13:17:03.797 [WorkManager Event Thread] DEBUG o.apache.drill.exec.work.WorkManager - Starting pending task org.apache.drill.exec.work.foreman.Foreman@771fa2d1\r\n13:17:03.806 [WorkManager-3] DEBUG o.a.d.e.s.dfs.WorkspaceSchemaFactory - File read failed.\r\njava.io.IOException: Open failed for file: /user/root/testJsonDir, error: Invalid argument (22)\r\n\tat com.mapr.fs.MapRClientImpl.open(MapRClientImpl.java:141) ~[maprfs-core-1.0.3-mapr-3.0.2.jar:na]\r\n\tat com.mapr.fs.MapRFileSystem.open(MapRFileSystem.java:499) ~[maprfs-core-1.0.3-mapr-3.0.2.jar:na]\r\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:463) ~[hadoop-0.20.2-dev-core.jar:na]\r\n\tat org.apache.drill.exec.store.dfs.shim.fallback.FallbackFileSystem.open(FallbackFileSystem.java:89) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.dfs.BasicFormatMatcher$MagicStringMatcher.matches(BasicFormatMatcher.java:104) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.dfs.BasicFormatMatcher.isReadable(BasicFormatMatcher.java:77) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.dfs.BasicFormatMatcher.isReadable(BasicFormatMatcher.java:64) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.parquet.ParquetFormatPlugin$ParquetFormatMatcher.isReadable(ParquetFormatPlugin.java:163) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory.create(WorkspaceSchemaFactory.java:77) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory.create(WorkspaceSchemaFactory.java:35) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.getNewEntry(ExpandingConcurrentMap.java:83) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.containsKey(ExpandingConcurrentMap.java:64) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.planner.sql.ExpandingConcurrentMap$DelegatingKeySet.contains(ExpandingConcurrentMap.java:160) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat com.google.common.collect.Maps$AsMapView.containsKey(Maps.java:788) [guava-14.0.1.jar:na]\r\n\tat net.hydromatic.optiq.util.CompositeMap.get(CompositeMap.java:91) [optiq-core-0.4.18.jar:na]\r\n\tat net.hydromatic.optiq.prepare.OptiqCatalogReader.getTableFrom(OptiqCatalogReader.java:89) [optiq-core-0.4.18.jar:na]\r\n\tat net.hydromatic.optiq.prepare.OptiqCatalogReader.getTable(OptiqCatalogReader.java:77) [optiq-core-0.4.18.jar:na]\r\n\tat net.hydromatic.optiq.prepare.OptiqCatalogReader.getTable(OptiqCatalogReader.java:43) [optiq-core-0.4.18.jar:na]\r\n\tat org.eigenbase.sql.validate.IdentifierNamespace.validateImpl(IdentifierNamespace.java:67) [optiq-core-0.4.18.jar:na]\r\n\tat org.eigenbase.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:80) [optiq-core-0.4.18.jar:na]\r\n\tat org.eigenbase.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:753) [optiq-core-0.4.18.jar:na]\r\n\tat org.eigenbase.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:742) [optiq-core-0.4.18.jar:na]\r\n\tat org.eigenbase.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:2672) [optiq-core-0.4.18.jar:na]\r\n\tat org.eigenbase.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:2899) [optiq-core-0.4.18.jar:na]\r\n\tat org.eigenbase.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) [optiq-core-0.4.18.jar:na]\r\n\tat org.eigenbase.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:80) [optiq-core-0.4.18.jar:na]\r\n\tat org.eigenbase.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:753) [optiq-core-0.4.18.jar:na]\r\n\tat org.eigenbase.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:742) [optiq-core-0.4.18.jar:na]\r\n\tat org.eigenbase.sql.SqlSelect.validate(SqlSelect.java:135) [optiq-core-0.4.18.jar:na]\r\n\tat org.eigenbase.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:707) [optiq-core-0.4.18.jar:na]\r\n\tat org.eigenbase.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:423) [optiq-core-0.4.18.jar:na]\r\n\tat net.hydromatic.optiq.prepare.PlannerImpl.validate(PlannerImpl.java:153) [optiq-core-0.4.18.jar:na]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:76) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:350) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:175) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n\tat java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n"
    ],
    [
        "DRILL-456",
        "DRILL-220",
        "Inconsistent results between Hive and Drill (an extra row appeared in Drill) Tried a simple query as below.\r\nThe results look different between Hive and Drill (note the extra result row in case of Drill)\r\n\r\nQuery:\r\nselect l_returnflag,l_linestatus, count(*)  from lineitem  group by l_returnflag, l_linestatus order by l_returnflag,l_linestatus;\r\n\r\nHive results:\r\nA\tF\t147790\r\nN\tF\t3765\r\nN\tO\t300716\r\nR\tF\t148301\r\n\r\nDrill results:\r\n+------------+--------------+--------------+\r\n|   EXPR$2   | l_returnflag | l_linestatus |\r\n+------------+--------------+--------------+\r\n| 147790     | A            | F            |\r\n| 3765       | N            | F            |\r\n| 0          | N            | O            |\r\n| 300716     | N            | O            |\r\n| 148301     | R            | F            |\r\n+------------+--------------+--------------+\r\n\r\n",
        "streaming-aggregate returning an extra row Here is a physical plan:\r\n\r\n{\r\n  head : {\r\n    type : \"APACHE_DRILL_PHYSICAL\",\r\n    version : 1,\r\n    generator : {\r\n      type : \"optiq\",\r\n      info : \"na\"\r\n    }\r\n  },\r\n  graph : [ {\r\n    pop : \"parquet-scan\",\r\n    @id : 1,\r\n    entries : [ {\r\n      path : \"/tmp/tpc-h/customer\"\r\n    } ],\r\n    storageengine : {\r\n      type : \"parquet\",\r\n      dfsName : \"file:///\"\r\n    },\r\n    ref : \"_MAP\",\r\n    fragmentPointer : 0\r\n  }, {\r\n    pop : \"project\",\r\n    @id : 2,\r\n    exprs : [ {\r\n      ref : \"output.NATIONKEY\",\r\n      expr : \"_MAP.C_NATIONKEY\"\r\n    }, {\r\n      ref : \"output.CUSTKEY\",\r\n      expr : \"_MAP.C_CUSTKEY\"\r\n    } ],\r\n    child : 1\r\n  }, {\r\n    pop : \"sort\",\r\n    @id : 3,\r\n    child : 2,\r\n    orderings : [ {\r\n      order : \"ASC\",\r\n      expr : \"NATIONKEY\"\r\n    } ],\r\n    reverse : false\r\n  }, {\r\n    pop : \"streaming-aggregate\",\r\n    @id : 4,\r\n    child : 3,\r\n    keys : [ {\r\n      ref : \"NATIONKEY\",\r\n      expr : \"NATIONKEY\"\r\n    } ],\r\n    exprs : [ {\r\n      ref : \"CUSTOMERS\",\r\n      expr : \"count(1) \"\r\n    } ]\r\n  }, {\r\n    pop : \"screen\",\r\n    @id : 5,\r\n    child : 4\r\n  } ]\r\n}\r\n\r\nIt is returning these results:\r\n\r\n-----------------------------------\r\n| NATIONKEY      | CUSTOMERS      |\r\n-----------------------------------\r\n| 4              | 0              |\r\n| 0              | 59916          |\r\n| 1              | 59841          |\r\n| 2              | 59952          |\r\n| 3              | 59849          |\r\n| 4              | 59969          |\r\n| 5              | 60471          |\r\n| 6              | 60316          |\r\n| 7              | 60153          |\r\n| 8              | 60215          |\r\n| 9              | 60236          |\r\n| 10             | 60101          |\r\n| 11             | 60056          |\r\n| 12             | 59757          |\r\n| 13             | 59909          |\r\n| 14             | 59476          |\r\n| 15             | 59834          |\r\n| 16             | 59796          |\r\n| 17             | 59788          |\r\n| 18             | 60065          |\r\n| 19             | 60048          |\r\n| 20             | 59803          |\r\n| 21             | 59997          |\r\n| 22             | 60065          |\r\n| 23             | 60381          |\r\n| 24             | 60006          |\r\n-----------------------------------\r\n\r\n"
    ],
    [
        "DRILL-709",
        "DRILL-520",
        "FLOOR function should return an int instead of decimal Drill's FLOOR function returns a decimal instead of an int.  For example:\r\nselect floor(100.75) from student where student_id=11;\r\nReturns 100.0\r\n\r\n",
        "ceiling/ceil and floor functions return decimal value instead of an integer Ran the following queries in drill:\r\n0: jdbc:drill:schema=dfs> select ceiling(55.8) from dfs.`student` where rownum=11;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 56.0       |\r\n+------------+\r\n\r\n0: jdbc:drill:schema=dfs> select floor(55.8) from dfs.`student` where rownum=11;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 55.0       |\r\n+------------+\r\n\r\nThe same queries executed from oracle, postgres and mysql returned integer values of 56 and 55.\r\n\r\nFound the following description of the two functions from http://users.atw.hu/sqlnut/sqlnut2-chp-4-sect-4.html :\r\n\r\nCeil/Ceiling:\r\nRounds a noninteger value upwards to the next greatest integer. Returns an integer value unchanged.\r\nFloor:\r\nRounds a noninteger value downwards to the next least integer. Returns an integer value unchanged."
    ],
    [
        "DRILL-744",
        "DRILL-699",
        "Functions should be case insensitive If the function names are in upper case, the query fails.  For example:\r\n\r\n0: jdbc:drill:schema=dfs> select to_date('2003/07/09', 'yyyy/MM/dd') from voter where voter_id=10;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 2003-07-09T00:00:00.000-07:00 |\r\n+------------+\r\n\r\nSame query with function name in uppercase:\r\nselect TO_DATE('2003/07/09', 'yyyy/MM/dd') from voter where voter_id=10;\r\n\r\nmessage: \"Failure while parsing sql. < ValidationException:[ org.eigenbase.util.EigenbaseContextException: From line 1, column 8 to line 1, column 42 ] < EigenbaseContextException:[ From line 1, column 8 to line 1, column 42 ] < SqlValidatorException:[ No match found for function signature TO_DATE(<CHARACTER>, <CHARACTER>) ]\"\r\n]",
        "Function resolutions fails to find functions in registry during the materialization due to case sensitive issues We maintain a Map<String, DrillFuncHolder> for functions that map the given function name to holder. Here the function name is added to map as it is given in FunctionTemplate without any case changes. The problem is after the Optiq-Drill conversion, all function names are converted to lowercase (in DrillOptiq.java). So if a function name is \"isTrue\" in the registry, we search for \"istrue\" and don't find any function during materialization."
    ],
    [
        "DRILL-746",
        "DRILL-600",
        "Union all operator not working with tables in hive Attached the DDL and a small data set that I used. The below query fails to be parsed\r\n\r\nselect * from hive.hivestudents1 union all select * from hive.hivestudents2;\r\n\r\nError: exception while executing query (state=,code=0)\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"2602c8e6-2654-40bd-b000-3f015e7d699f\"\r\nendpoint {\r\n  address: \"qa-node191.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < CannotPlanException:[ Node [rel#125:Subset#8.PHYSICAL.SINGLETON([]).[]] could not be implemented; planner state:\r\n\r\nRoot: rel#125:Subset#8.PHYSICAL.SINGLETON([]).[]",
        "CannotPlanException for a query with UNION ALL select c_custkey, c_nationkey from cp.`tpch/customer.parquet` c where c_acctbal between 1000 and 1200 and c_nationkey in (2, 4)\r\nunion all\r\nselect c_custkey, c_nationkey from cp.`tpch/customer.parquet` c where c_acctbal between 1000 and 1200 and c_nationkey in (1, 3)\r\n \r\norg.eigenbase.relopt.RelOptPlanner$CannotPlanException: Node [rel#2234:Subset#18.PHYSICAL.SINGLETON([]).[]] could not be implemented; planner state:\r\n\r\nRoot: rel#2234:Subset#18.PHYSICAL.SINGLETON([]).[]\r\nOriginal rel:\r\nAbstractConverter(subset=[rel#2234:Subset#18.PHYSICAL.SINGLETON([]).[]], convention=[PHYSICAL], DrillDistributionTraitDef=[SINGLETON([])], sort=[[]]): rowcount = 18.75, cumulative cost = {inf}, id = 2236\r\n  DrillScreenRel(subset=[rel#2233:Subset#18.LOGICAL.ANY([]).[]]): rowcount = 18.75, cumulative cost = {1.875 rows, 1.875 cpu, 0.0 io}, id = 2232\r\n    DrillUnionRel(subset=[rel#2231:Subset#17.LOGICAL.ANY([]).[]], all=[false]): rowcount = 18.75, cumulative cost = {9.375 rows, 9.375 cpu, 0.0 io}, id = 2230\r\n      DrillUnionRel(subset=[rel#2225:Subset#14.LOGICAL.ANY([]).[]], all=[false]): rowcount = 12.5, cumulative cost = {6.25 rows, 6.25 cpu, 0.0 io}, id = 2224\r\n        DrillProjectRel(subset=[rel#2219:Subset#11.LOGICAL.ANY([]).[]], c_custkey=[$3], c_nationkey=[$1]): rowcount = 6.25, cumulative cost = {0.625 rows, 1.25 cpu, 0.0 io}, id = 2218\r\n          DrillFilterRel(subset=[rel#2217:Subset#10.LOGICAL.ANY([]).[]], condition=[AND(>=($2, 1000), <=($2, 1200), OR(=($1, 2), =($1, 4)))]): rowcount = 6.25, cumulative cost = {0.625 rows, 10.0 cpu, 0.0 io}, id = 2216\r\n            DrillScanRel(subset=[rel#2215:Subset#9.LOGICAL.ANY([]).[]], table=[[cp, tpch/customer.parquet]]): rowcount = 100.0, cumulative cost = {100.0 rows, 101.0 cpu, 0.0 io}, id = 2193\r\n        DrillProjectRel(subset=[rel#2223:Subset#13.LOGICAL.ANY([]).[]], c_custkey=[$3], c_nationkey=[$1]): rowcount = 6.25, cumulative cost = {0.625 rows, 1.25 cpu, 0.0 io}, id = 2222\r\n          DrillFilterRel(subset=[rel#2221:Subset#12.LOGICAL.ANY([]).[]], condition=[AND(>=($2, 1000), <=($2, 1200), OR(=($1, 1), =($1, 3)))]): rowcount = 6.25, cumulative cost = {0.625 rows, 10.0 cpu, 0.0 io}, id = 2220\r\n            DrillScanRel(subset=[rel#2215:Subset#9.LOGICAL.ANY([]).[]], table=[[cp, tpch/customer.parquet]]): rowcount = 100.0, cumulative cost = {100.0 rows, 101.0 cpu, 0.0 io}, id = 2193\r\n      DrillProjectRel(subset=[rel#2229:Subset#16.LOGICAL.ANY([]).[]], c_custkey=[$3], c_nationkey=[$1]): rowcount = 6.25, cumulative cost = {0.625 rows, 1.25 cpu, 0.0 io}, id = 2228\r\n        DrillFilterRel(subset=[rel#2227:Subset#15.LOGICAL.ANY([]).[]], condition=[AND(>=($2, 1000), <=($2, 1200), OR(=($1, 2), =($1, 5)))]): rowcount = 6.25, cumulative cost = {0.625 rows, 10.0 cpu, 0.0 io}, id = 2226\r\n          DrillScanRel(subset=[rel#2215:Subset#9.LOGICAL.ANY([]).[]], table=[[cp, tpch/customer.parquet]]): rowcount = 100.0, cumulative cost = {100.0 rows, 101.0 cpu, 0.0 io}, id = 2193\r\n\r\nSets:\r\nSet#9, type: (DrillRecordRow[*, c_nationkey, c_acctbal, c_custkey])\r\n\trel#2215:Subset#9.LOGICAL.ANY([]).[], best=rel#2193, importance=0.531441\r\n\t\trel#2193:DrillScanRel.LOGICAL.ANY([]).[](table=[cp, tpch/customer.parquet]), rowcount=100.0, cumulative cost={100.0 rows, 101.0 cpu, 0.0 io}\r\n\t\trel#2246:AbstractConverter.LOGICAL.ANY([]).[](child=rel#2245:Subset#9.PHYSICAL.ANY([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=100.0, cumulative cost={inf}\r\n\t\trel#2250:AbstractConverter.LOGICAL.ANY([]).[](child=rel#2249:Subset#9.PHYSICAL.SINGLETON([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=100.0, cumulative cost={inf}\r\n\trel#2245:Subset#9.PHYSICAL.ANY([]).[], best=rel#2248, importance=0.4782969000000001\r\n\t\trel#2247:AbstractConverter.PHYSICAL.ANY([]).[](child=rel#2215:Subset#9.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=100.0, cumulative cost={inf}\r\n\t\trel#2251:AbstractConverter.PHYSICAL.ANY([]).[](child=rel#2249:Subset#9.PHYSICAL.SINGLETON([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=100.0, cumulative cost={inf}\r\n\t\trel#2252:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2215:Subset#9.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=100.0, cumulative cost={inf}\r\n\t\trel#2253:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2245:Subset#9.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=100.0, cumulative cost={inf}\r\n\t\trel#2248:ScanPrel.PHYSICAL.SINGLETON([]).[](table=[cp, tpch/customer.parquet]), rowcount=100.0, cumulative cost={100.0 rows, 101.0 cpu, 0.0 io}\r\n\trel#2249:Subset#9.PHYSICAL.SINGLETON([]).[], best=rel#2248, importance=0.4304672100000001\r\n\t\trel#2252:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2215:Subset#9.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=100.0, cumulative cost={inf}\r\n\t\trel#2253:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2245:Subset#9.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=100.0, cumulative cost={inf}\r\n\t\trel#2248:ScanPrel.PHYSICAL.SINGLETON([]).[](table=[cp, tpch/customer.parquet]), rowcount=100.0, cumulative cost={100.0 rows, 101.0 cpu, 0.0 io}\r\nSet#10, type: (DrillRecordRow[*, c_nationkey, c_acctbal, c_custkey])\r\n\trel#2217:Subset#10.LOGICAL.ANY([]).[], best=rel#2216, importance=0.5904900000000001\r\n\t\trel#2216:DrillFilterRel.LOGICAL.ANY([]).[](child=rel#2215:Subset#9.LOGICAL.ANY([]).[],condition=AND(>=($2, 1000), <=($2, 1200), OR(=($1, 2), =($1, 4)))), rowcount=6.25, cumulative cost={100.625 rows, 111.0 cpu, 0.0 io}\r\n\t\trel#2276:AbstractConverter.LOGICAL.ANY([]).[](child=rel#2275:Subset#10.PHYSICAL.ANY([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2303:AbstractConverter.LOGICAL.ANY([]).[](child=rel#2302:Subset#10.PHYSICAL.SINGLETON([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\trel#2275:Subset#10.PHYSICAL.ANY([]).[], best=rel#2301, importance=0.531441\r\n\t\trel#2277:AbstractConverter.PHYSICAL.ANY([]).[](child=rel#2217:Subset#10.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2304:AbstractConverter.PHYSICAL.ANY([]).[](child=rel#2302:Subset#10.PHYSICAL.SINGLETON([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2305:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2217:Subset#10.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2306:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2275:Subset#10.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2301:FilterPrel.PHYSICAL.SINGLETON([]).[](child=rel#2245:Subset#9.PHYSICAL.ANY([]).[],condition=AND(>=($2, 1000), <=($2, 1200), OR(=($1, 2), =($1, 4)))), rowcount=6.25, cumulative cost={100.625 rows, 111.0 cpu, 0.0 io}\r\n\trel#2302:Subset#10.PHYSICAL.SINGLETON([]).[], best=rel#2301, importance=0.4782969000000001\r\n\t\trel#2305:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2217:Subset#10.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2306:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2275:Subset#10.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2301:FilterPrel.PHYSICAL.SINGLETON([]).[](child=rel#2245:Subset#9.PHYSICAL.ANY([]).[],condition=AND(>=($2, 1000), <=($2, 1200), OR(=($1, 2), =($1, 4)))), rowcount=6.25, cumulative cost={100.625 rows, 111.0 cpu, 0.0 io}\r\nSet#11, type: RecordType(ANY c_custkey, ANY c_nationkey)\r\n\trel#2219:Subset#11.LOGICAL.ANY([]).[], best=rel#2218, importance=0.6561\r\n\t\trel#2218:DrillProjectRel.LOGICAL.ANY([]).[](child=rel#2217:Subset#10.LOGICAL.ANY([]).[],c_custkey=$3,c_nationkey=$1), rowcount=6.25, cumulative cost={101.25 rows, 112.25 cpu, 0.0 io}\r\n\t\trel#2312:AbstractConverter.LOGICAL.ANY([]).[](child=rel#2311:Subset#11.PHYSICAL.SINGLETON([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\trel#2311:Subset#11.PHYSICAL.SINGLETON([]).[], best=rel#2310, importance=0.5904900000000001\r\n\t\trel#2313:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2219:Subset#11.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2310:ProjectPrel.PHYSICAL.SINGLETON([]).[](child=rel#2302:Subset#10.PHYSICAL.SINGLETON([]).[],c_custkey=$3,c_nationkey=$1), rowcount=6.25, cumulative cost={101.25 rows, 112.25 cpu, 0.0 io}\r\nSet#12, type: (DrillRecordRow[*, c_nationkey, c_acctbal, c_custkey])\r\n\trel#2221:Subset#12.LOGICAL.ANY([]).[], best=rel#2220, importance=0.5904900000000001\r\n\t\trel#2220:DrillFilterRel.LOGICAL.ANY([]).[](child=rel#2215:Subset#9.LOGICAL.ANY([]).[],condition=AND(>=($2, 1000), <=($2, 1200), OR(=($1, 1), =($1, 3)))), rowcount=6.25, cumulative cost={100.625 rows, 111.0 cpu, 0.0 io}\r\n\t\trel#2273:AbstractConverter.LOGICAL.ANY([]).[](child=rel#2272:Subset#12.PHYSICAL.ANY([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2280:AbstractConverter.LOGICAL.ANY([]).[](child=rel#2279:Subset#12.PHYSICAL.SINGLETON([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\trel#2272:Subset#12.PHYSICAL.ANY([]).[], best=rel#2278, importance=0.531441\r\n\t\trel#2274:AbstractConverter.PHYSICAL.ANY([]).[](child=rel#2221:Subset#12.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2281:AbstractConverter.PHYSICAL.ANY([]).[](child=rel#2279:Subset#12.PHYSICAL.SINGLETON([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2282:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2221:Subset#12.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2283:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2272:Subset#12.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2278:FilterPrel.PHYSICAL.SINGLETON([]).[](child=rel#2245:Subset#9.PHYSICAL.ANY([]).[],condition=AND(>=($2, 1000), <=($2, 1200), OR(=($1, 1), =($1, 3)))), rowcount=6.25, cumulative cost={100.625 rows, 111.0 cpu, 0.0 io}\r\n\trel#2279:Subset#12.PHYSICAL.SINGLETON([]).[], best=rel#2278, importance=0.4782969000000001\r\n\t\trel#2282:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2221:Subset#12.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2283:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2272:Subset#12.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2278:FilterPrel.PHYSICAL.SINGLETON([]).[](child=rel#2245:Subset#9.PHYSICAL.ANY([]).[],condition=AND(>=($2, 1000), <=($2, 1200), OR(=($1, 1), =($1, 3)))), rowcount=6.25, cumulative cost={100.625 rows, 111.0 cpu, 0.0 io}\r\nSet#13, type: RecordType(ANY c_custkey, ANY c_nationkey)\r\n\trel#2223:Subset#13.LOGICAL.ANY([]).[], best=rel#2222, importance=0.6561\r\n\t\trel#2222:DrillProjectRel.LOGICAL.ANY([]).[](child=rel#2221:Subset#12.LOGICAL.ANY([]).[],c_custkey=$3,c_nationkey=$1), rowcount=6.25, cumulative cost={101.25 rows, 112.25 cpu, 0.0 io}\r\n\t\trel#2289:AbstractConverter.LOGICAL.ANY([]).[](child=rel#2288:Subset#13.PHYSICAL.SINGLETON([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\trel#2288:Subset#13.PHYSICAL.SINGLETON([]).[], best=rel#2287, importance=0.5904900000000001\r\n\t\trel#2290:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2223:Subset#13.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2287:ProjectPrel.PHYSICAL.SINGLETON([]).[](child=rel#2279:Subset#12.PHYSICAL.SINGLETON([]).[],c_custkey=$3,c_nationkey=$1), rowcount=6.25, cumulative cost={101.25 rows, 112.25 cpu, 0.0 io}\r\nSet#14, type: RecordType(ANY c_custkey, ANY c_nationkey)\r\n\trel#2225:Subset#14.LOGICAL.ANY([]).[], best=rel#2224, importance=0.7290000000000001\r\n\t\trel#2224:DrillUnionRel.LOGICAL.ANY([]).[](input#0=rel#2219:Subset#11.LOGICAL.ANY([]).[],input#1=rel#2223:Subset#13.LOGICAL.ANY([]).[],all=false), rowcount=12.5, cumulative cost={208.75 rows, 230.75 cpu, 0.0 io}\r\nSet#15, type: (DrillRecordRow[*, c_nationkey, c_acctbal, c_custkey])\r\n\trel#2227:Subset#15.LOGICAL.ANY([]).[], best=rel#2226, importance=0.6561\r\n\t\trel#2226:DrillFilterRel.LOGICAL.ANY([]).[](child=rel#2215:Subset#9.LOGICAL.ANY([]).[],condition=AND(>=($2, 1000), <=($2, 1200), OR(=($1, 2), =($1, 5)))), rowcount=6.25, cumulative cost={100.625 rows, 111.0 cpu, 0.0 io}\r\n\t\trel#2243:AbstractConverter.LOGICAL.ANY([]).[](child=rel#2242:Subset#15.PHYSICAL.ANY([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2256:AbstractConverter.LOGICAL.ANY([]).[](child=rel#2255:Subset#15.PHYSICAL.SINGLETON([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\trel#2242:Subset#15.PHYSICAL.ANY([]).[], best=rel#2254, importance=0.5904900000000001\r\n\t\trel#2244:AbstractConverter.PHYSICAL.ANY([]).[](child=rel#2227:Subset#15.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2257:AbstractConverter.PHYSICAL.ANY([]).[](child=rel#2255:Subset#15.PHYSICAL.SINGLETON([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2258:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2227:Subset#15.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2259:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2242:Subset#15.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2254:FilterPrel.PHYSICAL.SINGLETON([]).[](child=rel#2245:Subset#9.PHYSICAL.ANY([]).[],condition=AND(>=($2, 1000), <=($2, 1200), OR(=($1, 2), =($1, 5)))), rowcount=6.25, cumulative cost={100.625 rows, 111.0 cpu, 0.0 io}\r\n\trel#2255:Subset#15.PHYSICAL.SINGLETON([]).[], best=rel#2254, importance=0.531441\r\n\t\trel#2258:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2227:Subset#15.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2259:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2242:Subset#15.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2254:FilterPrel.PHYSICAL.SINGLETON([]).[](child=rel#2245:Subset#9.PHYSICAL.ANY([]).[],condition=AND(>=($2, 1000), <=($2, 1200), OR(=($1, 2), =($1, 5)))), rowcount=6.25, cumulative cost={100.625 rows, 111.0 cpu, 0.0 io}\r\nSet#16, type: RecordType(ANY c_custkey, ANY c_nationkey)\r\n\trel#2229:Subset#16.LOGICAL.ANY([]).[], best=rel#2228, importance=0.7290000000000001\r\n\t\trel#2228:DrillProjectRel.LOGICAL.ANY([]).[](child=rel#2227:Subset#15.LOGICAL.ANY([]).[],c_custkey=$3,c_nationkey=$1), rowcount=6.25, cumulative cost={101.25 rows, 112.25 cpu, 0.0 io}\r\n\t\trel#2265:AbstractConverter.LOGICAL.ANY([]).[](child=rel#2264:Subset#16.PHYSICAL.SINGLETON([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\trel#2264:Subset#16.PHYSICAL.SINGLETON([]).[], best=rel#2263, importance=0.6561\r\n\t\trel#2266:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2229:Subset#16.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=6.25, cumulative cost={inf}\r\n\t\trel#2263:ProjectPrel.PHYSICAL.SINGLETON([]).[](child=rel#2255:Subset#15.PHYSICAL.SINGLETON([]).[],c_custkey=$3,c_nationkey=$1), rowcount=6.25, cumulative cost={101.25 rows, 112.25 cpu, 0.0 io}\r\nSet#17, type: RecordType(ANY c_custkey, ANY c_nationkey)\r\n\trel#2231:Subset#17.LOGICAL.ANY([]).[], best=rel#2230, importance=0.81\r\n\t\trel#2230:DrillUnionRel.LOGICAL.ANY([]).[](input#0=rel#2225:Subset#14.LOGICAL.ANY([]).[],input#1=rel#2229:Subset#16.LOGICAL.ANY([]).[],all=false), rowcount=18.75, cumulative cost={319.375 rows, 352.375 cpu, 0.0 io}\r\n\t\trel#2238:AbstractConverter.LOGICAL.ANY([]).[](child=rel#2237:Subset#17.PHYSICAL.SINGLETON([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=1.7976931348623157E308, cumulative cost={inf}\r\n\trel#2237:Subset#17.PHYSICAL.SINGLETON([]).[], best=null, importance=0.7290000000000001\r\n\t\trel#2239:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2231:Subset#17.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=18.75, cumulative cost={inf}\r\nSet#18, type: RecordType(ANY c_custkey, ANY c_nationkey)\r\n\trel#2233:Subset#18.LOGICAL.ANY([]).[], best=rel#2232, importance=0.9\r\n\t\trel#2232:DrillScreenRel.LOGICAL.ANY([]).[](child=rel#2231:Subset#17.LOGICAL.ANY([]).[]), rowcount=18.75, cumulative cost={321.25 rows, 354.25 cpu, 0.0 io}\r\n\t\trel#2235:AbstractConverter.LOGICAL.ANY([]).[](child=rel#2234:Subset#18.PHYSICAL.SINGLETON([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=1.7976931348623157E308, cumulative cost={inf}\r\n\trel#2234:Subset#18.PHYSICAL.SINGLETON([]).[], best=null, importance=1.0\r\n\t\trel#2236:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#2233:Subset#18.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=18.75, cumulative cost={inf}\r\n\t\trel#2240:ScreenPrel.PHYSICAL.SINGLETON([]).[](child=rel#2237:Subset#17.PHYSICAL.SINGLETON([]).[]), rowcount=1.7976931348623157E308, cumulative cost={inf}\r\n\r\n\r\n\torg.eigenbase.relopt.volcano.RelSubset$CheapestPlanReplacer.visit(RelSubset.java:445) ~[optiq-core-0.7-20140421.161434-1.jar:na]\r\n\torg.eigenbase.relopt.volcano.RelSubset.buildCheapestPlan(RelSubset.java:287) ~[optiq-core-0.7-20140421.161434-1.jar:na]\r\n\torg.eigenbase.relopt.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:669) ~[optiq-core-0.7-20140421.161434-1.jar:na]\r\n\tnet.hydromatic.optiq.prepare.PlannerImpl.transform(PlannerImpl.java:234) ~[optiq-core-0.7-20140421.161434-1.jar:na]\r\n\torg.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToPrel(DefaultSqlHandler.java:120) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\torg.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:91) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\torg.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:100) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\torg.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:323) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\torg.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:175) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n\tjava.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n\r\n"
    ],
    [
        "DRILL-796",
        "DRILL-670",
        "Selecting * from hbase/m7 tables filtered with row_key returns empty arrays git.commit.id.abbrev=5d7e3d3\r\n\r\n0: jdbc:drill:schema=hbase> select * from voter where row_key=650;\r\n+------------+------------+------------+------------+------------+\r\n|  row_key   |   fourcf   |   onecf    |  threecf   |   twocf    |\r\n+------------+------------+------------+------------+------------+\r\n| [B@484265d9 | {}         | {}         | {}         | {}         |\r\n+------------+------------+------------+------------+------------+\r\n\r\nThe above table contains multiple regions.\r\n\r\nI have another table that belongs only to 1 region and the same query returns non-empty arrays.\r\n\r\n0: jdbc:drill:schema=hbase> select * from student where row_key=650;\r\n+------------+------------+------------+------------+------------+------------+\r\n|  row_key   |   fivecf   |   fourcf   |   onecf    |  threecf   |   twocf    |\r\n+------------+------------+------------+------------+------------+------------+\r\n| [B@59aa724f | {\"create_date\":\"MjAxNC0wMi0yMCAwMzoxMDoxOA==\"} | {\"studentnum\":\"OTQ1OTU5NTM4MTI5\"} | {\"name\":\"cHJpc2NpbGxhIGljaGFib2Q=\"} | {\"gpa\":\"MS41\"} | {\"age\":\"MjI=\"} |\r\n+------------+------------+------------+------------+------------+------------+\r\n",
        "Queries selecting specific column names from hbase table hang  mapr-drill-1.0.0.25536-1.noarch\r\n\r\nI have an hbase table with the following properties:\r\nhbase(main):001:0> describe 'voter'\r\nDESCRIPTION                                                          ENABLED                              \r\n 'voter', {NAME => 'fourcf', DATA_BLOCK_ENCODING => 'NONE', BLOOMFIL true                                 \r\n TER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', MIN_VERSI                                      \r\n ONS => '0', TTL => '2147483647', KEEP_DELETED_CELLS => 'false', BLO                                      \r\n CKSIZE => '65536', IN_MEMORY => 'false', ENCODE_ON_DISK => 'true',                                       \r\n BLOCKCACHE => 'true'}, {NAME => 'onecf', DATA_BLOCK_ENCODING => 'NO                                      \r\n NE', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '                                      \r\n 3', MIN_VERSIONS => '0', TTL => '2147483647', KEEP_DELETED_CELLS =>                                      \r\n  'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', ENCODE_ON_DIS                                      \r\n K => 'true', BLOCKCACHE => 'true'}, {NAME => 'threecf', DATA_BLOCK_                                      \r\n ENCODING => 'NONE', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0'                                      \r\n , VERSIONS => '3', MIN_VERSIONS => '0', TTL => '2147483647', KEEP_D                                      \r\n ELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false'                                      \r\n , ENCODE_ON_DISK => 'true', BLOCKCACHE => 'true'}, {NAME => 'twocf'                                      \r\n , DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'NONE', REPLICATION                                      \r\n _SCOPE => '0', VERSIONS => '3', MIN_VERSIONS => '0', TTL => '214748                                      \r\n 3647', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMO                                      \r\n RY => 'false', ENCODE_ON_DISK => 'true', BLOCKCACHE => 'true'}                                           \r\n\r\nRan the following query from sqlline:\r\nselect name, age from voter limit 5;\r\n\r\nThe query hangs.  Here is part of the java stack trace for the sqlline process:\r\n\"Finalizer\" daemon prio=10 tid=0x00007f3ec4073000 nid=0x25c4 in Object.wait() [0x00007f3ec8d79000]\r\n   java.lang.Thread.State: WAITING (on object monitor)\r\n\tat java.lang.Object.wait(Native Method)\r\n\t- waiting on <0x00000000c4190f98> (a java.lang.ref.ReferenceQueue$Lock)\r\n\tat java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)\r\n\t- locked <0x00000000c4190f98> (a java.lang.ref.ReferenceQueue$Lock)\r\n\tat java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)\r\n\tat java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:189)\r\n\r\n\"Reference Handler\" daemon prio=10 tid=0x00007f3ec406f000 nid=0x25c3 in Object.wait() [0x00007f3ec8e7a000]\r\n   java.lang.Thread.State: WAITING (on object monitor)\r\n\tat java.lang.Object.wait(Native Method)\r\n\t- waiting on <0x00000000c4190b10> (a java.lang.ref.Reference$Lock)\r\n\tat java.lang.Object.wait(Object.java:503)\r\n\tat java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)\r\n\t- locked <0x00000000c4190b10> (a java.lang.ref.Reference$Lock)\r\n\r\n\"main\" prio=10 tid=0x00007f3ec4011800 nid=0x25bf waiting on condition [0x00007f3ecada7000]\r\n   java.lang.Thread.State: WAITING (parking)\r\n\tat sun.misc.Unsafe.park(Native Method)\r\n\t- parking to wait for  <0x00000000ecd4dd18> (a java.util.concurrent.CountDownLatch$Sync)\r\n\tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)\r\n\tat java.util.concurrent.CountDownLatch.await(CountDownLatch.java:236)\r\n\tat org.apache.drill.jdbc.DrillResultSet.execute(DrillResultSet.java:88)\r\n\tat org.apache.drill.jdbc.DrillResultSet.execute(DrillResultSet.java:43)\r\n\tat net.hydromatic.avatica.AvaticaConnection.executeQueryInternal(AvaticaConnection.java:404)\r\n\tat net.hydromatic.avatica.AvaticaStatement.executeQueryInternal(AvaticaStatement.java:350)\r\n\tat net.hydromatic.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:337)\r\n\tat net.hydromatic.avatica.AvaticaStatement.execute(AvaticaStatement.java:69)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3755)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n\r\nSelecting all column (*) returns data."
    ],
    [
        "DRILL-822",
        "DRILL-586",
        "Conversion to relational algebra failed to preserve datatypes Datasources: TPCH (10MB), three-way split parquet files\r\ngit.commit.id.abbrev=2fad21d\r\ngit.commit.id=2fad21d5a6ec43bb68fb989e48b6da180f23f73a\r\n\r\n\r\n0: jdbc:drill:schema=dfs.TpcHMulti> select * from region order by r_regionkey;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"2f293a00-2cf6-4663-b961-7cde388efeb5\"\r\nendpoint {\r\n  address: \"perfnode104.perf.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while setting up Foreman. < AssertionError:[ Internal error: Conversion to relational algebra failed to preserve datatypes:\r\nvalidated type:\r\nRecordType(ANY *) NOT NULL\r\nconverted type:\r\nRecordType(ANY *, ANY r_regionkey) NOT NULL\r\nrel:\r\nProjectRel(*=[$0], r_regionkey=[$1])\r\n  SortRel(sort0=[$1], dir0=[ASC])\r\n    ProjectRel(*=[$0], r_regionkey=[$1], r_regionkey0=[$1])\r\n      TableAccessRel(table=[[dfs, TpcHMulti, region]])\r\n ]\"\r\n]\r\nError: exception while executing query (state=,code=0)",
        "Order by on select * fails 0: jdbc:drill:schema=dfs.drillTestDirP1> explain plan for select * from student order by age;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"e2eb9f49-d82b-41f3-a518-8ab698ed5812\"\r\nendpoint {\r\n  address: \"drillats3.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while setting up Foreman. < AssertionError:[ Internal error: Conversion to relational algebra failed to preserve datatypes:\\nvalidated type:\\nRecordType(ANY NOT NULL *) NOT NULL\\nconverted type:\\nRecordType(ANY NOT NULL *, ANY NOT NULL age) NOT NULL\\nrel:\\nProjectRel(*=[$0], age=[$1])\\n  SortRel(sort0=[$1], dir0=[ASC])\\n    ProjectRel(*=[$0], age=[$1], age0=[$1])\\n      TableAccessRel(table=[[dfs, drillTestDirP1, student]])\\n ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n\r\nThe query fails with the same error."
    ],
    [
        "DRILL-828",
        "DRILL-672",
        "Functions against split hbase table hang git.commit.id.abbrev=f948d71\r\n\r\nWhen a count(), max(), min(), etc., against partitioned hbase table, the query hangs.\r\n\r\nThe following stack trace is from query \"select count(*) from voter;\"\r\n\r\norg.apache.drill.common.exceptions.ExecutionSetupException: A scan batch must contain at least one reader.\r\norg.apache.drill.exec.physical.impl.ScanBatch.<init>(ScanBatch.java:86) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.impl.ScanBatch.<init>(ScanBatch.java:99) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.store.hbase.HBaseScanBatchCreator.getBatch(HBaseScanBatchCreator.java:48) ~[drill-storage-hbase-1.0.0-m2-incubating-SNAPSHOT.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.store.hbase.HBaseScanBatchCreator.getBatch(HBaseScanBatchCreator.java:32) ~[drill-storage-hbase-1.0.0-m2-incubating-SNAPSHOT.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.impl.ImplCreator.visitOp(ImplCreator.java:62) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.impl.ImplCreator.visitOp(ImplCreator.java:39) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.base.AbstractPhysicalVisitor.visitSubScan(AbstractPhysicalVisitor.java:92) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.store.hbase.HBaseSubScan.accept(HBaseSubScan.java:111) ~[drill-storage-hbase-1.0.0-m2-incubating-SNAPSHOT.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.impl.ImplCreator.getChildren(ImplCreator.java:74) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.impl.ImplCreator.visitOp(ImplCreator.java:62) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.impl.ImplCreator.visitOp(ImplCreator.java:39) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.base.AbstractPhysicalVisitor.visitProject(AbstractPhysicalVisitor.java:47) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.config.Project.accept(Project.java:52) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.impl.ImplCreator.getChildren(ImplCreator.java:74) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.impl.ImplCreator.visitOp(ImplCreator.java:62) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.impl.ImplCreator.visitOp(ImplCreator.java:39) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.base.AbstractPhysicalVisitor.visitStreamingAggregate(AbstractPhysicalVisitor.java:67) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.config.StreamingAggregate.accept(StreamingAggregate.java:59) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.impl.ImplCreator.getChildren(ImplCreator.java:74) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.impl.ImplCreator.visitOp(ImplCreator.java:59) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.impl.ImplCreator.visitOp(ImplCreator.java:39) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.base.AbstractPhysicalVisitor.visitSender(AbstractPhysicalVisitor.java:77) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.base.AbstractPhysicalVisitor.visitSingleSender(AbstractPhysicalVisitor.java:160) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.config.SingleSender.accept(SingleSender.java:69) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.physical.impl.ImplCreator.getExec(ImplCreator.java:87) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:87) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\njava.lang.Thread.run(Thread.java:744) [na:1.7.0_45]",
        "Queries against hbase table do not close after the data is returned.  Ran the following query via sqlline against an hbase table that spans multiple regions:\r\nselect * from voter limit 5;\r\n+------------+-------------+------------+---------------+------------+------------+--------------+\r\n|  row_key   | create_date |    name    | contributions | voterzone  |    age     | registration |\r\n+------------+-------------+------------+---------------+------------+------------+--------------+\r\n| [B@32cb56e6 | [B@43b63017 | [B@6e0f58bb | [B@318a9570   | [B@1590579 | [B@7b764b9f | [B@135ef30c  |\r\n| [B@3ea6a2c3 | [B@2e07d057 | [B@2ff175d | [B@3956dc34   | [B@7592f754 | [B@3abe81a | [B@b1304d8   |\r\n| [B@da30696 | [B@4a0568d8 | [B@292662df | [B@17253394   | [B@b5d456b | [B@187879a1 | [B@d794594   |\r\n| [B@14c6fe75 | [B@146bbdfb | [B@2d85b436 | [B@24fa5c93   | [B@30d3c9dc | [B@2069a38f | [B@777111e8  |\r\n| [B@204ed39b | [B@172675af | [B@5df84008 | [B@5533fd6a   | [B@484265d9 | [B@7f2cae9e | [B@4e7c484c  |\r\n\r\nIt displays the 5 rows but then just hang without returning to the sqlline prompt.  This could be due to this table spans multiple regions.  Select against different table that resides in 1 partition does not this problem."
    ],
    [
        "DRILL-857",
        "DRILL-585",
        "localtime function returns incorrect result git.commit.id.abbrev=01bf849\r\n\r\n0: jdbc:drill:schema=dfs> select localtime from voter where voter_id=10;\r\n+------------+\r\n| localtime  |\r\n+------------+\r\n| 1970-01-01T08:02:38.332-08:00 |\r\n",
        "timestamp and time data not showing up correctly in sqlline Executing the following query -\r\nselect first, last, age, sex, salary, registered, cast(join_date as timestamp) TIMEST, cast(join_date as date) DT, cast(join_time as TIME) TM   from dfs.`testTypes.json`\r\n\r\nreturned the following results in sqlline- \r\n+------------+------------+------------+------------+------------+------------+------------+------------+------------+\r\n|   first    |    last    |    age     |    sex     |   salary   | registered |   TIMEST   |     DT     |     TM     |\r\n+------------+------------+------------+------------+------------+------------+------------+------------+------------+\r\n| Jimmy      | James      | 29         | M          | 6300.1     | false      | 2011-11-30 | 2011-11-30 | 00:00:00   |\r\n| anderson   | carr       | null       | M          | 6300.1     | false      | null       | null       | null       |\r\n| John       | James      | 29         | M          | 6300.1     | true       | 2011-11-30 | 2011-11-30 | 00:00:00   |\r\n+------------+------------+------------+------------+------------+------------+------------+------------+------------+\r\n\r\nThe timestamp column does not have time, and the time column has incorrect values.\r\n\r\nData file contains the following data -\r\n\r\n{\r\n \"first\": \"Jimmy\",\r\n \"last\": \"James\",\r\n \"age\": 29,\r\n \"sex\": \"M\",\r\n \"salary\": 6300.10,\r\n \"registered\": false,\r\n \"join_date\": \"2011-11-30\",\r\n \"join_time\": \"9:11:10.000\"\r\n}\r\n{\r\n \"first\": \"anderson\",\r\n \"last\": \"carr\",\r\n \"sex\": \"M\",\r\n \"salary\": 6300.10,\r\n \"registered\": false\r\n}\r\n{\r\n \"first\": \"John\",\r\n \"last\": \"James\",\r\n \"age\": 29,\r\n \"sex\": \"M\",\r\n \"salary\": 6300.10,\r\n \"registered\": true,\r\n \"join_date\": \"2011-11-30\",\r\n \"join_time\": \"8:59:32.000\"\r\n}\r\n\r\n\r\n\r\n\r\n"
    ],
    [
        "DRILL-895",
        "DRILL-891",
        "Convert_from function against split hbase table hangs git.commit.id.abbrev=8490d74\r\ngit.build.time=02.06.2014 @ 10\\:11\\:56 PDT\r\n\r\nselect convert_from(onecf['name'], 'UTF8') name from voter where row_key=1;\r\n\r\nThe above query hangs if run against a split hbase table.  Against a non-split hbase table, it run successfully.",
        "Casts used in certain queries fail against split hbase table git.commit.id.abbrev=8490d74\r\ngit.commit.time=02.06.2014 @ 09\\:15\\:29 PDT\r\n\r\nI have a split hbase table.  Here is one record in the table:\r\n\r\nhbase(main):004:0> get 'voter', '1'\r\nCOLUMN                       CELL                                                                             \r\n fourcf:create_date          timestamp=1400799413902, value=2014-05-25 03:41:54                               \r\n onecf:name                  timestamp=1400799413902, value=nick miller                                       \r\n threecf:contributions       timestamp=1400799413902, value=717.12                                            \r\n threecf:voterzone           timestamp=1400799413902, value=13809                                             \r\n twocf:age                   timestamp=1400799413902, value=68                                                \r\n twocf:registration          timestamp=1400799413902, value=green                                             \r\n6 row(s) in 0.4660 seconds\r\n\r\nThe following sql query against this table failed.\r\n\r\n0: jdbc:drill:schema=hbase> select date_add(cast(fourcf['create_date'] as timestamp), interval '3' hour) from voter where row_key=5;\r\n\r\nmessage: \"Failure while running fragment. < UnsupportedOperationException:[ CastExpression is not expected here. It should have been converted to FunctionHolderExpression in materialization ]\"\r\n\r\nThe same query runs fine against a table without split:\r\n0: jdbc:drill:schema=hbase> select date_add(cast(fourcf['create_date'] as timestamp), interval '3' hour) from s_voter where row_key=5;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 2015-02-01T05:02:37.000-08:00 |\r\n+------------+\r\n"
    ],
    [
        "DRILL-1011",
        "DRILL-991",
        "Query appears to hang after displaying result This issue was observed with TPC-DS data-set. After displaying results, the query appears to hang.\r\n\r\nQuery:\r\nselect * from store_sales limit 10;\r\n\r\nApproximate size of table: 250 GB\r\n\r\nLogs are updated constantly. No error messages are seen in any logs. The issue can be reproduced consistently. Can provide more details as needed.  ",
        "Limit should terminate upstream fragments immediately upon completion "
    ],
    [
        "DRILL-1039",
        "DRILL-891",
        "Queries against hbase table with multiple regions return null  git.commit.id.abbrev=894037a\r\n\r\nThis is a regression. Ran the following query in sqlline:\r\n0: jdbc:drill:schema=dfs> select cast(row_key as integer) voter_id, cast(onecf['name'] as varchar(30)) name, cast(twocf['age'] as integer) age, cast(twocf['registration'] as varchar(20)) registration, cast(threecf['contributions'] as decimal(6,2)) contributions, cast(threecf['voterzone'] as integer) voterzone,cast(fourcf['create_date'] as timestamp) create_date from hbase.voter where row_key = 5;\r\n+------------+------------+------------+--------------+---------------+------------+-------------+\r\n|  voter_id  |    name    |    age     | registration | contributions | voterzone  | create_date |\r\n+------------+------------+------------+--------------+---------------+------------+-------------+\r\n| 5          | null       | null       | null         | null          | null       | 1970-01-01 00:00:00.0 |\r\n+------------+------------+------------+--------------+---------------+------------+-------------+\r\n\r\nThe same query returns expected data against an hbase table that resides in single partition. ",
        "Casts used in certain queries fail against split hbase table git.commit.id.abbrev=8490d74\r\ngit.commit.time=02.06.2014 @ 09\\:15\\:29 PDT\r\n\r\nI have a split hbase table.  Here is one record in the table:\r\n\r\nhbase(main):004:0> get 'voter', '1'\r\nCOLUMN                       CELL                                                                             \r\n fourcf:create_date          timestamp=1400799413902, value=2014-05-25 03:41:54                               \r\n onecf:name                  timestamp=1400799413902, value=nick miller                                       \r\n threecf:contributions       timestamp=1400799413902, value=717.12                                            \r\n threecf:voterzone           timestamp=1400799413902, value=13809                                             \r\n twocf:age                   timestamp=1400799413902, value=68                                                \r\n twocf:registration          timestamp=1400799413902, value=green                                             \r\n6 row(s) in 0.4660 seconds\r\n\r\nThe following sql query against this table failed.\r\n\r\n0: jdbc:drill:schema=hbase> select date_add(cast(fourcf['create_date'] as timestamp), interval '3' hour) from voter where row_key=5;\r\n\r\nmessage: \"Failure while running fragment. < UnsupportedOperationException:[ CastExpression is not expected here. It should have been converted to FunctionHolderExpression in materialization ]\"\r\n\r\nThe same query runs fine against a table without split:\r\n0: jdbc:drill:schema=hbase> select date_add(cast(fourcf['create_date'] as timestamp), interval '3' hour) from s_voter where row_key=5;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 2015-02-01T05:02:37.000-08:00 |\r\n+------------+\r\n"
    ],
    [
        "DRILL-1039",
        "DRILL-895",
        "Queries against hbase table with multiple regions return null  git.commit.id.abbrev=894037a\r\n\r\nThis is a regression. Ran the following query in sqlline:\r\n0: jdbc:drill:schema=dfs> select cast(row_key as integer) voter_id, cast(onecf['name'] as varchar(30)) name, cast(twocf['age'] as integer) age, cast(twocf['registration'] as varchar(20)) registration, cast(threecf['contributions'] as decimal(6,2)) contributions, cast(threecf['voterzone'] as integer) voterzone,cast(fourcf['create_date'] as timestamp) create_date from hbase.voter where row_key = 5;\r\n+------------+------------+------------+--------------+---------------+------------+-------------+\r\n|  voter_id  |    name    |    age     | registration | contributions | voterzone  | create_date |\r\n+------------+------------+------------+--------------+---------------+------------+-------------+\r\n| 5          | null       | null       | null         | null          | null       | 1970-01-01 00:00:00.0 |\r\n+------------+------------+------------+--------------+---------------+------------+-------------+\r\n\r\nThe same query returns expected data against an hbase table that resides in single partition. ",
        "Convert_from function against split hbase table hangs git.commit.id.abbrev=8490d74\r\ngit.build.time=02.06.2014 @ 10\\:11\\:56 PDT\r\n\r\nselect convert_from(onecf['name'], 'UTF8') name from voter where row_key=1;\r\n\r\nThe above query hangs if run against a split hbase table.  Against a non-split hbase table, it run successfully."
    ],
    [
        "DRILL-1098",
        "DRILL-1089",
        "Trim function in Drill should default to 'space' when no trim characters are provided like Oracle and Postgres git.commit.id.abbrev=a1a6144\r\nBuild # 26266\r\n\r\nAny query where we use trim function without mentioning a 'trim character' fails\r\n\r\nselect trim(pageURL) from rankings;\r\n",
        "SUBSTRING(string, from) function fails git.commit.id.abbrev=3ea8eb5\r\ngit.build.time=26.06.2014 @ 14\\:15\\:29 PDT\r\n\r\nThe following query ran successfully in the last build; but failed in the latest build.\r\n0: jdbc:drill:schema=dfs> select substring(name, 5) from voter where age < 20;\r\nmessage: \"Failure while setting up Foreman. < AssertionError:[ typeName.allowsPrecScale(true, false) ]\"\r\n]\r\n\r\nIf I use substr instead of substring, then the query succeeds.\r\n"
    ],
    [
        "DRILL-1109",
        "DRILL-606",
        "Implement downward casting functions for decimal data type Currently we don't have casting functions for decimal that decrease precision and scale. For eg if we want to move to Decimal38 --> Decimal28 etc. ",
        "double cast for decimals fails postgres:\r\n\r\nfoodmart=# select c_row, c_decimal38, cast(cast(c_decimal38 as decimal(38,10)) as decimal(18,7)) from data where c_row = 9;\r\n c_row |     c_decimal38     |    c_decimal38\r\n-------+---------------------+-------------------\r\n     9 | 123456789.123456789 | 123456789.1234568\r\n(1 row)\r\n\r\ndrill:\r\n\r\n0: jdbc:drill:schema=dfs> select c_row, c_decimal38, cast(cast(c_decimal38 as decimal(38,10)) as decimal(18,7)) from data where c_row = 9;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"1314b28c-90ac-4e47-b38e-e7f36bb2ca99\"\r\nendpoint {\r\n  address: \"qa-node119.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < UnsupportedOperationException:[ DECIMAL38DENSE ]\"\r\n]\r\nError: exception while executing query (state=,code=0)"
    ],
    [
        "DRILL-1129",
        "DRILL-1086",
        "Get NPE when select from information_schema after updating storage plugins git.commit.id.abbrev=5b57294\r\n\r\nAfter updating the Storage plugins from the UI, selecting from INFORMATION_SCHEMA gives a NPE.  For example, I have the following setting in dfs schema:\r\n\r\n    \"default\" : {\r\n      \"location\" : \"/drill/testdata/p1tests\",\r\n      \"writable\" : true,\r\n      \"storageformat\" : null\r\n    },\r\n\r\nI updated the location to  \"location\" : \"/drill/testdata\",\r\n\r\nAfter this, I went to sqlline and ran the following query:\r\n0: jdbc:drill:schema=dfs> SELECT * FROM INFORMATION_SCHEMA.`TABLES`;\r\nmessage: \"Failure while running fragment. < NullPointerException\"\r\n\r\nAfter I changed the location back to its previous value, the query runs successfully.",
        "Deleting a .view.drill file will cause INFORMATION_SCHEMA.`TABLES` to fail If a view is created by \"CREATE VIEW .....\", the view gets placed into INFORMATION_SCHEMA.`TABLES` as expected. If you go into the file system and delete the .view.drill file, then query SELECT * FROM INFORMATION_SCHEMA.`TABLES`, a null pointer exception will be thrown and nothing will be returned. \r\nNote that re-creating the deleted file will amend the issue."
    ],
    [
        "DRILL-1148",
        "DRILL-1134",
        "Using multiple filters with Text files fails to compile the runtime generated code git.commit.id.abbrev=810a204\r\n\r\nThe below query fails :\r\n\r\nselect columns[0] widecol1 from `file1.tsv` where columns[0] > 999 and columns[1] > 999;\r\n\r\nHowever things work as expected when we use only one filter:\r\n\r\nselect columns[0] widecol1 from `file1.tsv` where columns[0] > 999\r\n\r\nI attached the relevant logs for the failed case\r\n",
        "CompilationException for 2 or more Filters on an array type  The following query's generated code for the Filter consists of the doEval() block which has redefinition of variable isNull which causes a compilation failure.  Note that isNull is not even referenced in the generated code. \r\n\r\nSELECT columns[0] \r\n  FROM dfs.`/Users/asinha/data/regions.csv` \r\nWHERE cast(columns[0] as int) > 1 \r\n  ANDcast(columns[1] as varchar(20))='ASIA';\r\n\r\n}\r\n ] < CompileException:[ Line 82, Column 28: Redefinition of local variable \"isNull\"  ]\"\r\n\r\nI also get a Deadbuf access for this query, but that could be a separate issue. "
    ],
    [
        "DRILL-1156",
        "DRILL-1062",
        "mondrian117.q - result set with group by stitched wrong #Mon Jul 14 10:10:52 PDT 2014\r\ngit.commit.id.abbrev=699851b\r\n\r\nFor this mondrian query (query117.q), with group by, result set is stitched together in a wrong order. For example, for some columns, like c6, c7, c8, c9, row oder is wrong.\r\n\r\npostgres:\r\nfoodmart=# select store.store_country as c0, store.store_state as c1, store.store_city as c2, store.store_name as c3, store.store_type as c4, store.store_manager as c5, store.store_sqft as c6, store.grocery_sqft as c7, store.frozen_sqft as c8, store.meat_sqft as c9, store.coffee_bar as c10, store.store_street_address as c11 from store as store group by store.store_country, store.store_state, store.store_city, store.store_name, store.store_type, store.store_manager, store.store_sqft, store.grocery_sqft, store.frozen_sqft, store.meat_sqft, store.coffee_bar, store.store_street_address order by store.store_country ASC NULLS LAST, store.store_state ASC NULLS LAST, store.store_city ASC NULLS LAST, store.store_name ASC NULLS LAST;\r\n   c0   |    c1     |      c2       |    c3    |         c4          |    c5    |  c6   |  c7   |  c8  |  c9  | c10 |            c11\r\n--------+-----------+---------------+----------+---------------------+----------+-------+-------+------+------+-----+---------------------------\r\n Canada | BC        | Vancouver     | Store 19 | Deluxe Supermarket  | Ruth     | 23112 | 16418 | 4016 | 2678 | t   | 6644 Sudance Drive\r\n Canada | BC        | Victoria      | Store 20 | Mid-Size Grocery    | Cobb     | 34452 | 27463 | 4193 | 2795 | t   | 3706 Marvelle Ln\r\n Mexico | DF        | Mexico City   | Store 9  | Mid-Size Grocery    | Stuber   | 36509 | 22450 | 8435 | 5624 | f   | 1872 El Pintado Road\r\n Mexico | DF        | San Andres    | Store 21 | Deluxe Supermarket  | Jones    |       |       |      |      | t   | 4093 Steven Circle\r\n Mexico | Guerrero  | Acapulco      | Store 1  | Supermarket         | Jones    | 23593 | 17475 | 3671 | 2447 | f   | 2853 Bailey Rd\r\n Mexico | Jalisco   | Guadalajara   | Store 5  | Small Grocery       | Green    | 24597 | 15012 | 5751 | 3834 | t   | 1250 Coggins Drive\r\n Mexico | Veracruz  | Orizaba       | Store 10 | Supermarket         | Merz     | 34791 | 26354 | 5062 | 3375 | f   | 7894 Rotherham Dr\r\n Mexico | Yucatan   | Merida        | Store 8  | Deluxe Supermarket  | Williams | 30797 | 20141 | 6393 | 4262 | t   | 3173 Buena Vista Ave\r\n Mexico | Zacatecas | Camacho       | Store 4  | Gourmet Supermarket | Johnson  | 23759 | 16844 | 4149 | 2766 | t   | 433 St George Dr\r\n Mexico | Zacatecas | Hidalgo       | Store 12 | Deluxe Supermarket  | Kalman   | 30584 | 21938 | 5188 | 3458 | t   | 1120 Westchester Pl\r\n Mexico | Zacatecas | Hidalgo       | Store 18 | Mid-Size Grocery    | Brown    | 38382 | 30351 | 4819 | 3213 | f   | 6764 Glen Road\r\n USA    | CA        | Alameda       | HQ       | HeadQuarters        |          |       |       |      |      | f   | 1 Alameda Way\r\n USA    | CA        | Beverly Hills | Store 6  | Gourmet Supermarket | Maris    | 23688 | 15337 | 5011 | 3340 | t   | 5495 Mitchell Canyon Road\r\n USA    | CA        | Los Angeles   | Store 7  | Supermarket         | White    | 23598 | 14210 | 5633 | 3755 | f   | 1077 Wharf Drive\r\n USA    | CA        | San Diego     | Store 24 | Supermarket         | Byrd     |       |       |      |      | t   | 2342 Waltham St.\r\n USA    | CA        | San Francisco | Store 14 | Small Grocery       | Strehlo  | 22478 | 15321 | 4294 | 2863 | t   | 4365 Indigo Ct\r\n\r\ndrill:\r\nCanada\tBC\tVancouver\tStore 19\tDeluxe Supermarket\tRuth\t23112\t16418\t4016\t2678\tt\t6644 Sudance Drive\r\nCanada\tBC\tVictoria\tStore 20\tMid-Size Grocery\tCobb\t34452\t27463\t4193\t2795\tt\t3706 Marvelle Ln\r\nMexico\tDF\tMexico City\tStore 9\tMid-Size Grocery\tStuber\t36509\t22450\t8435\t5624\tf\t1872 El Pintado Road\r\nMexico\tDF\tSan Andres\tStore 21\tDeluxe Supermarket\tJones\t\\N\t\\N\t\\N\t\\N\tt\t4093 Steven Circle\r\nMexico\tGuerrero\tAcapulco\tStore 1\tSupermarket\tJones\t23593\t17475\t3671\t2447\tf\t2853 Bailey Rd\r\nMexico\tJalisco\tGuadalajara\tStore 5\tSmall Grocery\tGreen\t24597\t15012\t5751\t3834\tt\t1250 Coggins Drive\r\nMexico\tVeracruz\tOrizaba\tStore 10\tSupermarket\tMerz\t34791\t26354\t5062\t3375\tf\t7894 Rotherham Dr\r\nMexico\tYucatan\tMerida\tStore 8\tDeluxe Supermarket\tWilliams\t30797\t20141\t6393\t4262\tt\t3173 Buena Vista Ave\r\nMexico\tZacatecas\tCamacho\tStore 4\tGourmet Supermarket\tJohnson\t23759\t16844\t4149\t2766\tt\t433 St George Dr\r\nMexico\tZacatecas\tHidalgo\tStore 12\tDeluxe Supermarket\tKalman\t30584\t21938\t5188\t3458\tt\t1120 Westchester Pl\r\nMexico\tZacatecas\tHidalgo\tStore 18\tMid-Size Grocery\tBrown\t38382\t30351\t4819\t3213\tf\t6764 Glen Road\r\nUSA\tCA\tAlameda\tHQ\tHeadQuarters\t\\N\t\\N\t\\N\t\\N\t\\N\tf\t1 Alameda Way\r\nUSA\tCA\tBeverly Hills\tStore 6\tGourmet Supermarket\tMaris\t23688\t15337\t5011\t3340\tt\t5495 Mitchell Canyon Road\r\nUSA\tCA\tLos Angeles\tStore 7\tSupermarket\tWhite\t23598\t14210\t5633\t3755\tf\t1077 Wharf Drive\r\nUSA\tCA\tSan Diego\tStore 24\tSupermarket\tByrd\t\\N\t\\N\t\\N\t\\N\tt\t2342 Waltham St.\r\nUSA\tCA\tSan Francisco\tStore 14\tSmall Grocery\tStrehlo\t22478\t15321\t4294\t2863\tt\t4365 Indigo Ct",
        "DRILL does not handle NULLS FIRST/LAST correctly in ORDER BY clause ORDER BY clause could specify nulls first or nulls last.  Currently, DRILL will always use nulls last policy. \r\n\r\nselect tbl.topping[3].type from dfs.`/Users/jni/work/incubator-drill/exec/ref/target/test-classes/donuts.json` as tbl order by 1 nulls last;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| Chocolate  |\r\n| Maple      |\r\n| Powdered Sugar |\r\n| Powdered Sugar |\r\n| null       |\r\n+------------+\r\n5 rows selected (0.156 seconds)\r\n0: jdbc:drill:zk=local> select tbl.topping[3].type from dfs.`/Users/jni/work/incubator-drill/exec/ref/target/test-classes/donuts.json` as tbl order by 1 nulls first;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| Chocolate  |\r\n| Maple      |\r\n| Powdered Sugar |\r\n| Powdered Sugar |\r\n| null       |\r\n+------------+\r\n5 rows selected (0.186 seconds)\r\n"
    ],
    [
        "DRILL-1157",
        "DRILL-1138",
        "Views do not support boolean types I have a hive table with boolean values:\r\n{code}\r\n0: jdbc:drill:zk=localhost:5181> select * from `hive43.default`.`bit_table`;\r\n+------------+------------+\r\n| keycolumn  |  column1   |\r\n+------------+------------+\r\n| Zero       | false      |\r\n| One        | true       |\r\n+------------+------------+\r\n2 rows selected (0.496 seconds)\r\n{code}\r\n\r\nIf I create a view from this table, I cannot query the view:\r\n{code}\r\n0: jdbc:drill:zk=localhost:5181> create view `dfs.test`.`bitview` as select * from `hive43.default`.`bit_table`;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | View 'bitview' created successfully in 'dfs.test' schema |\r\n+------------+------------+\r\n1 row selected (0.629 seconds)\r\n0: jdbc:drill:zk=localhost:5181> select * from `dfs.test`.`bitview`;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"f68f6088-a748-4e07-9039-1d13d086a4f1\"\r\nendpoint {\r\n  address: \"192.168.39.43\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < IllegalArgumentException:[ No enum constant org.apache.drill.common.types.TypeProtos.MinorType.BOOLEAN ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n{code}",
        "Explicit casting to boolean fails 0: jdbc:drill:schema=dfs.TpcHMulti> select cast(C_CUSTKEY as boolean) as booleantest from customer limit 1;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"c20bd69f-bb5c-443e-a01b-8571b4c86490\"\r\nendpoint {\r\n  address: \"drillats1.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < IllegalArgumentException:[ No enum constant org.apache.drill.common.types.TypeProtos.MinorType.BOOLEAN ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n\r\n"
    ],
    [
        "DRILL-1172",
        "DRILL-541",
        "Unable to query Hive tables from Drill on CDH4 I am running Drill on CDH4.\r\nI have created a Hive Storage plugin as follows:\r\n{code}\r\n{\r\n  \"type\" : \"hive\",\r\n  \"enabled\" : true,\r\n  \"configProps\" : {\r\n    \"hive.metastore.uris\" : \"thrift://10.10.30.156:9083\",\r\n    \"fs.default.name\" : \"hdfs://10.10.30.156:8020/\",\r\n    \"hive.metastore.sasl.enabled\" : \"false\"\r\n  }\r\n}\r\n{code}\r\n\r\nI connect to sqlline as follows:\r\n /opt/drill/apache-drill-1.0.0-m2-incubating-SNAPSHOT/bin/sqlline -u \"jdbc:drill:schema=hive;zk=10.10.30.156:2181\"\r\n\r\nWhen I run a query against a hive table it errors out as follows:\r\n{code}\r\n0: jdbc:drill:schema=hive> select * from `student`;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"1a2b3324-66bf-46cd-8353-2c5a7d52c2aa\"\r\nendpoint {\r\n  address: \"perfnode156.perf.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while setting up Foreman. < AssertionError:[ Internal error: Error while applying rule DrillPushProjIntoScan, args [rel#5:ProjectRel.NONE.ANY([]).[](child=rel#4:Subset#0.ENUMERABLE.ANY([]).[],rownum=$0,name=$1,age=$2,gpa=$3,studentnum=$4), rel#2:EnumerableTableAccessRel.ENUMERABLE.ANY([]).[](table=[hive, student])] ] < IllegalArgumentException:[ Wrong FS: hdfs://perfnode156.perf.lab:8020/user/hive/warehouse/student, expected: file:/// ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n{code}\r\n\r\nI have even tried setting the below property in the hive storage plugin but it still errors out:\r\n\"fs.defaultFS\" : \"hdfs://10.10.30.156:8020/\"\r\n\r\nThe exception in the drillbit.log is \r\n{code}\r\n2014-07-22 23:46:16,056 [4454e978-5f18-440e-97c8-1f01cf4475cf:foreman] ERROR o.a.drill.exec.work.foreman.Foreman - Error 07265edc-756e-4aa2-9bef-a244501a072e: Failure while setting up Foreman.\r\njava.lang.IllegalArgumentException: Wrong FS: hdfs://perfnode156.perf.lab:8020/user/hive/warehouse/student, expected: file:///\r\n\tat org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:627) ~[hadoop-common-2.0.0-cdh4.7.0.jar:na]\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:69) ~[hadoop-common-2.0.0-cdh4.7.0.jar:na]\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:464) ~[hadoop-common-2.0.0-cdh4.7.0.jar:na]\r\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:380) ~[hadoop-common-2.0.0-cdh4.7.0.jar:na]\r\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1380) ~[hadoop-common-2.0.0-cdh4.7.0.jar:na]\r\n\tat org.apache.drill.exec.store.hive.HiveScan.getSplits(HiveScan.java:164) ~[drill-storage-hive-core-1.0.0-m2-incubating-SNAPSHOT.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.hive.HiveScan.<init>(HiveScan.java:121) ~[drill-storage-hive-core-1.0.0-m2-incubating-SNAPSHOT.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.hive.HiveStoragePlugin.getPhysicalScan(HiveStoragePlugin.java:75) ~[drill-storage-hive-core-1.0.0-m2-incubating-SNAPSHOT.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.hive.HiveStoragePlugin.getPhysicalScan(HiveStoragePlugin.java:39) ~[drill-storage-hive-core-1.0.0-m2-incubating-SNAPSHOT.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.AbstractStoragePlugin.getPhysicalScan(AbstractStoragePlugin.java:53) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.planner.logical.DrillTable.getGroupScan(DrillTable.java:54) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.planner.logical.DrillPushProjIntoScan.onMatch(DrillPushProjIntoScan.java:53) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.eigenbase.relopt.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:221) ~[optiq-core-0.7-20140710.204128-10.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:653) ~[optiq-core-0.7-20140710.204128-10.jar:na]\r\n\tat net.hydromatic.optiq.prepare.PlannerImpl.transform(PlannerImpl.java:271) ~[optiq-core-0.7-20140710.204128-10.jar:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToDrel(DefaultSqlHandler.java:136) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:116) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:129) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:400) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:216) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:242) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_55]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_55]\r\n\tat java.lang.Thread.run(Thread.java:744) [na:1.7.0_55]\r\n{code}\r\n\r\nIn the incubator-drill/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveScan.java class, the error is occurring because the FileSystem object being created in the getSplits() method is pointing to file:///\r\nThis FileSystem object is being created from a JobConf object.\r\nThe contents of the JobConf object are:\r\nConfiguration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\r\n\r\nThe fs.default.name is set to file:/// in /opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/share/doc/hadoop-2.0.0+1604/hadoop-project-dist/hadoop-common/core-default.xml.\r\nHowever changing that to hdfs://10.10.30.156:8020/ doesn't make any difference.",
        "hive-hbase tables query hangs [root@perfnode166 drilltests]# hive -e \"desc extended hbase_table_1\";\r\n\r\nLogging initialized using configuration in jar:file:/opt/mapr/hive/hive-0.12/lib/hive-common-0.12-mapr-1401-140130.jar!/hive-log4j.properties\r\nOK\r\nkey                 \tint                 \tfrom deserializer   \r\nvalue               \tstring              \tfrom deserializer   \r\n\t \t \r\nDetailed Table Information\tTable(tableName:hbase_table_1, dbName:default, owner:root, createTime:1397769348, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:maprfs:/user/hive/warehouse/hbase_table_1, inputFormat:org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.hbase.HBaseSerDe, parameters:{serialization.format=1, hbase.columns.mapping=:key,cf1:val}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{hbase.table.name=xyz, transient_lastDdlTime=1397769348, storage_handler=org.apache.hadoop.hive.hbase.HBaseStorageHandler}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)\t\r\nTime taken: 1.947 seconds, Fetched: 4 row(s)\r\n\r\nCan be queried fine from hive.\r\n\r\nFrom drill the query hangs. I have lilith setup but I am not sure what kind of information you need from there."
    ],
    [
        "DRILL-1186",
        "DRILL-1097",
        "NullableTimeStampAccessor asserts and returns wrong value when the value is null Repro:\r\nEnable asserts: export DRILL_SHELL_JAVA_OPTS=\"-ea\"\r\nstart sqlline\r\nselect cast(null as timestamp) from v1 limit 1;",
        "Sqlline seems to have issues with null values in hive table Accessing a hive table from sqlline, Query succeeds when executed through jdbc test framework or through submitplan.  Fails with an assert when executed through sqlline.\r\n\r\njava.lang.AssertionError: Tried to get null value\r\n\tat org.apache.drill.exec.vector.NullableTimeStampVector$Accessor.get(NullableTimeStampVector.java:299)\r\n\tat org.apache.drill.exec.vector.accessor.NullableTimeStampAccessor.getTimestamp(NullableTimeStampAccessor.java:100)\r\n\tat org.apache.drill.exec.vector.accessor.NullableTimeStampAccessor.getObject(NullableTimeStampAccessor.java:95)\r\n\tat org.apache.drill.jdbc.AvaticaDrillSqlAccessor.getObject(AvaticaDrillSqlAccessor.java:136)\r\n\tat net.hydromatic.avatica.AvaticaResultSet.getObject(AvaticaResultSet.java:336)\r\n\tat sqlline.SqlLine$Rows$Row.<init>(SqlLine.java:2388)\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2504)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n"
    ],
    [
        "DRILL-1214",
        "DRILL-1199",
        "Query with filter conditions fail while querying against Views built against Hbase tables I created a Hbase table called hbase_student as follows in hbase shell:\r\n{code}\r\ncreate 'hbase_student', 'stats'\r\nput 'hbase_student', '1', 'stats:name', 'fred ovid'\r\nput 'hbase_student', '1', 'stats:age', '76'\r\nput 'hbase_student', '1', 'stats:gpa', '1.55'\r\nput 'hbase_student', '1', 'stats:studentnum', '692315658449'\r\nput 'hbase_student', '1', 'stats:create_time', '2014-05-27 00:26:07'\r\nput 'hbase_student', '2', 'stats:name', 'bob brown'\r\nput 'hbase_student', '2', 'stats:age', '63'\r\nput 'hbase_student', '2', 'stats:gpa', '3.18'\r\nput 'hbase_student', '2', 'stats:studentnum', '650706039334'\r\nput 'hbase_student', '2', 'stats:create_time', '2014-12-04 21:43:14'\r\nput 'hbase_student', '3', 'stats:name', 'bob hernandez'\r\nput 'hbase_student', '3', 'stats:age', '28'\r\nput 'hbase_student', '3', 'stats:gpa', '1.09'\r\nput 'hbase_student', '3', 'stats:studentnum', '293612255322'\r\nput 'hbase_student', '3', 'stats:create_time', '2014-05-31 14:33:06'\r\n{code}\r\n\r\nI then logged in to sqlline using dfs.root schema and am able to successfully create a view & query it as long as filter conditions are not present\r\n\r\n{code}\r\ncreate view student_view as\r\nselect cast(tbl.row_key as int)rownum, \r\ncast(tbl.stats.name as varchar(20))name,\r\ncast(tbl.stats.age as int)age, \r\ncast(tbl.stats.gpa as float)gpa,\r\ncast(tbl.stats.studentnum as bigint)studentnum, \r\ncast(tbl.stats.create_time as varchar(20))create_time \r\nfrom hbase.hbase_student tbl\r\norder by rownum;\r\n\r\nSelect * from student_view;\r\n\r\n+------------+------------+------------+------------+------------+-------------+\r\n|   rownum   |    name    |    age     |    gpa     | studentnum | create_time |\r\n+------------+------------+------------+------------+------------+-------------+\r\n| 1          | fred ovid  | 76         | 1.55       | 692315658449 | 2014-05-27 00:26:07 |\r\n| 2          | bob brown  | 63         | 3.18       | 650706039334 | 2014-12-04 21:43:14 |\r\n| 3          | bob hernandez | 28         | 1.09       | 293612255322 | 2014-05-31 14:33:06 |\r\n+------------+------------+------------+------------+------------+-------------+\r\n\r\nSelect name,age,create_time from student_view;\r\n\r\n+------------+------------+-------------+\r\n|    name    |    age     | create_time |\r\n+------------+------------+-------------+\r\n| fred ovid  | 76         | 2014-05-27 00:26:07 |\r\n| bob brown  | 63         | 2014-12-04 21:43:14 |\r\n| bob hernandez | 28         | 2014-05-31 14:33:06 |\r\n+------------+------------+-------------+\r\n{code}\r\n\r\nHowever if I have a filter condition, the query fails as follows:\r\n\r\n{code}\r\nselect * from student_view where age > 50;\r\n\r\nerror_type: 0\r\nmessage: \"Screen received stop request sent. < SchemaChangeException:[ Failure while attempting to load generated class ] < ClassTransformationException:[ Failure generating transformation classes for value: \r\n \r\npackage org.apache.drill.exec.test.generated;\r\n\r\nimport org.apache.drill.exec.exception.SchemaChangeException;\r\nimport org.apache.drill.exec.ops.FragmentContext;\r\nimport org.apache.drill.exec.record.RecordBatch;\r\nimport org.apache.drill.exec.vector.IntVector;\r\nimport org.apache.drill.exec.vector.NullableBigIntVector;\r\nimport org.apache.drill.exec.vector.NullableFloat4Vector;\r\nimport org.apache.drill.exec.vector.NullableIntVector;\r\nimport org.apache.drill.exec.vector.NullableVarCharVector;\r\n\r\npublic class CopierGen287 {\r\n\r\n    IntVector vv0;\r\n    IntVector vv3;\r\n    NullableVarCharVector vv6;\r\n    NullableVarCharVector vv9;\r\n    NullableIntVector vv12;\r\n    NullableIntVector vv15;\r\n    NullableFloat4Vector vv18;\r\n    NullableFloat4Vector vv21;\r\n    NullableBigIntVector vv24;\r\n    NullableBigIntVector vv27;\r\n    NullableVarCharVector vv30;\r\n    NullableVarCharVector vv33;\r\n\r\n    public void doSetup(FragmentContext context, RecordBatch incoming, RecordBatch outgoing)\r\n        throws SchemaChangeException\r\n    {\r\n        {\r\n            int[] fieldIds1 = new int[ 1 ] ;\r\n            fieldIds1 [ 0 ] = 0;\r\n            Object tmp2 = (incoming).getValueAccessorById(IntVector.class, fieldIds1).getValueVector();\r\n            if (tmp2 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv0 with id: org.apache.drill.exec.record.TypedFieldId@21b323dc.\");\r\n            }\r\n            vv0 = ((IntVector) tmp2);\r\n            int[] fieldIds4 = new int[ 1 ] ;\r\n            fieldIds4 [ 0 ] = 0;\r\n            Object tmp5 = (outgoing).getValueAccessorById(IntVector.class, fieldIds4).getValueVector();\r\n            if (tmp5 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv3 with id: org.apache.drill.exec.record.TypedFieldId@21b323dc.\");\r\n            }\r\n            vv3 = ((IntVector) tmp5);\r\n            int[] fieldIds7 = new int[ 1 ] ;\r\n            fieldIds7 [ 0 ] = 1;\r\n            Object tmp8 = (incoming).getValueAccessorById(NullableVarCharVector.class, fieldIds7).getValueVector();\r\n            if (tmp8 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv6 with id: org.apache.drill.exec.record.TypedFieldId@f66d7cdd.\");\r\n            }\r\n            vv6 = ((NullableVarCharVector) tmp8);\r\n            int[] fieldIds10 = new int[ 1 ] ;\r\n            fieldIds10 [ 0 ] = 1;\r\n            Object tmp11 = (outgoing).getValueAccessorById(NullableVarCharVector.class, fieldIds10).getValueVector();\r\n            if (tmp11 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv9 with id: org.apache.drill.exec.record.TypedFieldId@f66d7cdd.\");\r\n            }\r\n            vv9 = ((NullableVarCharVector) tmp11);\r\n            int[] fieldIds13 = new int[ 1 ] ;\r\n            fieldIds13 [ 0 ] = 2;\r\n            Object tmp14 = (incoming).getValueAccessorById(NullableIntVector.class, fieldIds13).getValueVector();\r\n            if (tmp14 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv12 with id: org.apache.drill.exec.record.TypedFieldId@2376fc9d.\");\r\n            }\r\n            vv12 = ((NullableIntVector) tmp14);\r\n            int[] fieldIds16 = new int[ 1 ] ;\r\n            fieldIds16 [ 0 ] = 2;\r\n            Object tmp17 = (outgoing).getValueAccessorById(NullableIntVector.class, fieldIds16).getValueVector();\r\n            if (tmp17 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv15 with id: org.apache.drill.exec.record.TypedFieldId@2376fc9d.\");\r\n            }\r\n            vv15 = ((NullableIntVector) tmp17);\r\n            int[] fieldIds19 = new int[ 1 ] ;\r\n            fieldIds19 [ 0 ] = 3;\r\n            Object tmp20 = (incoming).getValueAccessorById(NullableFloat4Vector.class, fieldIds19).getValueVector();\r\n            if (tmp20 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv18 with id: org.apache.drill.exec.record.TypedFieldId@3d6b2cfd.\");\r\n            }\r\n            vv18 = ((NullableFloat4Vector) tmp20);\r\n            int[] fieldIds22 = new int[ 1 ] ;\r\n            fieldIds22 [ 0 ] = 3;\r\n            Object tmp23 = (outgoing).getValueAccessorById(NullableFloat4Vector.class, fieldIds22).getValueVector();\r\n            if (tmp23 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv21 with id: org.apache.drill.exec.record.TypedFieldId@3d6b2cfd.\");\r\n            }\r\n            vv21 = ((NullableFloat4Vector) tmp23);\r\n            int[] fieldIds25 = new int[ 1 ] ;\r\n            fieldIds25 [ 0 ] = 4;\r\n            Object tmp26 = (incoming).getValueAccessorById(NullableBigIntVector.class, fieldIds25).getValueVector();\r\n            if (tmp26 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv24 with id: org.apache.drill.exec.record.TypedFieldId@c6480360.\");\r\n            }\r\n            vv24 = ((NullableBigIntVector) tmp26);\r\n            int[] fieldIds28 = new int[ 1 ] ;\r\n            fieldIds28 [ 0 ] = 4;\r\n            Object tmp29 = (outgoing).getValueAccessorById(NullableBigIntVector.class, fieldIds28).getValueVector();\r\n            if (tmp29 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv27 with id: org.apache.drill.exec.record.TypedFieldId@c6480360.\");\r\n            }\r\n            vv27 = ((NullableBigIntVector) tmp29);\r\n            int[] fieldIds31 = new int[ 1 ] ;\r\n            fieldIds31 [ 0 ] = 5;\r\n            Object tmp32 = (incoming).getValueAccessorById(NullableVarCharVector.class, fieldIds31).getValueVector();\r\n            if (tmp32 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv30 with id: org.apache.drill.exec.record.TypedFieldId@fd40df59.\");\r\n            }\r\n            vv30 = ((NullableVarCharVector) tmp32);\r\n            int[] fieldIds34 = new int[ 1 ] ;\r\n            fieldIds34 [ 0 ] = 5;\r\n            Object tmp35 = (outgoing).getValueAccessorById(NullableVarCharVector.class, fieldIds34).getValueVector();\r\n            if (tmp35 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv33 with id: org.apache.drill.exec.record.TypedFieldId@fd40df59.\");\r\n            }\r\n            vv33 = ((NullableVarCharVector) tmp35);\r\n        }\r\n    }\r\n\r\n    public boolean doEval(int inIndex, int outIndex)\r\n        throws SchemaChangeException\r\n    {\r\n        {\r\n            if (!vv3 .copyFromSafe(((inIndex)& 65535), (outIndex), vv0 [((inIndex)>>> 16)])) {\r\n                return false;\r\n            }\r\n            if (!vv9 .copyFromSafe(((inIndex)& 65535), (outIndex), vv6 [((inIndex)>>> 16)])) {\r\n                return false;\r\n            }\r\n            if (!vv15 .copyFromSafe(((inIndex)& 65535), (outIndex), vv12 [((inIndex)>>> 16)])) {\r\n                return false;\r\n            }\r\n            if (!vv21 .copyFromSafe(((inIndex)& 65535), (outIndex), vv18 [((inIndex)>>> 16)])) {\r\n                return false;\r\n            }\r\n            if (!vv27 .copyFromSafe(((inIndex)& 65535), (outIndex), vv24 [((inIndex)>>> 16)])) {\r\n                return false;\r\n            }\r\n            if (!vv33 .copyFromSafe(((inIndex)& 65535), (outIndex), vv30 [((inIndex)>>> 16)])) {\r\n                return false;\r\n            }\r\n        }\r\n        {\r\n            return true;\r\n        }\r\n    }\r\n\r\n}\r\n ] < CompileException:[ Line 123, Column 36: No applicable constructor/method found for actual parameters \"int, int, java.lang.Object\"; candidates are: \"public boolean org.apache.drill.exec.vector.IntVector.copyFromSafe(int, int, org.apache.drill.exec.vector.IntVector)\" ]\"\r\n]\r\n{code}\r\n\r\nThe Exception in the drillbit.log is:\r\n{code}\r\n2014-07-28 17:33:31,496 [7b0e3219-8f05-480e-a5f5-bcf1e505c044:frag:0:0] ERROR o.a.d.e.w.f.AbstractStatusReporter - Error 373d229a-9697-46c0-a776-a747d5b0cf7a: Failure while running fragment.\r\norg.codehaus.commons.compiler.CompileException: Line 123, Column 36: No applicable constructor/method found for actual parameters \"int, int, java.lang.Object\"; candidates are: \"public boolean org.apache.drill.exec.vector.IntVector.copyFromSafe(int, int, org.apache.drill.exec.vector.IntVector)\"\r\n\tat org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10056) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7466) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7336) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7239) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3860) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:182) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3261) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3978) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3288) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4354) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileBoolean2(UnitCompiler.java:2854) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.access$4800(UnitCompiler.java:182) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler$8.visitMethodInvocation(UnitCompiler.java:2815) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3978) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileBoolean(UnitCompiler.java:2842) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileBoolean2(UnitCompiler.java:2872) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.access$4900(UnitCompiler.java:182) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler$8.visitUnaryOperation(UnitCompiler.java:2808) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.Java$UnaryOperation.accept(Java.java:3651) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileBoolean(UnitCompiler.java:2842) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1743) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:182) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:941) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.Java$IfStatement.accept(Java.java:2145) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:962) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1004) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:989) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:182) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:939) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.Java$Block.accept(Java.java:2005) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:962) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1004) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2284) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:826) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:798) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:503) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:389) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:182) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:343) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1136) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:350) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:318) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.apache.drill.exec.compile.JaninoClassCompiler.getByteCode(JaninoClassCompiler.java:48) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.compile.AbstractClassCompiler.getClassByteCode(AbstractClassCompiler.java:43) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.compile.QueryClassLoader$ClassCompilerSelector.getClassByteCode(QueryClassLoader.java:127) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.compile.QueryClassLoader$ClassCompilerSelector.access$000(QueryClassLoader.java:100) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.compile.QueryClassLoader.getClassByteCode(QueryClassLoader.java:93) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.compile.ClassTransformer.getImplementationClass(ClassTransformer.java:254) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.ops.FragmentContext.getImplementationClass(FragmentContext.java:182) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.getGenerated4Copier(RemovingRecordBatch.java:264) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.getGenerated4Copier(RemovingRecordBatch.java:250) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.setupNewSchema(RemovingRecordBatch.java:80) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:66) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:96) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:91) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:116) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:72) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:65) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:45) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:95) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:91) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:116) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:58) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:97) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:48) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:100) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:242) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_55]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_55]\r\n\tat java.lang.Thread.run(Thread.java:744) [na:1.7.0_55]\r\n{code}\r\n\r\nBut The same filter condition works if I apply it directly on Hbase table instead of the view\r\n{code}\r\nselect cast(tbl.row_key as int)rownum, \r\ncast(tbl.stats.name as varchar(20))name,\r\ncast(tbl.stats.age as int)age, \r\ncast(tbl.stats.gpa as float)gpa,\r\ncast(tbl.stats.studentnum as bigint)studentnum, \r\ncast(tbl.stats.create_time as varchar(20))create_time \r\nfrom hbase.hbase_student tbl\r\nwhere tbl.stats.age > 50;\r\n\r\n+------------+------------+------------+------------+------------+-------------+\r\n|   rownum   |    name    |    age     |    gpa     | studentnum | create_time |\r\n+------------+------------+------------+------------+------------+-------------+\r\n| 1          | fred ovid  | 76         | 1.55       | 692315658449 | 2014-05-27 00:26:07 |\r\n| 2          | bob brown  | 63         | 3.18       | 650706039334 | 2014-12-04 21:43:14 |\r\n+------------+------------+------------+------------+------------+-------------+\r\n{code}\r\n\r\n",
        "Order by nested inside a where clause fails Tableau wraps all Custom SQL queries in a SELECT * FROM ( <query> ) `TableauSQL` WHERE (0=1)\r\nIf the query contains an order by, the query will fail\r\n{code}\r\n0: jdbc:drill:zk=localhost:5181> select * from (select cast(c_time as time) c_time from `dfs`.`default`.`opt/drill/raw-files/data` order by c_time) `TableauSQL` where (0=1);\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"250c3758-7254-43d5-b354-ca77f5765876\"\r\nendpoint {\r\n  address: \"192.168.39.43\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Screen received stop request sent. < SchemaChangeException:[ Failure while attempting to load generated class ] < ClassTransformationException:[ Failure generating transformation classes for value: \r\n \r\npackage org.apache.drill.exec.test.generated;\r\n\r\nimport org.apache.drill.exec.exception.SchemaChangeException;\r\nimport org.apache.drill.exec.ops.FragmentContext;\r\nimport org.apache.drill.exec.record.RecordBatch;\r\nimport org.apache.drill.exec.vector.NullableTimeVector;\r\n\r\npublic class CopierGen167 {\r\n\r\n    NullableTimeVector vv0;\r\n    NullableTimeVector vv3;\r\n\r\n    public boolean doEval(int inIndex, int outIndex)\r\n        throws SchemaChangeException\r\n    {\r\n        {\r\n            if (!vv3 .copyFromSafe(((inIndex)& 65535), (outIndex), vv0 [((inIndex)>>> 16)])) {\r\n                return false;\r\n            }\r\n        }\r\n        {\r\n            return true;\r\n        }\r\n    }\r\n\r\n    public void doSetup(FragmentContext context, RecordBatch incoming, RecordBatch outgoing)\r\n        throws SchemaChangeException\r\n    {\r\n        {\r\n            int[] fieldIds1 = new int[ 1 ] ;\r\n            fieldIds1 [ 0 ] = 0;\r\n            Object tmp2 = (incoming).getValueAccessorById(NullableTimeVector.class, fieldIds1).getValueVector();\r\n            if (tmp2 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv0 with id: org.apache.drill.exec.record.TypedFieldId@6c3316c7.\");\r\n            }\r\n            vv0 = ((NullableTimeVector) tmp2);\r\n            int[] fieldIds4 = new int[ 1 ] ;\r\n            fieldIds4 [ 0 ] = 0;\r\n            Object tmp5 = (outgoing).getValueAccessorById(NullableTimeVector.class, fieldIds4).getValueVector();\r\n            if (tmp5 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv3 with id: org.apache.drill.exec.record.TypedFieldId@6c3316c7.\");\r\n            }\r\n            vv3 = ((NullableTimeVector) tmp5);\r\n        }\r\n    }\r\n\r\n}\r\n ] < CompileException:[ Line 18, Column 36: No applicable constructor/method found for actual parameters \"int, int, java.lang.Object\"; candidates are: \"public boolean org.apache.drill.exec.vector.NullableTimeVector.copyFromSafe(int, int, org.apache.drill.exec.vector.TimeVector)\", \"public boolean org.apache.drill.exec.vector.NullableTimeVector.copyFromSafe(int, int, org.apache.drill.exec.vector.NullableTimeVector)\" ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n{code}\r\n\r\nThe query runs fine without the order by clause."
    ],
    [
        "DRILL-1271",
        "DRILL-1240",
        "Update NOTICE file to 2014. ",
        "Copyright out of date in NOTICE NOTICE has copyright 2013. Should be 2013-2014 or something."
    ],
    [
        "DRILL-1274",
        "DRILL-1267",
        "Update License file to correctly identify source inclusions versus binary inclusions Issues found with last release:\r\n\r\n- LICENSE file is not correct - looks like quite a few things need to be removed as they are not actually bundled in the source artefact. The LICENSE file in the source artefact should only reference software that is actually bundled in the source release.\r\n- LICENSE file doesn't need to contain references to Apache licensed software only MIT and BSD software. Each bundled Apache software may modify the NOTICE file.\r\n- LICENSE issue \"The compiled Apache Drill distribution includes the following sources/binaries.\" is incorrect as a source distribution shouldn't normally contain binaries.\r\n- LICENSE refers to CDDL licensed, CPL licensed, EPL licensed and MPL licensed software - all which are category B licences. Are these actually included in the source release and if so how? And if they are included why are they not in the NOTICE file (as per [2]) If not included why are these mentioned in the the LICENCES file at all?\r\n\r\n\r\nI think the issue is we're using the same license file for both source and binary tarballs.  We need to separate these out.",
        "Do not include internal module dependencies in license "
    ],
    [
        "DRILL-1288",
        "DRILL-1278",
        "With json data drill is not returning all the columns in the query involving a join git.commit.id.abbrev\r\n\r\nIn the below  case drill does not return the 3rd column\r\n{code}\r\nselect\r\n      s.s_suppkey,\r\n      s.s_nationkey,\r\n      n.n_name\r\n    from\r\n      supplier s,\r\n      nation n\r\n    where\r\n      s.s_nationkey = n.n_nationkey limit 1;\r\n\r\n+------------+-------------+\r\n| s_suppkey  | s_nationkey |\r\n+------------+-------------+\r\n| 1          | 17          |\r\n+------------+-------------+\r\n\r\n{code}\r\n\r\nIn the below case drill does not return the second column\r\n{code}\r\nselect\r\n      n.n_name,\r\n      s.s_suppkey\r\n    from\r\n      supplier s,\r\n      nation n\r\n    where\r\n      s.s_nationkey = n.n_nationkey limit 1;\r\n\r\n+------------+\r\n|   n_name   |\r\n+------------+\r\n| PERU       |\r\n+------------+\r\n{code}\r\n\r\nLooks like drill is only returning the columns from table/file of the first column. If the first column is from nation, drill returns all subsequent columns from nation but does not return anything from supplier.\r\n\r\nHowever if use a function on columns from the second table, drill does return everything normally\r\n\r\n{code}\r\nselect\r\n      s.s_suppkey,\r\n      char_length(n.n_name) as name_length\r\n    from\r\n      supplier s,\r\n      nation n\r\n    where\r\n      s.s_nationkey = n.n_nationkey limit 1;\r\n\r\n+------------+-------------+\r\n| s_suppkey  | name_length |\r\n+------------+-------------+\r\n| 1          | 4           |\r\n+------------+-------------+\r\n{code}\r\n\r\nThis issue is affecting TPCH queries 10, 7, 3 on top of JSON data.\r\nData used is provided as an attachement",
        "Selecting individual fields from a map with a join fails with IllegalStateException  I have the following json file\r\nbug.json\r\n{\"col\" : {\"name\" : \"foo\", \"id\": 1}, \"col1\": 1}\r\n{\"col\" : {\"name\" : \"bar\", \"id\": 2}, \"col1\": 2}\r\n\r\nHave the same data in another file bug1.json.\r\n\r\nThe following query succeeds and returns the right result.\r\nselect b.col from dfs.`bug.json` b, dfs.`bug1.json` b1 where b.col1 = b1.col1;\r\n\r\nHowever selecting an individual field from the 'col' map fails.\r\nselect b.col.name from dfs.`bug.json` b, dfs.`bug1.json` b1 where b.col1 = b1.col1;\r\n\r\nBelow is the stack trace\r\n\r\njava.lang.IllegalStateException\r\n\r\nIncoming batch has an empty schema. This is not allowed.\r\n\r\nat\u00a0org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:128)\u00a0~[drill-java-exec-0.5.0-incubating-SNAPSHOT-rebuffed.jar:0.5.0-incubating-SNAPSHOT]\r\n\r\nat\u00a0org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:75)\u00a0~[drill-java-exec-0.5.0-incubating-SNAPSHOT-rebuffed.jar:0.5.0-incubating-SNAPSHOT]\r\n\r\nat\u00a0org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:65)\u00a0~[drill-java-exec-0.5.0-incubating-SNAPSHOT-rebuffed.jar:0.5.0-incubating-SNAPSHOT]\r\n\r\nat\u00a0org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:45)\u00a0~[drill-java-exec-0.5.0-incubating-SNAPSHOT-rebuffed.jar:0.5.0-incubating-SNAPSHOT]\r\n\r\nat\u00a0org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:120)\u00a0~[drill-java-exec-0.5.0-incubating-SNAPSHOT-rebuffed.jar:0.5.0-incubating-SNAPSHOT]\r\n\r\nat\u00a0org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:95)\u00a0~[drill-java-exec-0.5.0-incubating-SNAPSHOT-rebuffed.jar:0.5.0-incubating-SNAPSHOT]\r\n\r\nat\u00a0org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:116)\u00a0~[drill-java-exec-0.5.0-incubating-SNAPSHOT-rebuffed.jar:0.5.0-incubating-SNAPSHOT]\r\n\r\nat\u00a0org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:59)\u00a0~[drill-java-exec-0.5.0-incubating-SNAPSHOT-rebuffed.jar:0.5.0-incubating-SNAPSHOT]\r\n\r\nat\u00a0org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:98)\u00a0~[drill-java-exec-0.5.0-incubating-SNAPSHOT-rebuffed.jar:0.5.0-incubating-SNAPSHOT]\r\n\r\nat\u00a0org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:49)\u00a0~[drill-java-exec-0.5.0-incubating-SNAPSHOT-rebuffed.jar:0.5.0-incubating-SNAPSHOT]\r\n\r\nat\u00a0org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:105)\u00a0~[drill-java-exec-0.5.0-incubating-SNAPSHOT-rebuffed.jar:0.5.0-incubating-SNAPSHOT]\r\n\r\nat\u00a0org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:250)\u00a0[drill-java-exec-0.5.0-incubating-SNAPSHOT-rebuffed.jar:0.5.0-incubating-SNAPSHOT]\r\n\r\nat\u00a0java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\u00a0[na:1.7.0_45]\r\n\r\nat\u00a0java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\u00a0[na:1.7.0_45]\r\n\r\nat\u00a0java.lang.Thread.run(Thread.java:744)\u00a0[na:1.7.0_45]\r\n"
    ],
    [
        "DRILL-1301",
        "DRILL-1168",
        "Fail to query from hive tables that have decimal columns git.commit.id.abbrev=687b9b0\r\n\r\nI have a table in hive with the following schema:\r\n\r\nhive> describe voter_test;\r\nOK\r\nvoter_id            \tint                 \t                    \r\nname                \tvarchar(30)         \t                    \r\nage                 \ttinyint             \t                    \r\nregistration        \tstring              \t                    \r\ncontributions       \tdecimal(6,2)        \t                    \r\nvoterzone           \tsmallint            \t                    \r\ncreate_time         \ttimestamp  \r\n\r\nWhen I run any select against this table from drill, it fails with parsing error:\r\n0: jdbc:drill:schema=hive> select * from voter_test limit 5;\r\nQuery failed: Failure while parsing sql. Error: ',', ':', or ';' expected at position 7 from 'decimal(6,2)' [0:decimal, 7:(, 8:6, 9:,, 10:2, 11:)] [2a97a3ae-b9cd-409b-bab6-359febb023b3]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n\r\nI went back to hive and created another table using float instead of decimal for the contributions field and the queries against this table run successfully.\r\n\r\nhive> describe voter_test1;\r\nOK\r\nvoter_id            \tint                 \t                    \r\nname                \tvarchar(30)         \t                    \r\nage                 \ttinyint             \t                    \r\nregistration        \tstring              \t                    \r\ncontributions       \tfloat               \t                    \r\nvoterzone           \tsmallint            \t                    \r\ncreate_time         \ttimestamp                    \t         ",
        "Describe on table/view gives IllegalArgumentException error git.commit.id.abbrev=e5c2da0\r\n\r\ncreate view student_v as select CAST(student_id AS INTEGER) AS student_id, convert_from(name, 'UTF8') AS name, CAST(age AS INTEGER) AS age, CAST(gpa AS DECIMAL(4, 2)) AS gpa, CAST(studentnum AS BIGINT) AS student_num, CAST(create_time AS TIMESTAMP) AS create_time from student;\r\n\r\n0: jdbc:drill:schema=dfs1> describe student_v;\r\nmessage: \"Failure while running fragment. < IllegalArgumentException:[ Error: ',', ':', or ';' expected at position 7 from 'decimal(7,2)' [0:decimal, 7:(, 8:7, 9:,, 10:2, 11:)] ]\""
    ],
    [
        "DRILL-1310",
        "DRILL-1296",
        "Star select with join throws AssertionError Take two simple csv files:\r\n\r\n{panel:title=t1.csv}\r\nID,Name,Lastname\r\n9711942,name0,last0\r\n9707867,name1,last1\r\n{panel}\r\n\r\n{panel:title=t2.csv}\r\nID,Case Number\r\n9711942,HX362083\r\n9707867,HX357851\r\n{panel}\r\n\r\nMaking a simple join on them with a star projection:\r\n\r\n{code:sql}\r\nselect *\r\nfrom \r\ndfs.`/path/to/t1.csv` t1,\r\ndfs.`/path/to/t2.csv` t2\r\nwhere t1.columns[0] = t2.columns[0];\r\n{code}\r\n\r\nthrows \r\n{panel}\r\njava.lang.AssertionError: Unexpected project expression or reference.\r\n\tat org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.classifyExpr(ProjectRecordBatch.java:597)\r\n\tat org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.setupNewSchema(ProjectRecordBatch.java:260)\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:66)\r\n\tat org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:120)\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:95)\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:116)\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:75)\r\n\tat org.apache.drill.exec.physical.impl.join.HashJoinBatch.executeBuildPhase(HashJoinBatch.java:307)\r\n\tat org.apache.drill.exec.physical.impl.join.HashJoinBatch.innerNext(HashJoinBatch.java:196)\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:95)\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:116)\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:75)\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:65)\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:45)\r\n\tat org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:120)\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:95)\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:116)\r\n{panel}",
        "Star query fails with a nested select Take two simple csv files:\r\n{panel:title=t1.csv}\r\nID,Name,Lastname\r\n9711942,name0,last0\r\n9707867,name1,last1\r\n{panel}\r\n\r\n{panel:title=t2.csv}\r\nID,Case Number\r\n9711942,HX362083\r\n9707867,HX357851\r\n{panel}\r\n\r\nRunning the following fails:\r\n{code:sql}\r\nselect *\r\nFROM dfs.`/path/to/t1.csv` \r\nwhere columns[0] in \r\n(SELECT columns[0] FROM dfs.`/path/to/t2.csv`);\r\n{code}\r\n\r\n{panel}\r\njava.lang.UnsupportedOperationException: Failure finding function that runtime code generation expected.  Signature: compare_to( INT:OPTIONALVARCHAR:OPTIONAL,  ) returns INT:REQUIRED\r\n\tat org.apache.drill.exec.expr.fn.FunctionGenerationHelper.getFunctionExpression(FunctionGenerationHelper.java:78)\r\n\tat org.apache.drill.exec.expr.fn.FunctionGenerationHelper.getComparator(FunctionGenerationHelper.java:50)\r\n\tat org.apache.drill.exec.physical.impl.common.ChainedHashTable.setupIsKeyMatchInternal(ChainedHashTable.java:214)\r\n\tat org.apache.drill.exec.physical.impl.common.ChainedHashTable.createAndSetupHashTable(ChainedHashTable.java:173)\r\n\tat org.apache.drill.exec.physical.impl.join.HashJoinBatch.setupHashTable(HashJoinBatch.java:301)\r\n\tat org.apacheError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n.drill.exec.physical.impl.join.HashJoinBatch.executeBuildPhase(HashJoinBatch.java:328)\r\n\tat org.apache.drill.exec.physical.impl.join.HashJoinBatch.innerNext(HashJoinBatch.java:196)\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:95)\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:116)\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:75)\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:65)\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:45)\r\n\tat org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:120)\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:95)\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:116)\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:75)\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:65)\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:45)\r\n\tat org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:120)\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:95)\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:116)\r\n\tat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:59)\r\n\tat org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:98)\r\n\tat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:49)\r\n\tat org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:105)\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:250)\r\n{panel}\r\n"
    ],
    [
        "DRILL-1324",
        "DRILL-1311",
        "Detect schema changes when complex vector changes internally  Currently we indicate a schema change only if the top level vector in the container changes. We need to detect changes when we add a vector to the top level Map, Repeated Map, Repeated List in the container. ",
        "Hash join does not support schema changes error - Create a directory with a couple of JSON files. One with columns a,b and second with columns a,b,c. \r\n- a & b attributes have same data types across both the files\r\n- create a view by selecting columns a, b from the directory\r\n- Join the view with any other table\r\n\r\nAn error shows up indicating that 'Hash join does not support schema changes'.\r\nThere is a schema change across the files with a new element being added, however given that specific columns a,b are selected in the view, expect that query works fine."
    ],
    [
        "DRILL-1347",
        "DRILL-1168",
        "Apache Drill compatibility with Hive 0.13 This is a tracking item to ensure Drill works with Hive 0.13 version.\r\n",
        "Describe on table/view gives IllegalArgumentException error git.commit.id.abbrev=e5c2da0\r\n\r\ncreate view student_v as select CAST(student_id AS INTEGER) AS student_id, convert_from(name, 'UTF8') AS name, CAST(age AS INTEGER) AS age, CAST(gpa AS DECIMAL(4, 2)) AS gpa, CAST(studentnum AS BIGINT) AS student_num, CAST(create_time AS TIMESTAMP) AS create_time from student;\r\n\r\n0: jdbc:drill:schema=dfs1> describe student_v;\r\nmessage: \"Failure while running fragment. < IllegalArgumentException:[ Error: ',', ':', or ';' expected at position 7 from 'decimal(7,2)' [0:decimal, 7:(, 8:7, 9:,, 10:2, 11:)] ]\""
    ],
    [
        "DRILL-1347",
        "DRILL-1301",
        "Apache Drill compatibility with Hive 0.13 This is a tracking item to ensure Drill works with Hive 0.13 version.\r\n",
        "Fail to query from hive tables that have decimal columns git.commit.id.abbrev=687b9b0\r\n\r\nI have a table in hive with the following schema:\r\n\r\nhive> describe voter_test;\r\nOK\r\nvoter_id            \tint                 \t                    \r\nname                \tvarchar(30)         \t                    \r\nage                 \ttinyint             \t                    \r\nregistration        \tstring              \t                    \r\ncontributions       \tdecimal(6,2)        \t                    \r\nvoterzone           \tsmallint            \t                    \r\ncreate_time         \ttimestamp  \r\n\r\nWhen I run any select against this table from drill, it fails with parsing error:\r\n0: jdbc:drill:schema=hive> select * from voter_test limit 5;\r\nQuery failed: Failure while parsing sql. Error: ',', ':', or ';' expected at position 7 from 'decimal(6,2)' [0:decimal, 7:(, 8:6, 9:,, 10:2, 11:)] [2a97a3ae-b9cd-409b-bab6-359febb023b3]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n\r\nI went back to hive and created another table using float instead of decimal for the contributions field and the queries against this table run successfully.\r\n\r\nhive> describe voter_test1;\r\nOK\r\nvoter_id            \tint                 \t                    \r\nname                \tvarchar(30)         \t                    \r\nage                 \ttinyint             \t                    \r\nregistration        \tstring              \t                    \r\ncontributions       \tfloat               \t                    \r\nvoterzone           \tsmallint            \t                    \r\ncreate_time         \ttimestamp                    \t         "
    ],
    [
        "DRILL-1349",
        "DRILL-1348",
        "Could not find schemas when using drill ODBC connecting to Tableau 8.2 Documentation is based on Tableau 8.1:\r\nhttp://doc.mapr.com/display/MapR/Tableau+Examples\r\n\r\nHowever when I am using Tableau 8.2, after connecting to drill ODBC DSN, Tableau could not find any schemas. \r\nHowever this DSN works fine in Drill explorer.\r\nSee attached pictures for details.\r\n\r\nAsk:\r\n1. Does drill ODBC support/verify Tableau desktop 8.2 or not?\r\n2. If so, could we add the new steps in the documentation?\r\n3. If not, where could we find the support matrix between Drill ODBC and other BI tools?\r\n",
        "Could not find schemas when using drill ODBC connecting to Tableau 8.2 Documentation is based on Tableau 8.1:\r\nhttp://doc.mapr.com/display/MapR/Tableau+Examples\r\n\r\nHowever when I am using Tableau 8.2, after connecting to drill ODBC DSN, Tableau could not find any schemas. \r\nHowever this DSN works fine in Drill explorer.\r\nSee attached pictures for details.\r\n\r\nAsk:\r\n1. Does drill ODBC support/verify Tableau desktop 8.2 or not?\r\n2. If so, could we add the new steps in the documentation?\r\n3. If not, where could we find the support matrix between Drill ODBC and other BI tools?\r\n"
    ],
    [
        "DRILL-1397",
        "DRILL-1396",
        "Query with IN clause and correlation fails (for Text files) The following query fails. This could be related to https://issues.apache.org/jira/browse/DRILL-1396, but filing separate issue as the error is different.\r\n\r\n0: jdbc:drill:> select t.trans_info.purch_flag,\r\n. . . . . . . >           t.user_info.cust_id, t.trans_info.prod_id[0]\r\n. . . . . . . > from `Clickstream.clicks`.`/json/clicks.json` t \r\n. . . . . . . > where  t.user_info.cust_id IN (select o.cust_id from hive.orders o where o.order_total >100 );\r\n\r\nQuery failed: Failure while running fragment. Incoming batch has an empty schema. This is not allowed. [2b441a79-be49-4116-a459-513f97418738]\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n\r\nBelow is the explain plan.\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(EXPR$0=[$0], EXPR$1=[$1], EXPR$2=[$2])\r\n00-02        Project(EXPR$0=[ITEM($1, 'purch_flag')], EXPR$1=[ITEM($0, 'cust_id')], EXPR$2=[ITEM(ITEM($1, 'prod_id'), 0)])\r\n00-03          HashJoin(condition=[=($2, $3)], joinType=[inner])\r\n00-05            Project(T27\u00a6\u00a6user_info=[$1], T27\u00a6\u00a6trans_info=[$2], $f3=[ITEM($1, 'cust_id')])\r\n00-07              Project(T27\u00a6\u00a6*=[$0], T27\u00a6\u00a6user_info=[$1], T27\u00a6\u00a6trans_info=[$2])\r\n00-09                Scan(groupscan=[EasyGroupScan [selectionRoot=/mapr/my.cluster.com/demo/clicks/json/clicks.json, columns = null]])\r\n00-04            HashAgg(group=[{0}])\r\n00-06              Project(cust_id=[$0])\r\n00-08                SelectionVectorRemover\r\n00-10                  Filter(condition=[>($1, 100)])\r\n00-11                    Project(cust_id=[$1], order_total=[$0])\r\n00-12                      Scan(groupscan=[HiveScan [table=Table(tableName:orders, dbName:default, owner:root, createTime:1409956843, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:order_id, type:bigint, comment:null), FieldSchema(name:month, type:string, comment:null), FieldSchema(name:purchdate, type:timestamp, comment:null), FieldSchema(name:cust_id, type:bigint, comment:null), FieldSchema(name:state, type:string, comment:null), FieldSchema(name:prod_id, type:bigint, comment:null), FieldSchema(name:order_total, type:int, comment:null)], location:maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE, transient_lastDdlTime=1409956843}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE), inputSplits=[maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month2.agg.orders.csv:0+640155, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month5.agg.orders.csv:0+775506, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month6.agg.orders.csv:0+791685, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month8.agg.orders.csv:0+805072, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month4.agg.orders.csv:0+603886, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month9.agg.orders.csv:0+846270, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month1.agg.orders.csv:0+461090, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month7.agg.orders.csv:0+771399, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month3.agg.orders.csv:0+806738], columns=[SchemaPath [`cust_id`], SchemaPath [`order_total`]]]])\r\n | {\r\n  \"head\" : {\r\n    \"version\" : 1,\r\n    \"generator\" : {\r\n      \"type\" : \"ExplainHandler\",\r\n      \"info\" : \"\"\r\n    },\r\n    \"type\" : \"APACHE_DRILL_PHYSICAL\",\r\n    \"options\" : [ ],\r\n    \"queue\" : 0,\r\n    \"resultMode\" : \"EXEC\"\r\n  },\r\n  \"graph\" : [ {\r\n    \"pop\" : \"hive-scan\",\r\n    \"@id\" : 12,\r\n    \"hive-table\" : {\r\n      \"table\" : {\r\n        \"tableName\" : \"orders\",\r\n        \"dbName\" : \"default\",\r\n        \"owner\" : \"root\",\r\n        \"createTime\" : 1409956843,\r\n        \"lastAccessTime\" : 0,\r\n        \"retention\" : 0,\r\n        \"sd\" : {\r\n          \"cols\" : [ {\r\n            \"name\" : \"order_id\",\r\n            \"type\" : \"bigint\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"month\",\r\n            \"type\" : \"string\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"purchdate\",\r\n            \"type\" : \"timestamp\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"cust_id\",\r\n            \"type\" : \"bigint\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"state\",\r\n            \"type\" : \"string\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"prod_id\",\r\n            \"type\" : \"bigint\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"order_total\",\r\n            \"type\" : \"int\",\r\n            \"comment\" : null\r\n          } ],\r\n          \"location\" : \"maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders\",\r\n          \"inputFormat\" : \"org.apache.hadoop.mapred.TextInputFormat\",\r\n          \"outputFormat\" : \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\r\n          \"compressed\" : false,\r\n          \"numBuckets\" : -1,\r\n          \"serDeInfo\" : {\r\n            \"name\" : null,\r\n            \"serializationLib\" : \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\r\n            \"parameters\" : {\r\n              \"serialization.format\" : \",\",\r\n              \"field.delim\" : \",\"\r\n            }\r\n          },\r\n          \"sortCols\" : [ ],\r\n          \"parameters\" : { }\r\n        },\r\n        \"partitionKeys\" : [ ],\r\n        \"parameters\" : {\r\n          \"EXTERNAL\" : \"TRUE\",\r\n          \"transient_lastDdlTime\" : \"1409956843\"\r\n        },\r\n        \"viewOriginalText\" : null,\r\n        \"viewExpandedText\" : null,\r\n        \"tableType\" : \"EXTERNAL_TABLE\"\r\n      },\r\n      \"partitions\" : null,\r\n      \"hiveConfigOverride\" : {\r\n        \"hive.metastore.uris\" : \"thrift://192.168.208.143:9083\",\r\n        \"hive.metastore.sasl.enabled\" : \"false\"\r\n      }\r\n    },\r\n    \"storage-plugin\" : \"hive\",\r\n    \"columns\" : [ \"`cust_id`\", \"`order_total`\" ],\r\n    \"cost\" : 6349.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 11,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`cust_id`\",\r\n      \"expr\" : \"`cust_id`\"\r\n    }, {\r\n      \"ref\" : \"`order_total`\",\r\n      \"expr\" : \"`order_total`\"\r\n    } ],\r\n    \"child\" : 12,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 6349.0\r\n  }, {\r\n    \"pop\" : \"filter\",\r\n    \"@id\" : 10,\r\n    \"child\" : 11,\r\n    \"expr\" : \"greater_than(`order_total`, 100) \",\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 3174.5\r\n  }, {\r\n    \"pop\" : \"selection-vector-remover\",\r\n    \"@id\" : 8,\r\n    \"child\" : 10,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 3174.5\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 6,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`cust_id`\",\r\n      \"expr\" : \"`cust_id`\"\r\n    } ],\r\n    \"child\" : 8,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 3174.5\r\n  }, {\r\n    \"pop\" : \"hash-aggregate\",\r\n    \"@id\" : 4,\r\n    \"child\" : 6,\r\n    \"cardinality\" : 1.0,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 1587.25,\r\n    \"groupByExprs\" : [ {\r\n      \"ref\" : \"`cust_id`\",\r\n      \"expr\" : \"`cust_id`\"\r\n    } ],\r\n    \"aggrExprs\" : [ ]\r\n  }, {\r\n    \"pop\" : \"fs-scan\",\r\n    \"@id\" : 9,\r\n    \"files\" : [ \"maprfs:/mapr/my.cluster.com/demo/clicks/json/clicks.json\" ],\r\n    \"storage\" : {\r\n      \"type\" : \"file\",\r\n      \"enabled\" : true,\r\n      \"connection\" : \"maprfs:///\",\r\n      \"workspaces\" : {\r\n        \"root\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo\",\r\n          \"writable\" : false,\r\n          \"storageformat\" : null\r\n        },\r\n        \"clicks\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo/clicks\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        },\r\n        \"views\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo/views\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        }\r\n      },\r\n      \"formats\" : {\r\n        \"psv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tbl\" ],\r\n          \"delimiter\" : \"|\"\r\n        },\r\n        \"csv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"csv\" ],\r\n          \"delimiter\" : \",\"\r\n        },\r\n        \"tsv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tsv\" ],\r\n          \"delimiter\" : \"\\t\"\r\n        },\r\n        \"parquet\" : {\r\n          \"type\" : \"parquet\"\r\n        },\r\n        \"json\" : {\r\n          \"type\" : \"json\"\r\n        }\r\n      }\r\n    },\r\n    \"format\" : {\r\n      \"type\" : \"json\"\r\n    },\r\n    \"selectionRoot\" : \"/mapr/my.cluster.com/demo/clicks/json/clicks.json\",\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 7,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`T27\u00a6\u00a6*`\",\r\n      \"expr\" : \"`*`\"\r\n    } ],\r\n    \"child\" : 9,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 5,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`T27\u00a6\u00a6user_info`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6user_info`\"\r\n    }, {\r\n      \"ref\" : \"`T27\u00a6\u00a6trans_info`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6trans_info`\"\r\n    }, {\r\n      \"ref\" : \"`$f3`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6user_info`.`cust_id`\"\r\n    } ],\r\n    \"child\" : 7,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"hash-join\",\r\n    \"@id\" : 3,\r\n    \"left\" : 5,\r\n    \"right\" : 4,\r\n    \"conditions\" : [ {\r\n      \"relationship\" : \"==\",\r\n      \"left\" : \"`$f3`\",\r\n      \"right\" : \"`cust_id`\"\r\n    } ],\r\n    \"joinType\" : \"INNER\",\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 2,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`EXPR$0`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6trans_info`.`purch_flag`\"\r\n    }, {\r\n      \"ref\" : \"`EXPR$1`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6user_info`.`cust_id`\"\r\n    }, {\r\n      \"ref\" : \"`EXPR$2`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6trans_info`.`prod_id`[0]\"\r\n    } ],\r\n    \"child\" : 3,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 1,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`EXPR$0`\",\r\n      \"expr\" : \"`EXPR$0`\"\r\n    }, {\r\n      \"ref\" : \"`EXPR$1`\",\r\n      \"expr\" : \"`EXPR$1`\"\r\n    }, {\r\n      \"ref\" : \"`EXPR$2`\",\r\n      \"expr\" : \"`EXPR$2`\"\r\n    } ],\r\n    \"child\" : 2,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"screen\",\r\n    \"@id\" : 0,\r\n    \"child\" : 1,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  } ]\r\n} |\r\n+------------+------------+\r\n\r\n",
        "Query with EXISTS clause and correlation fails The following query fails.\r\n//Get the clickstream activity for for all the customers who have order total >100\r\n\r\nselect t.trans_info.purch_flag,\r\n t.user_info.cust_id, t.trans_info.prod_id\r\nfrom `Clickstream.clicks`.`/json/clicks.json` t \r\nwhere  exists (select * from hive.orders o where o.cust_id = t.user_info.cust_id and o.order_total > 100)\r\n\r\nQuery failed: Failure while running fragment. Failure finding function that runtime code generation expected.  Signature: compare_to( MAP:REQUIREDMAP:REQUIRED,  ) returns INT:REQUIRED [d6401ddd-f9bc-496d-ae0c-b5cde35bf289]\r\n\r\n\r\nBelow is the explain plan:\r\n\r\n\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(EXPR$0=[$0], EXPR$1=[$1], EXPR$2=[$2])\r\n00-02        Project(EXPR$0=[ITEM($2, 'purch_flag')], EXPR$1=[ITEM($1, 'cust_id')], EXPR$2=[ITEM($2, 'prod_id')])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[IS TRUE($4)])\r\n00-05              HashJoin(condition=[=($1, $3)], joinType=[left])\r\n00-07                Project(T24\u00a6\u00a6*=[$0], T24\u00a6\u00a6user_info=[$1], T24\u00a6\u00a6trans_info=[$2])\r\n00-09                  Scan(groupscan=[EasyGroupScan [selectionRoot=/mapr/my.cluster.com/demo/clicks/json/clicks.json, columns = null]])\r\n00-06                HashAgg(group=[{0}], agg#0=[MIN($1)])\r\n00-08                  Project(T25\u00a6\u00a6user_info=[$1], $f0=[true])\r\n00-10                    HashJoin(condition=[=($0, $2)], joinType=[inner])\r\n00-12                      Project($f7=[CAST($0):ANY])\r\n00-14                        SelectionVectorRemover\r\n00-16                          Filter(condition=[>($1, 100)])\r\n00-18                            Project(cust_id=[$1], order_total=[$0])\r\n00-20                              Scan(groupscan=[HiveScan [table=Table(tableName:orders, dbName:default, owner:root, createTime:1409956843, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:order_id, type:bigint, comment:null), FieldSchema(name:month, type:string, comment:null), FieldSchema(name:purchdate, type:timestamp, comment:null), FieldSchema(name:cust_id, type:bigint, comment:null), FieldSchema(name:state, type:string, comment:null), FieldSchema(name:prod_id, type:bigint, comment:null), FieldSchema(name:order_total, type:int, comment:null)], location:maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE, transient_lastDdlTime=1409956843}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE), inputSplits=[maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month2.agg.orders.csv:0+640155, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month5.agg.orders.csv:0+775506, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month6.agg.orders.csv:0+791685, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month8.agg.orders.csv:0+805072, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month4.agg.orders.csv:0+603886, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month9.agg.orders.csv:0+846270, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month1.agg.orders.csv:0+461090, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month7.agg.orders.csv:0+771399, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month3.agg.orders.csv:0+806738], columns=[SchemaPath [`cust_id`], SchemaPath [`order_total`]]]])\r\n00-11                      Project(T25\u00a6\u00a6user_info=[$0], $f1=[ITEM($0, 'cust_id')])\r\n00-13                        HashAgg(group=[{0}])\r\n00-15                          Project(T25\u00a6\u00a6user_info=[$1])\r\n00-17                            Project(T25\u00a6\u00a6*=[$0], T25\u00a6\u00a6user_info=[$1], T25\u00a6\u00a6trans_info=[$2])\r\n00-19                              Scan(groupscan=[EasyGroupScan [selectionRoot=/mapr/my.cluster.com/demo/clicks/json/clicks.json, columns = null]])\r\n | {\r\n  \"head\" : {\r\n    \"version\" : 1,\r\n    \"generator\" : {\r\n      \"type\" : \"ExplainHandler\",\r\n      \"info\" : \"\"\r\n    },\r\n    \"type\" : \"APACHE_DRILL_PHYSICAL\",\r\n    \"options\" : [ ],\r\n    \"queue\" : 0,\r\n    \"resultMode\" : \"EXEC\"\r\n  },\r\n  \"graph\" : [ {\r\n    \"pop\" : \"hive-scan\",\r\n    \"@id\" : 20,\r\n    \"hive-table\" : {\r\n      \"table\" : {\r\n        \"tableName\" : \"orders\",\r\n        \"dbName\" : \"default\",\r\n        \"owner\" : \"root\",\r\n        \"createTime\" : 1409956843,\r\n        \"lastAccessTime\" : 0,\r\n        \"retention\" : 0,\r\n        \"sd\" : {\r\n          \"cols\" : [ {\r\n            \"name\" : \"order_id\",\r\n            \"type\" : \"bigint\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"month\",\r\n            \"type\" : \"string\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"purchdate\",\r\n            \"type\" : \"timestamp\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"cust_id\",\r\n            \"type\" : \"bigint\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"state\",\r\n            \"type\" : \"string\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"prod_id\",\r\n            \"type\" : \"bigint\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"order_total\",\r\n            \"type\" : \"int\",\r\n            \"comment\" : null\r\n          } ],\r\n          \"location\" : \"maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders\",\r\n          \"inputFormat\" : \"org.apache.hadoop.mapred.TextInputFormat\",\r\n          \"outputFormat\" : \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\r\n          \"compressed\" : false,\r\n          \"numBuckets\" : -1,\r\n          \"serDeInfo\" : {\r\n            \"name\" : null,\r\n            \"serializationLib\" : \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\r\n            \"parameters\" : {\r\n              \"serialization.format\" : \",\",\r\n              \"field.delim\" : \",\"\r\n            }\r\n          },\r\n          \"sortCols\" : [ ],\r\n          \"parameters\" : { }\r\n        },\r\n        \"partitionKeys\" : [ ],\r\n        \"parameters\" : {\r\n          \"EXTERNAL\" : \"TRUE\",\r\n          \"transient_lastDdlTime\" : \"1409956843\"\r\n        },\r\n        \"viewOriginalText\" : null,\r\n        \"viewExpandedText\" : null,\r\n        \"tableType\" : \"EXTERNAL_TABLE\"\r\n      },\r\n      \"partitions\" : null,\r\n      \"hiveConfigOverride\" : {\r\n        \"hive.metastore.uris\" : \"thrift://192.168.208.143:9083\",\r\n        \"hive.metastore.sasl.enabled\" : \"false\"\r\n      }\r\n    },\r\n    \"storage-plugin\" : \"hive\",\r\n    \"columns\" : [ \"`cust_id`\", \"`order_total`\" ],\r\n    \"cost\" : 6349.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 18,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`cust_id`\",\r\n      \"expr\" : \"`cust_id`\"\r\n    }, {\r\n      \"ref\" : \"`order_total`\",\r\n      \"expr\" : \"`order_total`\"\r\n    } ],\r\n    \"child\" : 20,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 6349.0\r\n  }, {\r\n    \"pop\" : \"filter\",\r\n    \"@id\" : 16,\r\n    \"child\" : 18,\r\n    \"expr\" : \"greater_than(`order_total`, 100) \",\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 3174.5\r\n  }, {\r\n    \"pop\" : \"selection-vector-remover\",\r\n    \"@id\" : 14,\r\n    \"child\" : 16,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 3174.5\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 12,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`$f7`\",\r\n      \"expr\" : \"`cust_id`\"\r\n    } ],\r\n    \"child\" : 14,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 3174.5\r\n  }, {\r\n    \"pop\" : \"fs-scan\",\r\n    \"@id\" : 9,\r\n    \"files\" : [ \"maprfs:/mapr/my.cluster.com/demo/clicks/json/clicks.json\" ],\r\n    \"storage\" : {\r\n      \"type\" : \"file\",\r\n      \"enabled\" : true,\r\n      \"connection\" : \"maprfs:///\",\r\n      \"workspaces\" : {\r\n        \"root\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo\",\r\n          \"writable\" : false,\r\n          \"storageformat\" : null\r\n        },\r\n        \"clicks\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo/clicks\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        },\r\n        \"views\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo/views\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        }\r\n      },\r\n      \"formats\" : {\r\n        \"psv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tbl\" ],\r\n          \"delimiter\" : \"|\"\r\n        },\r\n        \"csv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"csv\" ],\r\n          \"delimiter\" : \",\"\r\n        },\r\n        \"tsv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tsv\" ],\r\n          \"delimiter\" : \"\\t\"\r\n        },\r\n        \"parquet\" : {\r\n          \"type\" : \"parquet\"\r\n        },\r\n        \"json\" : {\r\n          \"type\" : \"json\"\r\n        }\r\n      }\r\n    },\r\n    \"format\" : {\r\n      \"type\" : \"json\"\r\n    },\r\n    \"selectionRoot\" : \"/mapr/my.cluster.com/demo/clicks/json/clicks.json\",\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 7,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`T24\u00a6\u00a6*`\",\r\n      \"expr\" : \"`*`\"\r\n    } ],\r\n    \"child\" : 9,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"fs-scan\",\r\n    \"@id\" : 19,\r\n    \"files\" : [ \"maprfs:/mapr/my.cluster.com/demo/clicks/json/clicks.json\" ],\r\n    \"storage\" : {\r\n      \"type\" : \"file\",\r\n      \"enabled\" : true,\r\n      \"connection\" : \"maprfs:///\",\r\n      \"workspaces\" : {\r\n        \"root\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo\",\r\n          \"writable\" : false,\r\n          \"storageformat\" : null\r\n        },\r\n        \"clicks\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo/clicks\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        },\r\n        \"views\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo/views\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        }\r\n      },\r\n      \"formats\" : {\r\n        \"psv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tbl\" ],\r\n          \"delimiter\" : \"|\"\r\n        },\r\n        \"csv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"csv\" ],\r\n          \"delimiter\" : \",\"\r\n        },\r\n        \"tsv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tsv\" ],\r\n          \"delimiter\" : \"\\t\"\r\n        },\r\n        \"parquet\" : {\r\n          \"type\" : \"parquet\"\r\n        },\r\n        \"json\" : {\r\n          \"type\" : \"json\"\r\n        }\r\n      }\r\n    },\r\n    \"format\" : {\r\n      \"type\" : \"json\"\r\n    },\r\n    \"selectionRoot\" : \"/mapr/my.cluster.com/demo/clicks/json/clicks.json\",\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 17, |\r\n+------------+------------+"
    ],
    [
        "DRILL-1408",
        "DRILL-1401",
        "SELECT column from CSV with JOIN returns null if not part of JOIN condition A SELECT for a column from a CSV file with a JOIN condition always returns null for columns not included in the JOIN condition.  When querying each table separately, the values are returned as expected.\r\n\r\nNote that this works fine for any combination of JSON and Parquet, but fails when at least one of the files is CSV.\r\n\r\nSimple example with two small CSV files:\r\n\r\nbeatles.csv:\r\n---------------------------\r\n1,John,Lennon\r\n2,Paul,McCartney\r\n3,George,Harrison\r\n4,Ringo,Starr\r\n----------------------------\r\n\r\nsongs.csv:\r\n----------------------------\r\n1,Help\r\n2,Yesterday\r\n3,Blue Jay Way\r\n4,Yellow Submarine\r\n----------------------------\r\n\r\nThis queries returns values as expected:\r\n\r\nSELECT columns[0] AS id, CONCAT(columns[1], ' ', columns[2]) AS singer FROM dfs.`beatles.csv`;\r\n\r\n\r\nThis query returns 4 results, all with null values:\r\n\r\nSELECT S.columns[1] AS song, CONCAT(B.columns[1], ' ', B.columns[2]) AS singer \r\nFROM dfs.`beatles.csv` AS B\r\nINNER JOIN dfs.`songs.csv` AS S ON B.columns[0] = S.columns[0];\r\n\r\n\r\nThe only columns that return non-null values are the ones from the JOIN condition (B.columns[0] and S.columns[0] in the following query):\r\n\r\nSELECT S.columns[1] AS song, CONCAT(B.columns[1], ' ', B.columns[2]) AS singer, S.columns[0] AS beatles_id, B.columns[0] AS id \r\nFROM dfs.`beatles.csv` AS B\r\nINNER JOIN dfs.`songs.csv` AS S ON B.columns[0] = S.columns[0];\r\n",
        "Wrong result with csv data when projecting columns not part of the Join or Filter #Wed Sep 10 13:35:05 PDT 2014\r\ngit.commit.id.abbrev=686eb9e\r\n\r\njoin (inner,full outer,left,right) does not work if the join is directly applied to csv files. It will produce either null values on the projected columns or NumberFormatException depends on joining condition applied on which columns (first or second columns). If you create views on the csv files, then join works on the views.\r\n\r\nFor example, the following join give null values:\r\n\r\n0: jdbc:drill:schema=dfs> select cast(`aggregate_100r.csv`.columns[0] as int), cast(`aggregate_100r.csv`.columns[1] as int), cast(`join_100r.csv`.columns[1] as int) from `aggregate_100r.csv` inner join `join_100r.csv` on cast(`aggregate_100r.csv`.columns[0] as int) = cast(`join_100r.csv`.columns[0] as int);\r\n+------------+------------+------------+\r\n|   EXPR$0   |   EXPR$1   |   EXPR$2   |\r\n+------------+------------+------------+\r\n| 0          | null       | null       |\r\n| 0          | null       | null       |\r\n| 0          | null       | null       |\r\n| 0          | null       | null       |\r\n| 0          | null       | null       |\r\n| 0          | null       | null       |\r\n| 0          | null       | null       |\r\n| 0          | null       | null       |\r\n| 0          | null       | null       |\r\n| 0          | null       | null       |\r\n| 0          | null       | null       |\r\n| 0          | null       | null       |\r\n| 0          | null       | null       |\r\n| 0          | null       | null       |\r\n| 0          | null       | null       |\r\n| 0          | null       | null       |\r\n| 1          | null       | null       |\r\n| 1          | null       | null       |\r\n| 1          | null       | null       |\r\n| 1          | null       | null       |\r\n\r\nThe following give NumberFormatException:\r\n\r\n0: jdbc:drill:schema=dfs> select cast(`aggregate_100r.csv`.columns[0] as int), cast(`aggregate_100r.csv`.columns[1] as int), cast(`join_100r.csv`.columns[1] as int) from `aggregate_100r.csv` inner join `join_100r.csv` on cast(`aggregate_100r.csv`.columns[1] as int) = cast(`join_100r.csv`.columns[1] as int);\r\nQuery failed: Failure while running fragment.  [3f67299e-f312-445b-8d6b-74984a820f0c]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n\r\nThe following with views works:\r\n\r\n0: jdbc:drill:schema=dfs> select `aggregate_100r_v`.c0, `aggregate_100r_v`.c1, `join_100r_v`.c1 from `aggregate_100r_v` inner join `join_100r_v` on `aggregate_100r_v`.c0 = `join_100r_v`.c0;\r\n+------------+------------+------------+\r\n|     c0     |     c1     |    c10     |\r\n+------------+------------+------------+\r\n| 0          | 0          | 0          |\r\n| 0          | 0          | 2          |\r\n| 0          | 0          | 1          |\r\n| 0          | 0          | 0          |\r\n| 0          | 0          | 0          |\r\n| 0          | 0          | 2          |\r\n| 0          | 0          | 1          |\r\n| 0          | 0          | 0          |\r\n| 0          | 1          | 0          |\r\n| 0          | 1          | 2          |\r\n| 0          | 1          | 1          |\r\n| 0          | 1          | 0          |\r\n| 0          | 2          | 0          |\r\n| 0          | 2          | 2          |\r\n| 0          | 2          | 1          |\r\n| 0          | 2          | 0          |\r\n| 1          | 1          | 1          |\r\n| 1          | 1          | 2          |\r\n| 1          | 1          | 1          |\r\n| 1          | 1          | 0          |"
    ],
    [
        "DRILL-1420",
        "DRILL-1398",
        "Hive query fails with uncaught Exception Hi Drill-Team,\r\n\r\neverytime I try to query a hive table from drill-0.5.0 the following happens:\r\n\r\n0: jdbc:drill:zk=hadoop2:5181,hadoop3:5181,ha> use hive;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | Default schema changed to 'hive' |\r\n+------------+------------+\r\n1 row selected (0,024 seconds)\r\n0: jdbc:drill:zk=hadoop2:5181,hadoop3:5181,ha> show tables;\r\n+--------------+------------+\r\n| TABLE_SCHEMA | TABLE_NAME |\r\n+--------------+------------+\r\n| hive.default | bondaten   |\r\n+--------------+------------+\r\n1 row selected (0,09 seconds)\r\n0: jdbc:drill:zk=hadoop2:5181,hadoop3:5181,ha> describe bondaten;\r\n+-------------+------------+-------------+\r\n| COLUMN_NAME | DATA_TYPE  | IS_NULLABLE |\r\n+-------------+------------+-------------+\r\n| id          | VARCHAR    | YES         |\r\n| type        | INTEGER    | YES         |\r\n+-------------+------------+-------------+\r\n2 rows selected (0,092 seconds)\r\n0: jdbc:drill:zk=hadoop2:5181,hadoop3:5181,ha> select * from bondaten limit 10;\r\nQuery failed: Failure due to uncaught exception Instantiation of [simple type, class org.apache.drill.exec.store.hive.HiveTable$HivePartition] value failed: null (through reference chain: org.apache.drill.exec.store.hive.HivePartition[\"parameters\"]) [5128c99b-0dfb-46ae-913d-7baf5542df3e]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)",
        "Clone HivePartition only when InputSplit has non-null partition in InputSplit->Partition map If we try to clone HivePartition with a null source, during deserialization of fragment we end up with a NPE:\r\n\r\n{code}\r\njava.lang.NullPointerException: null\r\n\tat org.apache.drill.exec.store.hive.HiveTable$HivePartition.<init>(HiveTable.java:149) ~[drill-storage-hive-core-0.5.0-incubating-SNAPSHOT.jar:0.5.0-incubating-SNAPSHOT]\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.7.0_60]\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[na:1.7.0_60]\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.7.0_60]\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526) ~[na:1.7.0_60]\r\n\tat com.fasterxml.jackson.databind.introspect.AnnotatedConstructor.call(AnnotatedConstructor.java:125) ~[jackson-databind-2.2.0.jar:2.2.0]\r\n\tat com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromObjectWith(StdValueInstantiator.java:240) ~[jackson-databind-2.2.0.jar:2.2.0]\r\n\tat com.fasterxml.jackson.databind.deser.impl.PropertyBasedCreator.build(PropertyBasedCreator.java:158) ~[jackson-databind-2.2.0.jar:2.2.0]\r\n{code}"
    ],
    [
        "DRILL-1423",
        "DRILL-1396",
        "\"exists\" function does not work with text files git.commit.id.abbrev=1ce7c93\r\n\r\nThe below query does not work on text files. However it works properly on parquet files along with views on top of text files\r\n{code}\r\nselect\r\n  o.columns[5]\r\nfrom\r\n  `orders.tbl` o\r\nwhere\r\n  exists (\r\n    select\r\n      *\r\n    from\r\n      `lineitem.tbl` l\r\n    where\r\n       l.columns[0] = o.columns[0]\r\n  )\r\n{code}\r\n\r\nI attached the error log and the data files used. Let me know if you have any questions.",
        "Query with EXISTS clause and correlation fails The following query fails.\r\n//Get the clickstream activity for for all the customers who have order total >100\r\n\r\nselect t.trans_info.purch_flag,\r\n t.user_info.cust_id, t.trans_info.prod_id\r\nfrom `Clickstream.clicks`.`/json/clicks.json` t \r\nwhere  exists (select * from hive.orders o where o.cust_id = t.user_info.cust_id and o.order_total > 100)\r\n\r\nQuery failed: Failure while running fragment. Failure finding function that runtime code generation expected.  Signature: compare_to( MAP:REQUIREDMAP:REQUIRED,  ) returns INT:REQUIRED [d6401ddd-f9bc-496d-ae0c-b5cde35bf289]\r\n\r\n\r\nBelow is the explain plan:\r\n\r\n\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(EXPR$0=[$0], EXPR$1=[$1], EXPR$2=[$2])\r\n00-02        Project(EXPR$0=[ITEM($2, 'purch_flag')], EXPR$1=[ITEM($1, 'cust_id')], EXPR$2=[ITEM($2, 'prod_id')])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[IS TRUE($4)])\r\n00-05              HashJoin(condition=[=($1, $3)], joinType=[left])\r\n00-07                Project(T24\u00a6\u00a6*=[$0], T24\u00a6\u00a6user_info=[$1], T24\u00a6\u00a6trans_info=[$2])\r\n00-09                  Scan(groupscan=[EasyGroupScan [selectionRoot=/mapr/my.cluster.com/demo/clicks/json/clicks.json, columns = null]])\r\n00-06                HashAgg(group=[{0}], agg#0=[MIN($1)])\r\n00-08                  Project(T25\u00a6\u00a6user_info=[$1], $f0=[true])\r\n00-10                    HashJoin(condition=[=($0, $2)], joinType=[inner])\r\n00-12                      Project($f7=[CAST($0):ANY])\r\n00-14                        SelectionVectorRemover\r\n00-16                          Filter(condition=[>($1, 100)])\r\n00-18                            Project(cust_id=[$1], order_total=[$0])\r\n00-20                              Scan(groupscan=[HiveScan [table=Table(tableName:orders, dbName:default, owner:root, createTime:1409956843, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:order_id, type:bigint, comment:null), FieldSchema(name:month, type:string, comment:null), FieldSchema(name:purchdate, type:timestamp, comment:null), FieldSchema(name:cust_id, type:bigint, comment:null), FieldSchema(name:state, type:string, comment:null), FieldSchema(name:prod_id, type:bigint, comment:null), FieldSchema(name:order_total, type:int, comment:null)], location:maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE, transient_lastDdlTime=1409956843}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE), inputSplits=[maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month2.agg.orders.csv:0+640155, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month5.agg.orders.csv:0+775506, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month6.agg.orders.csv:0+791685, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month8.agg.orders.csv:0+805072, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month4.agg.orders.csv:0+603886, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month9.agg.orders.csv:0+846270, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month1.agg.orders.csv:0+461090, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month7.agg.orders.csv:0+771399, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month3.agg.orders.csv:0+806738], columns=[SchemaPath [`cust_id`], SchemaPath [`order_total`]]]])\r\n00-11                      Project(T25\u00a6\u00a6user_info=[$0], $f1=[ITEM($0, 'cust_id')])\r\n00-13                        HashAgg(group=[{0}])\r\n00-15                          Project(T25\u00a6\u00a6user_info=[$1])\r\n00-17                            Project(T25\u00a6\u00a6*=[$0], T25\u00a6\u00a6user_info=[$1], T25\u00a6\u00a6trans_info=[$2])\r\n00-19                              Scan(groupscan=[EasyGroupScan [selectionRoot=/mapr/my.cluster.com/demo/clicks/json/clicks.json, columns = null]])\r\n | {\r\n  \"head\" : {\r\n    \"version\" : 1,\r\n    \"generator\" : {\r\n      \"type\" : \"ExplainHandler\",\r\n      \"info\" : \"\"\r\n    },\r\n    \"type\" : \"APACHE_DRILL_PHYSICAL\",\r\n    \"options\" : [ ],\r\n    \"queue\" : 0,\r\n    \"resultMode\" : \"EXEC\"\r\n  },\r\n  \"graph\" : [ {\r\n    \"pop\" : \"hive-scan\",\r\n    \"@id\" : 20,\r\n    \"hive-table\" : {\r\n      \"table\" : {\r\n        \"tableName\" : \"orders\",\r\n        \"dbName\" : \"default\",\r\n        \"owner\" : \"root\",\r\n        \"createTime\" : 1409956843,\r\n        \"lastAccessTime\" : 0,\r\n        \"retention\" : 0,\r\n        \"sd\" : {\r\n          \"cols\" : [ {\r\n            \"name\" : \"order_id\",\r\n            \"type\" : \"bigint\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"month\",\r\n            \"type\" : \"string\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"purchdate\",\r\n            \"type\" : \"timestamp\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"cust_id\",\r\n            \"type\" : \"bigint\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"state\",\r\n            \"type\" : \"string\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"prod_id\",\r\n            \"type\" : \"bigint\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"order_total\",\r\n            \"type\" : \"int\",\r\n            \"comment\" : null\r\n          } ],\r\n          \"location\" : \"maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders\",\r\n          \"inputFormat\" : \"org.apache.hadoop.mapred.TextInputFormat\",\r\n          \"outputFormat\" : \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\r\n          \"compressed\" : false,\r\n          \"numBuckets\" : -1,\r\n          \"serDeInfo\" : {\r\n            \"name\" : null,\r\n            \"serializationLib\" : \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\r\n            \"parameters\" : {\r\n              \"serialization.format\" : \",\",\r\n              \"field.delim\" : \",\"\r\n            }\r\n          },\r\n          \"sortCols\" : [ ],\r\n          \"parameters\" : { }\r\n        },\r\n        \"partitionKeys\" : [ ],\r\n        \"parameters\" : {\r\n          \"EXTERNAL\" : \"TRUE\",\r\n          \"transient_lastDdlTime\" : \"1409956843\"\r\n        },\r\n        \"viewOriginalText\" : null,\r\n        \"viewExpandedText\" : null,\r\n        \"tableType\" : \"EXTERNAL_TABLE\"\r\n      },\r\n      \"partitions\" : null,\r\n      \"hiveConfigOverride\" : {\r\n        \"hive.metastore.uris\" : \"thrift://192.168.208.143:9083\",\r\n        \"hive.metastore.sasl.enabled\" : \"false\"\r\n      }\r\n    },\r\n    \"storage-plugin\" : \"hive\",\r\n    \"columns\" : [ \"`cust_id`\", \"`order_total`\" ],\r\n    \"cost\" : 6349.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 18,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`cust_id`\",\r\n      \"expr\" : \"`cust_id`\"\r\n    }, {\r\n      \"ref\" : \"`order_total`\",\r\n      \"expr\" : \"`order_total`\"\r\n    } ],\r\n    \"child\" : 20,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 6349.0\r\n  }, {\r\n    \"pop\" : \"filter\",\r\n    \"@id\" : 16,\r\n    \"child\" : 18,\r\n    \"expr\" : \"greater_than(`order_total`, 100) \",\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 3174.5\r\n  }, {\r\n    \"pop\" : \"selection-vector-remover\",\r\n    \"@id\" : 14,\r\n    \"child\" : 16,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 3174.5\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 12,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`$f7`\",\r\n      \"expr\" : \"`cust_id`\"\r\n    } ],\r\n    \"child\" : 14,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 3174.5\r\n  }, {\r\n    \"pop\" : \"fs-scan\",\r\n    \"@id\" : 9,\r\n    \"files\" : [ \"maprfs:/mapr/my.cluster.com/demo/clicks/json/clicks.json\" ],\r\n    \"storage\" : {\r\n      \"type\" : \"file\",\r\n      \"enabled\" : true,\r\n      \"connection\" : \"maprfs:///\",\r\n      \"workspaces\" : {\r\n        \"root\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo\",\r\n          \"writable\" : false,\r\n          \"storageformat\" : null\r\n        },\r\n        \"clicks\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo/clicks\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        },\r\n        \"views\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo/views\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        }\r\n      },\r\n      \"formats\" : {\r\n        \"psv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tbl\" ],\r\n          \"delimiter\" : \"|\"\r\n        },\r\n        \"csv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"csv\" ],\r\n          \"delimiter\" : \",\"\r\n        },\r\n        \"tsv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tsv\" ],\r\n          \"delimiter\" : \"\\t\"\r\n        },\r\n        \"parquet\" : {\r\n          \"type\" : \"parquet\"\r\n        },\r\n        \"json\" : {\r\n          \"type\" : \"json\"\r\n        }\r\n      }\r\n    },\r\n    \"format\" : {\r\n      \"type\" : \"json\"\r\n    },\r\n    \"selectionRoot\" : \"/mapr/my.cluster.com/demo/clicks/json/clicks.json\",\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 7,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`T24\u00a6\u00a6*`\",\r\n      \"expr\" : \"`*`\"\r\n    } ],\r\n    \"child\" : 9,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"fs-scan\",\r\n    \"@id\" : 19,\r\n    \"files\" : [ \"maprfs:/mapr/my.cluster.com/demo/clicks/json/clicks.json\" ],\r\n    \"storage\" : {\r\n      \"type\" : \"file\",\r\n      \"enabled\" : true,\r\n      \"connection\" : \"maprfs:///\",\r\n      \"workspaces\" : {\r\n        \"root\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo\",\r\n          \"writable\" : false,\r\n          \"storageformat\" : null\r\n        },\r\n        \"clicks\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo/clicks\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        },\r\n        \"views\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo/views\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        }\r\n      },\r\n      \"formats\" : {\r\n        \"psv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tbl\" ],\r\n          \"delimiter\" : \"|\"\r\n        },\r\n        \"csv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"csv\" ],\r\n          \"delimiter\" : \",\"\r\n        },\r\n        \"tsv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tsv\" ],\r\n          \"delimiter\" : \"\\t\"\r\n        },\r\n        \"parquet\" : {\r\n          \"type\" : \"parquet\"\r\n        },\r\n        \"json\" : {\r\n          \"type\" : \"json\"\r\n        }\r\n      }\r\n    },\r\n    \"format\" : {\r\n      \"type\" : \"json\"\r\n    },\r\n    \"selectionRoot\" : \"/mapr/my.cluster.com/demo/clicks/json/clicks.json\",\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 17, |\r\n+------------+------------+"
    ],
    [
        "DRILL-1423",
        "DRILL-1397",
        "\"exists\" function does not work with text files git.commit.id.abbrev=1ce7c93\r\n\r\nThe below query does not work on text files. However it works properly on parquet files along with views on top of text files\r\n{code}\r\nselect\r\n  o.columns[5]\r\nfrom\r\n  `orders.tbl` o\r\nwhere\r\n  exists (\r\n    select\r\n      *\r\n    from\r\n      `lineitem.tbl` l\r\n    where\r\n       l.columns[0] = o.columns[0]\r\n  )\r\n{code}\r\n\r\nI attached the error log and the data files used. Let me know if you have any questions.",
        "Query with IN clause and correlation fails (for Text files) The following query fails. This could be related to https://issues.apache.org/jira/browse/DRILL-1396, but filing separate issue as the error is different.\r\n\r\n0: jdbc:drill:> select t.trans_info.purch_flag,\r\n. . . . . . . >           t.user_info.cust_id, t.trans_info.prod_id[0]\r\n. . . . . . . > from `Clickstream.clicks`.`/json/clicks.json` t \r\n. . . . . . . > where  t.user_info.cust_id IN (select o.cust_id from hive.orders o where o.order_total >100 );\r\n\r\nQuery failed: Failure while running fragment. Incoming batch has an empty schema. This is not allowed. [2b441a79-be49-4116-a459-513f97418738]\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n\r\nBelow is the explain plan.\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(EXPR$0=[$0], EXPR$1=[$1], EXPR$2=[$2])\r\n00-02        Project(EXPR$0=[ITEM($1, 'purch_flag')], EXPR$1=[ITEM($0, 'cust_id')], EXPR$2=[ITEM(ITEM($1, 'prod_id'), 0)])\r\n00-03          HashJoin(condition=[=($2, $3)], joinType=[inner])\r\n00-05            Project(T27\u00a6\u00a6user_info=[$1], T27\u00a6\u00a6trans_info=[$2], $f3=[ITEM($1, 'cust_id')])\r\n00-07              Project(T27\u00a6\u00a6*=[$0], T27\u00a6\u00a6user_info=[$1], T27\u00a6\u00a6trans_info=[$2])\r\n00-09                Scan(groupscan=[EasyGroupScan [selectionRoot=/mapr/my.cluster.com/demo/clicks/json/clicks.json, columns = null]])\r\n00-04            HashAgg(group=[{0}])\r\n00-06              Project(cust_id=[$0])\r\n00-08                SelectionVectorRemover\r\n00-10                  Filter(condition=[>($1, 100)])\r\n00-11                    Project(cust_id=[$1], order_total=[$0])\r\n00-12                      Scan(groupscan=[HiveScan [table=Table(tableName:orders, dbName:default, owner:root, createTime:1409956843, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:order_id, type:bigint, comment:null), FieldSchema(name:month, type:string, comment:null), FieldSchema(name:purchdate, type:timestamp, comment:null), FieldSchema(name:cust_id, type:bigint, comment:null), FieldSchema(name:state, type:string, comment:null), FieldSchema(name:prod_id, type:bigint, comment:null), FieldSchema(name:order_total, type:int, comment:null)], location:maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE, transient_lastDdlTime=1409956843}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE), inputSplits=[maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month2.agg.orders.csv:0+640155, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month5.agg.orders.csv:0+775506, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month6.agg.orders.csv:0+791685, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month8.agg.orders.csv:0+805072, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month4.agg.orders.csv:0+603886, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month9.agg.orders.csv:0+846270, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month1.agg.orders.csv:0+461090, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month7.agg.orders.csv:0+771399, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month3.agg.orders.csv:0+806738], columns=[SchemaPath [`cust_id`], SchemaPath [`order_total`]]]])\r\n | {\r\n  \"head\" : {\r\n    \"version\" : 1,\r\n    \"generator\" : {\r\n      \"type\" : \"ExplainHandler\",\r\n      \"info\" : \"\"\r\n    },\r\n    \"type\" : \"APACHE_DRILL_PHYSICAL\",\r\n    \"options\" : [ ],\r\n    \"queue\" : 0,\r\n    \"resultMode\" : \"EXEC\"\r\n  },\r\n  \"graph\" : [ {\r\n    \"pop\" : \"hive-scan\",\r\n    \"@id\" : 12,\r\n    \"hive-table\" : {\r\n      \"table\" : {\r\n        \"tableName\" : \"orders\",\r\n        \"dbName\" : \"default\",\r\n        \"owner\" : \"root\",\r\n        \"createTime\" : 1409956843,\r\n        \"lastAccessTime\" : 0,\r\n        \"retention\" : 0,\r\n        \"sd\" : {\r\n          \"cols\" : [ {\r\n            \"name\" : \"order_id\",\r\n            \"type\" : \"bigint\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"month\",\r\n            \"type\" : \"string\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"purchdate\",\r\n            \"type\" : \"timestamp\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"cust_id\",\r\n            \"type\" : \"bigint\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"state\",\r\n            \"type\" : \"string\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"prod_id\",\r\n            \"type\" : \"bigint\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"order_total\",\r\n            \"type\" : \"int\",\r\n            \"comment\" : null\r\n          } ],\r\n          \"location\" : \"maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders\",\r\n          \"inputFormat\" : \"org.apache.hadoop.mapred.TextInputFormat\",\r\n          \"outputFormat\" : \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\r\n          \"compressed\" : false,\r\n          \"numBuckets\" : -1,\r\n          \"serDeInfo\" : {\r\n            \"name\" : null,\r\n            \"serializationLib\" : \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\r\n            \"parameters\" : {\r\n              \"serialization.format\" : \",\",\r\n              \"field.delim\" : \",\"\r\n            }\r\n          },\r\n          \"sortCols\" : [ ],\r\n          \"parameters\" : { }\r\n        },\r\n        \"partitionKeys\" : [ ],\r\n        \"parameters\" : {\r\n          \"EXTERNAL\" : \"TRUE\",\r\n          \"transient_lastDdlTime\" : \"1409956843\"\r\n        },\r\n        \"viewOriginalText\" : null,\r\n        \"viewExpandedText\" : null,\r\n        \"tableType\" : \"EXTERNAL_TABLE\"\r\n      },\r\n      \"partitions\" : null,\r\n      \"hiveConfigOverride\" : {\r\n        \"hive.metastore.uris\" : \"thrift://192.168.208.143:9083\",\r\n        \"hive.metastore.sasl.enabled\" : \"false\"\r\n      }\r\n    },\r\n    \"storage-plugin\" : \"hive\",\r\n    \"columns\" : [ \"`cust_id`\", \"`order_total`\" ],\r\n    \"cost\" : 6349.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 11,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`cust_id`\",\r\n      \"expr\" : \"`cust_id`\"\r\n    }, {\r\n      \"ref\" : \"`order_total`\",\r\n      \"expr\" : \"`order_total`\"\r\n    } ],\r\n    \"child\" : 12,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 6349.0\r\n  }, {\r\n    \"pop\" : \"filter\",\r\n    \"@id\" : 10,\r\n    \"child\" : 11,\r\n    \"expr\" : \"greater_than(`order_total`, 100) \",\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 3174.5\r\n  }, {\r\n    \"pop\" : \"selection-vector-remover\",\r\n    \"@id\" : 8,\r\n    \"child\" : 10,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 3174.5\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 6,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`cust_id`\",\r\n      \"expr\" : \"`cust_id`\"\r\n    } ],\r\n    \"child\" : 8,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 3174.5\r\n  }, {\r\n    \"pop\" : \"hash-aggregate\",\r\n    \"@id\" : 4,\r\n    \"child\" : 6,\r\n    \"cardinality\" : 1.0,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 1587.25,\r\n    \"groupByExprs\" : [ {\r\n      \"ref\" : \"`cust_id`\",\r\n      \"expr\" : \"`cust_id`\"\r\n    } ],\r\n    \"aggrExprs\" : [ ]\r\n  }, {\r\n    \"pop\" : \"fs-scan\",\r\n    \"@id\" : 9,\r\n    \"files\" : [ \"maprfs:/mapr/my.cluster.com/demo/clicks/json/clicks.json\" ],\r\n    \"storage\" : {\r\n      \"type\" : \"file\",\r\n      \"enabled\" : true,\r\n      \"connection\" : \"maprfs:///\",\r\n      \"workspaces\" : {\r\n        \"root\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo\",\r\n          \"writable\" : false,\r\n          \"storageformat\" : null\r\n        },\r\n        \"clicks\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo/clicks\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        },\r\n        \"views\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo/views\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        }\r\n      },\r\n      \"formats\" : {\r\n        \"psv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tbl\" ],\r\n          \"delimiter\" : \"|\"\r\n        },\r\n        \"csv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"csv\" ],\r\n          \"delimiter\" : \",\"\r\n        },\r\n        \"tsv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tsv\" ],\r\n          \"delimiter\" : \"\\t\"\r\n        },\r\n        \"parquet\" : {\r\n          \"type\" : \"parquet\"\r\n        },\r\n        \"json\" : {\r\n          \"type\" : \"json\"\r\n        }\r\n      }\r\n    },\r\n    \"format\" : {\r\n      \"type\" : \"json\"\r\n    },\r\n    \"selectionRoot\" : \"/mapr/my.cluster.com/demo/clicks/json/clicks.json\",\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 7,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`T27\u00a6\u00a6*`\",\r\n      \"expr\" : \"`*`\"\r\n    } ],\r\n    \"child\" : 9,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 5,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`T27\u00a6\u00a6user_info`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6user_info`\"\r\n    }, {\r\n      \"ref\" : \"`T27\u00a6\u00a6trans_info`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6trans_info`\"\r\n    }, {\r\n      \"ref\" : \"`$f3`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6user_info`.`cust_id`\"\r\n    } ],\r\n    \"child\" : 7,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"hash-join\",\r\n    \"@id\" : 3,\r\n    \"left\" : 5,\r\n    \"right\" : 4,\r\n    \"conditions\" : [ {\r\n      \"relationship\" : \"==\",\r\n      \"left\" : \"`$f3`\",\r\n      \"right\" : \"`cust_id`\"\r\n    } ],\r\n    \"joinType\" : \"INNER\",\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 2,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`EXPR$0`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6trans_info`.`purch_flag`\"\r\n    }, {\r\n      \"ref\" : \"`EXPR$1`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6user_info`.`cust_id`\"\r\n    }, {\r\n      \"ref\" : \"`EXPR$2`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6trans_info`.`prod_id`[0]\"\r\n    } ],\r\n    \"child\" : 3,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 1,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`EXPR$0`\",\r\n      \"expr\" : \"`EXPR$0`\"\r\n    }, {\r\n      \"ref\" : \"`EXPR$1`\",\r\n      \"expr\" : \"`EXPR$1`\"\r\n    }, {\r\n      \"ref\" : \"`EXPR$2`\",\r\n      \"expr\" : \"`EXPR$2`\"\r\n    } ],\r\n    \"child\" : 2,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"screen\",\r\n    \"@id\" : 0,\r\n    \"child\" : 1,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  } ]\r\n} |\r\n+------------+------------+\r\n\r\n"
    ],
    [
        "DRILL-1430",
        "DRILL-975",
        "sum(cast(columns[0] as int)) fails if column contains empty value code base\r\n#Fri Sep 12 14:08:02 PDT 2014\r\ngit.commit.id.abbrev=9e16466\r\n\r\nI have a simple tsv file that contains an empty row in one of the columns:\r\n\r\n[root@qa-node120 tmp]# hadoop fs -cat /user/root/mondrian/tmp.tbl\r\n1|1\r\n2|2\r\n|3\r\n4|4\r\n\r\n0: jdbc:drill:schema=dfs> select * from `tmp.tbl`;\r\n+------------+\r\n|  columns   |\r\n+------------+\r\n| [\"1\",\"1\"]  |\r\n| [\"2\",\"2\"]  |\r\n| [\"\",\"3\"]   |\r\n| [\"4\",\"4\"]  |\r\n+------------+\r\n4 rows selected (0.183 seconds)\r\n0: jdbc:drill:schema=dfs> select columns[0], columns[1] from `tmp.tbl`;\r\n+------------+------------+\r\n|   EXPR$0   |   EXPR$1   |\r\n+------------+------------+\r\n| 1          | 1          |\r\n| 2          | 2          |\r\n|            | 3          |\r\n| 4          | 4          |\r\n+------------+------------+\r\n4 rows selected (0.217 seconds)\r\n\r\nsum over the column that is full works fine, but sum over the column that contains empty row fails:\r\n\r\n0: jdbc:drill:schema=dfs> select sum(cast(columns[1] as int)) from `tmp.tbl`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 10         |\r\n+------------+\r\n1 row selected (0.283 seconds)\r\n0: jdbc:drill:schema=dfs> select sum(cast(columns[0] as int)) from `tmp.tbl`;\r\nQuery failed: Failure while running fragment.  [a9d8771f-aeb3-4226-9623-fc2b635365f8]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n0: jdbc:drill:schema=dfs>\r\n\r\ndrillbit.log\r\n11:44:19.450 [9d14d90c-128c-4549-b77f-9b0051bc3d5c:frag:0:0] DEBUG o.a.d.e.w.fragment.FragmentExecutor - Error while initializing or executing fragment\r\njava.lang.NumberFormatException:\r\n        at org.apache.drill.exec.test.generated.ProjectorGen18.doEval(ProjectorTemplate.java:40) ~[na:na]\r\n        at org.apache.drill.exec.test.generated.ProjectorGen18.projectRecords(ProjectorTemplate.java:64) ~[na:na]\r\n\r\n",
        "Null-on-exception option for cast functions Currently, if a particular value cannot be cast to the target type, an exception is thrown and the query fails. We should have a mode that will treat the output of all cast functions as nullable, and return a null value if the cast fails, rather than throwing an exception.\r\n\r\nAn important example where this is important is when using the Text reader. The text reader always produces a single, RepeatedVarChar column. The columns are then cast to the appropriate type. For the columns that are cast to numeric types, if there is no value (i.e. it's an empty string), currently this will throw NumberFormatException. What we really want is for it to produce a Null value."
    ],
    [
        "DRILL-1430",
        "DRILL-986",
        "sum(cast(columns[0] as int)) fails if column contains empty value code base\r\n#Fri Sep 12 14:08:02 PDT 2014\r\ngit.commit.id.abbrev=9e16466\r\n\r\nI have a simple tsv file that contains an empty row in one of the columns:\r\n\r\n[root@qa-node120 tmp]# hadoop fs -cat /user/root/mondrian/tmp.tbl\r\n1|1\r\n2|2\r\n|3\r\n4|4\r\n\r\n0: jdbc:drill:schema=dfs> select * from `tmp.tbl`;\r\n+------------+\r\n|  columns   |\r\n+------------+\r\n| [\"1\",\"1\"]  |\r\n| [\"2\",\"2\"]  |\r\n| [\"\",\"3\"]   |\r\n| [\"4\",\"4\"]  |\r\n+------------+\r\n4 rows selected (0.183 seconds)\r\n0: jdbc:drill:schema=dfs> select columns[0], columns[1] from `tmp.tbl`;\r\n+------------+------------+\r\n|   EXPR$0   |   EXPR$1   |\r\n+------------+------------+\r\n| 1          | 1          |\r\n| 2          | 2          |\r\n|            | 3          |\r\n| 4          | 4          |\r\n+------------+------------+\r\n4 rows selected (0.217 seconds)\r\n\r\nsum over the column that is full works fine, but sum over the column that contains empty row fails:\r\n\r\n0: jdbc:drill:schema=dfs> select sum(cast(columns[1] as int)) from `tmp.tbl`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 10         |\r\n+------------+\r\n1 row selected (0.283 seconds)\r\n0: jdbc:drill:schema=dfs> select sum(cast(columns[0] as int)) from `tmp.tbl`;\r\nQuery failed: Failure while running fragment.  [a9d8771f-aeb3-4226-9623-fc2b635365f8]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n0: jdbc:drill:schema=dfs>\r\n\r\ndrillbit.log\r\n11:44:19.450 [9d14d90c-128c-4549-b77f-9b0051bc3d5c:frag:0:0] DEBUG o.a.d.e.w.fragment.FragmentExecutor - Error while initializing or executing fragment\r\njava.lang.NumberFormatException:\r\n        at org.apache.drill.exec.test.generated.ProjectorGen18.doEval(ProjectorTemplate.java:40) ~[na:na]\r\n        at org.apache.drill.exec.test.generated.ProjectorGen18.projectRecords(ProjectorTemplate.java:64) ~[na:na]\r\n\r\n",
        "Explicit cast of empty string to double/integer throws NumberFormatException  While running TPC-DS queries we hit an issue while reading Parquet. When narrowed down, it was seen to be an issue with explicit cast when data files consist of null/empty strings. \r\n\r\nSqlLine error:\r\n> select cast('' as integer) from `/user/root/item.tbl` limit 1;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"7c691e59-6998-4f5d-bd89-0b3635709aea\"\r\nendpoint {\r\n  address: \"drillats2.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < NumberFormatException:[  ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n0: jdbc:drill:schema=dfs.drillTestDirTPCDS> select cast('' as double) from `/user/root/item.tbl` limit 1;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"ce1b91bd-1892-434e-88e4-a535541888c0\"\r\nendpoint {\r\n  address: \"drillats2.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < NumberFormatException:[ empty String ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n"
    ],
    [
        "DRILL-1434",
        "DRILL-1362",
        "count of a nullable column in tpcds gives incorrect results code base \r\n#Fri Sep 12 14:08:02 PDT 2014\r\ngit.commit.id.abbrev=9e16466\r\n\r\nI have a parquet file (tpcds data) which contains null value on a column. The total count of the column:\r\n\r\n0: jdbc:drill:schema=dfs> select count(ss_quantity) from `tpcds/p1/store_sales.parquet`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 2880404    |\r\n+------------+\r\n\r\nThe count without considering null is:\r\n\r\n0: jdbc:drill:schema=dfs> select count(ss_quantity) from `tpcds/p1/store_sales.parquet` where ss_quantity is not null;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 2750408    |\r\n+------------+\r\n\r\nBut the count for null value is zero:\r\n\r\n0: jdbc:drill:schema=dfs> select count(ss_quantity) from `tpcds/p1/store_sales.parquet` where ss_quantity is null;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 0          |\r\n+------------+\r\n\r\nHere is the physical plan look like for this query:\r\n\r\n0: jdbc:drill:schema=dfs> explain plan for select count(ss_quantity) from `tpcds/p1/store_sales.parquet` where ss_quantity is null;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      StreamAgg(group=[{}], EXPR$0=[COUNT($0)])\r\n00-02        Filter(condition=[IS NULL($0)])\r\n00-03          ProducerConsumer\r\n00-04            Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/user/root/mondrian/tpcds/p1/store_sales.parquet]], selectionRoot=/user/root/mondrian/tpcds/p1/store_sales.parquet, columns=[SchemaPath [`ss_quantity`]]]])\r\n | {\r\n  \"head\" : {\r\n    \"version\" : 1,\r\n    \"generator\" : {\r\n      \"type\" : \"ExplainHandler\",\r\n      \"info\" : \"\"\r\n    },\r\n    \"type\" : \"APACHE_DRILL_PHYSICAL\",\r\n    \"options\" : [ ],\r\n    \"queue\" : 0,\r\n    \"resultMode\" : \"EXEC\"\r\n  },\r\n  \"graph\" : [ {\r\n    \"pop\" : \"parquet-scan\",\r\n    \"@id\" : 4,\r\n    \"entries\" : [ {\r\n      \"path\" : \"maprfs:/user/root/mondrian/tpcds/p1/store_sales.parquet\"\r\n    } ],\r\n    \"storage\" : {\r\n      \"type\" : \"file\",\r\n      \"enabled\" : true,\r\n      \"connection\" : \"maprfs:///\",\r\n      \"workspaces\" : {\r\n        \"default\" : {\r\n          \"location\" : \"/user/root/mondrian/\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : null\r\n        },\r\n        \"home\" : {\r\n          \"location\" : \"/\",\r\n          \"writable\" : false,\r\n          \"storageformat\" : null\r\n        },\r\n        \"root\" : {\r\n          \"location\" : \"/\",\r\n          \"writable\" : false,\r\n          \"storageformat\" : null\r\n        },\r\n        \"abhi\" : {\r\n          \"location\" : \"/tables\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"csv\"\r\n        },\r\n        \"chun\" : {\r\n          \"location\" : \"/drill/testdata/chun/\",\r\n          \"writable\" : false,\r\n          \"storageformat\" : null\r\n        },\r\n        \"tmp\" : {\r\n          \"location\" : \"/tmp\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"csv\"\r\n        }\r\n      },\r\n      \"formats\" : {\r\n        \"psv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tbl\" ],\r\n          \"delimiter\" : \"|\"\r\n        },\r\n        \"csv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"csv\" ],\r\n          \"delimiter\" : \",\"\r\n        },\r\n        \"tsv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tsv\" ],\r\n          \"delimiter\" : \"\\t\"\r\n        },\r\n        \"parquet\" : {\r\n          \"type\" : \"parquet\"\r\n        },\r\n        \"json\" : {\r\n          \"type\" : \"json\"\r\n        }\r\n      }\r\n    },\r\n    \"format\" : {\r\n      \"type\" : \"parquet\"\r\n    },\r\n    \"columns\" : [ \"`ss_quantity`\" ],\r\n    \"selectionRoot\" : \"/user/root/mondrian/tpcds/p1/store_sales.parquet\",\r\n    \"cost\" : 2880404.0\r\n  }, {\r\n    \"pop\" : \"producer-consumer\",\r\n    \"@id\" : 3,\r\n    \"child\" : 4,\r\n    \"size\" : 10,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 2880404.0\r\n  }, {\r\n    \"pop\" : \"filter\",\r\n    \"@id\" : 2,\r\n    \"child\" : 3,\r\n    \"expr\" : \"isnull(`ss_quantity`) \",\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 720101.0\r\n  }, {\r\n    \"pop\" : \"streaming-aggregate\",\r\n    \"@id\" : 1,\r\n    \"child\" : 2,\r\n    \"keys\" : [ ],\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`EXPR$0`\",\r\n      \"expr\" : \"count(`ss_quantity`) \"\r\n    } ],\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 1.0\r\n  }, {\r\n    \"pop\" : \"screen\",\r\n    \"@id\" : 0,\r\n    \"child\" : 1,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 72010.1\r\n  } ]\r\n} |\r\n+------------+------------+",
        "Count(nullable-column) is incorrectly pushed into group scan operator The following query on TPC-DS table web_returns produces wrong result because the aggregate count(wr_return_quantity) gets pushed into the parquet group scan operator even though wr_return_quantity is nullable and apparently the parquet metadata does not have stats on nullable column.  \r\n\r\n0: jdbc:drill:zk=local> select count(wr_return_quantity) from web_returns;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 71763      |\r\n+------------+\r\n\r\n0: jdbc:drill:zk=local> explain plan for select count(wr_return_quantity) from web_returns;\r\n{code:sql}\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(EXPR$0=[$0])\r\n00-02        Scan(groupscan=[org.apache.drill.exec.store.pojo.PojoRecordReader@e4acaad])\r\n{code}\r\n\r\nFor reference, here are the correct results:  \r\ntpcds=# select count(wr_return_quantity) from web_returns;\r\n count\r\n-------\r\n 68616\r\n(1 row)\r\n\r\ntpcds=# select count(*) from web_returns;\r\n count\r\n-------\r\n 71763\r\n(1 row)"
    ],
    [
        "DRILL-1440",
        "DRILL-1318",
        "Allow delimited files to have customizable quote characters Delimited files should have multiple options to configure:\r\n-1. Field separators (already in place - Defaults to \\n)-\r\n2. Line separators (DRILL-1318 - Defaults to \\n)\r\n3. Skip header row (missing - Defaults to false)\r\n4. Quote characters (missing - Defaults to \\\")\r\n5. Strip Quotes (missing - Defaults to false)\r\n\r\nThese defaults will keep the functionality as currently coded the same without any of these settings being specified.",
        "Definable line separators Please could you add the ability to define custom line separators other than the default \\n\r\n"
    ],
    [
        "DRILL-1447",
        "DRILL-1125",
        "Hash join implicit cast fails Join on two columns, where one is int, and the other is bigint, does not return any results when using hash join. Disabling hashjoin gives the expected result.\r\n\r\nIn this example, nation_int and nation_big int each contain a single column, but one has int, and the other bigint:\r\n\r\n0: jdbc:drill:> select i.n_nationkey as a, b.n_nationkey as b from nation_int i, nation_bigint b where i.n_nationkey = b.n_nationkey limit 5;\r\n+------------+------------+\r\n|     a      |     b      |\r\n+------------+------------+\r\n+------------+------------+\r\nNo rows selected (0.891 seconds)\r\n0: jdbc:drill:> alter session set `planner.enable_hashjoin` = false;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | planner.enable_hashjoin updated. |\r\n+------------+------------+\r\n1 row selected (0.04 seconds)\r\n0: jdbc:drill:> select i.n_nationkey as a, b.n_nationkey as b from nation_int i, nation_bigint b where i.n_nationkey = b.n_nationkey limit 5;\r\n+------------+------------+\r\n|     a      |     b      |\r\n+------------+------------+\r\n| 0          | 0          |\r\n| 1          | 1          |\r\n| 2          | 2          |\r\n| 3          | 3          |\r\n| 4          | 4          |\r\n+------------+------------+\r\n5 rows selected (1.275 seconds)",
        "Implicit cast not working in the join condition git.commit.id.abbrev=810a204\r\nBuild # 26322\r\n\r\nIn the below query there is no type attached to 'donuts.id' but we expect implicit cast to work in this case. Data files referenced are also attached\r\n\r\ncreate view v1 as select cast(id as int) a, cast(price as double) b from prices;\r\nselect v1.a from v1 inner join donuts on v1.a = donuts.id;\r\n\r\nError From Sqlline : UnsupportedOperationException:[ Failure finding function that runtime code generation expected.  Signature: compare_to( INT:OPTIONALVARCHAR:OPTIONAL,  ) returns INT:REQUIRED ]\r\n\r\n\r\nWith slight modification the below query works\r\n\r\nselect v1.a from v1 inner join donuts on v1.a = donuts.id + 1 -1;\r\n"
    ],
    [
        "DRILL-1451",
        "DRILL-1185",
        "Classpath storage plugin cannot be used in test queries referencing delimited text files, fails with file not found ",
        "Drill not picking up 'text' files from classpath git.commit.id.abbrev=e5c2da0\r\n\r\nI placed a json file and text file under '/etc/drill/conf/' folder. Drill picks up the json file but throws 'resource classpath:/region.tbl not found' error in case of a text file.\r\n\r\nQuery : select * from cp.`region.tbl`;\r\n\r\nAttached the error log."
    ],
    [
        "DRILL-1471",
        "DRILL-1433",
        "Hbase queries fail with / by zero git.commit.id.abbrev=81fb18a\r\n\r\nThe following hbase query fails:\r\n0: jdbc:drill:schema=dfs> select cast(row_key as integer) voter_id, convert_from(onecf['name'], 'UTF8') name, cast(twocf['age'] as integer) age, cast(twocf['registration'] as varchar(20)) registration, cast(threecf['contributions'] as decimal(6,2)) contributions, cast(threecf['voterzone'] as integer) voterzone,cast(fourcf['create_date'] as timestamp) create_date from M7.m7voter where onecf['name'] not similar to '%(young|u|a|i|e|m)%';\r\nQuery failed: Failure while setting up Foreman. / by zero [176bbea0-b240-4ed2-88a6-b0e707c0e442]\r\n",
        "Where query on HBase store fails when row_key doesn't exist in HBase Give any query on hbase using where clause. If the queried row_key does not exist on HBase,\r\nquery fails with the following exception\r\n\r\n{noformat}\r\njava.lang.ArithmeticException: / by zero\r\n\tat org.apache.drill.exec.store.hbase.TableStatsCalculator.<init>(TableStatsCalculator.java:88) ~[drill-storage-hbase-0.6.0-incubating-SNAPSHOT.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.hbase.HBaseGroupScan.init(HBaseGroupScan.java:150) ~[drill-storage-hbase-0.6.0-incubating-SNAPSHOT.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.hbase.HBaseGroupScan.<init>(HBaseGroupScan.java:116) ~[drill-storage-hbase-0.6.0-incubating-SNAPSHOT.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.hbase.HBaseStoragePlugin.getPhysicalScan(HBaseStoragePlugin.java:65) ~[drill-storage-hbase-0.6.0-incubating-SNAPSHOT.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.hbase.HBaseStoragePlugin.getPhysicalScan(HBaseStoragePlugin.java:35) ~[drill-storage-hbase-0.6.0-incubating-SNAPSHOT.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.planner.logical.DrillTable.getGroupScan(DrillTable.java:53) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n{noformat}"
    ],
    [
        "DRILL-1481",
        "DRILL-1320",
        "Allow specify default storage format at file workspaces Let users specify a default storage format at the workspace level. If the extensions don't exist for files in the directory and if Drill cannot auto detect , this default storage format , then this value will be used to parse the data.\r\n\r\nThis will be very useful for when Drill needs to parse JSON, Text files and no extensions are present for these files.\r\n\r\n",
        "Allow storage plugin to query formatted files with no extention I am pulling data in from a database into Hadoop using Sqoop. This creates .csv files that do not have any file extensions. I have looked at the Drill Wiki and tried to alter my storage plugin config by removing the \"csv\" format in the JSON and I have tried to leave empty quotes here too. I have found that whatever I try, Drill cannot find the 'table' (csv file) if there is no .csv file extension present. If I add this extension to the files, drill can find the table. \r\n\r\nWould it be possible to add the ability to set an 'empty' extention so csv/other separated files with no file extension can be queried without having to change the file name?\r\n\r\nIf this is already possible, please close this ticket.\r\n\r\nThanks,\r\nMike\r\n\r\n"
    ],
    [
        "DRILL-1515",
        "DRILL-1373",
        "Rest/Web interface hangs if there is any error/exception in query execution Web interface hangs if there is any error/exception in query. We should have a proper error handling mechanism in place.",
        "Executing a query with a semi-colon as the last character causes 500 error in the query GUI Attempting to execute a query on the Query GUI page (http://node:8047/query) that contains a semi-colon at the end of the query statement results in a 500 error.\r\n{noformat}\r\nselect count(*) from storageplugin.workspace;\r\n{noformat}\r\nRemoving the semi-colon returns the query result successfully."
    ],
    [
        "DRILL-1519",
        "DRILL-1504",
        "Concurrent access to WorkEventBus#getOrCreateFragmentManager leaks memory. WorkEventBus uses a ConcurrentHashMap to ensure there is one and only one FragmentManager corresponding to each unique FragmentHandle. The method creates a FragmentManager that reserves some initial memory if a thread observes that it is not in the map. However, in case of concurrent access, we still need to clean up after if multiple threads observe that FragmentManager does not exist and create multiple FragmentManagers.",
        "Enabling fragment memory limit causes out of memory error When fragment memory limit is enabled, running a query with a large number of fragments hits the fragment memory limit after being run a few times. \r\nIt appears there are two problems - 1) At the end of the query, the drillbit does not reset the fragment limit to the amount before the query was run, and 2) the fragment limit seems to be smaller than expected.\r\n\r\nThe cause seems to be the following -\r\n \r\nWhen a drillbit receives a request for a fragmentRecordBatch, the BitServer threads create a NonRootFragmentManager object each, corresponding to the FragmentHandle. Only one of the NonRootFragmentManager objects is actually used, the others are discarded and garbage collected. \r\nHowever, when fragment memory limit is enabled, the Allocator corresponding to each of these nonRootFragmentManager objects registers the corresponding FragmentContext with the top level allocator which then uses this information to recalculate the fragment limit.\r\nThis has two effects - 1) the top level allocator counts more fragments because it counts each fragment multiple times. 2) The top level allocator keeps a reference to the fragment context which prevents the object from being garbage collected. Worse, since no code actually 'closes' the fragment context, these objects remain registered with the top level allocator across queries, eventually causing an out of memory condition.\r\n\r\n"
    ],
    [
        "DRILL-1532",
        "DRILL-1196",
        "Query fails when specifying schema name with column name When the schema name is specified in the column names, the query fails with a table not found error.\r\n{code} \r\n0: jdbc:drill:zk=localhost:5181> SELECT `hive43.default`.double_table.keycolumn, `hive43.default`.double_table.column1 FROM `hive43.default`.double_table;\r\nQuery failed: Failure while parsing sql. Table 'hive43.default' not found [010fac69-c747-41f8-b5ec-274428c78a73]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n\r\n{code}",
        "Does not support query of the form SELECT `schema`.`table`.* FROM `schema`.table` Query of the form Select `schema`.`table`.* from `schema`.table` returns an error from the parser. This affects loading data to TIBCO Spotfire BI tool.\r\n\r\nsqlline query and returned error:\r\n\r\n0: jdbc:drill:schema=hbase> SELECT  `hive43.default`.`bella_table`.* FROM  `hive43.default`.`bella_table`;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"186471ae-6271-4b74-91d5-fbe80e6675d2\"\r\nendpoint {\r\n  address: \"192.168.39.43\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < SqlParseException:[ Encountered \". *\" at line 1, column 39.\r\nWas expecting one of:\r\n    \"FROM\" ...\r\n    \",\" ...\r\n    \"AS\" ...\r\n    <IDENTIFIER> ...\r\n    <QUOTED_IDENTIFIER> ...\r\n    <BACK_QUOTED_IDENTIFIER> ...\r\n    <BRACKET_QUOTED_IDENTIFIER> ...\r\n    <UNICODE_QUOTED_IDENTIFIER> ...\r\n    \"NOT\" ...\r\n    \"IN\" ...\r\n    \"BETWEEN\" ...\r\n    \"LIKE\" ...\r\n    \"SIMILAR\" ...\r\n    \"=\" ...\r\n    \">\" ...\r\n    \"<\" ...\r\n    \"<=\" ...\r\n    \">=\" ...\r\n    \"<>\" ...\r\n    \"+\" ...\r\n    \"-\" ...\r\n    \"*\" ...\r\n    \"/\" ...\r\n    \"||\" ...\r\n    \"AND\" ...\r\n    \"OR\" ...\r\n    \"IS\" ...\r\n    \"MEMBER\" ...\r\n    \"SUBMULTISET\" ...\r\n    \"MULTISET\" ...\r\n    \"[\" ...\r\n    \".\" <IDENTIFIER> ...\r\n    \".\" <QUOTED_IDENTIFIER> ...\r\n    \".\" <BACK_QUOTED_IDENTIFIER> ...\r\n    \".\" <BRACKET_QUOTED_IDENTIFIER> ...\r\n    \".\" <UNICODE_QUOTED_IDENTIFIER> ...\r\n    \"(\" ...\r\n     ]\"\r\n]\r\nError: exception while executing query (state=,code=0)"
    ],
    [
        "DRILL-1543",
        "DRILL-1521",
        "Implement partition pruning for Hive tables Umbrella JIRA for implementing partition pruning on hive tables. ",
        "Support partition pruning with Hive storage engine "
    ],
    [
        "DRILL-1547",
        "DRILL-1536",
        "Selecting records with multiple null fields fails with AssertionError If reader hits a records with multiple `null` valued fields whose vectors are not yet allocated, running capacity check twice throws AssertionError. \r\n\r\nTake\r\n{panel:title=a.json}\r\n\\{\"some\":null, \"field\": null\\}\r\n{panel}\r\n\r\nRunning\r\n{code:sql}\r\nselect * from dfs.`a.json`\r\n{code}\r\nfails with AssertionError.\r\n\r\nThe work around is to check value capacity only if vector is allocated. This is an issue that affects sparse JSON files.",
        "Selecting a null valued field fails on json input format with an irrelevant error message Running \r\n{code:sql}\r\nselect t.id from dfs.`data/simple.json` t;\r\n{code}\r\n\r\nagainst\r\n\r\n{panel:title=simple.json}\r\n\\{\"id\":null\\}\r\n{panel}\r\n\r\nthrows\r\n{panel}\r\norg.apache.drill.common.exceptions.DrillRuntimeException: Record is too big to fit into allocated ValueVector\r\n\tat org.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:121)\r\n\tat org.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:158)\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:117)\r\n\tat org.apache.drill.exec.physical.impl.producer.ProducerConsumerBatch$Producer.run(ProducerConsumerBatch.java:128)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n{panel}\r\n\r\nWe need to make sure a value vector for the field `id` is allocated before attempting to manipulate the vector. Currently JsonReader does nothing beyond a bound check when encountered with a null value. We should implement proper null value handling.\r\n\r\nAlso the above error message should precisely reflect the root cause of the problem. Thrown exception in this case is irrelevant and misleading."
    ],
    [
        "DRILL-1565",
        "DRILL-1561",
        "Invalid plan for select count(1) from (select with orderby) select count(*) from (select l_orderkey from lineitem l\r\nwhere l.l_commitdate < l.l_receiptdate\r\norder by l_orderkey);\r\n      \r\n\r\norg.apache.drill.common.exceptions.PhysicalOperatorSetupException: SingleMergeExchange only supports a single receiver endpoint\r\n        at org.apache.drill.exec.physical.config.SingleMergeExchange.setupReceivers(SingleMergeExchange.java:67)\r\n        at org.apache.drill.exec.physical.base.AbstractExchange.setupReceivers(AbstractExchange.java:57)\r\n        at org.apache.drill.exec.planner.fragment.Wrapper.assignEndpoints(Wrapper.java:203)\r\n        at org.apache.drill.exec.planner.fragment.SimpleParallelizer.assignEndpoints(SimpleParallelizer.java:200)\r\n        at org.apache.drill.exec.planner.fragment.SimpleParallelizer.getFragments(SimpleParallelizer.java:95)\r\n        at org.apache.drill.exec.work.foreman.Foreman.runPhysicalPlan(Foreman.java:357)\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:384)\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:201)",
        "Doing 2 phase aggregation after an ORDER BY fails  Doing a COUNT(*) on top of a subquery that has ORDER-BY causes an error 'SingleMergeExchange only supports a single receiver endpoint'.  See below.\r\n \r\n// set slice_target to 1 to force introducing Exchanges\r\n0: jdbc:drill:zk=local> alter session set `planner.slice_target` = 1;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | planner.slice_target updated. |\r\n+------------+------------+\r\n1 row selected (0.066 seconds)\r\n\r\n0: jdbc:drill:zk=local> select count(*) from (select o_custkey from cp.`tpch/orders.parquet` order by o_custkey);\r\nQuery failed: Failure while setting up query. SingleMergeExchange only supports a single receiver endpoint [624ab231-755b-4915-8efd-f19758e514a6]\r\n"
    ],
    [
        "DRILL-1572",
        "DRILL-1483",
        "accuracy issue with tpch query 01.q and 10.q code base:\r\n#Wed Oct 22 11:40:19 PDT 2014\r\ngit.commit.id.abbrev=ae2790e\r\n\r\nThe following two tpch queries failed verification due to accuracy in returned data.\r\n\r\n/home/work/drill-testing/testing/framework/resources/Advanced/Passing/tpch100/parquet/01.q :\r\n{noformat}\r\n-- using 1395599672 as a seed to the RNG\r\nselect\r\n  l_returnflag,\r\n  l_linestatus,\r\n  sum(l_quantity) as sum_qty,\r\n  sum(l_extendedprice) as sum_base_price,\r\n  sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,\r\n  sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,\r\n  avg(l_quantity) as avg_qty,\r\n  avg(l_extendedprice) as avg_price,\r\n  avg(l_discount) as avg_disc,\r\n  count(*) as count_order\r\nfrom\r\n  lineitem\r\nwhere\r\n  l_shipdate <= date '1998-12-01' - interval '120' day (3)\r\ngroup by\r\n  l_returnflag,\r\n  l_linestatus\r\norder by\r\n  l_returnflag,\r\n  l_linestatus\r\n{noformat}\r\n\r\n         Expected number of rows: 4\r\nActual number of rows from Drill: 4\r\n         Number of matching rows: 0\r\n          Number of rows missing: 4\r\n       Number of rows unexpected: 4\r\n{noformat}\r\nThese rows are not expected (first 10):\r\n\tA\tF\t3.775127758E9\t5.660776097194428E12\t5.377736398183944E12\t5.59284742951595E12\t25.499370423275426\t38236.11698430475\t0.05000224353079674\t148047881\t\r\n\tN\tO\t7.269911583E9\t1.0901214476134316E13\t1.0356163586785012E13\t1.0770418891237377E13\t25.499873337396807\t38236.997134222445\t0.04999763132401859\t285095988\t\r\n\tR\tF\t3.77572497E9\t5.661603032745363E12\t5.378513563915393E12\t5.593662252666899E12\t25.50006628406532\t38236.697258453125\t0.050001304339521574\t148067261\t\r\n\tN\tF\t9.8553062E7\t1.4777109838597995E11\t1.4038496596503476E11\t1.4599979303277576E11\t25.501556956882876\t38237.19938880449\t0.04998528433803116\t3864590\t\r\n\r\nThese rows are missing (first 10):\r\n\tA\tF\t3.775127758E9\t5.660776097197787E12\t5.377736398184481E12\t5.592847429514863E12\t25.499370423275426\t38236.11698432743\t0.05000224347714149\t148047881\t (1 time(s))\r\n\tN\tO\t7.269911583E9\t1.0901214476133223E13\t1.0356163586779275E13\t1.0770418891231504E13\t25.499873337396807\t38236.99713421861\t0.04999763124732218\t285095988\t (1 time(s))\r\n\tR\tF\t3.77572497E9\t5.661603032743618E12\t5.378513563916123E12\t5.593662252665821E12\t25.50006628406532\t38236.69725844134\t0.05000130428587516\t148067261\t (1 time(s))\r\n\tN\tF\t9.8553062E7\t1.4777109838598825E11\t1.4038496596503897E11\t1.4599979303278268E11\t25.501556956882876\t38237.19938880664\t0.04998528433773886\t3864590\t (1 time(s))\r\n{noformat}\r\nTest_Failed: 2014/10/22 11:26:11.0011 - Verification failed.\r\n\r\n\r\n/home/work/drill-testing/testing/framework/resources/Advanced/Passing/tpch100/parquet/10.q :\r\n{noformat}\r\n-- tpch10 using 1395599672 as a seed to the RNG\r\nselect\r\n  c.c_custkey,\r\n  c.c_name,\r\n  sum(l.l_extendedprice * (1 - l.l_discount)) as revenue,\r\n  c.c_acctbal,\r\n  n.n_name,\r\n  c.c_address,\r\n  c.c_phone,\r\n  c.c_comment\r\nfrom\r\n  customer c,\r\n  orders o,\r\n  lineitem l,\r\n  nation n\r\nwhere\r\n  c.c_custkey = o.o_custkey\r\n  and l.l_orderkey = o.o_orderkey\r\n  and o.o_orderdate >= date '1994-03-01'\r\n  and o.o_orderdate < date '1994-03-01' + interval '3' month\r\n  and l.l_returnflag = 'R'\r\n  and c.c_nationkey = n.n_nationkey\r\ngroup by\r\n  c.c_custkey,\r\n  c.c_name,\r\n  c.c_acctbal,\r\n  c.c_phone,\r\n  n.n_name,\r\n  c.c_address,\r\n  c.c_comment\r\norder by\r\n  revenue desc\r\nlimit 20\r\n{noformat}\r\n\r\n         Expected number of rows: 20\r\nActual number of rows from Drill: 20\r\n         Number of matching rows: 17\r\n          Number of rows missing: 3\r\n       Number of rows unexpected: 3\r\n{noformat}\r\nThese rows are not expected (first 10):\r\n\t6372220\tCustomer#006372220\t793123.1516\t2836.62\tFRANCE\tbfd3hpM99xDp6AFsGNOPP\t16-143-244-4177\t regular theodolites are according to the unusual \t\r\n\t14211121\tCustomer#014211121\t796135.1836\t7443.03\tMOROCCO\tks7nhxDqzdk72CfWM\t25-755-902-4219\tlyly final packages doubt furiously carefully bold theodolites. final \t\r\n\t246700\tCustomer#000246700\t801786.5193999999\t5244.71\tCHINA\to6FXqCXJjKy3JdCAvuU3XJNRFcz35rAoc\t28-466-828-8872\t even asymptotes cajole slyly with the furiously bold accounts. furiously unusual platelets believe quickly final, \t\r\n\r\nThese rows are missing (first 10):\r\n\t14211121\tCustomer#014211121\t796135.1835999999\t7443.03\tMOROCCO\tks7nhxDqzdk72CfWM\t25-755-902-4219\tlyly final packages doubt furiously carefully bold theodolites. final\t (1 time(s))\r\n\t246700\tCustomer#000246700\t801786.5194000001\t5244.71\tCHINA\to6FXqCXJjKy3JdCAvuU3XJNRFcz35rAoc\t28-466-828-8872\t even asymptotes cajole slyly with the furiously bold accounts. furiously unusual platelets believe quickly final,\t (1 time(s))\r\n\t6372220\tCustomer#006372220\t793123.1516000001\t2836.62\tFRANCE\tbfd3hpM99xDp6AFsGNOPP\t16-143-244-4177\t regular theodolites are according to the unusual\t (1 time(s))\r\n{noformat}\r\nTest_Failed: 2014/10/22 11:23:10.0010 - Verification failed.\r\n",
        "Tpch Query 10 fails for SF 100 over parquet with precision issues git.commit.id.abbrev=5c220e3\r\n\r\nThe below query fails with verification issues. This looks like a regression since this is passing with 0.5 release\r\n\r\n{code}\r\nselect\r\n  c.c_custkey,\r\n  c.c_name,\r\n  sum(l.l_extendedprice * (1 - l.l_discount)) as revenue,\r\n  c.c_acctbal,\r\n  n.n_name,\r\n  c.c_address,\r\n  c.c_phone,\r\n  c.c_comment\r\nfrom\r\n  customer c,\r\n  orders o,\r\n  lineitem l,\r\n  nation n\r\nwhere\r\n  c.c_custkey = o.o_custkey\r\n  and l.l_orderkey = o.o_orderkey\r\n  and o.o_orderdate >= date '1994-03-01'\r\n  and o.o_orderdate < date '1994-03-01' + interval '3' month\r\n  and l.l_returnflag = 'R'\r\n  and c.c_nationkey = n.n_nationkey\r\ngroup by\r\n  c.c_custkey,\r\n  c.c_name,\r\n  c.c_acctbal,\r\n  c.c_phone,\r\n  n.n_name,\r\n  c.c_address,\r\n  c.c_comment\r\norder by\r\n  revenue desc\r\nlimit 20\r\n{code}\r\n\r\nExpected number of rows: 20\r\nActual number of rows from Drill: 20\r\nNumber of matching rows: 17\r\nNumber of rows missing: 3\r\nNumber of rows unexpected: 3\r\n\r\nActual Data from drill :\r\n{code}\r\n6372220\tCustomer#006372220\t793123.1516000001\t2836.62\tFRANCE\tbfd3hpM99xDp6AFsGNOPP\t16-143-244-4177\t regular theodolites are according to the unusual \t\r\n\t14211121\tCustomer#014211121\t796135.1836\t7443.03\tMOROCCO\tks7nhxDqzdk72CfWM\t25-755-902-4219\tlyly final packages doubt furiously carefully bold theodolites. final \t\r\n\t246700\tCustomer#000246700\t801786.5193999999\t5244.71\tCHINA\to6FXqCXJjKy3JdCAvuU3XJNRFcz35rAoc\t28-466-828-8872\t even asymptotes cajole slyly with the furiously bold accounts. furiously unusual platelets believe quickly final,\r\n{code}\r\n\r\nExpected Data :\r\n{code}\r\n14211121\tCustomer#014211121\t796135.1835999999\t7443.03\tMOROCCO\tks7nhxDqzdk72CfWM\t25-755-902-4219\tlyly final packages doubt furiously carefully bold theodolites. final    \r\n\t246700\tCustomer#000246700\t801786.5194000001\t5244.71\tCHINA\to6FXqCXJjKy3JdCAvuU3XJNRFcz35rAoc\t28-466-828-8872\t even asymptotes cajole slyly with the furiously bold accounts. furiously unusual platelets believe quickly final,    \r\n\t6372220\tCustomer#006372220\t793123.1516000001\t2836.62\tFRANCE\tbfd3hpM99xDp6AFsGNOPP\t16-143-244-4177\t regular theodolites are according to the unusual\r\n{code}"
    ],
    [
        "DRILL-1594",
        "DRILL-1257",
        "SQL exception when querying JSON I can reproduce this with many queries, but narrowing down, it may actually have to do with the number of rows that are being retrieved or a specific record.\r\nI tried a query as follows (i.e limit 10034). It returns 10033 rows and then fails with an exception. A modified limit 10033 does succeed.\r\n\r\n select * from dfs.`/users/nrentachintala/Downloads/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_business.json` b  limit 10034;\r\n-------------------\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while trying to get next result batch.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n\r\nI enabled text mode using alter system set `store.json.all_text_mode`=true;\r\nThe same error occurs with this configuration as well.\r\n\r\nHere is another example where it fails.\r\nselect * from dfs.`/users/nrentachintala/Downloads/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_business.json` b where review_count > 1000;",
        "Handle schema mutations within a batch Drill is intended to provide flexibility for a wide variety of querying use cases, including those against file formats with changing schema such as JSON. Currently Drill does not have an efficient means of representing these cases, as a constant schema is tied to a record batch, which is best used to represent a few hundred or thousand records. We have have also not designed all of the operators and functions to gracefully handle changes in schema throughout the course of a query."
    ],
    [
        "DRILL-1606",
        "DRILL-1581",
        "Issue with casting to integer from a CSV file I have a CSV file with this data:\r\n{code}\r\n0: jdbc:drill:zk=localhost:5181> select columns[0], columns[1] from `dfs.optdrill`.`touchstone/Integer_Table.csv`;\r\n+------------+------------+\r\n|   EXPR$0   |   EXPR$1   |\r\n+------------+------------+\r\n| Zero       | 0          |\r\n| One        | 1          |\r\n| MinusOne   | -1         |\r\n| Two        | 2          |\r\n| MaxTInt    | 127        |\r\n| MinTInt    | -128       |\r\n| MaxUTInt   | 255        |\r\n| MaxTIntP1  | 128        |\r\n| MinTIntM1  | -129       |\r\n| MaxUTIntP1 | 256        |\r\n| MaxSInt    | 32767      |\r\n| MinSInt    | -32768     |\r\n| MaxUSInt   | 65535      |\r\n| MinSIntM1  | -32769     |\r\n| MaxSIntP1  | 32768      |\r\n| MaxUSIntP1 | 65536      |\r\n| MaxInt     | 2147483647 |\r\n| MinInt     | -2147483648 |\r\n+------------+------------+\r\n18 rows selected (0.099 seconds)\r\n{code}\r\n\r\nWhen I try to cast it to varchar and integer, it fails:\r\n{code}\r\n0: jdbc:drill:zk=localhost:5181> select cast(columns[0] as varchar(20)), cast(columns[1] as integer) from `dfs.optdrill`.`touchstone/Integer_Table.csv`;\r\n+------------+------------+\r\n|   EXPR$0   |   EXPR$1   |\r\n+------------+------------+\r\nQuery failed: Failure while running fragment.\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n        at sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n        at sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n        at sqlline.SqlLine.print(SqlLine.java:1809)\r\n        at sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n        at sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n        at sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n        at sqlline.SqlLine.begin(SqlLine.java:763)\r\n        at sqlline.SqlLine.start(SqlLine.java:498)\r\n        at sqlline.SqlLine.main(SqlLine.java:460)\r\n{code}\r\n\r\nCasting to varchar and varchar or varchar and bigint works perfectly fine.\r\nUsing the same data set in a parquet file works fine when casting to varchar and integer as well.\r\n{code}\r\n0: jdbc:drill:zk=localhost:5181> select cast(keycolumn as varchar(20)), cast(column1 as integer) from `dfs.optdrill`.`touchstone/integer.parquet`;\r\n+------------+------------+\r\n|   EXPR$0   |   EXPR$1   |\r\n+------------+------------+\r\n| Zero       | 0          |\r\n| One        | 1          |\r\n| MinusOne   | -1         |\r\n| Two        | 2          |\r\n| MaxTInt    | 127        |\r\n| MinTInt    | -128       |\r\n| MaxUTInt   | 255        |\r\n| MaxTIntP1  | 128        |\r\n| MinTIntM1  | -129       |\r\n| MaxUTIntP1 | 256        |\r\n| MaxSInt    | 32767      |\r\n| MinSInt    | -32768     |\r\n| MaxUSInt   | 65535      |\r\n| MinSIntM1  | -32769     |\r\n| MaxSIntP1  | 32768      |\r\n| MaxUSIntP1 | 65536      |\r\n| MaxInt     | 2147483647 |\r\n| MinInt     | -2147483648 |\r\n+------------+------------+\r\n18 rows selected (0.067 seconds)\r\n{code}",
        "tpch100 text queries are failing - regression code base:\r\n#Generated by Git-Commit-Id-Plugin\r\n#Fri Oct 24 11:15:44 PDT 2014\r\ngit.commit.id.abbrev=b956e45\r\n\r\nThe following tpch100 text query was working before the following checkin (not verified)\r\n\r\nCommit after which tests started failing\r\nhttps://github.com/apache/incubator-drill/commit/17a4d921417ff914cb81588926b692d1aea09e56\r\nor\r\nhttps://github.com/apache/incubator-drill/commit/38c5d4e4bb693fe65118f84463dce9246d7096fe\r\n\r\nThe query:\r\nTest_Started: 2014/10/24 16:29:15.0015\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\ncreate view nation as select cast(columns[0] as int) n_nationkey, columns[1] n_name, cast(columns[2] as int) n_regionkey, columns[3] n_comment from `nation_text`\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\ncreate view region as select cast(columns[0] as int) r_regionkey, columns[1] r_name, columns[2] r_comment from `region_text`\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\ncreate view part as select cast(columns[0] as int) p_partkey, columns[1] p_name, columns[2] p_mfgr, columns[3] p_brand, columns[4] p_type, cast(columns[5] as int) p_size, columns[6] p_container, cast(columns[7] as double) p_retailprice, columns[8] p_comment from `part_text`\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\ncreate view supplier as select cast(columns[0] as int) s_suppkey, columns[1] s_name, columns[2] s_address, cast(columns[3] as int) s_nationkey, columns[4] s_phone, cast(columns[5] as double) s_acctbal, columns[6] s_comment from `supplier_text`\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\ncreate view partsupp as select cast(columns[0] as int) ps_partkey, cast(columns[1] as int) ps_suppkey, cast(columns[2] as int) ps_availqty, cast(columns[3] as double) ps_supplycost, columns[4] ps_comment from `partsupp_text`\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\ncreate view customer as select cast(columns[0] as int) c_custkey, columns[1] c_name, columns[2] c_address, cast(columns[3] as int) c_nationkey, columns[4] c_phone, cast(columns[5] as double) c_acctbal, columns[6] c_mktsegment, columns[7] c_comment from `customer_text`\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\ncreate view orders as select cast(columns[0] as int) o_orderkey, cast(columns[1] as int) o_custkey, columns[2] o_orderstatus, cast(columns[3] as double) o_totalprice, cast(columns[4] as date)o_orderdate, columns[5] o_orderpriority, columns[6] o_clerk, cast(columns[7] as int) o_shippriority, columns[8] o_comment from `orders_text`\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\ncreate view lineitem as select cast(columns[0] as int) l_orderkey, cast(columns[1] as int) l_partkey, cast(columns[2] as int) l_suppkey, cast(columns[3] as int) l_linenumber, cast(columns[4] as double) l_quantity, cast(columns[5] as double) l_extendedprice, cast(columns[6] as double) l_discount, cast(columns[7] as double) l_tax, columns[8] l_returnflag, columns[9] l_linestatus, cast(columns[10] as date) l_shipdate, cast(columns[11] as date) l_commitdate, cast(columns[12] as date) l_receiptdate, columns[13] l_shipinstruct, columns[14] l_shipmode, columns[15] l_comment from `lineitem_text`\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\n-- tpch3 using 1395599672 as a seed to the RNG\r\nselect\r\n  l.l_orderkey,\r\n  sum(l.l_extendedprice * (1 - l.l_discount)) as revenue,\r\n  o.o_orderdate,\r\n  o.o_shippriority\r\nfrom\r\n  customer c,\r\n  orders o,\r\n  lineitem l\r\nwhere\r\n  c.c_mktsegment = 'HOUSEHOLD'\r\n  and c.c_custkey = o.o_custkey\r\n  and l.l_orderkey = o.o_orderkey\r\n  and o.o_orderdate < date '1995-03-25'\r\n  and l.l_shipdate > date '1995-03-25'\r\ngroup by\r\n  l.l_orderkey,\r\n  o.o_orderdate,\r\n  o.o_shippriority\r\norder by\r\n  revenue desc,\r\n  o.o_orderdate\r\nlimit 10\r\n\r\nQuery failed: Failure while running fragment. 299064069 [f197040e-0b70-4ef7-bbb2-a1dc88e340da]\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\ndrop view nation\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\ndrop view region\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\ndrop view part\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\ndrop view supplier\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\ndrop view partsupp\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\ndrop view customer\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\ndrop view orders\r\n\r\n/root/forgitcommit/private-sql-hadoop-test/framework/resources/Advanced/Passing/tpch100/text/03.q :\r\n\r\ndrop view lineitem\r\n\r\n\r\nTest_Failed: 2014/10/24 16:29:21.0021 - Failed to execute.\r\nEND of Test\r\n\r\nIn drillbit.log, I saw interrupted exception:\r\n16:29:15.664 [68a9af00-8c19-4327-bc6a-4f3672a841e0:frag:0:0] WARN  o.a.d.e.p.impl.SendingAccountor - Failure while waiting for send complete.\r\njava.lang.InterruptedException: null\r\n  at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:996) ~[na:1.7.0_45]\r\n  at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303) ~[na:1.7.0_45]\r\n  at java.util.concurrent.Semaphore.acquire(Semaphore.java:472) ~[na:1.7.0_45]\r\n  at org.apache.drill.exec.physical.impl.SendingAccountor.waitForSendComplete(SendingAccountor.java:44) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.stop(ScreenCreator.java:186) [drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.fragment.FragmentExecutor.closeOutResources(FragmentExecutor.java:134) [drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:109) [drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:250) [drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n  at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n\r\n\r\nAnd error messages such as:\r\n\r\n16:29:20.712 [BitServer-4] ERROR o.a.d.exec.rpc.control.WorkEventBus - A fragment message arrived but there was no registered listener for that message for handle query_id {\r\n  part1: 5422968853476163806\r\n  part2: -5881537513090378761\r\n}\r\nmajor_fragment_id: 5\r\nminor_fragment_id: 25\r\n\r\n\r\nThere is also RpcException:\r\n16:29:20.852 [2a0c0d77-65a1-4109-ab57-32476ceaab18:frag:0:0] ERROR o.a.d.e.p.i.ScreenCreator$ScreenRoot - Failure while sending data to user.\r\norg.apache.drill.exec.rpc.RpcException: java.lang.InterruptedException\r\n  at org.apache.drill.exec.rpc.RemoteConnection.blockOnNotWritable(RemoteConnection.java:56) [drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.rpc.RpcBus.send(RpcBus.java:89) [drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.rpc.user.UserServer$UserClientConnection.sendResult(UserServer.java:132) [drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:139) [drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) [drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:104) [drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:250) [drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n  at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\nCaused by: java.lang.InterruptedException: null\r\n  at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1301) ~[na:1.7.0_45]\r\n  at org.apache.drill.exec.rpc.ResettableBarrier.await(ResettableBarrier.java:70) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.rpc.RemoteConnection$WriteManager.waitForWritable(RemoteConnection.java:80) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.rpc.RemoteConnection.blockOnNotWritable(RemoteConnection.java:53) [drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  ... 9 common frames omitted\r\n\r\n\r\nAnd NumberFormatException:\r\n\r\n16:29:20.912 [4b424170-ab74-40de-ae60-94cb271d13f7:frag:5:59] WARN  o.a.d.e.w.fragment.FragmentExecutor - Error while initializing or executing fragment\r\njava.lang.NumberFormatException: 540000001\r\n  at org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.nfeI(StringFunctionHelpers.java:93) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.varCharToInt(StringFunctionHelpers.java:124) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.test.generated.ProjectorGen267.doEval(ProjectorTemplate.java:135) ~[na:na]\r\n  at org.apache.drill.exec.test.generated.ProjectorGen267.projectRecords(ProjectorTemplate.java:64) ~[na:na]\r\n  at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.doWork(ProjectRecordBatch.java:145) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:85) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:127) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:105) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:117) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:85) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:75) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:50) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:105) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:117) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.physical.impl.partitionsender.PartitionSenderRootExec.innerNext(PartitionSenderRootExec.java:124) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:104) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:250) [drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n  at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n\r\n\r\nAnd RuntimeException accessing zookeeper:\r\n\r\n16:29:20.925 [a000da5d-a42c-4bb9-bbb0-f203fc0769e4:frag:0:0] WARN  o.a.d.e.w.fragment.FragmentExecutor - Error while initializing or executing fragment\r\njava.lang.RuntimeException: Failure while accessing Zookeeper\r\n  at org.apache.drill.exec.store.sys.zk.ZkPStore.put(ZkPStore.java:111) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.foreman.QueryStatus.updateCache(QueryStatus.java:125) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.foreman.QueryStatus.update(QueryStatus.java:119) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.foreman.QueryManager.updateStatus(QueryManager.java:173) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.foreman.QueryManager.finished(QueryManager.java:189) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.foreman.QueryManager.statusUpdate(QueryManager.java:162) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.foreman.QueryManager$RootStatusHandler.statusChange(QueryManager.java:284) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.fragment.AbstractStatusReporter.finished(AbstractStatusReporter.java:101) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.fragment.AbstractStatusReporter.stateChanged(AbstractStatusReporter.java:73) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.fragment.FragmentExecutor.updateState(FragmentExecutor.java:172) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:110) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:250) [drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n  at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n\r\n\r\n"
    ],
    [
        "DRILL-1613",
        "DRILL-1311",
        "Drill does not honor nested schema changes Drill's current definition of schema change is an incremental update to the top-level schema structure. This is inappropriate for complex data types where schema change may occur at any level in the hierarchy of vectors. Current design does not fit into this requirement neatly as only top-level containers are able to mutate the schema. We need a minor to major re-design to handle this situation.\r\n\r\nThis issue exacerbates when querying partitions or batches of files that do not necessarily conform to the exact same schema. New fields from upcoming batches will be omitted in either case. ",
        "Hash join does not support schema changes error - Create a directory with a couple of JSON files. One with columns a,b and second with columns a,b,c. \r\n- a & b attributes have same data types across both the files\r\n- create a view by selecting columns a, b from the directory\r\n- Join the view with any other table\r\n\r\nAn error shows up indicating that 'Hash join does not support schema changes'.\r\nThere is a schema change across the files with a new element being added, however given that specific columns a,b are selected in the view, expect that query works fine."
    ],
    [
        "DRILL-1613",
        "DRILL-1324",
        "Drill does not honor nested schema changes Drill's current definition of schema change is an incremental update to the top-level schema structure. This is inappropriate for complex data types where schema change may occur at any level in the hierarchy of vectors. Current design does not fit into this requirement neatly as only top-level containers are able to mutate the schema. We need a minor to major re-design to handle this situation.\r\n\r\nThis issue exacerbates when querying partitions or batches of files that do not necessarily conform to the exact same schema. New fields from upcoming batches will be omitted in either case. ",
        "Detect schema changes when complex vector changes internally  Currently we indicate a schema change only if the top level vector in the container changes. We need to detect changes when we add a vector to the top level Map, Repeated Map, Repeated List in the container. "
    ],
    [
        "DRILL-1623",
        "DRILL-1417",
        "COUNT (*) query with a filter does not work over partitioned data in text and json formats git.commit.id.abbrev=6dca24a\r\n\r\nThe below 2 queries fail. The underlying data is partitioned and is one level deep. \r\n\r\nText Query :\r\n{code}\r\nselect count(*) from lineitem_text where dir0=1993;\r\n\r\nQuery failed: Failure while running fragment. Readers needs at least a column to read. [499ab624-616a-409a-a93c-c1dc9a0d31b1]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n{code}\r\n\r\nJson Query :\r\n{code}\r\nselect count(*) from orders_json where dir0=1993\r\n\r\nQuery failed: Failure while running fragment. json record reader requires at least a column [dc7cfa54-6d6f-4bf3-a0f2-38da9e58ace9]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n{code}\r\n\r\nWhen I tested this against parquet and hive they seem to be having no issues. \r\n\r\nAttached the data, logs and plans for the above 2 queries",
        "Query against directory of JSON files no longer works in 0.5 This query was working on an earlier build (somewhere between the 0.4 and 0.5 releases) but no longer works:\r\n\r\n0: jdbc:drill:> select dir1 month_no, count(*) month_count from logs where dir0=2014 group by dir1 order by dir1;\r\nQuery failed: Failure while running fragment. json record reader requires at least a column [90feae61-fcfa-427f-b0bf-4a06563aa9a9]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n\r\nLet me know if I should open a Jira, or if I need to rewrite the query. I am using the latest build of the MapR Sandbox for Drill from http://builds.qa.lab/vm/ova/?C=M;O=A\r\n\r\nThe old result of the query is here:\r\n\r\n0: jdbc:drill:> select dir1 month_no, count(*) month_count from logs where dir0=2014 group by dir1 order by dir1;\r\n+------------+-------------+\r\n|  month_no  | month_count |\r\n+------------+-------------+\r\n| 1          | 1741        |\r\n| 2          | 1538        |\r\n| 3          | 1689        |\r\n| 4          | 1675        |\r\n| 5          | 1738        |\r\n| 6          | 1653        |\r\n| 7          | 1745        |\r\n| 8          | 221         |\r\n+------------+-------------+\r\n8 rows selected\r\n\r\nEXPLAIN TEXT\r\n---------------------\r\n0: jdbc:drill:> explain plan for select dir1 month_no, count(*) month_count from logs where dir0=2014 group by dir1 order by dir1;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(month_no=[$0], month_count=[$1])\r\n00-02        SelectionVectorRemover\r\n00-03          Sort(sort0=[$0], dir0=[ASC])\r\n00-04            Project(month_no=[$0], month_count=[$1])\r\n00-05              HashAgg(group=[{0}], month_count=[COUNT()])\r\n00-06                Project(dir1=[$0])\r\n00-07                  SelectionVectorRemover\r\n00-08                    Filter(condition=[=(CAST($1):INTEGER, 2014)])\r\n00-09                      Scan(groupscan=[EasyGroupScan [selectionRoot=/mapr/demo.mapr.com/data/flat/logs, columns = [SchemaPath [`dir1`], SchemaPath [`dir0`]]]])\r\n | {\r\n  \"head\" : {\r\n    \"version\" : 1,\r\n    \"generator\" : {\r\n      \"type\" : \"ExplainHandler\",\r\n      \"info\" : \"\"\r\n    },\r\n    \"type\" : \"APACHE_DRILL_PHYSICAL\",\r\n    \"options\" : [ ],\r\n    \"queue\" : 0,\r\n    \"resultMode\" : \"EXEC\"\r\n  },\r\n  \"graph\" : [ {\r\n    \"pop\" : \"fs-scan\",\r\n    \"@id\" : 9,\r\n    \"files\" : [ \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2012/8/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2012/11/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2012/6/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2012/12/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2012/3/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2012/4/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2012/5/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2012/1/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2012/7/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2012/2/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2012/9/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2012/10/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2013/8/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2013/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2013/11/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2013/6/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2013/12/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2013/3/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2013/4/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2013/5/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2013/1/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2013/7/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2013/2/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2013/9/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2013/10/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2014/8/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2014/6/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2014/3/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2014/4/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2014/5/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2014/1/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2014/7/log.json\", \"maprfs:/mapr/demo.mapr.com/data/flat/logs/2014/2/log.json\" ],\r\n    \"storage\" : {\r\n      \"type\" : \"file\",\r\n      \"enabled\" : true,\r\n      \"connection\" : \"maprfs:///\",\r\n      \"workspaces\" : {\r\n        \"root\" : {\r\n          \"location\" : \"/\",\r\n          \"writable\" : false,\r\n          \"storageformat\" : null\r\n        },\r\n        \"data\" : {\r\n          \"location\" : \"/mapr/demo.mapr.com/data\",\r\n          \"writable\" : false,\r\n          \"storageformat\" : null\r\n        },\r\n        \"clicks\" : {\r\n          \"location\" : \"/mapr/demo.mapr.com/data/nested\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        },\r\n        \"logs\" : {\r\n          \"location\" : \"/mapr/demo.mapr.com/data/flat\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        },\r\n        \"views\" : {\r\n          \"location\" : \"/mapr/demo.mapr.com/data/views\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        },\r\n        \"tmp\" : {\r\n          \"location\" : \"/tmp\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"csv\"\r\n        }\r\n      },\r\n      \"formats\" : {\r\n        \"psv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tbl\" ],\r\n          \"delimiter\" : \"|\"\r\n        },\r\n        \"csv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"csv\" ],\r\n          \"delimiter\" : \",\"\r\n        },\r\n        \"tsv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tsv\" ],\r\n          \"delimiter\" : \"\\t\"\r\n        },\r\n        \"parquet\" : {\r\n          \"type\" : \"parquet\"\r\n        },\r\n        \"json\" : {\r\n          \"type\" : \"json\"\r\n        }\r\n      }\r\n    },\r\n    \"format\" : {\r\n      \"type\" : \"json\"\r\n    },\r\n    \"columns\" : [ \"`dir1`\", \"`dir0`\" ],\r\n    \"selectionRoot\" : \"/mapr/demo.mapr.com/data/flat/logs\",\r\n    \"cost\" : 7731.0\r\n  }, {\r\n    \"pop\" : \"filter\",\r\n    \"@id\" : 8,\r\n    \"child\" : 9,\r\n    \"expr\" : \"equal(cast( (`dir0` ) as INT ), 2014) \",\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 1159.6499999999999\r\n  }, {\r\n    \"pop\" : \"selection-vector-remover\",\r\n    \"@id\" : 7,\r\n    \"child\" : 8,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 1159.6499999999999\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 6,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`dir1`\",\r\n      \"expr\" : \"`dir1`\"\r\n    } ],\r\n    \"child\" : 7,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 1159.6499999999999\r\n  }, {\r\n    \"pop\" : \"hash-aggregate\",\r\n    \"@id\" : 5,\r\n    \"child\" : 6,\r\n    \"cardinality\" : 1.0,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 579.8249999999999,\r\n    \"groupByExprs\" : [ {\r\n      \"ref\" : \"`dir1`\",\r\n      \"expr\" : \"`dir1`\"\r\n    } ],\r\n    \"aggrExprs\" : [ {\r\n      \"ref\" : \"`month_count`\",\r\n      \"expr\" : \"count(1) \"\r\n    } ]\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 4,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`month_no`\",\r\n      \"expr\" : \"`dir1`\"\r\n    }, {\r\n      \"ref\" : \"`month_count`\",\r\n      \"expr\" : \"`month_count`\"\r\n    } ],\r\n    \"child\" : 5,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 115.96499999999999\r\n  }, {\r\n    \"pop\" : \"external-sort\",\r\n    \"@id\" : 3,\r\n    \"child\" : 4,\r\n    \"orderings\" : [ {\r\n      \"order\" : \"ASC\",\r\n      \"expr\" : \"`month_no`\",\r\n      \"nullDirection\" : \"UNSPECIFIED\"\r\n    } ],\r\n    \"reverse\" : false,\r\n    \"initialAllocation\" : 20000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 115.96499999999999\r\n  }, {\r\n    \"pop\" : \"selection-vector-remover\",\r\n    \"@id\" : 2,\r\n    \"child\" : 3,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 115.96499999999999\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 1,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`month_no`\",\r\n      \"expr\" : \"`month_no`\"\r\n    }, {\r\n      \"ref\" : \"`month_count`\",\r\n      \"expr\" : \"`month_count`\"\r\n    } ],\r\n    \"child\" : 2,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 115.96499999999999\r\n  }, {\r\n    \"pop\" : \"screen\",\r\n    \"@id\" : 0,\r\n    \"child\" : 1,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 115.96499999999999\r\n  } ]\r\n} |\r\n+------------+------------+\r\n1 row selected (0.86 seconds)"
    ],
    [
        "DRILL-1627",
        "DRILL-1449",
        "Writer needs to be transactional Tried to do a CTAS which failed for unknown reasons. Output starts out looking OK, but then gets an error:\r\n\r\n0: jdbc:drill:zk=local> create table donuts_parquet as select * from `donuts.json`;\r\ncreate table donuts_parquet as select * from `donuts.jso \r\nn`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 5                         |\r\nQuery failed: Failure while running fragment.\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n0: jdbc:drill:zk=local> \r\n\r\nNo indication of what caused the failure. But the non-zero \"Number of records written\" would seem to imply success. I checked the directory this workspace is configured to use, and while it did create the parquet file, it is zero sized:\r\n\r\nwormsign:json cwestin$ ls\r\ndonuts.json\tdonuts_parquet/\r\nwormsign:json cwestin$ ls donuts_parquet\r\n0_0_0.parquet\r\nwormsign:json cwestin$ ls -l donuts_parquet\r\ntotal 0\r\n-rw-r--r--  1 cwestin  staff  0 Oct 31 16:06 0_0_0.parquet\r\nwormsign:json cwestin$\r\n",
        "CTAS in JSON format creates a table even when the SELECT query fails When a CTAS command fails for any reason, it should not create the target  table. It appears that when the storage output format is JSON,  we end up creating an empty table.  If the output format is Parquet, this works correctly. \r\n\r\n0: jdbc:drill:zk=local> alter session set `store.format` = 'json';\r\n..\r\n0: jdbc:drill:zk=local> create table orders3 as select * from cp.`tpch/orders.parquet` where o_orderdate between '1994-07-01' and '1994-09-31' limit 10;\r\nQuery failed: Failure while running fragment. Value 31 for dayOfMonth must be in the range [1,30] \r\n\r\n// Now fix the query and re-run : \r\n\r\n0: jdbc:drill:zk=local> create table orders3 as select * from cp.`tpch/orders.parquet` where o_orderdate between '1994-07-01' and '1994-09-30' limit 10;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| false      | Table 'orders3' already exists. |\r\n+------------+------------+\r\n\r\n0: jdbc:drill:zk=local> select * from orders3;\r\n+--+\r\n|  |\r\n+--+\r\n+--+\r\nNo rows selected"
    ],
    [
        "DRILL-1640",
        "DRILL-1374",
        "DrillColumnMetaDataList does not implement List methods DrillColumnMetaDataList extends from org.apache.drill.jdbc.BasicList, which does not implement much of the List interface (it usually throws UnsupportedOperationExceptions).\r\n\r\nOne of these methods, iterator(), is called when searching for columns by name in the result set. Using the JDBC API, if you call DrillResultSet.getString(\"myColumnName\"), you get something like the following stack trace:\r\n\r\nCaused by: java.lang.UnsupportedOperationException\r\n\tat org.apache.drill.jdbc.BasicList.iterator(BasicList.java:44)\r\n\tat org.apache.drill.jdbc.DrillColumnMetaDataList.iterator(DrillColumnMetaDataList.java:34)\r\n\tat net.hydromatic.avatica.AvaticaResultSet.findColumn0(AvaticaResultSet.java:69)\r\n\tat net.hydromatic.avatica.AvaticaResultSet.getAccessor(AvaticaResultSet.java:102)\r\n\tat net.hydromatic.avatica.AvaticaResultSet.getString(AvaticaResultSet.java:270)\r\n\r\n\r\nDrillColumnMetaDataList should behave as a proper list.",
        "JDBC meta data query fails Using Drill JDBC jar with Jaspersoft BI tool and discovering Drill meta data fails with UnsupportedOperationException."
    ],
    [
        "DRILL-1643",
        "DRILL-1642",
        "Group count incorrect in repeated map vector The group count should indicate the number of groups or lists in the entire batch, this should be equal to the record count of the batch, or the value count on a scalar vector. Currently the repeatedMapVector incorrectly returns the number of map fields instead.\r\n\r\nThis is causing issues with flatten.",
        "flatten function is dropping the last record when the json file contains even no of records git.commit.id.abbrev=5adadfa\r\n\r\nThe below query does not produce the right output. It is missing the last record from the data file. The record count is even for the data set. However if it is odd, the flatten operator has not issues .\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select rownum, flatten(complex) from `flatten.json`;\r\n+------------+------------+\r\n|   rownum   |   EXPR$1   |\r\n+------------+------------+\r\n| 1          | {\"col1\":3} |\r\n| 1          | {\"col2\":2,\"col3\":1} |\r\n| 1          | {\"col1\":7} |\r\n| 2          | {\"col2\":2,\"col3\":1} |\r\n| 2          | {\"col1\":7} |\r\n| 3          | {\"col1\":2,\"col3\":1} |\r\n+------------+------------+\r\n{code} \r\n\r\nAttached the data file. Let me know if you need anything more."
    ],
    [
        "DRILL-1648",
        "DRILL-1607",
        "Incorrect schema reported during fast schema phase of flatten causes downstream code compilation to fail ",
        "Unable to use Flatten with sum/avg aggregate functions Unable to use flatten with arithmetic aggregations such as sum, avg. count, min, max and plain listing of fields work.\r\n\r\n0: jdbc:drill:zk=local> select c.f.name, c.f.gender,c.f.age from \r\n. . . . . . . . . . . > (select flatten(children) as f from dfs.`/Users/nrentachintala/Downloads/drillreleases/apache-drill-0.7.0-incubating-SNAPSHOT/sample-data/test.json`) c;\r\n+------------+------------+------------+\r\n|   EXPR$0   |   EXPR$1   |   EXPR$2   |\r\n+------------+------------+------------+\r\n| Jane       | Female     | 6          |\r\n| John       | Male       | 15         |\r\n| Earl       | Male       | 10         |\r\n| Sam        | Male       | 6          |\r\n| Kit        | Male       | 8          |\r\n+------------+------------+------------+\r\n5 rows selected (0.271 seconds)\r\n0: jdbc:drill:zk=local> \r\n0: jdbc:drill:zk=local> select count(*) from \r\n. . . . . . . . . . . > (select flatten(children) as f from dfs.`/Users/nrentachintala/Downloads/drillreleases/apache-drill-0.7.0-incubating-SNAPSHOT/sample-data/test.json`) c;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 5          |\r\n+------------+\r\n1 row selected (0.219 seconds)\r\n0: jdbc:drill:zk=local> \r\n0: jdbc:drill:zk=local> \r\n0: jdbc:drill:zk=local> select avg(c.f.age) from \r\n. . . . . . . . . . . > (select flatten(children) as f from dfs.`/Users/nrentachintala/Downloads/drillreleases/apache-drill-0.7.0-incubating-SNAPSHOT/sample-data/test.json`) c\r\n. . . . . . . . . . . > ;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\nQuery failed: Failure while running fragment.\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n\r\nJSON file used is below:\r\n{\"kind\": \"person\", \"fullName\": \"John Doe\", \"age\": 22, \"gender\": \"Male\", \"phoneNumber\": { \"areaCode\": \"206\", \"number\": \"1234567\"}, \"children\": [{ \"name\": \"Jane\", \"gender\": \"Female\", \"age\": \"6\"}, {\"name\": \"John\", \"gender\": \"Male\", \"age\": \"15\"}], \"citiesLived\": [{ \"place\": \"Seattle\", \"yearsLived\": [\"1995\"]}, {\"place\": \"Stockholm\", \"yearsLived\": [\"2005\"]}]}\r\n{\"kind\": \"person\", \"fullName\": \"Mike Jones\", \"age\": 35, \"gender\": \"Male\", \"phoneNumber\": { \"areaCode\": \"622\", \"number\": \"1567845\"}, \"children\": [{ \"name\": \"Earl\", \"gender\": \"Male\", \"age\": \"10\"}, {\"name\": \"Sam\", \"gender\": \"Male\", \"age\": \"6\"}, { \"name\": \"Kit\", \"gender\": \"Male\", \"age\": \"8\"}], \"citiesLived\": [{\"place\": \"Los Angeles\", \"yearsLived\": [\"1989\", \"1993\", \"1998\", \"2002\"]}, {\"place\": \"Washington DC\", \"yearsLived\": [\"1990\", \"1993\", \"1998\", \"2008\"]}, {\"place\": \"Portland\", \"yearsLived\": [\"1993\", \"1998\", \"2003\", \"2005\"]}, {\"place\": \"Austin\", \"yearsLived\": [\"1973\", \"1998\", \"2001\", \"2005\"]}]}\r\n{\"kind\": \"person\", \"fullName\": \"Anna Karenina\", \"age\": 45, \"gender\": \"Female\", \"phoneNumber\": { \"areaCode\": \"425\", \"number\": \"1984783\"}, \"citiesLived\": [{\"place\": \"Stockholm\", \"yearsLived\": [\"1992\", \"1998\", \"2000\", \"2010\"]}, {\"place\": \"Russia\", \"yearsLived\": [\"1998\", \"2001\", \"2005\"]}, {\"place\": \"Austin\", \"yearsLived\": [\"1995\", \"1999\"]}]}\r\n\r\nExplain plan for the avg query is below.\r\n0: jdbc:drill:zk=local> explain plan for select avg(c.f.age) from \r\n. . . . . . . . . . . > (select flatten(children) as f from dfs.`/Users/nrentachintala/Downloads/drillreleases/apache-drill-0.7.0-incubating-SNAPSHOT/sample-data/test.json`) c\r\n. . . . . . . . . . . > ;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(EXPR$0=[CAST(/(CastHigh(CASE(=($1, 0), null, $0)), $1)):ANY])\r\n00-02        StreamAgg(group=[{}], agg#0=[$SUM0($0)], agg#1=[COUNT()])\r\n00-03     |\r\n+------------+------------+"
    ],
    [
        "DRILL-1654",
        "DRILL-970",
        "Drill in embedded mode dies when laptop goes to sleep When my Mac goes to sleep and comes back up, the sqlline shell is disconnected and I have to restart it. Other services like mongod can resume automatically in a graceful way, and Drill should do the same.",
        "sqllline shell hangs after long idle time or laptop suspend/resume This has happened to me a few times. I put my laptop to sleep, come back later, and sqlline hangs for a while. \r\n\r\n0: jdbc:drill:zk=local> select count(*) as incidents, columns[1] as category from dfs.`/tmp/SFPD_Incidents_-_Previous_Three_Months.csv` group by columns[1] order by incidents desc;\r\nQuery failed: org.apache.drill.exec.rpc.ChannelClosedException: Queue closed due to channel closure.\r\nError: exception while executing query (state=,code=0)\r\n0: jdbc:drill:zk=local> "
    ],
    [
        "DRILL-1676",
        "DRILL-1587",
        "Query Plan visualizer is broken The web ui has a reference to \r\n\r\nhttp://cpettitt.github.io/project/dagre-d3/latest/dagre-d3.js\r\n\r\nA little while ago, this artifact was updated, and our visualizer code no longer works.\r\n\r\nThe solution is to point to the version of dagre-d3 that was in place when visualizer was added.",
        "Drill Web UI does not show the plan graph in tab of \"Visualized Plan\".  The Web UI used to show a plan graph in the \"Visualized plan\" tab. However, the plan graph is not shown there any more. That seems to be a regression. The script to show the plan graph might run into some error. "
    ],
    [
        "DRILL-1681",
        "DRILL-1072",
        "select with limit on directory with csv files takes quite long to terminate query like select * from `/drill/data` limit 100 takes quite long to terminate, about 20+ seconds.\r\n\r\n/drill/data includes overall 1100 csv files, all in single directory.\r\n\r\nselect * from `/drill/data/d2.csv` limit 100; terminates in 0.2 seconds.",
        "Drill is very slow when we have a large number of text files git.commit.id.abbrev=efa3274\r\nBuild# 26178\r\n\r\nAs the total number of files under the below directory increase, drill becomes very slow. Check the results for different file counts for the below query.\r\n\r\nAll files just contain 1 number and have a '.tbl' extension\r\n\r\nselect count(*) from dfs.`/drill/testdata/morefiles`;\r\n\r\n100 files --- 5.183 seconds\r\n250 files --- 15.021 seconds\r\n500 files --- 26.846 seconds\r\n1000 files --- 69.835 seconds\r\n5000 files --- 1573.589 seconds\r\n\r\nThe logs contain these messages repeatedly when executing against 5000 files:\r\n\r\n22:02:22.818 [b5a7fdd3-f788-4a40-9fd7-bf525bad09e3:frag:0:0] DEBUG o.a.d.e.s.text.DrillTextRecordReader - vector value capacity 65536\r\n22:02:22.818 [b5a7fdd3-f788-4a40-9fd7-bf525bad09e3:frag:0:0] DEBUG o.a.d.e.s.text.DrillTextRecordReader - vector byte capacity 32767500\r\n22:02:22.819 [b5a7fdd3-f788-4a40-9fd7-bf525bad09e3:frag:0:0] DEBUG o.a.d.e.s.text.DrillTextRecordReader - text scan batch size 5\r\n22:02:22.840 [b5a7fdd3-f788-4a40-9fd7-bf525bad09e3:frag:0:0] DEBUG o.a.d.e.s.text.DrillTextRecordReader - vector value capacity 65536\r\n22:02:22.841 [b5a7fdd3-f788-4a40-9fd7-bf525bad09e3:frag:0:0] DEBUG o.a.d.e.s.text.DrillTextRecordReader - vector byte capacity 32767500\r\n22:02:22.841 [b5a7fdd3-f788-4a40-9fd7-bf525bad09e3:frag:0:0] DEBUG o.a.d.e.s.text.DrillTextRecordReader - text scan batch size 0\r\n22:02:22.863 [b5a7fdd3-f788-4a40-9fd7-bf525bad09e3:frag:0:0] DEBUG o.a.d.e.s.text.DrillTextRecordReader - vector value capacity 65536\r\n22:02:22.863 [b5a7fdd3-f788-4a40-9fd7-bf525bad09e3:frag:0:0] DEBUG o.a.d.e.s.text.DrillTextRecordReader - vector byte capacity 32767500\r\n22:02:22.864 [b5a7fdd3-f788-4a40-9fd7-bf525bad09e3:frag:0:0] DEBUG o.a.d.e.s.text.DrillTextRecordReader - text scan batch size 5\r\n22:02:23.035 [b5a7fdd3-f788-4a40-9fd7-bf525bad09e3:frag:0:0] DEBUG o.a.d.e.s.text.DrillTextRecordReader - vector value capacity 65536\r\n22:02:23.036 [b5a7fdd3-f788-4a40-9fd7-bf525bad09e3:frag:0:0] DEBUG o.a.d.e.s.text.DrillTextRecordReader - vector byte capacity 32767500\r\n22:02:23.036 [b5a7fdd3-f788-4a40-9fd7-bf525bad09e3:frag:0:0] DEBUG o.a.d.e.s.text.DrillTextRecordReader - text scan batch size 0\r\n22:02:23.059 [b5a7fdd3-f788-4a40-9fd7-bf525bad09e3:frag:0:0] DEBUG o.a.d.e.s.text.DrillTextRecordReader - vector value capacity 65536\r\n22:02:23.059 [b5a7fdd3-f788-4a40-9fd7-bf525bad09e3:frag:0:0] DEBUG o.a.d.e.s.text.DrillTextRecordReader - vector byte capacity 32767500\r\n22:02:23.060 [b5a7fdd3-f788-4a40-9fd7-bf525bad09e3:frag:0:0] DEBUG o.a.d.e.s.text.DrillTextRecordReader - text scan batch size 5"
    ],
    [
        "DRILL-1685",
        "DRILL-1586",
        "MongoDB plugin - Drill not showing correct error when using the wrong MongoDB database When connecting to MongoDB with multiple databases and collections drill fails to recognize the use of unavailable tables/collections.\r\n\r\nExample: \r\nIn the reviews database there are review, user, business collections/tables.\r\nIn the twitter database there is a tweets collection/table.\r\n\r\nWhen connecting to the reviews database (use mongo.reviews;) and then incorrectly trying to access the tweets table in the twitter database sqlline returns the following error:\r\n\r\nQuery failed: Failure while setting up Foreman. Internal error: Error while applying rule DrillPushProjIntoScan, args [rel#5522:ProjectRel.NONE.ANY([]).[](child=rel#5521:Subset#0.ENUMERABLE.ANY([]).[],date=SUBSTRING($1, 1, 10)), rel#5510:EnumerableTableAccessRel.ENUMERABLE.ANY([]).[](table=[mongo, reviews, tweets])] [68adba1a-3f55-4f95-ad61-be3a7550a26d]\r\n\r\nInstead of simply identifying that the tweets table/collection is not in the current database.",
        "NPE when the collection being queried for does not exist in Mongo DB NPE when the collection being queried for does not exist in Mongo DB."
    ],
    [
        "DRILL-1693",
        "DRILL-105",
        "Could not find any reference for configuring RDBMS (Oracle/DB2) as a data source in any of the product document. Could not found documentation on creating RDBMS data source, as mentioned in different presentations.\r\nCan you please share details, on how to configure Oracle or DB2 data sources.\r\nAs per my use case, I am trying to connect to Oracel / DB2 and parquet files. The SQL is suppose to fetch federated data from RDBMS and Parquet.\r\n\r\nCan you please suggest if this feature is a part of the current implementation. Thanks.",
        "JDBC Storage engine We should allow query pushdown into JDBC.  Note that this is very different from JDBC support that allows a Java program to send a query to Drill.  Instead, it is a way for Drill to push down a query into a JDBC driver."
    ],
    [
        "DRILL-1714",
        "DRILL-1257",
        "query fails with AssertionError when querying Yelp business data The query:\r\n\r\n{code:sql}\r\nSELECT\r\n    t.type,\r\n    t.business_id,\r\n    t.name,\r\n    t.neighborhoods,\r\n    t.full_address,\r\n    t.city,\r\n    t.state,\r\n    CAST(t.latitude AS FLOAT) as latitude,\r\n    CAST(t.longitude AS FLOAT) as longitude,\r\n    CAST(t.stars as FLOAT) as stars,\r\n    CAST(t.review_count as INT) as review_count,\r\n    t.categories,\r\n    t.`open` AS is_open,\r\n    t.hours,\r\n    t.attributes\r\nFROM dfs.`/Users/vince/Desktop/data/yelp/yelp_academic_dataset_business.json` AS t\r\n{code}\r\n\r\n{quote}\r\n| business   | AnqXhbdkHHimMBR3UMS1Jw | Homewood Suites | []            | 2001 E Highland Avenue\r\nPhoenix, AZ 85016 | Phoenix    | AZ         | 33.505135  | -112.03801 | 3.5        |  |\r\njava.lang.AssertionError\r\n\tat org.apache.drill.exec.vector.VarCharVector$Accessor.get(VarCharVector.java:367)\r\n\tat org.apache.drill.exec.vector.VarCharVector$Accessor.getObject(VarCharVector.java:393)\r\n\tat org.apache.drill.exec.vector.NullableVarCharVector$Accessor.getObject(NullableVarCharVector.java:381)\r\n\tat org.apache.drill.exec.vector.accessor.NullableVarCharAccessor.getObject(NullableVarCharAccessor.java:98)\r\n\tat org.apache.drill.jdbc.AvaticaDrillSqlAccessor.getObject(AvaticaDrillSqlAccessor.java:136)\r\n\tat net.hydromatic.avatica.AvaticaResultSet.getObject(AvaticaResultSet.java:351)\r\n\tat sqlline.SqlLine$Rows$Row.<init>(SqlLine.java:2388)\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2504)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n{quote}",
        "Handle schema mutations within a batch Drill is intended to provide flexibility for a wide variety of querying use cases, including those against file formats with changing schema such as JSON. Currently Drill does not have an efficient means of representing these cases, as a constant schema is tied to a record batch, which is best used to represent a few hundred or thousand records. We have have also not designed all of the operators and functions to gracefully handle changes in schema throughout the course of a query."
    ],
    [
        "DRILL-1714",
        "DRILL-1594",
        "query fails with AssertionError when querying Yelp business data The query:\r\n\r\n{code:sql}\r\nSELECT\r\n    t.type,\r\n    t.business_id,\r\n    t.name,\r\n    t.neighborhoods,\r\n    t.full_address,\r\n    t.city,\r\n    t.state,\r\n    CAST(t.latitude AS FLOAT) as latitude,\r\n    CAST(t.longitude AS FLOAT) as longitude,\r\n    CAST(t.stars as FLOAT) as stars,\r\n    CAST(t.review_count as INT) as review_count,\r\n    t.categories,\r\n    t.`open` AS is_open,\r\n    t.hours,\r\n    t.attributes\r\nFROM dfs.`/Users/vince/Desktop/data/yelp/yelp_academic_dataset_business.json` AS t\r\n{code}\r\n\r\n{quote}\r\n| business   | AnqXhbdkHHimMBR3UMS1Jw | Homewood Suites | []            | 2001 E Highland Avenue\r\nPhoenix, AZ 85016 | Phoenix    | AZ         | 33.505135  | -112.03801 | 3.5        |  |\r\njava.lang.AssertionError\r\n\tat org.apache.drill.exec.vector.VarCharVector$Accessor.get(VarCharVector.java:367)\r\n\tat org.apache.drill.exec.vector.VarCharVector$Accessor.getObject(VarCharVector.java:393)\r\n\tat org.apache.drill.exec.vector.NullableVarCharVector$Accessor.getObject(NullableVarCharVector.java:381)\r\n\tat org.apache.drill.exec.vector.accessor.NullableVarCharAccessor.getObject(NullableVarCharAccessor.java:98)\r\n\tat org.apache.drill.jdbc.AvaticaDrillSqlAccessor.getObject(AvaticaDrillSqlAccessor.java:136)\r\n\tat net.hydromatic.avatica.AvaticaResultSet.getObject(AvaticaResultSet.java:351)\r\n\tat sqlline.SqlLine$Rows$Row.<init>(SqlLine.java:2388)\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2504)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n{quote}",
        "SQL exception when querying JSON I can reproduce this with many queries, but narrowing down, it may actually have to do with the number of rows that are being retrieved or a specific record.\r\nI tried a query as follows (i.e limit 10034). It returns 10033 rows and then fails with an exception. A modified limit 10033 does succeed.\r\n\r\n select * from dfs.`/users/nrentachintala/Downloads/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_business.json` b  limit 10034;\r\n-------------------\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while trying to get next result batch.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n\r\nI enabled text mode using alter system set `store.json.all_text_mode`=true;\r\nThe same error occurs with this configuration as well.\r\n\r\nHere is another example where it fails.\r\nselect * from dfs.`/users/nrentachintala/Downloads/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_business.json` b where review_count > 1000;"
    ],
    [
        "DRILL-1714",
        "DRILL-1602",
        "query fails with AssertionError when querying Yelp business data The query:\r\n\r\n{code:sql}\r\nSELECT\r\n    t.type,\r\n    t.business_id,\r\n    t.name,\r\n    t.neighborhoods,\r\n    t.full_address,\r\n    t.city,\r\n    t.state,\r\n    CAST(t.latitude AS FLOAT) as latitude,\r\n    CAST(t.longitude AS FLOAT) as longitude,\r\n    CAST(t.stars as FLOAT) as stars,\r\n    CAST(t.review_count as INT) as review_count,\r\n    t.categories,\r\n    t.`open` AS is_open,\r\n    t.hours,\r\n    t.attributes\r\nFROM dfs.`/Users/vince/Desktop/data/yelp/yelp_academic_dataset_business.json` AS t\r\n{code}\r\n\r\n{quote}\r\n| business   | AnqXhbdkHHimMBR3UMS1Jw | Homewood Suites | []            | 2001 E Highland Avenue\r\nPhoenix, AZ 85016 | Phoenix    | AZ         | 33.505135  | -112.03801 | 3.5        |  |\r\njava.lang.AssertionError\r\n\tat org.apache.drill.exec.vector.VarCharVector$Accessor.get(VarCharVector.java:367)\r\n\tat org.apache.drill.exec.vector.VarCharVector$Accessor.getObject(VarCharVector.java:393)\r\n\tat org.apache.drill.exec.vector.NullableVarCharVector$Accessor.getObject(NullableVarCharVector.java:381)\r\n\tat org.apache.drill.exec.vector.accessor.NullableVarCharAccessor.getObject(NullableVarCharAccessor.java:98)\r\n\tat org.apache.drill.jdbc.AvaticaDrillSqlAccessor.getObject(AvaticaDrillSqlAccessor.java:136)\r\n\tat net.hydromatic.avatica.AvaticaResultSet.getObject(AvaticaResultSet.java:351)\r\n\tat sqlline.SqlLine$Rows$Row.<init>(SqlLine.java:2388)\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2504)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n{quote}",
        "Enhance Drill to support Heterogeneous fields "
    ],
    [
        "DRILL-1723",
        "DRILL-1591",
        "Drill Web UI doesn't work without Internet access It utilizes JavaScript libraries that are loaded from CDNs. The Drill Web UI needs to work even without Internet access. The required JavaScript libraries should be shipped with the release locally.",
        "Drill Web UI Page Source Has Links To External Sites The Drill web UI page source has links to external sites that can break the functionality on clusters that do not have access to the internet.\r\n<link href=\"http://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css\" rel=\"stylesheet\">\r\n<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js\"></script>\r\n<script src=\"http://netdna.bootstrapcdn.com/bootstrap/3.1.1/js/bootstrap.min.js\"></script>\r\nAn advisable thing would be to put the .js and .css files on the web server itself and not link them to external servers."
    ],
    [
        "DRILL-1731",
        "DRILL-1257",
        "Exception in a query on Yelp business data 0: jdbc:drill:zk=localhost:2181> SELECT * FROM dfs.root.`Users/tshiran/Development/demo/data/yelp/business.json` ORDER BY review_count DESC;\r\n+-------------+--------------+------------+------------+------------+------------+--------------+------------+------------+------------+------------+------------+------------+------------+---------------+\r\n| business_id | full_address |   hours    |    open    | categories |    city    | review_count |    name    | longitude  |   state    |   stars    |  latitude  | attributes |    type    | neighborhoods |\r\n+-------------+--------------+------------+------------+------------+------------+--------------+------------+------------+------------+------------+------------+------------+------------+---------------+\r\nQuery failed: Failure while running fragment., Attempted to close accountor with 2133 buffer(s) still allocatedfor QueryId: 25d74fce-e33a-4024-9e69-ff74a4a71b08, MajorFragmentId: 0, MinorFragmentId: 0.\r\n\r\n\r\n\tTotal 112 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 368 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 112 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 14 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 56 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 112 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 184 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 7 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 368 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 14 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 48 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 56 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 16 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 65536, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.allocate(SingleListWriter.java:116)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 184 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 24 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.Float8Vector.allocateNewSafe(Float8Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableFloat8WriterImpl.allocate(NullableFloat8WriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 33 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 48 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 16 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.allocate(SingleListWriter.java:116)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 56 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 48 allocation(s) of byte size(s): 65536, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 65536, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 10 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 24 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableFloat8WriterImpl.allocate(NullableFloat8WriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 7 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 6 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 2 allocation(s) of byte size(s): 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:228)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.varChar(SingleListWriter.java:659)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:310)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:385)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:222)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 6 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 16 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:228)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.allocate(SingleListWriter.java:116)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 33 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 7 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 10 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 6 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 14 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 2 allocation(s) of byte size(s): 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.varChar(SingleListWriter.java:659)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:310)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:385)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:222)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 3 allocation(s) of byte size(s): 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.Float8Vector.allocateNewSafe(Float8Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.float8(SingleMapWriter.java:366)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:265)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 3 allocation(s) of byte size(s): 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.float8(SingleMapWriter.java:366)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:265)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 2 allocation(s) of byte size(s): 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.varChar(SingleListWriter.java:659)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:310)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:385)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:222)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n [ e35a1281-e629-4f59-8ae5-9f4af49a14ea on 172.19.128.211:31010 ]\r\n\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n",
        "Handle schema mutations within a batch Drill is intended to provide flexibility for a wide variety of querying use cases, including those against file formats with changing schema such as JSON. Currently Drill does not have an efficient means of representing these cases, as a constant schema is tied to a record batch, which is best used to represent a few hundred or thousand records. We have have also not designed all of the operators and functions to gracefully handle changes in schema throughout the course of a query."
    ],
    [
        "DRILL-1731",
        "DRILL-1594",
        "Exception in a query on Yelp business data 0: jdbc:drill:zk=localhost:2181> SELECT * FROM dfs.root.`Users/tshiran/Development/demo/data/yelp/business.json` ORDER BY review_count DESC;\r\n+-------------+--------------+------------+------------+------------+------------+--------------+------------+------------+------------+------------+------------+------------+------------+---------------+\r\n| business_id | full_address |   hours    |    open    | categories |    city    | review_count |    name    | longitude  |   state    |   stars    |  latitude  | attributes |    type    | neighborhoods |\r\n+-------------+--------------+------------+------------+------------+------------+--------------+------------+------------+------------+------------+------------+------------+------------+---------------+\r\nQuery failed: Failure while running fragment., Attempted to close accountor with 2133 buffer(s) still allocatedfor QueryId: 25d74fce-e33a-4024-9e69-ff74a4a71b08, MajorFragmentId: 0, MinorFragmentId: 0.\r\n\r\n\r\n\tTotal 112 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 368 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 112 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 14 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 56 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 112 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 184 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 7 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 368 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 14 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 48 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 56 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 16 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 65536, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.allocate(SingleListWriter.java:116)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 184 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 24 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.Float8Vector.allocateNewSafe(Float8Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableFloat8WriterImpl.allocate(NullableFloat8WriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 33 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 48 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 16 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.allocate(SingleListWriter.java:116)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 56 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 48 allocation(s) of byte size(s): 65536, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 65536, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 10 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 24 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableFloat8WriterImpl.allocate(NullableFloat8WriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 7 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 6 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 2 allocation(s) of byte size(s): 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:228)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.varChar(SingleListWriter.java:659)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:310)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:385)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:222)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 6 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 16 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:228)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.allocate(SingleListWriter.java:116)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 33 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 7 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 10 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 6 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 14 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 2 allocation(s) of byte size(s): 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.varChar(SingleListWriter.java:659)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:310)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:385)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:222)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 3 allocation(s) of byte size(s): 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.Float8Vector.allocateNewSafe(Float8Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.float8(SingleMapWriter.java:366)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:265)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 3 allocation(s) of byte size(s): 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.float8(SingleMapWriter.java:366)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:265)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 2 allocation(s) of byte size(s): 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.varChar(SingleListWriter.java:659)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:310)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:385)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:222)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n [ e35a1281-e629-4f59-8ae5-9f4af49a14ea on 172.19.128.211:31010 ]\r\n\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n",
        "SQL exception when querying JSON I can reproduce this with many queries, but narrowing down, it may actually have to do with the number of rows that are being retrieved or a specific record.\r\nI tried a query as follows (i.e limit 10034). It returns 10033 rows and then fails with an exception. A modified limit 10033 does succeed.\r\n\r\n select * from dfs.`/users/nrentachintala/Downloads/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_business.json` b  limit 10034;\r\n-------------------\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while trying to get next result batch.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n\r\nI enabled text mode using alter system set `store.json.all_text_mode`=true;\r\nThe same error occurs with this configuration as well.\r\n\r\nHere is another example where it fails.\r\nselect * from dfs.`/users/nrentachintala/Downloads/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_business.json` b where review_count > 1000;"
    ],
    [
        "DRILL-1731",
        "DRILL-1602",
        "Exception in a query on Yelp business data 0: jdbc:drill:zk=localhost:2181> SELECT * FROM dfs.root.`Users/tshiran/Development/demo/data/yelp/business.json` ORDER BY review_count DESC;\r\n+-------------+--------------+------------+------------+------------+------------+--------------+------------+------------+------------+------------+------------+------------+------------+---------------+\r\n| business_id | full_address |   hours    |    open    | categories |    city    | review_count |    name    | longitude  |   state    |   stars    |  latitude  | attributes |    type    | neighborhoods |\r\n+-------------+--------------+------------+------------+------------+------------+--------------+------------+------------+------------+------------+------------+------------+------------+---------------+\r\nQuery failed: Failure while running fragment., Attempted to close accountor with 2133 buffer(s) still allocatedfor QueryId: 25d74fce-e33a-4024-9e69-ff74a4a71b08, MajorFragmentId: 0, MinorFragmentId: 0.\r\n\r\n\r\n\tTotal 112 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 368 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 112 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 14 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 56 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 112 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 184 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 7 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 368 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 14 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 48 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 56 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 16 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 65536, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.allocate(SingleListWriter.java:116)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 184 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 24 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.Float8Vector.allocateNewSafe(Float8Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableFloat8WriterImpl.allocate(NullableFloat8WriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 33 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 48 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 16 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.allocate(SingleListWriter.java:116)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 56 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 48 allocation(s) of byte size(s): 65536, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 65536, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 10 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 24 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableFloat8WriterImpl.allocate(NullableFloat8WriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 7 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 6 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 2 allocation(s) of byte size(s): 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:228)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.varChar(SingleListWriter.java:659)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:310)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:385)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:222)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 6 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 16 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:228)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.allocate(SingleListWriter.java:116)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 33 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 7 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 10 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 6 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 14 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 2 allocation(s) of byte size(s): 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.varChar(SingleListWriter.java:659)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:310)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:385)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:222)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 3 allocation(s) of byte size(s): 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.Float8Vector.allocateNewSafe(Float8Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.float8(SingleMapWriter.java:366)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:265)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 3 allocation(s) of byte size(s): 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.float8(SingleMapWriter.java:366)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:265)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 2 allocation(s) of byte size(s): 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.varChar(SingleListWriter.java:659)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:310)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:385)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:222)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n [ e35a1281-e629-4f59-8ae5-9f4af49a14ea on 172.19.128.211:31010 ]\r\n\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n",
        "Enhance Drill to support Heterogeneous fields "
    ],
    [
        "DRILL-1731",
        "DRILL-1714",
        "Exception in a query on Yelp business data 0: jdbc:drill:zk=localhost:2181> SELECT * FROM dfs.root.`Users/tshiran/Development/demo/data/yelp/business.json` ORDER BY review_count DESC;\r\n+-------------+--------------+------------+------------+------------+------------+--------------+------------+------------+------------+------------+------------+------------+------------+---------------+\r\n| business_id | full_address |   hours    |    open    | categories |    city    | review_count |    name    | longitude  |   state    |   stars    |  latitude  | attributes |    type    | neighborhoods |\r\n+-------------+--------------+------------+------------+------------+------------+--------------+------------+------------+------------+------------+------------+------------+------------+---------------+\r\nQuery failed: Failure while running fragment., Attempted to close accountor with 2133 buffer(s) still allocatedfor QueryId: 25d74fce-e33a-4024-9e69-ff74a4a71b08, MajorFragmentId: 0, MinorFragmentId: 0.\r\n\r\n\r\n\tTotal 112 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 368 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 112 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 14 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 56 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 112 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 184 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 7 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 368 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 14 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 48 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 56 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 16 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 65536, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.allocate(SingleListWriter.java:116)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 184 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 24 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.Float8Vector.allocateNewSafe(Float8Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableFloat8WriterImpl.allocate(NullableFloat8WriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 33 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 48 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 16 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.allocate(SingleListWriter.java:116)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 56 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 48 allocation(s) of byte size(s): 65536, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 65536, 32768, 32768, 32768, 65536, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableVarCharWriterImpl.allocate(NullableVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 10 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 24 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableFloat8WriterImpl.allocate(NullableFloat8WriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 7 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 6 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 2 allocation(s) of byte size(s): 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:228)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.varChar(SingleListWriter.java:659)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:310)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:385)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:222)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 13 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 6 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 16 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:228)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.allocate(SingleListWriter.java:116)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 33 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 7 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:170)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 10 allocation(s) of byte size(s): 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:238)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 6 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBigIntWriterImpl.allocate(NullableBigIntWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 8 allocation(s) of byte size(s): 512, 512, 512, 512, 512, 512, 512, 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.NullableBitWriterImpl.allocate(NullableBitWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:137)\r\n\t\torg.apache.drill.exec.vector.complex.impl.VectorContainerWriter.allocate(VectorContainerWriter.java:88)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:102)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:252)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:100)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n\t\torg.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n\t\torg.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:122)\r\n\t\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:113)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 14 allocation(s) of byte size(s): 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.NullableVarCharVector.allocateNewSafe(NullableVarCharVector.java:169)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.varChar(SingleMapWriter.java:546)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:305)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:277)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:225)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 2 allocation(s) of byte size(s): 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:314)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.varChar(SingleListWriter.java:659)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:310)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:385)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:222)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 3 allocation(s) of byte size(s): 32768, 32768, 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.Float8Vector.allocateNewSafe(Float8Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.float8(SingleMapWriter.java:366)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:265)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 32768, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 3 allocation(s) of byte size(s): 4096, 4096, 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableFloat8Vector.allocateNewSafe(NullableFloat8Vector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.float8(SingleMapWriter.java:366)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:265)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 512, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.BitVector.allocateNewSafe(BitVector.java:88)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:172)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bigInt(SingleMapWriter.java:336)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:274)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 1 allocation(s) of byte size(s): 4096, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.NullableBitVector.allocateNewSafe(NullableBitVector.java:173)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleMapWriter.bit(SingleMapWriter.java:576)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:248)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\tTotal 2 allocation(s) of byte size(s): 16384, 16384, at stack location:\r\n\t\torg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n\t\torg.apache.drill.exec.vector.UInt4Vector.allocateNewSafe(UInt4Vector.java:137)\r\n\t\torg.apache.drill.exec.vector.VarCharVector.allocateNewSafe(VarCharVector.java:320)\r\n\t\torg.apache.drill.exec.vector.RepeatedVarCharVector.allocateNewSafe(RepeatedVarCharVector.java:230)\r\n\t\torg.apache.drill.exec.vector.complex.impl.RepeatedVarCharWriterImpl.allocate(RepeatedVarCharWriterImpl.java:113)\r\n\t\torg.apache.drill.exec.vector.complex.impl.SingleListWriter.varChar(SingleListWriter.java:659)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.handleString(JsonReader.java:310)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:385)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:222)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:149)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:130)\r\n\t\torg.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:73)\r\n\t\torg.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:191)\r\n\t\torg.apache.drill.exec.physical.impl.ScanBatch.buildSchema(ScanBatch.java:125)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.buildSchema(ExternalSortBatch.java:202)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.buildSchema(RemovingRecordBatch.java:64)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema(ProjectRecordBatch.java:269)\r\n\t\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema(IteratorValidatorBatchIterator.java:80)\r\n\t\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema(ScreenCreator.java:95)\r\n\t\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:111)\r\n\t\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:249)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tjava.lang.Thread.run(Thread.java:745)\r\n [ e35a1281-e629-4f59-8ae5-9f4af49a14ea on 172.19.128.211:31010 ]\r\n\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n",
        "query fails with AssertionError when querying Yelp business data The query:\r\n\r\n{code:sql}\r\nSELECT\r\n    t.type,\r\n    t.business_id,\r\n    t.name,\r\n    t.neighborhoods,\r\n    t.full_address,\r\n    t.city,\r\n    t.state,\r\n    CAST(t.latitude AS FLOAT) as latitude,\r\n    CAST(t.longitude AS FLOAT) as longitude,\r\n    CAST(t.stars as FLOAT) as stars,\r\n    CAST(t.review_count as INT) as review_count,\r\n    t.categories,\r\n    t.`open` AS is_open,\r\n    t.hours,\r\n    t.attributes\r\nFROM dfs.`/Users/vince/Desktop/data/yelp/yelp_academic_dataset_business.json` AS t\r\n{code}\r\n\r\n{quote}\r\n| business   | AnqXhbdkHHimMBR3UMS1Jw | Homewood Suites | []            | 2001 E Highland Avenue\r\nPhoenix, AZ 85016 | Phoenix    | AZ         | 33.505135  | -112.03801 | 3.5        |  |\r\njava.lang.AssertionError\r\n\tat org.apache.drill.exec.vector.VarCharVector$Accessor.get(VarCharVector.java:367)\r\n\tat org.apache.drill.exec.vector.VarCharVector$Accessor.getObject(VarCharVector.java:393)\r\n\tat org.apache.drill.exec.vector.NullableVarCharVector$Accessor.getObject(NullableVarCharVector.java:381)\r\n\tat org.apache.drill.exec.vector.accessor.NullableVarCharAccessor.getObject(NullableVarCharAccessor.java:98)\r\n\tat org.apache.drill.jdbc.AvaticaDrillSqlAccessor.getObject(AvaticaDrillSqlAccessor.java:136)\r\n\tat net.hydromatic.avatica.AvaticaResultSet.getObject(AvaticaResultSet.java:351)\r\n\tat sqlline.SqlLine$Rows$Row.<init>(SqlLine.java:2388)\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2504)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n{quote}"
    ],
    [
        "DRILL-1740",
        "DRILL-1373",
        "Web UI returns HTTP 500 with no explanation When submitting a SQL query through the Web UI I get this error with no explanation on how to troubleshoot. I'm not sure whether the Web UI query tool is broken - it may be, as I am able to submit the same query through sqlline (Drill is in distributed mode, so it's the same drillbit). There has to be an actionable explanation/error message or at least a pointer to the relevant log file.\r\n\r\nHTTP ERROR 500\r\n\r\nProblem accessing /query. Reason:\r\n\r\n    Request failed.\r\n\r\n",
        "Executing a query with a semi-colon as the last character causes 500 error in the query GUI Attempting to execute a query on the Query GUI page (http://node:8047/query) that contains a semi-colon at the end of the query statement results in a 500 error.\r\n{noformat}\r\nselect count(*) from storageplugin.workspace;\r\n{noformat}\r\nRemoving the semi-colon returns the query result successfully."
    ],
    [
        "DRILL-1740",
        "DRILL-1515",
        "Web UI returns HTTP 500 with no explanation When submitting a SQL query through the Web UI I get this error with no explanation on how to troubleshoot. I'm not sure whether the Web UI query tool is broken - it may be, as I am able to submit the same query through sqlline (Drill is in distributed mode, so it's the same drillbit). There has to be an actionable explanation/error message or at least a pointer to the relevant log file.\r\n\r\nHTTP ERROR 500\r\n\r\nProblem accessing /query. Reason:\r\n\r\n    Request failed.\r\n\r\n",
        "Rest/Web interface hangs if there is any error/exception in query execution Web interface hangs if there is any error/exception in query. We should have a proper error handling mechanism in place."
    ],
    [
        "DRILL-1772",
        "DRILL-1765",
        "Updates to Foreman logic caused regression in error propogation ",
        "Exceptions are not propagated properly to the client anymore When running the following query:\r\n{code}\r\nshow databases.;\r\n{code}\r\nIn sqlline we used to get the following error message:\r\n{code}\r\nQuery failed: Failure while parsing sql. Encountered \".\" at line 1, column 15.\r\nWas expecting one of:\r\n    <EOF>\r\n    \"LIKE\" ...\r\n    \"WHERE\" ...\r\n     [8417fee2-88fc-4427-ac26-78af1f047be7]\r\n\r\nError: exception while executing query: Failure while trying to get next result\r\nbatch. (state=,code=0)\r\n{code}\r\nBut now we only get:\r\n{code}\r\nQuery failed: Query failed.\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}"
    ],
    [
        "DRILL-1794",
        "DRILL-1545",
        "Can not make files with extension \"log\" to be recognized as json format? If we want to use \".log\" as the file extension, and also want it to be recognized as json format, I tried to use below storage engine , but failed to read the .log file..\r\n\r\n\r\n{code}\r\n  \"formats\": {\r\n    \"log\": {\r\n      \"type\": \"json\"\r\n    },\r\n    \"csv\": {\r\n      \"type\": \"text\",\r\n      \"extensions\": [\r\n        \"csv\"\r\n      ],\r\n      \"delimiter\": \",\"\r\n    }\r\n  }\r\n{code} \r\n\r\n\r\n{code}\r\n0: jdbc:drill:zk=n1a:5181,n2a:5181,n3a:5181> select * from  logtest.`test.json`;\r\n+------------+------------+------------+------------+\r\n|   field1   |   field2   |   field3   |   field4   |\r\n+------------+------------+------------+------------+\r\n| data1      | 100.0      | more data1 | 123.001    |\r\n+------------+------------+------------+------------+\r\n1 row selected (0.159 seconds)\r\n0: jdbc:drill:zk=n1a:5181,n2a:5181,n3a:5181> select * from  logtest.`test.log`;\r\nQuery failed: Failure while validating sql : org.eigenbase.util.EigenbaseContextException: From line 1, column 16 to line 1, column 22: Table 'logtest.test.log' not found\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nDo we support above requirement?\r\nIf so, what is the storage plugin text?",
        "Json files can only be read when they have a .json extension It seems that Drill can only discover json data if the file extension is .json.   \r\n\r\nWe have tried to add the file extension.log as type json in the Storage Plugin (and validated the json) , but without success. \r\n\r\nWould be great if somebody can share a example config or has an idea.\r\n\r\nStorage Plugin Configuration.\r\n{\r\n  \"type\": \"file\",\r\n  \"enabled\": true,\r\n  \"connection\": \"maprfs:///\",\r\n  \"workspaces\": {\r\n    \"root\": {\r\n      \"location\": \"/\",\r\n      \"writable\": false,\r\n      \"storageformat\": null\r\n    },\r\n    \"tmp\": {\r\n      \"location\": \"/tmp\",\r\n      \"writable\": true,\r\n      \"storageformat\": \"csv\"\r\n    }\r\n  },\r\n  \"formats\": {\r\n    \"log\": {\r\n      \"type\": \"json\",\r\n      \"extensions\": [\r\n        \"log\"\r\n      ]\r\n    },\r\n    \"psv\": {\r\n      \"type\": \"text\",\r\n      \"extensions\": [\r\n        \"tbl\"\r\n      ],\r\n      \"delimiter\": \"|\"\r\n    },\r\n    \"csv\": {\r\n      \"type\": \"text\",\r\n      \"extensions\": [\r\n        \"csv\"\r\n      ],\r\n      \"delimiter\": \",\"\r\n    },\r\n    \"tsv\": {\r\n      \"type\": \"text\",\r\n      \"extensions\": [\r\n        \"tsv\"\r\n      ],\r\n      \"delimiter\": \"\\t\"\r\n    },\r\n    \"parquet\": {\r\n      \"type\": \"parquet\"\r\n    },\r\n    \"json\": {\r\n      \"type\": \"json\"\r\n}\r\n  }\r\n}"
    ],
    [
        "DRILL-1794",
        "DRILL-1546",
        "Can not make files with extension \"log\" to be recognized as json format? If we want to use \".log\" as the file extension, and also want it to be recognized as json format, I tried to use below storage engine , but failed to read the .log file..\r\n\r\n\r\n{code}\r\n  \"formats\": {\r\n    \"log\": {\r\n      \"type\": \"json\"\r\n    },\r\n    \"csv\": {\r\n      \"type\": \"text\",\r\n      \"extensions\": [\r\n        \"csv\"\r\n      ],\r\n      \"delimiter\": \",\"\r\n    }\r\n  }\r\n{code} \r\n\r\n\r\n{code}\r\n0: jdbc:drill:zk=n1a:5181,n2a:5181,n3a:5181> select * from  logtest.`test.json`;\r\n+------------+------------+------------+------------+\r\n|   field1   |   field2   |   field3   |   field4   |\r\n+------------+------------+------------+------------+\r\n| data1      | 100.0      | more data1 | 123.001    |\r\n+------------+------------+------------+------------+\r\n1 row selected (0.159 seconds)\r\n0: jdbc:drill:zk=n1a:5181,n2a:5181,n3a:5181> select * from  logtest.`test.log`;\r\nQuery failed: Failure while validating sql : org.eigenbase.util.EigenbaseContextException: From line 1, column 16 to line 1, column 22: Table 'logtest.test.log' not found\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nDo we support above requirement?\r\nIf so, what is the storage plugin text?",
        "Support specifying extensions for JSON/Parquet data format JSON data format configuration does not let one to specify file extensions. This limitation prevents us to analyze JSON data that resides in files with no *.json extension. This issue is to add support for specifying non json extensions. Note that parquet data format suffers from the same limitation as well."
    ],
    [
        "DRILL-1876",
        "DRILL-779",
        "Implement concatenation operator ( || ) It is documented, but it does not work at all (DRILL-779) \r\nWorkaround exists: concat function\r\n\r\nAs of version 0.7.0 (git.commit.id.abbrev=3b0ff5d)\r\n\r\n(1)\r\n0: jdbc:drill:schema=dfs> select 'abc' || 'def' from cp.`tpch/nation.parquet`;\r\nQuery failed: Query failed: Failure while running fragment.[ dc4727ed-4ac3-42bb-a60b-f8f370dd4fcf on atsqa4-133.qa.lab:31010 ]\r\n[ dc4727ed-4ac3-42bb-a60b-f8f370dd4fcf on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n(2)\r\n0: jdbc:drill:schema=dfs> select n_name || 'xyz' from cp.`tpch/nation.parquet`;\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: Internal error: pre-condition failed: SqlTypeUtil.sameNamedType(argTypes[0], argTypes[1])\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n(3)\r\n0: jdbc:drill:schema=dfs> select cast(n_nationkey as varchar(10)) || n_name from cp.`tpch/nation.parquet`;\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: Internal error: pre-condition failed: SqlTypeUtil.sameNamedType(argTypes[0], argTypes[1])\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n(4)\r\n0: jdbc:drill:schema=dfs> select 'abc' || 'cde' from `test.json`;\r\nQuery failed: Query failed: Failure while running fragment.[ 01194cdf-746c-45db-ae4e-014590bfbffd on atsqa4-133.qa.lab:31010 ]\r\n[ 01194cdf-746c-45db-ae4e-014590bfbffd on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n",
        "Add support for pipes Concat \"||\" operator git.commit.id.abbrev=70fab8c\r\n\r\n0: jdbc:drill:schema=dfs> select (name || age || contributions || registration) as COL from voter where name like '%young';\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"dd813164-2347-4c49-9480-bc5c6cc401d6\"\r\nendpoint {\r\n  address: \"qa-node64.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < UnsupportedOperationException:[ NULL ]\"\r\n]\r\n\r\nThe data set contains null values; however the result from the above query does not contain any null values.\r\n\r\nStack trace:\r\njava.lang.AssertionError: Internal error: pre-condition failed: SqlTypeUtil.sameNamedType(argTypes[0], argTypes[1])\r\norg.eigenbase.util.Util.newInternal(Util.java:760) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.util.Util.pre(Util.java:833) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.type.ReturnTypes$8.inferReturnType(ReturnTypes.java:540) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.type.SqlTypeTransformCascade.inferReturnType(SqlTypeTransformCascade.java:55) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.inferReturnType(SqlOperator.java:451) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.validateOperands(SqlOperator.java:418) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:480) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:476) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:476) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:476) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:476) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:476) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlAsOperator.deriveType(SqlAsOperator.java:117) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:418) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:3019) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:2772) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:80) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:747) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:736) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlSelect.validate(SqlSelect.java:209) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:710) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:426) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\nnet.hydromatic.optiq.prepare.PlannerImpl.validate(PlannerImpl.java:175) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateNode(DefaultSqlHandler.java:99) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:84) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:134) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:338) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:186) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\njava.lang.Thread.run(Thread.java:744) [na:1.7.0_45]"
    ],
    [
        "DRILL-1877",
        "DRILL-779",
        "Throw unsupported error message for || operator early in the parsing stage If we don't plan to support concat operator (||), we should throw a decent error message early.\r\n\r\nThe error is cryptic:\r\n\r\n0: jdbc:drill:schema=dfs> select cast(l_orderkey as varchar(10)) || '-' || cast(l_partkey as varchar(10)) from cp.`tpch/lineitem.parquet`;\r\nQuery failed: Query stopped., Line 65, Column 29: \"value\" is neither a method, a field, nor a member class of \"org.apache.drill.exec.expr.holders.VarCharHolder\" [ bc85685d-e08a-494b-aa9f-5b3fa4ee4033 on atsqa4-133.qa.lab:3\r\n1010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nI believe that error that I'm getting from the query below is the result of not throwing \"Unsupported\" message earlier.\r\nPlease correct me if I'm wrong.\r\n\r\n0: jdbc:drill:schema=dfs> select\r\n. . . . . . . . . . . . >         cast(l_orderkey as varchar(10)) || '-' || cast(l_partkey as varchar(10))\r\n. . . . . . . . . . . . > from\r\n. . . . . . . . . . . . >         (\r\n. . . . . . . . . . . . >         select  sum(l_quantity),\r\n. . . . . . . . . . . . >                 l_orderkey,\r\n. . . . . . . . . . . . >                 l_partkey\r\n. . . . . . . . . . . . >         from    cp.`tpch/lineitem.parquet`\r\n. . . . . . . . . . . . >         group by\r\n. . . . . . . . . . . . >                 l_orderkey,\r\n. . . . . . . . . . . . >                 l_partkey\r\n. . . . . . . . . . . . >         )\r\n. . . . . . . . . . . . > group by\r\n. . . . . . . . . . . . >         cast(l_orderkey as varchar(10)) || '-' || cast(l_partkey as varchar(10));\r\nQuery failed: Query failed: Failure while running fragment., You tried to do a batch data read operation when you were in a state of STOP.  You can only do this type of operation when you are in a state of OK or OK_NEW_SCHEMA. [ f5cb68a8-7c05-4e02-a104-56081e6d34ce on atsqa4-133.qa.lab:31010 ]\r\n[ f5cb68a8-7c05-4e02-a104-56081e6d34ce on atsqa4-133.qa.lab:31010 ]\r\n",
        "Add support for pipes Concat \"||\" operator git.commit.id.abbrev=70fab8c\r\n\r\n0: jdbc:drill:schema=dfs> select (name || age || contributions || registration) as COL from voter where name like '%young';\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"dd813164-2347-4c49-9480-bc5c6cc401d6\"\r\nendpoint {\r\n  address: \"qa-node64.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < UnsupportedOperationException:[ NULL ]\"\r\n]\r\n\r\nThe data set contains null values; however the result from the above query does not contain any null values.\r\n\r\nStack trace:\r\njava.lang.AssertionError: Internal error: pre-condition failed: SqlTypeUtil.sameNamedType(argTypes[0], argTypes[1])\r\norg.eigenbase.util.Util.newInternal(Util.java:760) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.util.Util.pre(Util.java:833) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.type.ReturnTypes$8.inferReturnType(ReturnTypes.java:540) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.type.SqlTypeTransformCascade.inferReturnType(SqlTypeTransformCascade.java:55) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.inferReturnType(SqlOperator.java:451) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.validateOperands(SqlOperator.java:418) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:480) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:476) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:476) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:476) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:476) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:476) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlAsOperator.deriveType(SqlAsOperator.java:117) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:418) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:3019) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:2772) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:80) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:747) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:736) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlSelect.validate(SqlSelect.java:209) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:710) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:426) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\nnet.hydromatic.optiq.prepare.PlannerImpl.validate(PlannerImpl.java:175) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateNode(DefaultSqlHandler.java:99) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:84) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:134) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:338) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:186) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\njava.lang.Thread.run(Thread.java:744) [na:1.7.0_45]"
    ],
    [
        "DRILL-1877",
        "DRILL-1876",
        "Throw unsupported error message for || operator early in the parsing stage If we don't plan to support concat operator (||), we should throw a decent error message early.\r\n\r\nThe error is cryptic:\r\n\r\n0: jdbc:drill:schema=dfs> select cast(l_orderkey as varchar(10)) || '-' || cast(l_partkey as varchar(10)) from cp.`tpch/lineitem.parquet`;\r\nQuery failed: Query stopped., Line 65, Column 29: \"value\" is neither a method, a field, nor a member class of \"org.apache.drill.exec.expr.holders.VarCharHolder\" [ bc85685d-e08a-494b-aa9f-5b3fa4ee4033 on atsqa4-133.qa.lab:3\r\n1010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nI believe that error that I'm getting from the query below is the result of not throwing \"Unsupported\" message earlier.\r\nPlease correct me if I'm wrong.\r\n\r\n0: jdbc:drill:schema=dfs> select\r\n. . . . . . . . . . . . >         cast(l_orderkey as varchar(10)) || '-' || cast(l_partkey as varchar(10))\r\n. . . . . . . . . . . . > from\r\n. . . . . . . . . . . . >         (\r\n. . . . . . . . . . . . >         select  sum(l_quantity),\r\n. . . . . . . . . . . . >                 l_orderkey,\r\n. . . . . . . . . . . . >                 l_partkey\r\n. . . . . . . . . . . . >         from    cp.`tpch/lineitem.parquet`\r\n. . . . . . . . . . . . >         group by\r\n. . . . . . . . . . . . >                 l_orderkey,\r\n. . . . . . . . . . . . >                 l_partkey\r\n. . . . . . . . . . . . >         )\r\n. . . . . . . . . . . . > group by\r\n. . . . . . . . . . . . >         cast(l_orderkey as varchar(10)) || '-' || cast(l_partkey as varchar(10));\r\nQuery failed: Query failed: Failure while running fragment., You tried to do a batch data read operation when you were in a state of STOP.  You can only do this type of operation when you are in a state of OK or OK_NEW_SCHEMA. [ f5cb68a8-7c05-4e02-a104-56081e6d34ce on atsqa4-133.qa.lab:31010 ]\r\n[ f5cb68a8-7c05-4e02-a104-56081e6d34ce on atsqa4-133.qa.lab:31010 ]\r\n",
        "Implement concatenation operator ( || ) It is documented, but it does not work at all (DRILL-779) \r\nWorkaround exists: concat function\r\n\r\nAs of version 0.7.0 (git.commit.id.abbrev=3b0ff5d)\r\n\r\n(1)\r\n0: jdbc:drill:schema=dfs> select 'abc' || 'def' from cp.`tpch/nation.parquet`;\r\nQuery failed: Query failed: Failure while running fragment.[ dc4727ed-4ac3-42bb-a60b-f8f370dd4fcf on atsqa4-133.qa.lab:31010 ]\r\n[ dc4727ed-4ac3-42bb-a60b-f8f370dd4fcf on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n(2)\r\n0: jdbc:drill:schema=dfs> select n_name || 'xyz' from cp.`tpch/nation.parquet`;\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: Internal error: pre-condition failed: SqlTypeUtil.sameNamedType(argTypes[0], argTypes[1])\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n(3)\r\n0: jdbc:drill:schema=dfs> select cast(n_nationkey as varchar(10)) || n_name from cp.`tpch/nation.parquet`;\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: Internal error: pre-condition failed: SqlTypeUtil.sameNamedType(argTypes[0], argTypes[1])\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n(4)\r\n0: jdbc:drill:schema=dfs> select 'abc' || 'cde' from `test.json`;\r\nQuery failed: Query failed: Failure while running fragment.[ 01194cdf-746c-45db-ae4e-014590bfbffd on atsqa4-133.qa.lab:31010 ]\r\n[ 01194cdf-746c-45db-ae4e-014590bfbffd on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n"
    ],
    [
        "DRILL-1889",
        "DRILL-1778",
        "when 'select *' is used along with an order by on length of a column, Drill is adding the computed length to the list of columns git.commit.id.abbrev=9dfa4a1\r\n\r\nDataset :\r\n{code}\r\n{\r\n \"col1\":1,\r\n \"col2\":\"a\"\r\n}\r\n{\r\n \"col1\":2,\r\n \"col2\":\"b\"\r\n}\r\n{\r\n \"col1\":2,\r\n \"col2\":\"abc\"\r\n}\r\n{code}\r\n\r\nQuery :\r\n{code}\r\n select * from `b.json` order by length(col2);\r\n+------------+------------+------------+\r\n|    col1    |    col2    |   EXPR$1   |\r\n+------------+------------+------------+\r\n| 1          | a          | 1          |\r\n| 2          | b          | 1          |\r\n| 2          | abc        | 3          |\r\n+------------+------------+------------+\r\n{code}\r\n\r\nDrill adds the length column. (EXPR$1) Not sure if this is intended behavior since postgres does not do this",
        "expressions along with * in the select-clause makes drill return duplicated columns *. The cases the operators pass through \r\n1. select *, upper(TABLE_NAME) from INFORMATION_SCHEMA.`TABLES`;\r\n2. select upper(TABLE_NAME), * from INFORMATION_SCHEMA.`TABLES`;\r\n\r\n*. The cases the operators fail\r\n1. select upper(first_name),* from cp.`employee.json` limit 2\r\n=> upper(first_name) was repeated \"twice\". \r\n2. select *, upper(first_name) from cp.`employee.json` limit 2\r\n=> Crashed JVM entirely"
    ],
    [
        "DRILL-1903",
        "DRILL-1784",
        "Boolean column can't be used by itself in certain condition in a where clause {code}\r\ngit.commit.id.abbrev=e3ab2c1\r\n{code}  \r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from test;\r\n+------------+------------+------------+\r\n|     a1     |     b1     |     c1     |\r\n+------------+------------+------------+\r\n| 0          | 0          | true       |\r\n| 0          | 0          | false      |\r\n| 0          | 0          | false      |\r\n| 1          | 1          | true       |\r\n| 1          | 1          | true       |\r\n+------------+------------+------------+\r\n5 rows selected (0.067 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from test where  c1;\r\nQuery failed: Query failed: Failure validating SQL. org.eigenbase.util.EigenbaseContextException: From line 1, column 27 to line 1, column 28: WHERE clause must be a condition\r\n{code}\r\n\r\nHowever, all the cases below seem to work correctly:\r\n{code}0: jdbc:drill:schema=dfs> select * from test where 1=1 and c1;\r\n+------------+------------+------------+\r\n|     a1     |     b1     |     c1     |\r\n+------------+------------+------------+\r\n| 0          | 0          | true       |\r\n| 1          | 1          | true       |\r\n| 1          | 1          | true       |\r\n+------------+------------+------------+\r\n3 rows selected (0.085 seconds)\r\n0: jdbc:drill:schema=dfs> select * from test where c1 and 1=1;\r\n+------------+------------+------------+\r\n|     a1     |     b1     |     c1     |\r\n+------------+------------+------------+\r\n| 0          | 0          | true       |\r\n| 1          | 1          | true       |\r\n| 1          | 1          | true       |\r\n+------------+------------+------------+\r\n3 rows selected (0.079 seconds)\r\n0: jdbc:drill:schema=dfs> select * from test where not c1 and 1=1;\r\n+------------+------------+------------+\r\n|     a1     |     b1     |     c1     |\r\n+------------+------------+------------+\r\n| 0          | 0          | false      |\r\n| 0          | 0          | false      |\r\n+------------+------------+------------+\r\n2 rows selected (0.095 seconds)\r\n0: jdbc:drill:schema=dfs> select * from test where not c1;\r\n+------------+------------+------------+\r\n|     a1     |     b1     |     c1     |\r\n+------------+------------+------------+\r\n| 0          | 0          | false      |\r\n| 0          | 0          | false      |\r\n+------------+------------+------------+\r\n2 rows selected (0.108 seconds)\r\n\r\n\r\n0: jdbc:drill:schema=dfs> select * from test where c1 is true;\r\n+------------+------------+------------+\r\n|     a1     |     b1     |     c1     |\r\n+------------+------------+------------+\r\n| 0          | 0          | true       |\r\n| 1          | 1          | true       |\r\n| 1          | 1          | true       |\r\n+------------+------------+------------+\r\n3 rows selected (0.093 seconds)\r\n0: jdbc:drill:schema=dfs> select * from test where c1 is false;\r\n+------------+------------+------------+\r\n|     a1     |     b1     |     c1     |\r\n+------------+------------+------------+\r\n| 0          | 0          | false      |\r\n| 0          | 0          | false      |\r\n+------------+------------+------------+\r\n2 rows selected (0.087 seconds)\r\n\r\n{code}",
        "Ignore boolean type enforcement on filter conditions during validation The title should be self describing. To give some more context on this, it would be nice if we stop boolean type enforcement on filter conditions as it is possible to create a scenario where we don't have a concrete return type but later bind it during execution. Currently we will need to `cast` condition to boolean explicitly. This does not reflect the flexibility of execution engine."
    ],
    [
        "DRILL-1919",
        "DRILL-1918",
        "Running DRILL with JRE results in failure with ambiguous errors A user reported at [MapR Answers Forum|http://answers.mapr.com/questions/161911/apache-drill-07-errors.html] that SQLLine hangs and the Drillbit log contains the following error\r\n\r\n{noformat}\r\n2014-12-30 20:34:03,060 [WorkManager-1] ERROR o.apache.drill.exec.work.WorkManager\r\n - Failure while running wrapper [Foreman: 2b5cf3c7-7705-4a18-3457-2d80f9800eb6] java.lang.NullPointerException: null\r\n at org.apache.drill.exec.work.foreman.Foreman.cancelExecutingFragments(Foreman.java:483) ~[drill-java-exec-0.7.0-SNAPSHOT-rebuffed.jar:0.7.0-SNAPSHOT] \r\n at org.apache.drill.exec.work.foreman.Foreman.moveToState(Foreman.java:436) ~[drill-java-exec-0.7.0-SNAPSHOT-rebuffed.jar:0.7.0-SNAPSHOT] \r\n at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:194) ~[drill-java-exec-0.7.0-SNAPSHOT-rebuffed.jar:0.7.0-SNAPSHOT] \r\n at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[drill-java-exec-0.7.0-SNAPSHOT-rebuffed.jar:0.7.0-SNAPSHOT] \r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_71] \r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71] \r\n at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]\r\n{noformat}\r\n\r\nThis happens if the JDK compiler can not be created during query execution, which results in {{org.apache.drill.exec.work.foreman.Foreman.rootRunner}} not getting initialized.",
        "Drill does very obscure things when a JRE is used instead of a JDK. In http://answers.mapr.com/questions/161911/apache-drill-07-errors.html#comment-161933 a user describes the consequences of running Drill with a JRE instead of a JDK.  \r\n\r\nSurely this could be detected and this confusion could be avoided.\r\n\r\n"
    ],
    [
        "DRILL-1937",
        "DRILL-1936",
        "Throw exception and give error message when Non-scalar sub-query used in an expression {code}\r\n#Fri Jan 02 21:20:47 EST 2015\r\ngit.commit.id.abbrev=b491cdb\r\n{code}\r\n\r\nIt is dangerous to have an internal function be exposed to users.\r\nWhat if one day user decided to write a UDF with the same signature ?\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select SINGLE_VALUE(1) from `t.json`;\r\n+--+\r\n|  |\r\n+--+\r\n+--+\r\nNo rows selected (0.111 seconds)\r\n{code}",
        "Throw an error if subquery in the where clause does not return scalar result {code}\r\n#Fri Jan 02 21:20:47 EST 2015\r\ngit.commit.id.abbrev=b491cdb\r\n{code}\r\n\r\nWhen result of a subquery is non scalar (regardless of if it is correlated or not) we should throw  an error either during planning time or during runtime when we know cardinality of the result set.\r\n\r\nCurrently, queries either fail to plan:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from cp.`tpch/nation.parquet`  where n_name = ( select r_name from cp.`tpch/region.parquet` where n_regionkey = r_regionkey);\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: Node [rel#24659:Subset#7.LOGICAL.ANY([]).[]] could not be implemented; planner state:\r\n\r\nRoot: rel#24659:Subset#7.LOGICAL.ANY([]).[]\r\nOriginal rel:\r\nAbstractConverter(subset=[rel#24659:Subset#7.LOGICAL.ANY([]).[]], convention=[LOGICAL], DrillDistributionTraitDef=[ANY([])], sort=[[]]): rowcount = 1.7976931348623157E308, cumulative cost = {inf}, id = 24660\r\n  ProjectRel(subset=[rel#24658:Subset#7.NONE.ANY([]).[]], *=[$0]): rowcount = 1.7976931348623157E308, cumulative cost = {1.7976931348623157E308 rows, 1.7976931348623157E308 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 24657\r\n    FilterRel(subset=[rel#24656:Subset#6.NONE.ANY([]).[]], condition=[=($1, $2)]): rowcount = 2.6965397022934733E307, cumulative cost = {2.6965397022934733E307 rows, 1.7976931348623157E308 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 24655\r\n      JoinRel(subset=[rel#24654:Subset#5.NONE.ANY([]).[]], condition=[true], joinType=[left]): rowcount = 1.7976931348623157E308, cumulative cost = {1.7976931348623157E308 rows, 0.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 24653\r\n        EnumerableTableAccessRel(subset=[rel#24645:Subset#0.ENUMERABLE.ANY([]).[]], table=[[cp, tpch/nation.parquet]]): rowcount = 100.0, cumulative cost = {100.0 rows, 101.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 24623\r\n        AggregateRel(subset=[rel#24652:Subset#4.NONE.ANY([]).[]], group=[{}], agg#0=[SINGLE_VALUE($0)]): rowcount = 1.7976931348623158E307, cumulative cost = {1.7976931348623158E307 rows, 0.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 24651\r\n          ProjectRel(subset=[rel#24650:Subset#3.NONE.ANY([]).[]], r_name=[$3]): rowcount = 1.7976931348623157E308, cumulative cost = {1.7976931348623157E308 rows, 1.7976931348623157E308 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 24649\r\n            FilterRel(subset=[rel#24648:Subset#2.NONE.ANY([]).[]], condition=[=($1, $2)]): rowcount = 15.0, cumulative cost = {15.0 rows, 100.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 24647\r\n              EnumerableTableAccessRel(subset=[rel#24646:Subset#1.ENUMERABLE.ANY([]).[]], table=[[cp, tpch/region.parquet]]): rowcount = 100.0, cumulative cost = {100.0 rows, 101.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 24624\r\n{code}\r\n\r\nor return strange error messages that are difficult to decipher:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select  a.emp_num,\r\n. . . . . . . . . . . . >         a.emp_name\r\n. . . . . . . . . . . . > from    `emp1.json` as a\r\n. . . . . . . . . . . . > where   a.salary > (\r\n. . . . . . . . . . . . >                 select  b.salary\r\n. . . . . . . . . . . . >                 from    `emp1.json` b\r\n. . . . . . . . . . . . >                 where   b.dept = a.dept)\r\n. . . . . . . . . . . . > order by 1;\r\nQuery failed: Query failed: Failure while running fragment., Schema is currently null.  You must call buildSchema(SelectionVectorMode) before this container can return a schema. [ d800ab5d-aa5b-4371-8cb6-819dccca40aa on atsqa4-134.qa.lab:31010 ]\r\n[ d800ab5d-aa5b-4371-8cb6-819dccca40aa on atsqa4-134.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n"
    ],
    [
        "DRILL-1941",
        "DRILL-1302",
        "select from subquery having an order by fails git.commit.id.abbrev=b491cdb\r\n\r\nThe following query fails:\r\n0: jdbc:drill:schema=dfs>  select q.name as name, q.registration as registration from (select cast(student.name as varchar(30)) as name, cast(voter.registration as varchar(20)) as registration from `dfs`.`default`.`voter` voter full outer join `dfs`.`default`.`./student` student on (student.name = voter.name) where student.age < 30 order by student.name) q;\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: -1\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nRunning the same query with \"explain plan for\" also fail with the same error.  If I remove the \"order by student.name\" from the sub-query, the query run successfully.",
        "Order by using table.col in subquery fails with ArrayIndexOutOfBoundsException  git.commit.id.abbrev=687b9b0\r\n\r\nThe following query runs successfully:\r\nselect cast(student.name as varchar(30)) name, cast(voter.registration as varchar(20)) registration from voter full outer join student on (student.name = voter.name) where student.age < 20 order by student.name;\r\n\r\nIf I put the above query in a sub-select, it fails:\r\n0: jdbc:drill:schema=dfs> select tbl.name, tbl.registration from (select cast(student.name as varchar(30)) name, cast(voter.registration as varchar(20)) registration from `dfs`.`default`.`./voter` voter full outer join `dfs`.`default`.`./student` student on (student.name = voter.name) where student.age < 20 order by student.name) tbl;\r\nQuery failed: Failure while parsing sql. -1 [fe42e70f-cf5b-4389-b879-5b1598ab887f]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n\r\nIf I change the query using \"order by name\" without specifying the student table, the query runs fine:\r\n0: jdbc:drill:schema=dfs> select tbl.name, tbl.registration from (select cast(student.name as varchar(30)) name, cast(voter.registration as varchar(20)) registration from `dfs`.`default`.`./voter` voter full outer join `dfs`.`default`.`./student` student on (student.name = voter.name) where student.age < 20 order by student.name) tbl;\r\n\r\nThe student.name in the order by clause should work."
    ],
    [
        "DRILL-1944",
        "DRILL-1829",
        "JsonReader fails to read arrays of size 2000 git.commit.id.abbrev=b491cdb\r\n\r\nI tried a select * on a json file which contained a single array of strings. Drill fails to read the file if the array has 2000 elements. However it works for an array size of 1500. So I am not sure if it is a known limitation or a bug.\r\n\r\nQuery :\r\n{code}\r\njdbc:drill:schema=dfs.drillTestDir> select * from `data-shapes/wide-records/single/wide-record1.json`;\r\nQuery failed: Query stopped., Record was too large to copy into vector. [ 5b320c2f-fff3-4de5-8095-8043c30510fd on qa-node191.qa.lab:31010 ]\r\n{code}\r\n\r\nI attached the data file and the error log. Let me know if you have any questions.",
        "Move to a reallocation rather than rewind strategy for vectors git.commit.id.abbrev=142e577\r\n\r\nI attached 2 files. Both of them have just 1 column. The length of the column in one file is 10000 and in the other it is 100000.\r\n\r\nThe below queries fail to produce any output\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select columns[0] from `data-shapes/100000.tbl`;\r\n+--+\r\n|  |\r\n+--+\r\n+--+\r\nNo rows selected (0.09 seconds)\r\n0: jdbc:drill:schema=dfs.drillTestDir> select count(*) from `data-shapes/100000.tbl`;\r\n+--+\r\n|  |\r\n+--+\r\n+--+\r\nNo rows selected (0.092 seconds)\r\n{code}\r\n\r\nThe same queries work when the column width is 10000.\r\n\r\nLet me know if you need anything."
    ],
    [
        "DRILL-1960",
        "DRILL-1829",
        "Automatically realloc buffers when vector runs out of space When reaching the end of a buffer, the current way drill handles this is for setSafe() to return false, and then whatever operator is doing the write will send the batch, and the redo the last record. This creates a lot of difficulty, because it sometimes requires being able to \"rewind\" the input stream to replay the last record.\r\n\r\nThe proposal is to move the handling of buffer sizing and allocation into the value vectors themselves, making it transparent to the user of the value vector. The operators will now no longer have to worry about the possibility that writing into a vector may fail due to lack of space.",
        "Move to a reallocation rather than rewind strategy for vectors git.commit.id.abbrev=142e577\r\n\r\nI attached 2 files. Both of them have just 1 column. The length of the column in one file is 10000 and in the other it is 100000.\r\n\r\nThe below queries fail to produce any output\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select columns[0] from `data-shapes/100000.tbl`;\r\n+--+\r\n|  |\r\n+--+\r\n+--+\r\nNo rows selected (0.09 seconds)\r\n0: jdbc:drill:schema=dfs.drillTestDir> select count(*) from `data-shapes/100000.tbl`;\r\n+--+\r\n|  |\r\n+--+\r\n+--+\r\nNo rows selected (0.092 seconds)\r\n{code}\r\n\r\nThe same queries work when the column width is 10000.\r\n\r\nLet me know if you need anything."
    ],
    [
        "DRILL-1960",
        "DRILL-1944",
        "Automatically realloc buffers when vector runs out of space When reaching the end of a buffer, the current way drill handles this is for setSafe() to return false, and then whatever operator is doing the write will send the batch, and the redo the last record. This creates a lot of difficulty, because it sometimes requires being able to \"rewind\" the input stream to replay the last record.\r\n\r\nThe proposal is to move the handling of buffer sizing and allocation into the value vectors themselves, making it transparent to the user of the value vector. The operators will now no longer have to worry about the possibility that writing into a vector may fail due to lack of space.",
        "JsonReader fails to read arrays of size 2000 git.commit.id.abbrev=b491cdb\r\n\r\nI tried a select * on a json file which contained a single array of strings. Drill fails to read the file if the array has 2000 elements. However it works for an array size of 1500. So I am not sure if it is a known limitation or a bug.\r\n\r\nQuery :\r\n{code}\r\njdbc:drill:schema=dfs.drillTestDir> select * from `data-shapes/wide-records/single/wide-record1.json`;\r\nQuery failed: Query stopped., Record was too large to copy into vector. [ 5b320c2f-fff3-4de5-8095-8043c30510fd on qa-node191.qa.lab:31010 ]\r\n{code}\r\n\r\nI attached the data file and the error log. Let me know if you have any questions."
    ],
    [
        "DRILL-1978",
        "DRILL-1302",
        "Using order by with a cast function and array index fails to plan. {code}\r\nCREATE TABLE newStar AS \r\n\r\nSELECT \r\n  columns[0], columns[1], columns[2], columns[3], columns[4], columns[5], \r\n  columns[6], columns[7], columns[8], columns[9], columns[10], columns[11], \r\n  columns[12], columns[13], columns[14], columns[15]\r\nFROM dfs.`file.csv` ORDER BY cast(columns[2] as int);\r\n{code}\r\n\r\n\r\n{code}\r\norg.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: Internal error: Error while applying rule ExpandConversionRule, args [rel#98:AbstractConverter.PHYSICAL.ANY([]).[16](child=rel#79:Subset#9.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[16])]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:194) [classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\nCaused by: java.lang.AssertionError: Internal error: Error while applying rule ExpandConversionRule, args [rel#98:AbstractConverter.PHYSICAL.ANY([]).[16](child=rel#79:Subset#9.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[16])]\r\n\tat org.eigenbase.util.Util.newInternal(Util.java:750) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:246) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:661) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.tools.Programs$RuleSetProgram.run(Programs.java:165) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.prepare.PlannerImpl.transform(PlannerImpl.java:273) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToPrel(DefaultSqlHandler.java:167) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.CreateTableHandler.getPlan(CreateTableHandler.java:112) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:507) [classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:185) [classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\n\t... 4 more\r\nCaused by: java.lang.IndexOutOfBoundsException: index (16) must be less than size (16)\r\n\tat com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305) ~[guava-14.0.1.jar:na]\r\n\tat com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284) ~[guava-14.0.1.jar:na]\r\n\tat com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:81) ~[guava-14.0.1.jar:na]\r\n\tat org.eigenbase.rex.RexBuilder.makeInputRef(RexBuilder.java:764) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.SortRel.<init>(SortRel.java:94) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.SortRel.<init>(SortRel.java:59) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:78) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:1) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1011) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1102) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.AbstractConverter$ExpandConversionRule.onMatch(AbstractConverter.java:108) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:223) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:661) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.tools.Programs$RuleSetProgram.run(Programs.java:165) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.prepare.PlannerImpl.transform(PlannerImpl.java:273) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToPrel(DefaultSqlHandler.java:167) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.CreateTableHandler.getPlan(CreateTableHandler.java:112) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:507) [classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:185) [classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\n\t... 12 more\r\n{code}\r\n",
        "Order by using table.col in subquery fails with ArrayIndexOutOfBoundsException  git.commit.id.abbrev=687b9b0\r\n\r\nThe following query runs successfully:\r\nselect cast(student.name as varchar(30)) name, cast(voter.registration as varchar(20)) registration from voter full outer join student on (student.name = voter.name) where student.age < 20 order by student.name;\r\n\r\nIf I put the above query in a sub-select, it fails:\r\n0: jdbc:drill:schema=dfs> select tbl.name, tbl.registration from (select cast(student.name as varchar(30)) name, cast(voter.registration as varchar(20)) registration from `dfs`.`default`.`./voter` voter full outer join `dfs`.`default`.`./student` student on (student.name = voter.name) where student.age < 20 order by student.name) tbl;\r\nQuery failed: Failure while parsing sql. -1 [fe42e70f-cf5b-4389-b879-5b1598ab887f]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n\r\nIf I change the query using \"order by name\" without specifying the student table, the query runs fine:\r\n0: jdbc:drill:schema=dfs> select tbl.name, tbl.registration from (select cast(student.name as varchar(30)) name, cast(voter.registration as varchar(20)) registration from `dfs`.`default`.`./voter` voter full outer join `dfs`.`default`.`./student` student on (student.name = voter.name) where student.age < 20 order by student.name) tbl;\r\n\r\nThe student.name in the order by clause should work."
    ],
    [
        "DRILL-1978",
        "DRILL-1941",
        "Using order by with a cast function and array index fails to plan. {code}\r\nCREATE TABLE newStar AS \r\n\r\nSELECT \r\n  columns[0], columns[1], columns[2], columns[3], columns[4], columns[5], \r\n  columns[6], columns[7], columns[8], columns[9], columns[10], columns[11], \r\n  columns[12], columns[13], columns[14], columns[15]\r\nFROM dfs.`file.csv` ORDER BY cast(columns[2] as int);\r\n{code}\r\n\r\n\r\n{code}\r\norg.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: Internal error: Error while applying rule ExpandConversionRule, args [rel#98:AbstractConverter.PHYSICAL.ANY([]).[16](child=rel#79:Subset#9.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[16])]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:194) [classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\nCaused by: java.lang.AssertionError: Internal error: Error while applying rule ExpandConversionRule, args [rel#98:AbstractConverter.PHYSICAL.ANY([]).[16](child=rel#79:Subset#9.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[16])]\r\n\tat org.eigenbase.util.Util.newInternal(Util.java:750) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:246) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:661) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.tools.Programs$RuleSetProgram.run(Programs.java:165) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.prepare.PlannerImpl.transform(PlannerImpl.java:273) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToPrel(DefaultSqlHandler.java:167) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.CreateTableHandler.getPlan(CreateTableHandler.java:112) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:507) [classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:185) [classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\n\t... 4 more\r\nCaused by: java.lang.IndexOutOfBoundsException: index (16) must be less than size (16)\r\n\tat com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305) ~[guava-14.0.1.jar:na]\r\n\tat com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284) ~[guava-14.0.1.jar:na]\r\n\tat com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:81) ~[guava-14.0.1.jar:na]\r\n\tat org.eigenbase.rex.RexBuilder.makeInputRef(RexBuilder.java:764) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.SortRel.<init>(SortRel.java:94) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.SortRel.<init>(SortRel.java:59) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:78) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:1) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1011) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1102) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.AbstractConverter$ExpandConversionRule.onMatch(AbstractConverter.java:108) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:223) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:661) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.tools.Programs$RuleSetProgram.run(Programs.java:165) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.prepare.PlannerImpl.transform(PlannerImpl.java:273) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToPrel(DefaultSqlHandler.java:167) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.CreateTableHandler.getPlan(CreateTableHandler.java:112) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:507) [classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:185) [classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\n\t... 12 more\r\n{code}\r\n",
        "select from subquery having an order by fails git.commit.id.abbrev=b491cdb\r\n\r\nThe following query fails:\r\n0: jdbc:drill:schema=dfs>  select q.name as name, q.registration as registration from (select cast(student.name as varchar(30)) as name, cast(voter.registration as varchar(20)) as registration from `dfs`.`default`.`voter` voter full outer join `dfs`.`default`.`./student` student on (student.name = voter.name) where student.age < 30 order by student.name) q;\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: -1\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nRunning the same query with \"explain plan for\" also fail with the same error.  If I remove the \"order by student.name\" from the sub-query, the query run successfully."
    ],
    [
        "DRILL-1995",
        "DRILL-1859",
        "UnlimitedRawBatchBuffer should not enqueue incoming batches when its state is KILLED UnlimitedRawBatchBuffer should release incoming batches and avoid any enqueueing when its state is KILLED. Otherwise execution might fail during the cleanup phase of a LIMIT query complaining that the buffer is not empty.",
        "IllegalReferenceCountException in the decoder inside Netty The following query does a LIMIT inside a subquery to force a UnionExchange and then does an ORDER-BY outside that will first re-distribute the data before sorting.  It results in a DecoderException in netty.\r\n\r\n{code}\r\n0: jdbc:drill:zk=local> alter session set `planner.slice_target` = 10;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | planner.slice_target updated. |\r\n+------------+------------+\r\n0: jdbc:drill:zk=local> select t2.o_custkey from (select o_orderkey, o_custkey from cp.`tpch/orders.parquet` t1 group by o_orderkey, o_custkey limit 10) t2 order by t2.o_custkey;\r\nQuery failed: Query failed: Failure while running fragment., refCnt: 0, decrement: 1 \r\n{code}\r\n\r\nHere's partial output from the logs: (will attach full error log).  \r\n{code}\r\nio.netty.handler.codec.DecoderException: io.netty.util.IllegalReferenceCountException: refCnt: 0, decrement: 1\r\n        io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:99) [netty-codec-4.0.24.Final.jar:4.0.24.Final]\r\n        io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n{code}"
    ],
    [
        "DRILL-2008",
        "DRILL-779",
        "select a || b from ... fails while select cast(a as varchar) || cast(b as varchar) from ... The first query would fail at planning by optiq/calcite. \r\n\r\nFor example,\r\nselect n_name || n_name from cp.`tpch/nation.parquet`;\r\norg.apache.drill.exec.rpc.RpcException: Query failed: Unexpected exception during fragment initialization: null\r\n\r\n\tat org.apache.drill.exec.rpc.user.QueryResultHandler.batchArrived(QueryResultHandler.java:79)\r\n\tat org.apache.drill.exec.rpc.user.UserClient.handleReponse(UserClient.java:93)\r\n\tat org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:52)\r\n\tat org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:34)\r\n\tat org.apache.drill.exec.rpc.RpcBus.handle(RpcBus.java:58)\r\n\tat org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:194)\r\n\tat org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:173)\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:161)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787)\r\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)\r\n\tat java.lang.Thread.run(Thread.java:745)",
        "Add support for pipes Concat \"||\" operator git.commit.id.abbrev=70fab8c\r\n\r\n0: jdbc:drill:schema=dfs> select (name || age || contributions || registration) as COL from voter where name like '%young';\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"dd813164-2347-4c49-9480-bc5c6cc401d6\"\r\nendpoint {\r\n  address: \"qa-node64.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < UnsupportedOperationException:[ NULL ]\"\r\n]\r\n\r\nThe data set contains null values; however the result from the above query does not contain any null values.\r\n\r\nStack trace:\r\njava.lang.AssertionError: Internal error: pre-condition failed: SqlTypeUtil.sameNamedType(argTypes[0], argTypes[1])\r\norg.eigenbase.util.Util.newInternal(Util.java:760) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.util.Util.pre(Util.java:833) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.type.ReturnTypes$8.inferReturnType(ReturnTypes.java:540) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.type.SqlTypeTransformCascade.inferReturnType(SqlTypeTransformCascade.java:55) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.inferReturnType(SqlOperator.java:451) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.validateOperands(SqlOperator.java:418) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:480) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:476) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:476) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:476) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:476) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:476) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlAsOperator.deriveType(SqlAsOperator.java:117) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:418) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:3019) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:2772) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:80) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:747) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:736) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.SqlSelect.validate(SqlSelect.java:209) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:710) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.eigenbase.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:426) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\nnet.hydromatic.optiq.prepare.PlannerImpl.validate(PlannerImpl.java:175) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\norg.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateNode(DefaultSqlHandler.java:99) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:84) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:134) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:338) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\norg.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:186) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\njava.lang.Thread.run(Thread.java:744) [na:1.7.0_45]"
    ],
    [
        "DRILL-2008",
        "DRILL-1876",
        "select a || b from ... fails while select cast(a as varchar) || cast(b as varchar) from ... The first query would fail at planning by optiq/calcite. \r\n\r\nFor example,\r\nselect n_name || n_name from cp.`tpch/nation.parquet`;\r\norg.apache.drill.exec.rpc.RpcException: Query failed: Unexpected exception during fragment initialization: null\r\n\r\n\tat org.apache.drill.exec.rpc.user.QueryResultHandler.batchArrived(QueryResultHandler.java:79)\r\n\tat org.apache.drill.exec.rpc.user.UserClient.handleReponse(UserClient.java:93)\r\n\tat org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:52)\r\n\tat org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:34)\r\n\tat org.apache.drill.exec.rpc.RpcBus.handle(RpcBus.java:58)\r\n\tat org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:194)\r\n\tat org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:173)\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:161)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787)\r\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)\r\n\tat java.lang.Thread.run(Thread.java:745)",
        "Implement concatenation operator ( || ) It is documented, but it does not work at all (DRILL-779) \r\nWorkaround exists: concat function\r\n\r\nAs of version 0.7.0 (git.commit.id.abbrev=3b0ff5d)\r\n\r\n(1)\r\n0: jdbc:drill:schema=dfs> select 'abc' || 'def' from cp.`tpch/nation.parquet`;\r\nQuery failed: Query failed: Failure while running fragment.[ dc4727ed-4ac3-42bb-a60b-f8f370dd4fcf on atsqa4-133.qa.lab:31010 ]\r\n[ dc4727ed-4ac3-42bb-a60b-f8f370dd4fcf on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n(2)\r\n0: jdbc:drill:schema=dfs> select n_name || 'xyz' from cp.`tpch/nation.parquet`;\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: Internal error: pre-condition failed: SqlTypeUtil.sameNamedType(argTypes[0], argTypes[1])\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n(3)\r\n0: jdbc:drill:schema=dfs> select cast(n_nationkey as varchar(10)) || n_name from cp.`tpch/nation.parquet`;\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: Internal error: pre-condition failed: SqlTypeUtil.sameNamedType(argTypes[0], argTypes[1])\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n(4)\r\n0: jdbc:drill:schema=dfs> select 'abc' || 'cde' from `test.json`;\r\nQuery failed: Query failed: Failure while running fragment.[ 01194cdf-746c-45db-ae4e-014590bfbffd on atsqa4-133.qa.lab:31010 ]\r\n[ 01194cdf-746c-45db-ae4e-014590bfbffd on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n"
    ],
    [
        "DRILL-2008",
        "DRILL-1877",
        "select a || b from ... fails while select cast(a as varchar) || cast(b as varchar) from ... The first query would fail at planning by optiq/calcite. \r\n\r\nFor example,\r\nselect n_name || n_name from cp.`tpch/nation.parquet`;\r\norg.apache.drill.exec.rpc.RpcException: Query failed: Unexpected exception during fragment initialization: null\r\n\r\n\tat org.apache.drill.exec.rpc.user.QueryResultHandler.batchArrived(QueryResultHandler.java:79)\r\n\tat org.apache.drill.exec.rpc.user.UserClient.handleReponse(UserClient.java:93)\r\n\tat org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:52)\r\n\tat org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:34)\r\n\tat org.apache.drill.exec.rpc.RpcBus.handle(RpcBus.java:58)\r\n\tat org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:194)\r\n\tat org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:173)\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:161)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787)\r\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)\r\n\tat java.lang.Thread.run(Thread.java:745)",
        "Throw unsupported error message for || operator early in the parsing stage If we don't plan to support concat operator (||), we should throw a decent error message early.\r\n\r\nThe error is cryptic:\r\n\r\n0: jdbc:drill:schema=dfs> select cast(l_orderkey as varchar(10)) || '-' || cast(l_partkey as varchar(10)) from cp.`tpch/lineitem.parquet`;\r\nQuery failed: Query stopped., Line 65, Column 29: \"value\" is neither a method, a field, nor a member class of \"org.apache.drill.exec.expr.holders.VarCharHolder\" [ bc85685d-e08a-494b-aa9f-5b3fa4ee4033 on atsqa4-133.qa.lab:3\r\n1010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nI believe that error that I'm getting from the query below is the result of not throwing \"Unsupported\" message earlier.\r\nPlease correct me if I'm wrong.\r\n\r\n0: jdbc:drill:schema=dfs> select\r\n. . . . . . . . . . . . >         cast(l_orderkey as varchar(10)) || '-' || cast(l_partkey as varchar(10))\r\n. . . . . . . . . . . . > from\r\n. . . . . . . . . . . . >         (\r\n. . . . . . . . . . . . >         select  sum(l_quantity),\r\n. . . . . . . . . . . . >                 l_orderkey,\r\n. . . . . . . . . . . . >                 l_partkey\r\n. . . . . . . . . . . . >         from    cp.`tpch/lineitem.parquet`\r\n. . . . . . . . . . . . >         group by\r\n. . . . . . . . . . . . >                 l_orderkey,\r\n. . . . . . . . . . . . >                 l_partkey\r\n. . . . . . . . . . . . >         )\r\n. . . . . . . . . . . . > group by\r\n. . . . . . . . . . . . >         cast(l_orderkey as varchar(10)) || '-' || cast(l_partkey as varchar(10));\r\nQuery failed: Query failed: Failure while running fragment., You tried to do a batch data read operation when you were in a state of STOP.  You can only do this type of operation when you are in a state of OK or OK_NEW_SCHEMA. [ f5cb68a8-7c05-4e02-a104-56081e6d34ce on atsqa4-133.qa.lab:31010 ]\r\n[ f5cb68a8-7c05-4e02-a104-56081e6d34ce on atsqa4-133.qa.lab:31010 ]\r\n"
    ],
    [
        "DRILL-2016",
        "DRILL-1921",
        "Remove \"multiset\" data type from supported grammar Should throw \"unsupported error\"\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select cast(a1 as multiset) from `t1.json`;\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: use createMultisetType() instead\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n",
        "Throw unsupported error message some set operators that are not currently supported Throw unsupported error message for these operators: (instead of \"Could not be implemented error below\")\r\n\r\nINTERSECT\r\nEXCEPT\r\nUNION\r\nCROSS JOIN\r\n\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: Node [rel#517133:Subset#3.LOGICAL.ANY([]).[]] could not be implemented; planner state:\r\n\r\nEnhacement requests in Jira for these above mentioned operators:\r\n\r\n{code}\r\nINTERSECT       DRILL-1308\r\nMINUS                No enhancement request ( not recognized in grammar)\r\nEXCEPT             No enhancement request\r\nUNION                DRILL-1169\r\nCROSS JOIN     DRILL-786\r\n{code}"
    ],
    [
        "DRILL-2018",
        "DRILL-1728",
        "Error message must be updated when Drill fails to handle JSON files with mutating schemas within fields. Currently Drill fails to read JSON files with the following structure:\r\n\r\nBelow is the snippet causing the issue: \r\n\r\n{\r\n  \"data\": {\r\n    \"games\": {\r\n      \"game\": [\r\n        {\r\n          \"home_runs\": {\r\n            \"player\": {\r\n              \"first\": \"Jason\"\r\n            }\r\n          }\r\n        },\r\n        {\r\n          \"home_runs\": {\r\n            \"player\": [\r\n              {\r\n                \"first\": \"Kosuke\"\r\n              },\r\n              {\r\n                \"first\": \"Alfonso\"\r\n              },\r\n              {\r\n                \"first\": \"Jeff\"\r\n              },\r\n              {\r\n                \"first\": \"Brandon\"\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n> select * from `snippet.json` limit 1;\r\n\r\nQuery failed: Query stopped., You tried to write a Map type when you are using a ValueWriter of type SingleMapWriter. [ 358d955b-d834-476c-b649-95059c264030 on abhi5.qa.lab:31010 ]\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nThe error message must be updated to indicate clearly that this feature is not currently not supported. It would also be helpful if the fields having the issue is also indicated, as it would help narrow down the issue. ",
        "Better error messages on Drill JSON read error {code}\r\n0: jdbc:drill:zk=localhost:2181> SELECT * FROM dfs.root.`Users/tshiran/Development/demo/data/yelp/business.json` WHERE true and REPEATED_CONTAINS(categories, 'Australian');\r\n+-------------+--------------+------------+------------+------------+------------+--------------+------------+------------+------------+------------+------------+------------+------------+---------------+\r\n| business_id | full_address |   hours    |    open    | categories |    city    | review_count |    name    | longitude  |   state    |   stars    |  latitude  | attributes |    type    | neighborhoods |\r\n+-------------+--------------+------------+------------+------------+------------+--------------+------------+------------+------------+------------+------------+------------+------------+---------------+\r\nQuery failed: Query stopeed., You tried to start when you are using a ValueWriter of type NullableBitWriterImpl. [ e5bafa1e-6226-443d-80fd-51e18f330899 on 172.17.3.132:31010 ]\r\n\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n{code}"
    ],
    [
        "DRILL-2021",
        "DRILL-1979",
        "select a, *, a from ... gives wrong result When, in the select-clause, there are star(s) and regular columns showing up more than once, some regular columns would failed to be printed out.\r\n\r\nFor example, \r\nselect n_name, *, n_name from cp.`tpch/nation.parquet` limit 2\r\n\r\n---------------------------------------------------------------------------------------------------------------------\r\nn_name\tn_nationkey\tn_name0\tn_regionkey\tn_comment\r\nALGERIA\t0\tALGERIA\t0\t haggle. carefully final deposits detect slyly agai\r\nARGENTINA\t1\tARGENTINA\t1\tal foxes promise slyly according to the regular accounts. bold requests alone\r\n---------------------------------------------------------------------------------------------------------------------\r\n\r\nNotice \"n_name\" show be printed out three times. ",
        "select multiple *, column as alias, with limit operator, produces extra column For example, \r\n\r\nselect *, r_name as extra, * from cp.`tpch/region.parquet` limit 2;\r\n\r\n-----------------------------------------------------------------------------------------------------------------------------------------\r\n| r_regionkey<IN | r_name<VARCHAR | r_comment<VARC | extra<VARCHAR( | r_regionkey0<I | r_name0<VARCHA | r_comment0<VAR | extra0<VARCHAR |\r\n-----------------------------------------------------------------------------------------------------------------------------------------\r\n| 0              | AFRICA         | lar deposits.  | AFRICA         | 0              | AFRICA         | lar deposits.  | AFRICA         |\r\n| 1              | AMERICA        | hs use ironic, | AMERICA        | 1              | AMERICA        | hs use ironic, | AMERICA        |\r\n-----------------------------------------------------------------------------------------------------------------------------------------\r\nTotal rows returned : 2\r\n\r\nNotice the last column \"extra0\", which is redundant!"
    ],
    [
        "DRILL-2027",
        "DRILL-1496",
        "SQL 'similar' functionality having issues with large datasets git.commit.id.abbrev=5a28066\r\n\r\nThe below query runs when the dataset has 10 (or even 1000) records. However if I just copy over the same record set 100000 times then the below query is failing\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select * from `wide-strings-large.tbl` where columns[3] similar to '%NqP%';\r\nQuery failed: Query failed: Failure while trying to start remote fragment, Expression has syntax error! line 1:0:no viable alternative at input 'similar' [ 48b1fa16-6f3e-4bf5-9eb7-9c4a385d09b9 on qa-node190.qa.lab:31010 ]\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nBelow are the contents of the log file :\r\n{code}\r\norg.apache.drill.exec.rpc.RemoteRpcException: Failure while trying to start remote fragment, Expression has syntax error! line 1:15:no viable alternative at input 'similar' [ e4e39df0-4811-4706-a5ad-7fd93e64b72c on qa-node190.qa.lab:31010 ]\r\n\r\n        at org.apache.drill.exec.rpc.CoordinationQueue.updateFailedFuture(CoordinationQueue.java:153) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:227) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:173) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89) [netty-codec-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:161) [netty-codec-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116) [netty-common-4.0.24.Final.jar:4.0.24.Final]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]\r\n{code}\r\n\r\nI attached the dataset with 10 records. For the larger dataset just copy over the smaller one multiple times\r\n",
        "Fix serialization of 'similar to' function git.commit.id.abbrev=5c220e3\r\n\r\nDill seems to be having problems when we use 'similar' as a filter in querying hbase tables. Queries on top of parquet with 'similar' keyword work fine. \r\n\r\nWe started seeing this with the 0.6 release and was working fine before\r\n\r\nBelow is the query that I used :\r\n\r\n{code}\r\nselect row_key  from voter where onecf['name'] similar to '%young%';\r\n{code}\r\n\r\nAttached the data and log files"
    ],
    [
        "DRILL-2063",
        "DRILL-1946",
        "Wrong result for query with aggregate expression The following query gives wrong result for avg_price: \r\n{code}\r\n0: jdbc:drill:zk=local> select l_suppkey, sum(l_extendedprice)/sum(l_quantity) as avg_price from cp.`tpch/lineitem.parquet` where l_orderkey in (select o_orderkey from cp.`tpch/orders.parquet` where o_custkey = 2) and l_suppkey = 4 group by l_suppkey;\r\n+------------+------------+\r\n| l_suppkey  | avg_price  |\r\n+------------+------------+\r\n| 4          | 0.1111111111111111 |\r\n+------------+------------+\r\n{code}\r\n\r\nIf I include the aggregate functions explicitly outside of the expression, I get the right result: \r\n{code}\r\n0: jdbc:drill:zk=local> select l_suppkey, sum(l_extendedprice) as total_price, sum(l_quantity) as total_qty, sum(l_extendedprice)/sum(l_quantity) as avg_price from cp.`tpch/lineitem.parquet` where l_orderkey in (select o_orderkey from cp.`tpch/orders.parquet` where o_custkey = 2) and l_suppkey = 4 group by l_suppkey;\r\n+------------+-------------+------------+------------+\r\n| l_suppkey  | total_price | total_qty  | avg_price  |\r\n+------------+-------------+------------+------------+\r\n| 4          | 49480.92    | 36.0       | 1374.47    |\r\n+------------+-------------+------------+------------+\r\n{code}\r\n\r\nNote that the wrong result  in the first query is because of using the wrong column for the numerator of the division.  It is actually doing l_suppkey/total_qty  (4/36 = 0.11111).   Since this is an egregious error, I am marking this critical. ",
        "Wrong result for aggregation query with lower slice_target Here's the result with the default value of slice_target for the following query: \r\n{code}\r\nselect count(*) from (\r\nSELECT l_suppkey, sum(l_extendedprice)/sum(l_quantity)\r\nFROM cp.`tpch/lineitem.parquet`\r\nWHERE (l_orderkey in (SELECT o_orderkey FROM cp.`tpch/orders.parquet`\r\n                       WHERE o_custkey < 2) )\r\nGROUP BY l_suppkey\r\n);\r\n\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 29         |\r\n+------------+\r\n{code}\r\n\r\nI lowered the slice_target in order to force exchanges in the following query.  The result is wrong.   \r\n{code}\r\n0: jdbc:drill:zk=local> alter session set `planner.slice_target` = 1;\r\nselect count(*) from (\r\nSELECT l_suppkey, sum(l_extendedprice)/sum(l_quantity)\r\nFROM cp.`tpch/lineitem.parquet`\r\nWHERE (l_orderkey in (SELECT o_orderkey FROM cp.`tpch/orders.parquet`\r\n                       WHERE o_custkey < 2) )\r\nGROUP BY l_suppkey\r\n);\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 14         |\r\n+------------+\r\n{code}\r\n\r\nThe Explain plan for the second query has multiphase aggregates.  When I set planner.enable_multiphase_agg = false, the result is correct, so it seems to be a multiphase aggregate issue based on a preliminary analysis (although there might be other things going on). "
    ],
    [
        "DRILL-2075",
        "DRILL-1302",
        "Subquery not projecting the order by column is causing issues when used along with flatten git.commit.id.abbrev=3c6d0ef\r\n\r\nData Set :\r\n{code}\r\n{\r\n  \"uid\" : 1,\r\n  \"uid_str\" : \"01\",\r\n  \"type\" : \"web\",\r\n  \"events\" : [\r\n        { \"evnt_id\":\"e1\", \"campaign_id\":\"c1\", \"event_name\":\"e1_name\", \"event_time\":1000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e2\", \"campaign_id\":\"c1\", \"event_name\":\"e2_name\", \"event_time\":2000000, \"type\" : \"cmpgn4\"},\r\n        { \"evnt_id\":\"e3\", \"campaign_id\":\"c1\", \"event_name\":\"e3_name\", \"event_time\":3000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e4\", \"campaign_id\":\"c1\", \"event_name\":\"e4_name\", \"event_time\":4000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e5\", \"campaign_id\":\"c2\", \"event_name\":\"e5_name\", \"event_time\":5000000, \"type\" : \"cmpgn3\"},\r\n        { \"evnt_id\":\"e6\", \"campaign_id\":\"c1\", \"event_name\":\"e6_name\", \"event_time\":6000000, \"type\" : \"cmpgn9\"},\r\n        { \"evnt_id\":\"e7\", \"campaign_id\":\"c1\", \"event_name\":\"e7_name\", \"event_time\":7000000, \"type\" : \"cmpgn3\"},\r\n        { \"evnt_id\":\"e8\", \"campaign_id\":\"c2\", \"event_name\":\"e8_name\", \"event_time\":8000000, \"type\" : \"cmpgn2\"},\r\n        { \"evnt_id\":\"e9\", \"campaign_id\":\"c2\", \"event_name\":\"e9_name\", \"event_time\":9000000, \"type\" : \"cmpgn4\"}\r\n  ]\r\n}\r\n{code}\r\n\r\nQuery with flatten :\r\n{code}\r\nselect s.type  from (select d.type type, flatten(d.events) evnts from `temp.json` d where d.type='web' order by d.uid) s where s.type='web';\r\nQuery failed: IndexOutOfBoundsException: index (2) must be less than size (1)\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nQuery without flatten :\r\n{code}\r\nselect s.type  from (select d.type type from `temp.json` d where d.type='web' order by d.uid) s where s.type='web';\r\n+------------+\r\n|    type    |\r\n+------------+\r\n| web        |\r\n+------------+\r\n{code}\r\n\r\nHowever even the above query fails when we remove the filter and this could be related to DRILL-1302\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select s.type  from (select d.type type from `temp.json` d where d.type='web' order by d.uid) s;\r\nQuery failed: ArrayIndexOutOfBoundsException: -1\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}",
        "Order by using table.col in subquery fails with ArrayIndexOutOfBoundsException  git.commit.id.abbrev=687b9b0\r\n\r\nThe following query runs successfully:\r\nselect cast(student.name as varchar(30)) name, cast(voter.registration as varchar(20)) registration from voter full outer join student on (student.name = voter.name) where student.age < 20 order by student.name;\r\n\r\nIf I put the above query in a sub-select, it fails:\r\n0: jdbc:drill:schema=dfs> select tbl.name, tbl.registration from (select cast(student.name as varchar(30)) name, cast(voter.registration as varchar(20)) registration from `dfs`.`default`.`./voter` voter full outer join `dfs`.`default`.`./student` student on (student.name = voter.name) where student.age < 20 order by student.name) tbl;\r\nQuery failed: Failure while parsing sql. -1 [fe42e70f-cf5b-4389-b879-5b1598ab887f]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n\r\nIf I change the query using \"order by name\" without specifying the student table, the query runs fine:\r\n0: jdbc:drill:schema=dfs> select tbl.name, tbl.registration from (select cast(student.name as varchar(30)) name, cast(voter.registration as varchar(20)) registration from `dfs`.`default`.`./voter` voter full outer join `dfs`.`default`.`./student` student on (student.name = voter.name) where student.age < 20 order by student.name) tbl;\r\n\r\nThe student.name in the order by clause should work."
    ],
    [
        "DRILL-2075",
        "DRILL-1941",
        "Subquery not projecting the order by column is causing issues when used along with flatten git.commit.id.abbrev=3c6d0ef\r\n\r\nData Set :\r\n{code}\r\n{\r\n  \"uid\" : 1,\r\n  \"uid_str\" : \"01\",\r\n  \"type\" : \"web\",\r\n  \"events\" : [\r\n        { \"evnt_id\":\"e1\", \"campaign_id\":\"c1\", \"event_name\":\"e1_name\", \"event_time\":1000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e2\", \"campaign_id\":\"c1\", \"event_name\":\"e2_name\", \"event_time\":2000000, \"type\" : \"cmpgn4\"},\r\n        { \"evnt_id\":\"e3\", \"campaign_id\":\"c1\", \"event_name\":\"e3_name\", \"event_time\":3000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e4\", \"campaign_id\":\"c1\", \"event_name\":\"e4_name\", \"event_time\":4000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e5\", \"campaign_id\":\"c2\", \"event_name\":\"e5_name\", \"event_time\":5000000, \"type\" : \"cmpgn3\"},\r\n        { \"evnt_id\":\"e6\", \"campaign_id\":\"c1\", \"event_name\":\"e6_name\", \"event_time\":6000000, \"type\" : \"cmpgn9\"},\r\n        { \"evnt_id\":\"e7\", \"campaign_id\":\"c1\", \"event_name\":\"e7_name\", \"event_time\":7000000, \"type\" : \"cmpgn3\"},\r\n        { \"evnt_id\":\"e8\", \"campaign_id\":\"c2\", \"event_name\":\"e8_name\", \"event_time\":8000000, \"type\" : \"cmpgn2\"},\r\n        { \"evnt_id\":\"e9\", \"campaign_id\":\"c2\", \"event_name\":\"e9_name\", \"event_time\":9000000, \"type\" : \"cmpgn4\"}\r\n  ]\r\n}\r\n{code}\r\n\r\nQuery with flatten :\r\n{code}\r\nselect s.type  from (select d.type type, flatten(d.events) evnts from `temp.json` d where d.type='web' order by d.uid) s where s.type='web';\r\nQuery failed: IndexOutOfBoundsException: index (2) must be less than size (1)\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nQuery without flatten :\r\n{code}\r\nselect s.type  from (select d.type type from `temp.json` d where d.type='web' order by d.uid) s where s.type='web';\r\n+------------+\r\n|    type    |\r\n+------------+\r\n| web        |\r\n+------------+\r\n{code}\r\n\r\nHowever even the above query fails when we remove the filter and this could be related to DRILL-1302\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select s.type  from (select d.type type from `temp.json` d where d.type='web' order by d.uid) s;\r\nQuery failed: ArrayIndexOutOfBoundsException: -1\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}",
        "select from subquery having an order by fails git.commit.id.abbrev=b491cdb\r\n\r\nThe following query fails:\r\n0: jdbc:drill:schema=dfs>  select q.name as name, q.registration as registration from (select cast(student.name as varchar(30)) as name, cast(voter.registration as varchar(20)) as registration from `dfs`.`default`.`voter` voter full outer join `dfs`.`default`.`./student` student on (student.name = voter.name) where student.age < 30 order by student.name) q;\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: -1\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nRunning the same query with \"explain plan for\" also fail with the same error.  If I remove the \"order by student.name\" from the sub-query, the query run successfully."
    ],
    [
        "DRILL-2075",
        "DRILL-1978",
        "Subquery not projecting the order by column is causing issues when used along with flatten git.commit.id.abbrev=3c6d0ef\r\n\r\nData Set :\r\n{code}\r\n{\r\n  \"uid\" : 1,\r\n  \"uid_str\" : \"01\",\r\n  \"type\" : \"web\",\r\n  \"events\" : [\r\n        { \"evnt_id\":\"e1\", \"campaign_id\":\"c1\", \"event_name\":\"e1_name\", \"event_time\":1000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e2\", \"campaign_id\":\"c1\", \"event_name\":\"e2_name\", \"event_time\":2000000, \"type\" : \"cmpgn4\"},\r\n        { \"evnt_id\":\"e3\", \"campaign_id\":\"c1\", \"event_name\":\"e3_name\", \"event_time\":3000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e4\", \"campaign_id\":\"c1\", \"event_name\":\"e4_name\", \"event_time\":4000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e5\", \"campaign_id\":\"c2\", \"event_name\":\"e5_name\", \"event_time\":5000000, \"type\" : \"cmpgn3\"},\r\n        { \"evnt_id\":\"e6\", \"campaign_id\":\"c1\", \"event_name\":\"e6_name\", \"event_time\":6000000, \"type\" : \"cmpgn9\"},\r\n        { \"evnt_id\":\"e7\", \"campaign_id\":\"c1\", \"event_name\":\"e7_name\", \"event_time\":7000000, \"type\" : \"cmpgn3\"},\r\n        { \"evnt_id\":\"e8\", \"campaign_id\":\"c2\", \"event_name\":\"e8_name\", \"event_time\":8000000, \"type\" : \"cmpgn2\"},\r\n        { \"evnt_id\":\"e9\", \"campaign_id\":\"c2\", \"event_name\":\"e9_name\", \"event_time\":9000000, \"type\" : \"cmpgn4\"}\r\n  ]\r\n}\r\n{code}\r\n\r\nQuery with flatten :\r\n{code}\r\nselect s.type  from (select d.type type, flatten(d.events) evnts from `temp.json` d where d.type='web' order by d.uid) s where s.type='web';\r\nQuery failed: IndexOutOfBoundsException: index (2) must be less than size (1)\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nQuery without flatten :\r\n{code}\r\nselect s.type  from (select d.type type from `temp.json` d where d.type='web' order by d.uid) s where s.type='web';\r\n+------------+\r\n|    type    |\r\n+------------+\r\n| web        |\r\n+------------+\r\n{code}\r\n\r\nHowever even the above query fails when we remove the filter and this could be related to DRILL-1302\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select s.type  from (select d.type type from `temp.json` d where d.type='web' order by d.uid) s;\r\nQuery failed: ArrayIndexOutOfBoundsException: -1\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}",
        "Using order by with a cast function and array index fails to plan. {code}\r\nCREATE TABLE newStar AS \r\n\r\nSELECT \r\n  columns[0], columns[1], columns[2], columns[3], columns[4], columns[5], \r\n  columns[6], columns[7], columns[8], columns[9], columns[10], columns[11], \r\n  columns[12], columns[13], columns[14], columns[15]\r\nFROM dfs.`file.csv` ORDER BY cast(columns[2] as int);\r\n{code}\r\n\r\n\r\n{code}\r\norg.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: Internal error: Error while applying rule ExpandConversionRule, args [rel#98:AbstractConverter.PHYSICAL.ANY([]).[16](child=rel#79:Subset#9.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[16])]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:194) [classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\nCaused by: java.lang.AssertionError: Internal error: Error while applying rule ExpandConversionRule, args [rel#98:AbstractConverter.PHYSICAL.ANY([]).[16](child=rel#79:Subset#9.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[16])]\r\n\tat org.eigenbase.util.Util.newInternal(Util.java:750) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:246) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:661) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.tools.Programs$RuleSetProgram.run(Programs.java:165) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.prepare.PlannerImpl.transform(PlannerImpl.java:273) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToPrel(DefaultSqlHandler.java:167) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.CreateTableHandler.getPlan(CreateTableHandler.java:112) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:507) [classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:185) [classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\n\t... 4 more\r\nCaused by: java.lang.IndexOutOfBoundsException: index (16) must be less than size (16)\r\n\tat com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305) ~[guava-14.0.1.jar:na]\r\n\tat com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284) ~[guava-14.0.1.jar:na]\r\n\tat com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:81) ~[guava-14.0.1.jar:na]\r\n\tat org.eigenbase.rex.RexBuilder.makeInputRef(RexBuilder.java:764) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.SortRel.<init>(SortRel.java:94) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.SortRel.<init>(SortRel.java:59) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:78) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:1) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1011) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1102) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.AbstractConverter$ExpandConversionRule.onMatch(AbstractConverter.java:108) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:223) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:661) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.tools.Programs$RuleSetProgram.run(Programs.java:165) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.prepare.PlannerImpl.transform(PlannerImpl.java:273) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToPrel(DefaultSqlHandler.java:167) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.CreateTableHandler.getPlan(CreateTableHandler.java:112) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:507) [classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:185) [classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\n\t... 12 more\r\n{code}\r\n"
    ],
    [
        "DRILL-2082",
        "DRILL-1597",
        "nested arrays of strings returned wrong results #Mon Jan 26 14:10:51 PST 2015\r\ngit.commit.id.abbrev=3c6d0ef\r\n\r\nQuerying Complex JSON data type nested array of strings returned wrong results when data size is large (1 million row). Smaller data size (a few rows) returned correct results. Test data can be accessed at http://apache-drill.s3.amazonaws.com/files/complex.json.gz\r\n\r\nFor small data size, I got correct results:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> select t.id, t.aaa from `aaa.json` t;\r\n+------------+------------+\r\n|     id     |    aaa     |\r\n+------------+------------+\r\n| 1          | [[[\"aa0 1\"],[\"ab0 1\"]],[[\"ba0 1\"],[\"bb0 1\"]],[[\"ca0 1\",\"ca1 1\"],[\"cb0 1\",\"cb1 1\",\"cb2 1\"]]] |\r\n| 2          | [[[\"aa0 2\"],[\"ab0 2\"]],[[\"ba0 2\"],[\"bb0 2\"]],[[\"ca0 2\",\"ca1 2\"],[\"cb0 2\",\"cb1 2\",\"cb2 2\"]]] |\r\n+------------+------------+\r\n{code}\r\n\r\nBut large data size returned wrong results:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> select t.id, t.aaa from `complex.json` t where t.id=1 limit 1;\r\n+------------+------------+\r\n|     id     |    aaa     |\r\n+------------+------------+\r\n| 1          | [[[\"ba0 56\"],[\"bb0 56\"],[\"ca0 56\",\"ca1 56\"],[\"cb0 56\",\"cb1 56\",\"cb2 56\"],[\"aa0 91\"],[\"ab0 91\"],[\"aa0 125\"],[\"ab0 125\"],[\"aa0 140\"],[\"ab0 140\"],[\"aa0 142\"],[\"ab0 142\"],[\"aa0 146\"],[\"ab0 146\"],[\"ba0 402\"],[\"bb0 402\"],[\"ca0 402\",\"ca1 402\"],[\"cb0 402\",\"cb1 402\",\"cb2 402\"],[\"aa0 403\"],[\"ab0 403\"],[\"ba0 403\"],[\"bb0 403\"],[\"ca0 403\",\"ca1 403\"],[\"cb0 403\",\"cb1 403\",\"cb2 403\"],[\"aa0 404\"],[\"ab0 404\"],[\"ba0 404\"],[\"bb0 404\"],[\"ca0 404\",\"ca1 404\"],[\"cb0 404\",\"cb1 404\",\"cb2 404\"],[\"aa0 405\"],[\"ab0 405\"],[\"ba0 405\"],[\"bb0 405\"],[\"ca0 405\",\"ca1 405\"],[\"cb0 405\",\"cb1 405\",\"cb2 405\"],[\"aa0 437\"],[\"ab0 437\"],[\"aa0 485\"],[\"ab0 485\"],[\"aa0 503\"],[\"ab0 503\"],[\"aa0 569\"],[\"ab0 569\"],[\"aa0 581\"],[\"ab0 581\"],[\"aa0 620\"],[\"ab0 620\"],[\"aa0 632\"],[\"ab0 632\"],[\"aa0 640\"],[\"ab0 640\"],[\"aa0 650\"],[\"ab0 650\"],[\"aa0 669\"],[\"ab0 669\"],[\"aa0 671\"],[\"ab0 671\"],[\"aa0 728\"],[\"ab0 728\"],[\"aa0 735\"],[\"ab0 735\"],[\"aa0 772\"],[\"ab0 772\"],[\"aa0 784\"],[\"ab0 784\"],[\"aa0 811\"],[\"ab0 811\"],[\"aa0 817\"],[\"ab0 817\"],[\"aa0 836\"],[\"ab0 836\"],[\"aa0 881\"],[\"ab0 881\"],[\"aa0 891\"],[\"ab0 891\"],[\"aa0 924\"],[\"ab0 924\"],[\"aa0 1005\"],[\"ab0 1005\"],[\"aa0 1057\"],[\"ab0 1057\"],[\"aa0 1086\"],[\"ab0 1086\"],[\"aa0 1089\"],[\"ab0 1089\"],[\"aa0 1097\"],[\"ab0 1097\"],[\"aa0 1133\"],[\"ab0 1133\"],[\"aa0 1136\"],[\"ab0 1136\"],[\"aa0 1146\"],[\"ab0 1146\"],[\"aa0 1169\"],[\"ab0 1169\"],[\"aa0 1178\"],[\"ab0 1178\"],[\"aa0 1184\"],[\"ab0 1184\"],[\"aa0 1189\"],[\"ab0 1189\"],[\"aa0 1223\"],[\"ab0 1223\"],[\"aa0 1275\"],[\"ab0 1275\"],[\"aa0 1290\"],[\"ab0 1290\"],[\"aa0 1295\"],[\"ab0 1295\"],[\"aa0 1320\"],[\"ab0 1320\"],[\"aa0 1343\"],[\"ab0 1343\"],[\"aa0 1400\"],[\"ab0 1400\"],[\"aa0 1426\"],[\"ab0 1426\"],[\"aa0 1442\"],[\"ab0 1442\"],[\"aa0 1455\"],[\"ab0 1455\"],[\"aa0 1499\"],[\"ab0 1499\"],[\"aa0 1521\"],[\"ab0 1521\"],[\"aa0 1541\"],[\"ab0 1541\"],[\"aa0 1557\"],[\"ab0 1557\"],[\"aa0 1578\"],[\"ab0 1578\"],[\"aa0 1633\"],[\"ab0 1633\"],[\"aa0 1635\"],[\"ab0 1635\"],[\"aa0 1651\"],[\"ab0 1651\"],[\"aa0 1665\"],[\"ab0 1665\"],[\"aa0 1689\"],[\"ab0 1689\"],[\"aa0 1760\"],[\"ab0 1760\"],[\"aa0 1784\"],[\"ab0 1784\"],[\"aa0 1796\"],[\"ab0 1796\"],[\"aa0 1801\"],[\"ab0 1801\"],[\"aa0 1817\"],[\"ab0 1817\"],[\"aa0 1861\"],[\"ab0 1861\"],[\"aa0 1872\"],[\"ab0 1872\"],[\"aa0 1895\"],[\"ab0 1895\"],[\"aa0 1897\"],[\"ab0 1897\"],[\"aa0 1911\"],[\"ab0 1911\"],[\"aa0 1975\"],[\"ab0 1975\"],[\"aa0 1983\"],[\"ab0 1983\"],[\"aa0 1996\"],[\"ab0 1996\"],[\"aa0 2005\"],[\"ab0 2005\"],[\"aa0 2048\"],[\"ab0 2048\"],[\"aa0 2063\"],[\"ab0 2063\"],[\"aa0 2150\"],[\"ab0 2150\"],[\"aa0 2159\"],[\"ab0 2159\"],[\"aa0 2214\"],[\"ab0 2214\"],[\"aa0 2218\"],[\"ab0 2218\"],[\"aa0 2220\"],[\"ab0 2220\"],[\"aa0 2250\"],[\"ab0 2250\"],[\"aa0 2256\"],[\"ab0 2256\"],[\"aa0 2265\"],[\"ab0 2265\"],[\"aa0 2296\"],[\"ab0 2296\"],[\"aa0 2319\"],[\"ab0 2319\"],[\"aa0 2327\"],[\"ab0 2327\"],[\"aa0 2333\"],[\"ab0 2333\"],[\"aa0 2361\"],[\"ab0 2361\"],[\"aa0 2392\"],[\"ab0 2392\"],[\"aa0 2399\"],[\"ab0 2399\"],[\"aa0 2424\"],[\"ab0 2424\"],[\"aa0 2466\"],[\"ab0 2466\"],[\"aa0 2473\"],[\"ab0 2473\"],[\"aa0 2508\"],[\"ab0 2508\"],[\"aa0 2524\"],[\"ab0 2524\"],[\"aa0 2550\"],[\"ab0 2550\"],[\"aa0 2553\"],[\"ab0 2553\"],[\"aa0 2560\"],[\"ab0 2560\"],[\"aa0 2563\"],[\"ab0 2563\"],[\"aa0 2574\"],[\"ab0 2574\"],[\"aa0 2592\"],[\"ab0 2592\"],[\"aa0 2600\"],[\"ab0 2600\"],[\"aa0 2606\"],[\"ab0 2606\"],[\"aa0 2639\"],[\"ab0 2639\"],[\"aa0 2670\"],[\"ab0 2670\"],[\"aa0 2684\"],[\"ab0 2684\"],[\"aa0 2720\"],[\"ab0 2720\"],[\"aa0 2745\"],[\"ab0 2745\"],[\"aa0 2763\"],[\"ab0 2763\"],[\"aa0 2786\"],[\"ab0 2786\"],[\"aa0 2831\"],[\"ab0 2831\"],[\"aa0 2834\"],[\"ab0 2834\"],[\"aa0 2838\"],[\"ab0 2838\"],[\"aa0 2842\"],[\"ab0 2842\"],[\"aa0 2909\"],[\"ab0 2909\"],[\"aa0 2982\"],[\"ab0 2982\"],[\"aa0 2989\"],[\"ab0 2989\"],[\"aa0 2992\"],[\"ab0 2992\"],[\"aa0 3027\"],[\"ab0 3027\"],[\"aa0 3033\"],[\"ab0 3033\"],[\"aa0 3052\"],[\"ab0 3052\"],[\"aa0 3072\"],[\"ab0 3072\"],[\"aa0 3078\"],[\"ab0 3078\"],[\"aa0 3104\"],[\"ab0 3104\"],[\"aa0 3116\"],[\"ab0 3116\"],[\"aa0 3152\"],[\"ab0 3152\"],[\"aa0 3168\"],[\"ab0 3168\"],[\"aa0 3195\"],[\"ab0 3195\"],[\"aa0 3202\"],[\"ab0 3202\"],[\"aa0 3212\"],[\"ab0 3212\"],[\"aa0 3227\"],[\"ab0 3227\"],[\"aa0 3252\"],[\"ab0 3252\"],[\"aa0 3258\"],[\"ab0 3258\"],[\"aa0 3269\"],[\"ab0 3269\"],[\"aa0 3308\"],[\"ab0 3308\"],[\"aa0 3332\"],[\"ab0 3332\"],[\"aa0 3351\"],[\"ab0 3351\"],[\"aa0 3359\"],[\"ab0 3359\"],[\"aa0 3382\"],[\"ab0 3382\"],[\"aa0 3400\"],[\"ab0 3400\"],[\"aa0 3450\"],[\"ab0 3450\"],[\"aa0 3455\"],[\"ab0 3455\"],[\"aa0 3478\"],[\"ab0 3478\"],[\"aa0 3484\"],[\"ab0 3484\"],[\"aa0 3504\"],[\"ab0 3504\"],[\"aa0 3531\"],[\"ab0 3531\"],[\"aa0 3557\"],[\"ab0 3557\"],[\"aa0 3582\"],[\"ab0 3582\"],[\"aa0 3631\"],[\"ab0 3631\"],[\"aa0 3658\"],[\"ab0 3658\"],[\"aa0 3703\"],[\"ab0 3703\"],[\"aa0 3710\"],[\"ab0 3710\"],[\"aa0 3716\"],[\"ab0 3716\"],[\"aa0 3741\"],[\"ab0 3741\"],[\"aa0 3759\"],[\"ab0 3759\"],[\"aa0 3803\"],[\"ab0 3803\"],[\"aa0 3852\"],[\"ab0 3852\"],[\"aa0 3874\"],[\"ab0 3874\"],[\"aa0 3884\"],[\"ab0 3884\"],[\"aa0 3887\"],[\"ab0 3887\"],[\"aa0 3889\"],[\"ab0 3889\"],[\"aa0 3981\"],[\"ab0 3981\"],[\"aa0 3993\"],[\"ab0 3993\"],[\"aa0 4012\"],[\"ab0 4012\"],[\"aa0 4024\"],[\"ab0 4024\"],[\"aa0 4032\"],[\"ab0 4032\"],[\"aa0 4042\"],[\"ab0 4042\"],[\"aa0 4066\"],[\"ab0 4066\"],[\"aa0 4088\"],[\"ab0 4088\"],[\"aa0 4095\"],[\"ab0 4095\"]],[[\"\"],[\"bb0 3741\"],[\"ba0 3759\"],[\"bb0 3759\"],[\"ba0 3803\"],[\"bb0 3803\"],[\"ba0 3814\"],[\"bb0 3814\"],[\"ba0 3852\"],[\"bb0 3852\"],[\"ba0 3874\"],[\"bb0 3874\"],[\"ba0 3884\"],[\"bb0 3884\"],[\"ba0 3887\"],[\"bb0 3887\"],[\"ba0 3889\"],[\"bb0 3889\"],[\"ba0 3957\"],[\"bb0 3957\"],[\"ba0 3981\"],[\"bb0 3981\"],[\"ba0 3993\"],[\"bb0 3993\"],[\"ba0 4012\"],[\"bb0 4012\"],[\"ba0 4024\"],[\"bb0 4024\"],[\"ba0 4032\"],[\"bb0 4032\"],[\"ba0 4042\"],[\"bb0 4042\"],[\"ba0 4066\"],[\"bb0 4066\"],[\"ba0 4088\"],[\"bb0 4088\"],[\"ba0 4095\"],[\"bb0 4095\"]],[[\"ca0 4095\",\"ca1 4095\"],[\"cb0 4095\",\"cb1 4095\",\"cb2 4095\"]]] |\r\n+------------+------------+\r\n{code}\r\n\r\nphysical plan\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> explain plan for select t.id, t.aaa from `complex.json` t where t.id=1 limit 1;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(id=[$0], aaa=[$1])\r\n00-02        SelectionVectorRemover\r\n00-03          Limit(fetch=[1])\r\n00-04            Filter(condition=[=($0, 1)])\r\n00-05              Project(id=[$1], aaa=[$0])\r\n00-06                Scan(groupscan=[EasyGroupScan [selectionRoot=/drill/testdata/complex_type/json/complex.json, numFiles=1, columns=[`id`, `aaa`], files=[maprfs:/drill/testdata/complex_type/json/complex.json]]])\r\n{code}",
        "RepeatedListVector does not populate empty values properly The issue is DRILL-1539 of RepeatedListVector. Symptoms are the same. Also we need to refactor RepeatedListVector as well as RepeatedMapVector so that both classes will reuse the same code path for empty value population."
    ],
    [
        "DRILL-2094",
        "DRILL-1302",
        "Drill has problems with reading json fields when used in a subquery git.commit.id.abbrev=3e33880\r\n\r\nData Set :\r\n{code}\r\n{\r\n  \"id\" : 1,\r\n  \"list\" : [1,2]\r\n}\r\n{code} \r\n\r\nThe below query works\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select id from `temp1.json` order by list[0];\r\n+------------+\r\n|     id     |\r\n+------------+\r\n| 1          |\r\n+------------+\r\n1 row selected (0.146 seconds)\r\n{code}\r\n\r\nHowever when I used the same exact query as part of a sub-query, I get an error from drill\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select s.id from (select id from `temp1.json` order by list[0]) s;\r\nQuery failed: SqlValidatorException: Table 'list' not found\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nExplain plan also does not work and it returns the same problem",
        "Order by using table.col in subquery fails with ArrayIndexOutOfBoundsException  git.commit.id.abbrev=687b9b0\r\n\r\nThe following query runs successfully:\r\nselect cast(student.name as varchar(30)) name, cast(voter.registration as varchar(20)) registration from voter full outer join student on (student.name = voter.name) where student.age < 20 order by student.name;\r\n\r\nIf I put the above query in a sub-select, it fails:\r\n0: jdbc:drill:schema=dfs> select tbl.name, tbl.registration from (select cast(student.name as varchar(30)) name, cast(voter.registration as varchar(20)) registration from `dfs`.`default`.`./voter` voter full outer join `dfs`.`default`.`./student` student on (student.name = voter.name) where student.age < 20 order by student.name) tbl;\r\nQuery failed: Failure while parsing sql. -1 [fe42e70f-cf5b-4389-b879-5b1598ab887f]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n\r\nIf I change the query using \"order by name\" without specifying the student table, the query runs fine:\r\n0: jdbc:drill:schema=dfs> select tbl.name, tbl.registration from (select cast(student.name as varchar(30)) name, cast(voter.registration as varchar(20)) registration from `dfs`.`default`.`./voter` voter full outer join `dfs`.`default`.`./student` student on (student.name = voter.name) where student.age < 20 order by student.name) tbl;\r\n\r\nThe student.name in the order by clause should work."
    ],
    [
        "DRILL-2094",
        "DRILL-1941",
        "Drill has problems with reading json fields when used in a subquery git.commit.id.abbrev=3e33880\r\n\r\nData Set :\r\n{code}\r\n{\r\n  \"id\" : 1,\r\n  \"list\" : [1,2]\r\n}\r\n{code} \r\n\r\nThe below query works\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select id from `temp1.json` order by list[0];\r\n+------------+\r\n|     id     |\r\n+------------+\r\n| 1          |\r\n+------------+\r\n1 row selected (0.146 seconds)\r\n{code}\r\n\r\nHowever when I used the same exact query as part of a sub-query, I get an error from drill\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select s.id from (select id from `temp1.json` order by list[0]) s;\r\nQuery failed: SqlValidatorException: Table 'list' not found\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nExplain plan also does not work and it returns the same problem",
        "select from subquery having an order by fails git.commit.id.abbrev=b491cdb\r\n\r\nThe following query fails:\r\n0: jdbc:drill:schema=dfs>  select q.name as name, q.registration as registration from (select cast(student.name as varchar(30)) as name, cast(voter.registration as varchar(20)) as registration from `dfs`.`default`.`voter` voter full outer join `dfs`.`default`.`./student` student on (student.name = voter.name) where student.age < 30 order by student.name) q;\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: -1\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nRunning the same query with \"explain plan for\" also fail with the same error.  If I remove the \"order by student.name\" from the sub-query, the query run successfully."
    ],
    [
        "DRILL-2094",
        "DRILL-1978",
        "Drill has problems with reading json fields when used in a subquery git.commit.id.abbrev=3e33880\r\n\r\nData Set :\r\n{code}\r\n{\r\n  \"id\" : 1,\r\n  \"list\" : [1,2]\r\n}\r\n{code} \r\n\r\nThe below query works\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select id from `temp1.json` order by list[0];\r\n+------------+\r\n|     id     |\r\n+------------+\r\n| 1          |\r\n+------------+\r\n1 row selected (0.146 seconds)\r\n{code}\r\n\r\nHowever when I used the same exact query as part of a sub-query, I get an error from drill\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select s.id from (select id from `temp1.json` order by list[0]) s;\r\nQuery failed: SqlValidatorException: Table 'list' not found\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nExplain plan also does not work and it returns the same problem",
        "Using order by with a cast function and array index fails to plan. {code}\r\nCREATE TABLE newStar AS \r\n\r\nSELECT \r\n  columns[0], columns[1], columns[2], columns[3], columns[4], columns[5], \r\n  columns[6], columns[7], columns[8], columns[9], columns[10], columns[11], \r\n  columns[12], columns[13], columns[14], columns[15]\r\nFROM dfs.`file.csv` ORDER BY cast(columns[2] as int);\r\n{code}\r\n\r\n\r\n{code}\r\norg.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: Internal error: Error while applying rule ExpandConversionRule, args [rel#98:AbstractConverter.PHYSICAL.ANY([]).[16](child=rel#79:Subset#9.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[16])]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:194) [classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\nCaused by: java.lang.AssertionError: Internal error: Error while applying rule ExpandConversionRule, args [rel#98:AbstractConverter.PHYSICAL.ANY([]).[16](child=rel#79:Subset#9.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[16])]\r\n\tat org.eigenbase.util.Util.newInternal(Util.java:750) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:246) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:661) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.tools.Programs$RuleSetProgram.run(Programs.java:165) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.prepare.PlannerImpl.transform(PlannerImpl.java:273) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToPrel(DefaultSqlHandler.java:167) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.CreateTableHandler.getPlan(CreateTableHandler.java:112) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:507) [classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:185) [classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\n\t... 4 more\r\nCaused by: java.lang.IndexOutOfBoundsException: index (16) must be less than size (16)\r\n\tat com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305) ~[guava-14.0.1.jar:na]\r\n\tat com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284) ~[guava-14.0.1.jar:na]\r\n\tat com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:81) ~[guava-14.0.1.jar:na]\r\n\tat org.eigenbase.rex.RexBuilder.makeInputRef(RexBuilder.java:764) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.SortRel.<init>(SortRel.java:94) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.SortRel.<init>(SortRel.java:59) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:78) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:1) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1011) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1102) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.AbstractConverter$ExpandConversionRule.onMatch(AbstractConverter.java:108) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:223) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:661) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.tools.Programs$RuleSetProgram.run(Programs.java:165) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.prepare.PlannerImpl.transform(PlannerImpl.java:273) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToPrel(DefaultSqlHandler.java:167) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.CreateTableHandler.getPlan(CreateTableHandler.java:112) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:507) [classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:185) [classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\n\t... 12 more\r\n{code}\r\n"
    ],
    [
        "DRILL-2094",
        "DRILL-2075",
        "Drill has problems with reading json fields when used in a subquery git.commit.id.abbrev=3e33880\r\n\r\nData Set :\r\n{code}\r\n{\r\n  \"id\" : 1,\r\n  \"list\" : [1,2]\r\n}\r\n{code} \r\n\r\nThe below query works\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select id from `temp1.json` order by list[0];\r\n+------------+\r\n|     id     |\r\n+------------+\r\n| 1          |\r\n+------------+\r\n1 row selected (0.146 seconds)\r\n{code}\r\n\r\nHowever when I used the same exact query as part of a sub-query, I get an error from drill\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select s.id from (select id from `temp1.json` order by list[0]) s;\r\nQuery failed: SqlValidatorException: Table 'list' not found\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nExplain plan also does not work and it returns the same problem",
        "Subquery not projecting the order by column is causing issues when used along with flatten git.commit.id.abbrev=3c6d0ef\r\n\r\nData Set :\r\n{code}\r\n{\r\n  \"uid\" : 1,\r\n  \"uid_str\" : \"01\",\r\n  \"type\" : \"web\",\r\n  \"events\" : [\r\n        { \"evnt_id\":\"e1\", \"campaign_id\":\"c1\", \"event_name\":\"e1_name\", \"event_time\":1000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e2\", \"campaign_id\":\"c1\", \"event_name\":\"e2_name\", \"event_time\":2000000, \"type\" : \"cmpgn4\"},\r\n        { \"evnt_id\":\"e3\", \"campaign_id\":\"c1\", \"event_name\":\"e3_name\", \"event_time\":3000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e4\", \"campaign_id\":\"c1\", \"event_name\":\"e4_name\", \"event_time\":4000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e5\", \"campaign_id\":\"c2\", \"event_name\":\"e5_name\", \"event_time\":5000000, \"type\" : \"cmpgn3\"},\r\n        { \"evnt_id\":\"e6\", \"campaign_id\":\"c1\", \"event_name\":\"e6_name\", \"event_time\":6000000, \"type\" : \"cmpgn9\"},\r\n        { \"evnt_id\":\"e7\", \"campaign_id\":\"c1\", \"event_name\":\"e7_name\", \"event_time\":7000000, \"type\" : \"cmpgn3\"},\r\n        { \"evnt_id\":\"e8\", \"campaign_id\":\"c2\", \"event_name\":\"e8_name\", \"event_time\":8000000, \"type\" : \"cmpgn2\"},\r\n        { \"evnt_id\":\"e9\", \"campaign_id\":\"c2\", \"event_name\":\"e9_name\", \"event_time\":9000000, \"type\" : \"cmpgn4\"}\r\n  ]\r\n}\r\n{code}\r\n\r\nQuery with flatten :\r\n{code}\r\nselect s.type  from (select d.type type, flatten(d.events) evnts from `temp.json` d where d.type='web' order by d.uid) s where s.type='web';\r\nQuery failed: IndexOutOfBoundsException: index (2) must be less than size (1)\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nQuery without flatten :\r\n{code}\r\nselect s.type  from (select d.type type from `temp.json` d where d.type='web' order by d.uid) s where s.type='web';\r\n+------------+\r\n|    type    |\r\n+------------+\r\n| web        |\r\n+------------+\r\n{code}\r\n\r\nHowever even the above query fails when we remove the filter and this could be related to DRILL-1302\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select s.type  from (select d.type type from `temp.json` d where d.type='web' order by d.uid) s;\r\nQuery failed: ArrayIndexOutOfBoundsException: -1\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}"
    ],
    [
        "DRILL-2102",
        "DRILL-1373",
        "Empty query yields HTTP 500 error and poor error message. In the Drill web UI, going to the query page and clicking Submit (without entering any query text) yields a page saying:\r\n\r\nHTTP ERROR 500\r\n\r\nProblem accessing /query. Reason:\r\n\r\n    Request failed.\r\n\r\n(rather than giving, say, a syntax error reporting that the query is empty).\r\n\r\n",
        "Executing a query with a semi-colon as the last character causes 500 error in the query GUI Attempting to execute a query on the Query GUI page (http://node:8047/query) that contains a semi-colon at the end of the query statement results in a 500 error.\r\n{noformat}\r\nselect count(*) from storageplugin.workspace;\r\n{noformat}\r\nRemoving the semi-colon returns the query result successfully."
    ],
    [
        "DRILL-2102",
        "DRILL-1515",
        "Empty query yields HTTP 500 error and poor error message. In the Drill web UI, going to the query page and clicking Submit (without entering any query text) yields a page saying:\r\n\r\nHTTP ERROR 500\r\n\r\nProblem accessing /query. Reason:\r\n\r\n    Request failed.\r\n\r\n(rather than giving, say, a syntax error reporting that the query is empty).\r\n\r\n",
        "Rest/Web interface hangs if there is any error/exception in query execution Web interface hangs if there is any error/exception in query. We should have a proper error handling mechanism in place."
    ],
    [
        "DRILL-2102",
        "DRILL-1740",
        "Empty query yields HTTP 500 error and poor error message. In the Drill web UI, going to the query page and clicking Submit (without entering any query text) yields a page saying:\r\n\r\nHTTP ERROR 500\r\n\r\nProblem accessing /query. Reason:\r\n\r\n    Request failed.\r\n\r\n(rather than giving, say, a syntax error reporting that the query is empty).\r\n\r\n",
        "Web UI returns HTTP 500 with no explanation When submitting a SQL query through the Web UI I get this error with no explanation on how to troubleshoot. I'm not sure whether the Web UI query tool is broken - it may be, as I am able to submit the same query through sqlline (Drill is in distributed mode, so it's the same drillbit). There has to be an actionable explanation/error message or at least a pointer to the relevant log file.\r\n\r\nHTTP ERROR 500\r\n\r\nProblem accessing /query. Reason:\r\n\r\n    Request failed.\r\n\r\n"
    ],
    [
        "DRILL-2115",
        "DRILL-1921",
        "Disable function CARDINALITY in grammar Since we don't support multiset type, we should disable CARDINALITY function as well:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select cardinality(list) from `test.json`;\r\nQuery failed: Query stopped., Failure while trying to materialize incoming schema.  Errors:\r\n \r\nError in expression at index -1.  Error: Missing function implementation: [cardinality(BIGINT-REPEATED)].  Full expression: --UNKNOWN EXPRESSION--.. [ db86cf79-6083-4ad7-afa1-fac534b942bc on atsqa4-134.qa.lab:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n0: jdbc:drill:schema=dfs> select cardinality(id) from `test.json`;\r\nQuery failed: Query stopped., Failure while trying to materialize incoming schema.  Errors:\r\n \r\nError in expression at index -1.  Error: Missing function implementation: [cardinality(BIGINT-OPTIONAL)].  Full expression: --UNKNOWN EXPRESSION--.. [ 1f209cb2-4d60-4c1d-b1d0-d921f8e8a913 on atsqa4-134.qa.lab:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n0: jdbc:drill:schema=dfs> select cardinality() from `test.json`;\r\nQuery failed: SqlValidatorException: Invalid number of arguments to function 'CARDINALITY'. Was expecting 1 arguments\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n0: jdbc:drill:schema=dfs> select cardinality('aa') from `test.json`;\r\nQuery failed: SqlValidatorException: Cannot apply 'CARDINALITY' to arguments of type 'CARDINALITY(<CHAR(2)>)'. Supported form(s): 'CARDINALITY(<MULTISET>)'\r\n'CARDINALITY(<ARRAY>)'\r\n'CARDINALITY(<MAP>)'\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}",
        "Throw unsupported error message some set operators that are not currently supported Throw unsupported error message for these operators: (instead of \"Could not be implemented error below\")\r\n\r\nINTERSECT\r\nEXCEPT\r\nUNION\r\nCROSS JOIN\r\n\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: Node [rel#517133:Subset#3.LOGICAL.ANY([]).[]] could not be implemented; planner state:\r\n\r\nEnhacement requests in Jira for these above mentioned operators:\r\n\r\n{code}\r\nINTERSECT       DRILL-1308\r\nMINUS                No enhancement request ( not recognized in grammar)\r\nEXCEPT             No enhancement request\r\nUNION                DRILL-1169\r\nCROSS JOIN     DRILL-786\r\n{code}"
    ],
    [
        "DRILL-2115",
        "DRILL-2016",
        "Disable function CARDINALITY in grammar Since we don't support multiset type, we should disable CARDINALITY function as well:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select cardinality(list) from `test.json`;\r\nQuery failed: Query stopped., Failure while trying to materialize incoming schema.  Errors:\r\n \r\nError in expression at index -1.  Error: Missing function implementation: [cardinality(BIGINT-REPEATED)].  Full expression: --UNKNOWN EXPRESSION--.. [ db86cf79-6083-4ad7-afa1-fac534b942bc on atsqa4-134.qa.lab:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n0: jdbc:drill:schema=dfs> select cardinality(id) from `test.json`;\r\nQuery failed: Query stopped., Failure while trying to materialize incoming schema.  Errors:\r\n \r\nError in expression at index -1.  Error: Missing function implementation: [cardinality(BIGINT-OPTIONAL)].  Full expression: --UNKNOWN EXPRESSION--.. [ 1f209cb2-4d60-4c1d-b1d0-d921f8e8a913 on atsqa4-134.qa.lab:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n0: jdbc:drill:schema=dfs> select cardinality() from `test.json`;\r\nQuery failed: SqlValidatorException: Invalid number of arguments to function 'CARDINALITY'. Was expecting 1 arguments\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n0: jdbc:drill:schema=dfs> select cardinality('aa') from `test.json`;\r\nQuery failed: SqlValidatorException: Cannot apply 'CARDINALITY' to arguments of type 'CARDINALITY(<CHAR(2)>)'. Supported form(s): 'CARDINALITY(<MULTISET>)'\r\n'CARDINALITY(<ARRAY>)'\r\n'CARDINALITY(<MAP>)'\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}",
        "Remove \"multiset\" data type from supported grammar Should throw \"unsupported error\"\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select cast(a1 as multiset) from `t1.json`;\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: use createMultisetType() instead\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n"
    ],
    [
        "DRILL-2137",
        "DRILL-2132",
        "ResultsSetMetaData.getColumnName() returns \"none\" (rather than right class name) The implementation of {{java.sql.ResultsSetMetaData.getColumnName()}} returns \"none\" rather than returning the correct class name (\"the class ... that would be used by the method {{ResultSet.getObject}} to retrieve the value in the specified column\").\r\n\r\nDrillColumnMetaData.updateColumnMetaData(...) has the hard-coded \"none\" value.",
        "ResultSetMetaData.getColumnClassName(...) returns \"none\" (rather than a class name) {{ResultSetMetaData}}'s {{getColumnClassName(...)}} returns the string {{\"none\"}} (rather than the name of a class), and least for the result set returned by {{DatabaseMetaData.getColumns(...)}}.\r\n"
    ],
    [
        "DRILL-2151",
        "DRILL-1588",
        "VariableLengthVector.copyFromSafe() unnecessary sets the offsetVector The following method defined in {{VariableLengthVectors.java}}:\r\n\r\n{code}\r\npublic boolean copyFromSafe(int fromIndex, int thisIndex, ${minor.class}Vector from){\r\n    int start = from.offsetVector.getAccessor().get(fromIndex);\r\n    int end =   from.offsetVector.getAccessor().get(fromIndex+1);\r\n    int len = end - start;\r\n    \r\n    int outputStart = offsetVector.data.get${(minor.javaType!type.javaType)?cap_first}(thisIndex * ${type.width});\r\n    \r\n    if(data.capacity() < outputStart + len) {\r\n        reAlloc();\r\n    }\r\n\r\n    offsetVector.getMutator().setSafe(thisIndex + 1, outputStart + len);\r\n\r\n    from.data.getBytes(start, data, outputStart, len);\r\n    \r\n    offsetVector.getMutator().setSafe( (thisIndex+1) * ${type.width}, outputStart + len);\r\n\r\n    return true;\r\n}\r\n{code}\r\n\r\nat the very end, it multiplies {{thisIndex+1}} by the type width, but the mutator will also multiply the index by the type width. This line should be removed because the offset is already set correctly in the same method.\r\n\r\nOne possible query where this actually causes problem is the one defined in DRILL-1588 (on TPCH SF 1):\r\n{noformat}\r\n// set slice target to 1 to ensure exchanges are used\r\n0: jdbc:drill:zk=local> alter session set `planner.slice_target` = 1;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | planner.slice_target updated. |\r\n+------------+------------+\r\n1 row selected (0.1 seconds)\r\n0: jdbc:drill:zk=local> select * from lineitem l left outer join orders o on (l.l_orderkey = o.o_orderkey) left outer join  customer c on (l.l_orderkey = c.c_custkey) left outer join  nation n on (l.l_partkey = n.n_nationkey) left outer join  region r  on (l.l_suppkey = r.r_regionkey) limit 10;\r\n{noformat}\r\n\r\nHere is a copy of the execution in the logs:\r\n{noformat}\r\njava.lang.IndexOutOfBoundsException: index: 60496, length: 4 (expected: range(0, 32768))\r\nat io.netty.buffer.DrillBuf.checkIndexD(DrillBuf.java:156) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\nat io.netty.buffer.DrillBuf.chk(DrillBuf.java:178) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\nat io.netty.buffer.DrillBuf.setInt(DrillBuf.java:473) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\nat org.apache.drill.exec.vector.UInt4Vector$Mutator.set(UInt4Vector.java:359) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\nat org.apache.drill.exec.vector.UInt4Vector$Mutator.setSafe(UInt4Vector.java:366) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\nat org.apache.drill.exec.vector.VarBinaryVector.copyFromSafe(VarBinaryVector.java:259) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\nat org.apache.drill.exec.vector.NullableVarBinaryVector.copyFromSafe(NullableVarBinaryVector.java:301) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\nat org.apache.drill.exec.test.generated.HashJoinProbeGen28.projectBuildRecord(HashJoinProbeTemplate.java:718) ~[na:na]\r\nat org.apache.drill.exec.test.generated.HashJoinProbeGen28.executeProbePhase(HashJoinProbeTemplate.java:173) ~[na:na]\r\nat org.apache.drill.exec.test.generated.HashJoinProbeGen28.probeAndProject(HashJoinProbeTemplate.java:223) ~[na:na]\r\nat org.apache.drill.exec.physical.impl.join.HashJoinBatch.innerNext(HashJoinBatch.java:227) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\nat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\nat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\nat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\nat org.apache.drill.exec.physical.impl.partitionsender.PartitionSenderRootExec.innerNext(PartitionSenderRootExec.java:133) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\nat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\nat org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:110) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\nat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_71]\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]\r\nat java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]\r\n{noformat}",
        "IndexOutOfBoundException for a query with left outer joins Use TPCH  SF1. \r\n\r\n{code:sql}\r\n// set slice target to 1 to ensure exchanges are used\r\n0: jdbc:drill:zk=local> alter session set `planner.slice_target` = 1;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | planner.slice_target updated. |\r\n+------------+------------+\r\n1 row selected (0.1 seconds)\r\n0: jdbc:drill:zk=local> select * from lineitem l left outer join orders o on (l.l_orderkey = o.o_orderkey) left outer join  customer c on (l.l_orderkey = c.c_custkey) left outer join  nation n on (l.l_partkey = n.n_nationkey) left outer join  region r  on (l.l_suppkey = r.r_regionkey) limit 10;\r\n{code}\r\n\r\njava.lang.IndexOutOfBoundsException: writerIndex: 8196 (expected: readerIndex(0) <= writerIndex <= capacity(8192))\r\n        io.netty.buffer.AbstractByteBuf.writerIndex(AbstractByteBuf.java:88) ~[netty-buffer-4.0.20.Final.jar:4.0.20.Final]\r\n        org.apache.drill.exec.vector.VectorTrimmer.trim(VectorTrimmer.java:27) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n        org.apache.drill.exec.vector.UInt4Vector$Mutator.setValueCount(UInt4Vector.java:424) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n        org.apache.drill.exec.vector.VarBinaryVector$Mutator.setValueCount(VarBinaryVector.java:587) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n        org.apache.drill.exec.vector.NullableVarBinaryVector$Mutator.setValueCount(NullableVarBinaryVector.java:546) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n        org.apache.drill.exec.physical.impl.join.HashJoinBatch.innerNext(HashJoinBatch.java:238) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n        org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n        org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n        org.apache.drill.exec.physical.impl.partitionsender.PartitionSenderRootExec.innerNext(PartitionSenderRootExec.java:141) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n"
    ],
    [
        "DRILL-2159",
        "DRILL-1900",
        "TableStatsCalculator In TableStatsCalculator.getRegionSizeInBytes method, if avgRowSizeInBytes is to large, the return value will be out of int range. So the code should be fixed like \"return ((long)avgRowSizeInBytes)*1024L*1024L\".",
        "Hbase statistics collector could hit numeric overflow problem, when average row size in HBase table goes beyond a threshold.   "
    ],
    [
        "DRILL-2163",
        "DRILL-2122",
        "Projecting nested types past join returns invalid result Take the following data files and query:\r\n\r\n{code:title=a.json}\r\n{\r\n  \"uid\": 1,\r\n  \"events\" : [\r\n    { \"evnt_id\":\"e1\"}\r\n  ]\r\n}\r\n{code}\r\n\r\n\r\n{code:title=b.json}\r\n{\r\n  \"uid\": 1,\r\n  \"transactions\" : [\r\n    { \"trans_id\":\"t1\"}\r\n  ]\r\n}\r\n{code}\r\n\r\n{code}\r\nselect t1.events, t2.transactions from dfs.`local`.`2122/a.json` t1, dfs.`local`.`2122/b.json` t2 where t1.uid = t2.uid\r\n{code}\r\n\r\nThe projection of a nested value -coming out of right side of join-, {{transactions}} as of b6a3878, reports empty result. ",
        "Projecting a repeated map after a join fails with an obscure error git.commit.id.abbrev=3e33880\r\n\r\nData Set 1:\r\n{code}\r\n{\r\n    \"uid\": 1,\r\n    \"events\" : [\r\n        { \"evnt_id\":\"e1\"}\r\n  ]\r\n}\r\n{code}\r\n\r\nData Set 2:\r\n{code}\r\n{\r\n  \"uid1\": 1,\r\n  \"transactions\" : [\r\n       { \"trans_id\":\"t1\"}\r\n  ]\r\n}\r\n{code}\r\n\r\nQuery :\r\n{code}\r\nselect t1.uid, t1.events from  `data1.json` t1, `data2.json` t2 where t1.uid = t2.uid1;\r\nError: exception while executing query: null (state=,code=0)\r\n{code}\r\n\r\nThe logs were not helpful in this case:\r\n{code}\r\n2015-01-29 23:12:54,474 [2b354189-5439-e82d-e85e-3728b466c6dc:foreman] INFO  o.a.drill.exec.work.foreman.Foreman - State change requested.  PENDING --> RUNNING\r\n2015-01-29 23:12:54,532 [UserServer-1] INFO  o.a.drill.exec.work.foreman.Foreman - State change requested.  RUNNING --> CANCELED\r\n2015-01-29 23:12:54,542 [UserServer-1] INFO  o.a.drill.exec.work.foreman.Foreman - State change requested.  CANCELED --> COMPLETED\r\n2015-01-29 23:12:54,543 [UserServer-1] WARN  o.a.drill.exec.work.foreman.Foreman - Dropping request to move to COMPLETED state as query is already at CANCELED state (which is terminal).\r\n{code}\r\n\r\nExplain Plan :\r\n{code}\r\n00-00    Screen\r\n00-01      Project(uid=[$0], events=[$1])\r\n00-02        Project(uid=[$0], events=[$1])\r\n00-03          HashJoin(condition=[=($0, $2)], joinType=[inner])\r\n00-05            Project(uid=[$1], events=[$0])\r\n00-06              Scan(groupscan=[EasyGroupScan [selectionRoot=/drill/testdata/flatten_operators/data1.json, numFiles=1, columns=[`uid`, `events`], files=[maprfs:/drill/testdata/flatten_operators/temp3.json]]])\r\n00-04            Scan(groupscan=[EasyGroupScan [selectionRoot=/drill/testdata/flatten_operators/data2.json, numFiles=1, columns=[`uid1`], files=[maprfs:/drill/testdata/flatten_operators/temp4.json]]])\r\n{code}"
    ],
    [
        "DRILL-2176",
        "DRILL-1302",
        "IndexOutOfBoundsException for count(*) on a subquery which does order-by The IOBE occurs in creating the collation trait in Calcite.  \r\n{code}\r\n0: jdbc:drill:zk=local> select count(*) from (select n_nationkey, n_regionkey from cp.`tpch/nation.parquet` order by 1, 2);\r\nQuery failed: IndexOutOfBoundsException: index (1) must be less than size (1)\r\n{code}\r\n\r\nFull stack trace: \r\n{code}\r\naused by: java.lang.IndexOutOfBoundsException: index (1) must be less than size (1)\r\n        at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305) ~[guava-14.0.1.jar:na]\r\n        at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284) ~[guava-14.0.1.jar:na]\r\n        at com.google.common.collect.SingletonImmutableList.get(SingletonImmutableList.java:45) ~[guava-14.0.1.jar:na]\r\n        at org.eigenbase.rex.RexBuilder.makeInputRef(RexBuilder.java:764) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.SortRel.<init>(SortRel.java:94) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.SortRel.<init>(SortRel.java:59) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:78) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:1) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1011) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1102) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.AbstractConverter$ExpandConversionRule.onMatch(AbstractConverter.java:108) ~[optiq-core-0.9-drill-r18.jar:na]\r\n{code}\r\n\r\nThis might be related to CALCITE-569 (and possibly DRILL-1978) but the stack traces are different, so I am treating this as a separate issue.  ",
        "Order by using table.col in subquery fails with ArrayIndexOutOfBoundsException  git.commit.id.abbrev=687b9b0\r\n\r\nThe following query runs successfully:\r\nselect cast(student.name as varchar(30)) name, cast(voter.registration as varchar(20)) registration from voter full outer join student on (student.name = voter.name) where student.age < 20 order by student.name;\r\n\r\nIf I put the above query in a sub-select, it fails:\r\n0: jdbc:drill:schema=dfs> select tbl.name, tbl.registration from (select cast(student.name as varchar(30)) name, cast(voter.registration as varchar(20)) registration from `dfs`.`default`.`./voter` voter full outer join `dfs`.`default`.`./student` student on (student.name = voter.name) where student.age < 20 order by student.name) tbl;\r\nQuery failed: Failure while parsing sql. -1 [fe42e70f-cf5b-4389-b879-5b1598ab887f]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n\r\nIf I change the query using \"order by name\" without specifying the student table, the query runs fine:\r\n0: jdbc:drill:schema=dfs> select tbl.name, tbl.registration from (select cast(student.name as varchar(30)) name, cast(voter.registration as varchar(20)) registration from `dfs`.`default`.`./voter` voter full outer join `dfs`.`default`.`./student` student on (student.name = voter.name) where student.age < 20 order by student.name) tbl;\r\n\r\nThe student.name in the order by clause should work."
    ],
    [
        "DRILL-2176",
        "DRILL-1941",
        "IndexOutOfBoundsException for count(*) on a subquery which does order-by The IOBE occurs in creating the collation trait in Calcite.  \r\n{code}\r\n0: jdbc:drill:zk=local> select count(*) from (select n_nationkey, n_regionkey from cp.`tpch/nation.parquet` order by 1, 2);\r\nQuery failed: IndexOutOfBoundsException: index (1) must be less than size (1)\r\n{code}\r\n\r\nFull stack trace: \r\n{code}\r\naused by: java.lang.IndexOutOfBoundsException: index (1) must be less than size (1)\r\n        at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305) ~[guava-14.0.1.jar:na]\r\n        at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284) ~[guava-14.0.1.jar:na]\r\n        at com.google.common.collect.SingletonImmutableList.get(SingletonImmutableList.java:45) ~[guava-14.0.1.jar:na]\r\n        at org.eigenbase.rex.RexBuilder.makeInputRef(RexBuilder.java:764) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.SortRel.<init>(SortRel.java:94) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.SortRel.<init>(SortRel.java:59) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:78) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:1) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1011) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1102) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.AbstractConverter$ExpandConversionRule.onMatch(AbstractConverter.java:108) ~[optiq-core-0.9-drill-r18.jar:na]\r\n{code}\r\n\r\nThis might be related to CALCITE-569 (and possibly DRILL-1978) but the stack traces are different, so I am treating this as a separate issue.  ",
        "select from subquery having an order by fails git.commit.id.abbrev=b491cdb\r\n\r\nThe following query fails:\r\n0: jdbc:drill:schema=dfs>  select q.name as name, q.registration as registration from (select cast(student.name as varchar(30)) as name, cast(voter.registration as varchar(20)) as registration from `dfs`.`default`.`voter` voter full outer join `dfs`.`default`.`./student` student on (student.name = voter.name) where student.age < 30 order by student.name) q;\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: -1\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nRunning the same query with \"explain plan for\" also fail with the same error.  If I remove the \"order by student.name\" from the sub-query, the query run successfully."
    ],
    [
        "DRILL-2176",
        "DRILL-1978",
        "IndexOutOfBoundsException for count(*) on a subquery which does order-by The IOBE occurs in creating the collation trait in Calcite.  \r\n{code}\r\n0: jdbc:drill:zk=local> select count(*) from (select n_nationkey, n_regionkey from cp.`tpch/nation.parquet` order by 1, 2);\r\nQuery failed: IndexOutOfBoundsException: index (1) must be less than size (1)\r\n{code}\r\n\r\nFull stack trace: \r\n{code}\r\naused by: java.lang.IndexOutOfBoundsException: index (1) must be less than size (1)\r\n        at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305) ~[guava-14.0.1.jar:na]\r\n        at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284) ~[guava-14.0.1.jar:na]\r\n        at com.google.common.collect.SingletonImmutableList.get(SingletonImmutableList.java:45) ~[guava-14.0.1.jar:na]\r\n        at org.eigenbase.rex.RexBuilder.makeInputRef(RexBuilder.java:764) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.SortRel.<init>(SortRel.java:94) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.SortRel.<init>(SortRel.java:59) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:78) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:1) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1011) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1102) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.AbstractConverter$ExpandConversionRule.onMatch(AbstractConverter.java:108) ~[optiq-core-0.9-drill-r18.jar:na]\r\n{code}\r\n\r\nThis might be related to CALCITE-569 (and possibly DRILL-1978) but the stack traces are different, so I am treating this as a separate issue.  ",
        "Using order by with a cast function and array index fails to plan. {code}\r\nCREATE TABLE newStar AS \r\n\r\nSELECT \r\n  columns[0], columns[1], columns[2], columns[3], columns[4], columns[5], \r\n  columns[6], columns[7], columns[8], columns[9], columns[10], columns[11], \r\n  columns[12], columns[13], columns[14], columns[15]\r\nFROM dfs.`file.csv` ORDER BY cast(columns[2] as int);\r\n{code}\r\n\r\n\r\n{code}\r\norg.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: Internal error: Error while applying rule ExpandConversionRule, args [rel#98:AbstractConverter.PHYSICAL.ANY([]).[16](child=rel#79:Subset#9.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[16])]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:194) [classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\nCaused by: java.lang.AssertionError: Internal error: Error while applying rule ExpandConversionRule, args [rel#98:AbstractConverter.PHYSICAL.ANY([]).[16](child=rel#79:Subset#9.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[16])]\r\n\tat org.eigenbase.util.Util.newInternal(Util.java:750) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:246) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:661) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.tools.Programs$RuleSetProgram.run(Programs.java:165) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.prepare.PlannerImpl.transform(PlannerImpl.java:273) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToPrel(DefaultSqlHandler.java:167) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.CreateTableHandler.getPlan(CreateTableHandler.java:112) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:507) [classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:185) [classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\n\t... 4 more\r\nCaused by: java.lang.IndexOutOfBoundsException: index (16) must be less than size (16)\r\n\tat com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305) ~[guava-14.0.1.jar:na]\r\n\tat com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284) ~[guava-14.0.1.jar:na]\r\n\tat com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:81) ~[guava-14.0.1.jar:na]\r\n\tat org.eigenbase.rex.RexBuilder.makeInputRef(RexBuilder.java:764) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.SortRel.<init>(SortRel.java:94) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.SortRel.<init>(SortRel.java:59) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:78) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:1) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1011) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1102) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.AbstractConverter$ExpandConversionRule.onMatch(AbstractConverter.java:108) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:223) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.eigenbase.relopt.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:661) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.tools.Programs$RuleSetProgram.run(Programs.java:165) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat net.hydromatic.optiq.prepare.PlannerImpl.transform(PlannerImpl.java:273) ~[optiq-core-0.9-drill-r16.jar:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToPrel(DefaultSqlHandler.java:167) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.CreateTableHandler.getPlan(CreateTableHandler.java:112) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:507) [classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:185) [classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) ~[classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\n\t... 12 more\r\n{code}\r\n"
    ],
    [
        "DRILL-2176",
        "DRILL-2075",
        "IndexOutOfBoundsException for count(*) on a subquery which does order-by The IOBE occurs in creating the collation trait in Calcite.  \r\n{code}\r\n0: jdbc:drill:zk=local> select count(*) from (select n_nationkey, n_regionkey from cp.`tpch/nation.parquet` order by 1, 2);\r\nQuery failed: IndexOutOfBoundsException: index (1) must be less than size (1)\r\n{code}\r\n\r\nFull stack trace: \r\n{code}\r\naused by: java.lang.IndexOutOfBoundsException: index (1) must be less than size (1)\r\n        at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305) ~[guava-14.0.1.jar:na]\r\n        at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284) ~[guava-14.0.1.jar:na]\r\n        at com.google.common.collect.SingletonImmutableList.get(SingletonImmutableList.java:45) ~[guava-14.0.1.jar:na]\r\n        at org.eigenbase.rex.RexBuilder.makeInputRef(RexBuilder.java:764) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.SortRel.<init>(SortRel.java:94) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.SortRel.<init>(SortRel.java:59) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:78) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:1) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1011) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1102) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.AbstractConverter$ExpandConversionRule.onMatch(AbstractConverter.java:108) ~[optiq-core-0.9-drill-r18.jar:na]\r\n{code}\r\n\r\nThis might be related to CALCITE-569 (and possibly DRILL-1978) but the stack traces are different, so I am treating this as a separate issue.  ",
        "Subquery not projecting the order by column is causing issues when used along with flatten git.commit.id.abbrev=3c6d0ef\r\n\r\nData Set :\r\n{code}\r\n{\r\n  \"uid\" : 1,\r\n  \"uid_str\" : \"01\",\r\n  \"type\" : \"web\",\r\n  \"events\" : [\r\n        { \"evnt_id\":\"e1\", \"campaign_id\":\"c1\", \"event_name\":\"e1_name\", \"event_time\":1000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e2\", \"campaign_id\":\"c1\", \"event_name\":\"e2_name\", \"event_time\":2000000, \"type\" : \"cmpgn4\"},\r\n        { \"evnt_id\":\"e3\", \"campaign_id\":\"c1\", \"event_name\":\"e3_name\", \"event_time\":3000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e4\", \"campaign_id\":\"c1\", \"event_name\":\"e4_name\", \"event_time\":4000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e5\", \"campaign_id\":\"c2\", \"event_name\":\"e5_name\", \"event_time\":5000000, \"type\" : \"cmpgn3\"},\r\n        { \"evnt_id\":\"e6\", \"campaign_id\":\"c1\", \"event_name\":\"e6_name\", \"event_time\":6000000, \"type\" : \"cmpgn9\"},\r\n        { \"evnt_id\":\"e7\", \"campaign_id\":\"c1\", \"event_name\":\"e7_name\", \"event_time\":7000000, \"type\" : \"cmpgn3\"},\r\n        { \"evnt_id\":\"e8\", \"campaign_id\":\"c2\", \"event_name\":\"e8_name\", \"event_time\":8000000, \"type\" : \"cmpgn2\"},\r\n        { \"evnt_id\":\"e9\", \"campaign_id\":\"c2\", \"event_name\":\"e9_name\", \"event_time\":9000000, \"type\" : \"cmpgn4\"}\r\n  ]\r\n}\r\n{code}\r\n\r\nQuery with flatten :\r\n{code}\r\nselect s.type  from (select d.type type, flatten(d.events) evnts from `temp.json` d where d.type='web' order by d.uid) s where s.type='web';\r\nQuery failed: IndexOutOfBoundsException: index (2) must be less than size (1)\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nQuery without flatten :\r\n{code}\r\nselect s.type  from (select d.type type from `temp.json` d where d.type='web' order by d.uid) s where s.type='web';\r\n+------------+\r\n|    type    |\r\n+------------+\r\n| web        |\r\n+------------+\r\n{code}\r\n\r\nHowever even the above query fails when we remove the filter and this could be related to DRILL-1302\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select s.type  from (select d.type type from `temp.json` d where d.type='web' order by d.uid) s;\r\nQuery failed: ArrayIndexOutOfBoundsException: -1\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}"
    ],
    [
        "DRILL-2176",
        "DRILL-2094",
        "IndexOutOfBoundsException for count(*) on a subquery which does order-by The IOBE occurs in creating the collation trait in Calcite.  \r\n{code}\r\n0: jdbc:drill:zk=local> select count(*) from (select n_nationkey, n_regionkey from cp.`tpch/nation.parquet` order by 1, 2);\r\nQuery failed: IndexOutOfBoundsException: index (1) must be less than size (1)\r\n{code}\r\n\r\nFull stack trace: \r\n{code}\r\naused by: java.lang.IndexOutOfBoundsException: index (1) must be less than size (1)\r\n        at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305) ~[guava-14.0.1.jar:na]\r\n        at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284) ~[guava-14.0.1.jar:na]\r\n        at com.google.common.collect.SingletonImmutableList.get(SingletonImmutableList.java:45) ~[guava-14.0.1.jar:na]\r\n        at org.eigenbase.rex.RexBuilder.makeInputRef(RexBuilder.java:764) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.SortRel.<init>(SortRel.java:94) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.SortRel.<init>(SortRel.java:59) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:78) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:1) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1011) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1102) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.AbstractConverter$ExpandConversionRule.onMatch(AbstractConverter.java:108) ~[optiq-core-0.9-drill-r18.jar:na]\r\n{code}\r\n\r\nThis might be related to CALCITE-569 (and possibly DRILL-1978) but the stack traces are different, so I am treating this as a separate issue.  ",
        "Drill has problems with reading json fields when used in a subquery git.commit.id.abbrev=3e33880\r\n\r\nData Set :\r\n{code}\r\n{\r\n  \"id\" : 1,\r\n  \"list\" : [1,2]\r\n}\r\n{code} \r\n\r\nThe below query works\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select id from `temp1.json` order by list[0];\r\n+------------+\r\n|     id     |\r\n+------------+\r\n| 1          |\r\n+------------+\r\n1 row selected (0.146 seconds)\r\n{code}\r\n\r\nHowever when I used the same exact query as part of a sub-query, I get an error from drill\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select s.id from (select id from `temp1.json` order by list[0]) s;\r\nQuery failed: SqlValidatorException: Table 'list' not found\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nExplain plan also does not work and it returns the same problem"
    ],
    [
        "DRILL-2179",
        "DRILL-1938",
        "better handle column called 'Timestamp' When querying a JSON file with a key called 'Timestamp' (select timestamp from <file>), Drill throws a non-intuitive error message (below).  It works when backticking.  Ideally the parser would look for keys called 'Timestamp' before erroring out.  If that can't be done, a better error should be printed.\r\n\r\njdbc:drill:zk=local> select Timestamp, Operation from dfs.`/...json`;\r\nQuery failed: Query failed: Failure parsing SQL. Encountered \"Timestamp ,\" at line 1, column 8.\r\nWas expecting one of:\r\n    \"UNION\" ...\r\n    \"INTERSECT\" ...\r\n    \"EXCEPT\" ...\r\n    \"ORDER\" ...",
        "Fix error message when reserved words are used in query. When reserved words are used in a query, the error message thrown should be more user-friendly.\r\n\r\n0: jdbc:drill:zk=10.10.82.51:5181> select user from\r\ndfs.`/mr_output/output-2015-01-06_10-43-40_parquet` LIMIT 10;\r\n\r\nQuery failed: Query stopped., Failure while trying to materialize\r\nincoming schema.  Errors:\r\n\r\nError in expression at index -1.  Error: Missing function\r\nimplementation: [user()].  Full expression: --UNKNOWN EXPRESSION--.. [\r\n79c75e68-ac33-47b4-b7d3-0d65e96c7357 on CentOS001:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query.\r\n(state=,code=0)"
    ],
    [
        "DRILL-2186",
        "DRILL-2180",
        "select * is not working with flatten git.commit.id.abbrev=c54bd6a\r\n\r\nData Set :\r\n{code}\r\n{\r\n  \"id\" : 1,\r\n  \"lst\" : [1,2,3]\r\n}\r\n{code}\r\n\r\nThe below queries incorrectly return null\r\n{code}\r\n0: jdbc:drill:schema=dfs> select *, flatten(lst) from `temp.json`;\r\n+------------+------------+\r\n|     *      |   EXPR$1   |\r\n+------------+------------+\r\n| null       | 1          |\r\n| null       | 2          |\r\n| null       | 3          |\r\n+------------+------------+\r\n{code}\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from `temp.json` order by flatten(lst);\r\n+------------+\r\n|     *      |\r\n+------------+\r\n| null       |\r\n| null       |\r\n| null       |\r\n+------------+\r\n{code}",
        "Star is not expanded when being used with flatten For example,\r\nselect *, \"flatten(j.topping) tt \" +\r\n              \"from dfs_test.`%s` j \"\r\n(using the same data set in DRILL-2012)\r\n\r\n*\ttt\r\nnull\t{\"id\":\"5001\",\"type\":\"None\"}\r\nnull\t{\"id\":\"5002\",\"type\":\"Glazed\"}\r\nnull\t{\"id\":\"5005\",\"type\":\"Sugar\"}\r\nnull\t{\"id\":\"5007\",\"type\":\"Powdered Sugar\"}\r\nnull\t{\"id\":\"5006\",\"type\":\"Chocolate with Sprinkles\"}\r\nnull\t{\"id\":\"5003\",\"type\":\"Chocolate\"}\r\nnull\t{\"id\":\"5004\",\"type\":\"Maple\"}\r\n\r\nNote that the first column is messed. "
    ],
    [
        "DRILL-2211",
        "DRILL-2178",
        "Define realloc(), allocationMonitor behavior when using fixed size allocation methods in ValueVectors In our value vectors implementation we have two major schemes to perform allocation. \r\nallocateNew(int)  -> Allocate for exact number of records\r\nallocateNew() -> Allocate based on allocation monitors.\r\n\r\nCurrently we don't have a paradigm for when a physical operator wants to exercise allocation using both the above schemes causing a few issues for such operators. ",
        "Update outgoing record batch size and allocation in PartitionSender Currently we allocate memory for vectors in partition sender OutgoingRecordBatch using allocateNew() which for most ValueVectors allocates space for 4096 record capacity, but we flush the current record batch as soon as we reach 1000 records causing wasted memory. Automatic resizing kicks in after flushing few record batches, but auto resize always doubles or halves the capacity. This cause the buffer record capacity to flip between  512 and 2048.\r\n\r\nThis JIRA is to:\r\n1. Decide on the outgoing record batch depending upon the number of receivers of partition sender. Default value is 1024, but when the number of receivers exceeds 1000 change it to 512.\r\n2. Allocate value vector space for storing the outgoing record batch size decided in (1). For this we make use of {{AllocationHelper.allocate(ValueVector v, int valueCount, int bytesPerValue, int repeatedPerTop)}}. {{bytesPerValue}} and {{repeatedPerTop}} is currently hard coded to 50 and 10, but this shouldn't matter as these values are applicable for variable and repeated vectors which have realloc facility if they run out of space.\r\n\r\n\r\n\r\n\r\n "
    ],
    [
        "DRILL-2213",
        "DRILL-2029",
        "JSON parsing error message doesn't give file name or context of error The error message resulting from trying to read this ill-formed JSON (from a local file): \r\n\r\n  {\r\n   \"key\":\"abc\"\r\n   \"a1\":1,\r\n  }\r\n\r\ndoesn't say anything about where the error was:\r\n\r\n  org.apache.drill.exec.rpc.RpcException: RemoteRpcException: Failure while running fragment., Unexpected character ('\"' (code 34)): was expecting comma to separate OBJECT entries\r\n\r\nAt absolute minimum, it should include the name of the file or a quote of the text surrounding the error.\r\n\r\nIt should include:\r\n- the name of the file\r\n- the line and column within the file, and/or a byte (or character) offset\r\n- some of the text surrounding the error \r\n- (the \"Unexpected ... expecting ...\" part it already includes.\r\n\r\n\r\n(Note:  The \"code\" in \"code 34\" is ambiguous.  \"Code point\" or \"character number\" would be better.  \r\n\r\nActually, the report of the character should include the standard (or just conventional?) Unicode hex notation (\"U+0022\").  (Including the decimal value too is fine, but that should probably be labeled as decimal, perhaps as ... \"U+0022 (34 decimal)\".)\r\n",
        "All readers should show the filename where it encountered error.  If possible, also position The Parquet reader (and possibly other file system readers) may encounter an error (e.g an IndexOutOfBounds)  while reading one out of hundreds or thousands of files in a directory.  The stack does not show the exact file where this error occurred which makes diagnosing the problem much harder.   We should show the filename in the error message."
    ],
    [
        "DRILL-2243",
        "DRILL-2220",
        "Table created as cast of literal to any decimal type either can not be read back or produces incrorrect result This bug looks suspiciously similar to drill-2220, but assert is different + wrong result.\r\n\r\nDecimal9 : failure to read from the table\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select cast('1.2' as decimal(8,2)) from `test.json`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n+------------+\r\n6 rows selected (0.081 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> create table temp(c1) as select cast('1.2' as decimal(8,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.193 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.Decimal9Vector cannot be cast to org.apache.drill.exec.vector.IntVector [ 33d801e0-a1d4-4999-9aac-e35f445018bb on atsqa4-133.qa.lab:31010 ]\r\n[ 33d801e0-a1d4-4999-9aac-e35f445018bb on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nDecimal18 : failure to read from the table\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select cast('1.2' as decimal(18,2)) from `test.json`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n+------------+\r\n6 rows selected (0.064 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> create table temp1(c1) as select cast('1.2' as decimal(18,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.257 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp1;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.Decimal18Vector cannot be cast to org.apache.drill.exec.vector.BigIntVector [ 5a23c757-9723-43cc-874b-18aaf62640a4 on atsqa4-133.qa.lab:31010 ]\r\n[ 5a23c757-9723-43cc-874b-18aaf62640a4 on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nDecimal28 : wrong result\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> create table temp2(c1) as select cast('1.2' as decimal(28,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.194 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp2;\r\n+------------+\r\n|     c1     |\r\n+------------+\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n+------------+\r\n6 rows selected (0.057 seconds)\r\n{code}\r\n\r\nDecimal38: wrong result\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> create table temp4(c1) as select cast('1.2' as decimal(38,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.214 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp4;\r\n+------------+\r\n|     c1     |\r\n+------------+\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n+------------+\r\n6 rows selected (0.048 seconds)\r\n{code}",
        "Complex reader unable to read FIXED_LEN_BYTE_ARRAY types in parquet file File created using Create table as having all types.\r\nFile can be read fine using normal scalar parquet reader.\r\n\r\nSwitching to the complex reader results in \r\n\r\nQuery failed: RemoteRpcException: Failure while running fragment., Unsupported type: FIXED_LEN_BYTE_ARRAY [ bb9b5e4d-185c-40f9-998d-c32740273bf7 on 10.10.30.167:31010 ]\r\n[ bb9b5e4d-185c-40f9-998d-c32740273bf7 on 10.10.30.167:31010 ]\r\n\r\nAttached are the source parquet files and the query file."
    ],
    [
        "DRILL-2249",
        "DRILL-2220",
        "Parquet reader hit IOBE when reading decimal type columns.  On today's master branch:\r\n\r\nselect commit_id from sys.version;\r\n+------------+\r\n| commit_id |\r\n+------------+\r\n| 4ed0a8d68ec5ef344fb54ff7c9d857f7f3f153aa |\r\n+------------+\r\n\r\nIf I create a parquet file containing two decimal(10,2) columns as:\r\n\r\n{code}\r\ncreate table my_dec_table as select *, cast(o_totalprice as decimal(10,2)) dec1, cast(o_totalprice as decimal(10,2)) dec2 from cp.`tpch/orders.parquet`;\r\n\r\n+------------+---------------------------+\r\n| Fragment | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0 | 15000 |\r\n+------------+---------------------------+\r\n1 row selected (1.977 seconds)\r\n{code}\r\n\r\nHowever, when I try to read from the new created parquet file, Drill report IOBE in parquet reader.\r\n\r\n{code}\r\nselect * from my_dec_table;\r\nQuery failed: Query stopped., index: 22531, length: 1 (expected: range(0, 22531)) [ ee35bc67-5c70-4677-bf7f-8db12e4a5491 on 10.250.0.8:31010 ]\r\n{code}\r\n\r\nThe plan looks fine to me for this query:\r\n\r\n{code}\r\nxplain plan for select * from my_dec_table;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=file:/Users/jni/work/data/tpcds/my_dec_table]], selectionRoot=/Users/jni/work/data/tpcds/my_dec_table, numFiles=1, columns=[`*`]]])\r\n{code}\r\n\r\nHere is part of the stack trace:\r\n\r\n{code}\r\njava.lang.IndexOutOfBoundsException: index: 22531, length: 1 (expected: range(0, 22531))\r\n\tat io.netty.buffer.DrillBuf.checkIndexD(DrillBuf.java:156) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n\tat io.netty.buffer.DrillBuf.chk(DrillBuf.java:178) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n\tat io.netty.buffer.DrillBuf.getByte(DrillBuf.java:673) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n\tat org.apache.drill.exec.store.parquet.columnreaders.FixedByteAlignedReader$DateReader.readIntLittleEndian(FixedByteAlignedReader.java:144) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.parquet.columnreaders.FixedByteAlignedReader.......\r\n{code}\r\n",
        "Complex reader unable to read FIXED_LEN_BYTE_ARRAY types in parquet file File created using Create table as having all types.\r\nFile can be read fine using normal scalar parquet reader.\r\n\r\nSwitching to the complex reader results in \r\n\r\nQuery failed: RemoteRpcException: Failure while running fragment., Unsupported type: FIXED_LEN_BYTE_ARRAY [ bb9b5e4d-185c-40f9-998d-c32740273bf7 on 10.10.30.167:31010 ]\r\n[ bb9b5e4d-185c-40f9-998d-c32740273bf7 on 10.10.30.167:31010 ]\r\n\r\nAttached are the source parquet files and the query file."
    ],
    [
        "DRILL-2249",
        "DRILL-2243",
        "Parquet reader hit IOBE when reading decimal type columns.  On today's master branch:\r\n\r\nselect commit_id from sys.version;\r\n+------------+\r\n| commit_id |\r\n+------------+\r\n| 4ed0a8d68ec5ef344fb54ff7c9d857f7f3f153aa |\r\n+------------+\r\n\r\nIf I create a parquet file containing two decimal(10,2) columns as:\r\n\r\n{code}\r\ncreate table my_dec_table as select *, cast(o_totalprice as decimal(10,2)) dec1, cast(o_totalprice as decimal(10,2)) dec2 from cp.`tpch/orders.parquet`;\r\n\r\n+------------+---------------------------+\r\n| Fragment | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0 | 15000 |\r\n+------------+---------------------------+\r\n1 row selected (1.977 seconds)\r\n{code}\r\n\r\nHowever, when I try to read from the new created parquet file, Drill report IOBE in parquet reader.\r\n\r\n{code}\r\nselect * from my_dec_table;\r\nQuery failed: Query stopped., index: 22531, length: 1 (expected: range(0, 22531)) [ ee35bc67-5c70-4677-bf7f-8db12e4a5491 on 10.250.0.8:31010 ]\r\n{code}\r\n\r\nThe plan looks fine to me for this query:\r\n\r\n{code}\r\nxplain plan for select * from my_dec_table;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=file:/Users/jni/work/data/tpcds/my_dec_table]], selectionRoot=/Users/jni/work/data/tpcds/my_dec_table, numFiles=1, columns=[`*`]]])\r\n{code}\r\n\r\nHere is part of the stack trace:\r\n\r\n{code}\r\njava.lang.IndexOutOfBoundsException: index: 22531, length: 1 (expected: range(0, 22531))\r\n\tat io.netty.buffer.DrillBuf.checkIndexD(DrillBuf.java:156) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n\tat io.netty.buffer.DrillBuf.chk(DrillBuf.java:178) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n\tat io.netty.buffer.DrillBuf.getByte(DrillBuf.java:673) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n\tat org.apache.drill.exec.store.parquet.columnreaders.FixedByteAlignedReader$DateReader.readIntLittleEndian(FixedByteAlignedReader.java:144) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.parquet.columnreaders.FixedByteAlignedReader.......\r\n{code}\r\n",
        "Table created as cast of literal to any decimal type either can not be read back or produces incrorrect result This bug looks suspiciously similar to drill-2220, but assert is different + wrong result.\r\n\r\nDecimal9 : failure to read from the table\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select cast('1.2' as decimal(8,2)) from `test.json`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n+------------+\r\n6 rows selected (0.081 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> create table temp(c1) as select cast('1.2' as decimal(8,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.193 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.Decimal9Vector cannot be cast to org.apache.drill.exec.vector.IntVector [ 33d801e0-a1d4-4999-9aac-e35f445018bb on atsqa4-133.qa.lab:31010 ]\r\n[ 33d801e0-a1d4-4999-9aac-e35f445018bb on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nDecimal18 : failure to read from the table\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select cast('1.2' as decimal(18,2)) from `test.json`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n+------------+\r\n6 rows selected (0.064 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> create table temp1(c1) as select cast('1.2' as decimal(18,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.257 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp1;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.Decimal18Vector cannot be cast to org.apache.drill.exec.vector.BigIntVector [ 5a23c757-9723-43cc-874b-18aaf62640a4 on atsqa4-133.qa.lab:31010 ]\r\n[ 5a23c757-9723-43cc-874b-18aaf62640a4 on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nDecimal28 : wrong result\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> create table temp2(c1) as select cast('1.2' as decimal(28,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.194 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp2;\r\n+------------+\r\n|     c1     |\r\n+------------+\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n+------------+\r\n6 rows selected (0.057 seconds)\r\n{code}\r\n\r\nDecimal38: wrong result\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> create table temp4(c1) as select cast('1.2' as decimal(38,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.214 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp4;\r\n+------------+\r\n|     c1     |\r\n+------------+\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n+------------+\r\n6 rows selected (0.048 seconds)\r\n{code}"
    ],
    [
        "DRILL-2253",
        "DRILL-2220",
        "Vectorized Parquet reader fails to read correctly against RLE Dictionary encoded DATE column ",
        "Complex reader unable to read FIXED_LEN_BYTE_ARRAY types in parquet file File created using Create table as having all types.\r\nFile can be read fine using normal scalar parquet reader.\r\n\r\nSwitching to the complex reader results in \r\n\r\nQuery failed: RemoteRpcException: Failure while running fragment., Unsupported type: FIXED_LEN_BYTE_ARRAY [ bb9b5e4d-185c-40f9-998d-c32740273bf7 on 10.10.30.167:31010 ]\r\n[ bb9b5e4d-185c-40f9-998d-c32740273bf7 on 10.10.30.167:31010 ]\r\n\r\nAttached are the source parquet files and the query file."
    ],
    [
        "DRILL-2253",
        "DRILL-2243",
        "Vectorized Parquet reader fails to read correctly against RLE Dictionary encoded DATE column ",
        "Table created as cast of literal to any decimal type either can not be read back or produces incrorrect result This bug looks suspiciously similar to drill-2220, but assert is different + wrong result.\r\n\r\nDecimal9 : failure to read from the table\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select cast('1.2' as decimal(8,2)) from `test.json`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n+------------+\r\n6 rows selected (0.081 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> create table temp(c1) as select cast('1.2' as decimal(8,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.193 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.Decimal9Vector cannot be cast to org.apache.drill.exec.vector.IntVector [ 33d801e0-a1d4-4999-9aac-e35f445018bb on atsqa4-133.qa.lab:31010 ]\r\n[ 33d801e0-a1d4-4999-9aac-e35f445018bb on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nDecimal18 : failure to read from the table\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select cast('1.2' as decimal(18,2)) from `test.json`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n+------------+\r\n6 rows selected (0.064 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> create table temp1(c1) as select cast('1.2' as decimal(18,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.257 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp1;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.Decimal18Vector cannot be cast to org.apache.drill.exec.vector.BigIntVector [ 5a23c757-9723-43cc-874b-18aaf62640a4 on atsqa4-133.qa.lab:31010 ]\r\n[ 5a23c757-9723-43cc-874b-18aaf62640a4 on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nDecimal28 : wrong result\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> create table temp2(c1) as select cast('1.2' as decimal(28,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.194 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp2;\r\n+------------+\r\n|     c1     |\r\n+------------+\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n+------------+\r\n6 rows selected (0.057 seconds)\r\n{code}\r\n\r\nDecimal38: wrong result\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> create table temp4(c1) as select cast('1.2' as decimal(38,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.214 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp4;\r\n+------------+\r\n|     c1     |\r\n+------------+\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n+------------+\r\n6 rows selected (0.048 seconds)\r\n{code}"
    ],
    [
        "DRILL-2253",
        "DRILL-2249",
        "Vectorized Parquet reader fails to read correctly against RLE Dictionary encoded DATE column ",
        "Parquet reader hit IOBE when reading decimal type columns.  On today's master branch:\r\n\r\nselect commit_id from sys.version;\r\n+------------+\r\n| commit_id |\r\n+------------+\r\n| 4ed0a8d68ec5ef344fb54ff7c9d857f7f3f153aa |\r\n+------------+\r\n\r\nIf I create a parquet file containing two decimal(10,2) columns as:\r\n\r\n{code}\r\ncreate table my_dec_table as select *, cast(o_totalprice as decimal(10,2)) dec1, cast(o_totalprice as decimal(10,2)) dec2 from cp.`tpch/orders.parquet`;\r\n\r\n+------------+---------------------------+\r\n| Fragment | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0 | 15000 |\r\n+------------+---------------------------+\r\n1 row selected (1.977 seconds)\r\n{code}\r\n\r\nHowever, when I try to read from the new created parquet file, Drill report IOBE in parquet reader.\r\n\r\n{code}\r\nselect * from my_dec_table;\r\nQuery failed: Query stopped., index: 22531, length: 1 (expected: range(0, 22531)) [ ee35bc67-5c70-4677-bf7f-8db12e4a5491 on 10.250.0.8:31010 ]\r\n{code}\r\n\r\nThe plan looks fine to me for this query:\r\n\r\n{code}\r\nxplain plan for select * from my_dec_table;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=file:/Users/jni/work/data/tpcds/my_dec_table]], selectionRoot=/Users/jni/work/data/tpcds/my_dec_table, numFiles=1, columns=[`*`]]])\r\n{code}\r\n\r\nHere is part of the stack trace:\r\n\r\n{code}\r\njava.lang.IndexOutOfBoundsException: index: 22531, length: 1 (expected: range(0, 22531))\r\n\tat io.netty.buffer.DrillBuf.checkIndexD(DrillBuf.java:156) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n\tat io.netty.buffer.DrillBuf.chk(DrillBuf.java:178) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n\tat io.netty.buffer.DrillBuf.getByte(DrillBuf.java:673) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n\tat org.apache.drill.exec.store.parquet.columnreaders.FixedByteAlignedReader$DateReader.readIntLittleEndian(FixedByteAlignedReader.java:144) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.parquet.columnreaders.FixedByteAlignedReader.......\r\n{code}\r\n"
    ],
    [
        "DRILL-2254",
        "DRILL-1754",
        "When we use flatten on list within a list, drill returns null \r\n\r\ngit.commit.id.abbrev=6676f2d\r\n\r\nApplying flatten on a list within a list seems to be broken now. This was working prior to \"02d51ddaad0e8940cf4a3ab1384b7fac31d482cb\".\r\n\r\n{code}\r\n{\r\n  \"id\":1,\r\n  \"lst_lst\" : [[1,2,3,4,5],[2,3,4,5,6]]\r\n}\r\n{code}\r\n\r\n{code}\r\nselect uid, flatten(lst_lst) from `data.json`;\r\n+------------+------------+\r\n|    uid     |   EXPR$1   |\r\n+------------+------------+\r\n| 1          | null       |\r\n| 1          | null       |\r\n{code}\r\n\r\nMarking this as critical since this was working earlier.",
        "Flatten nested within another flatten fails A query that tried to flatten a repeated list, and then flatten the resulting simple lists fails in execution."
    ],
    [
        "DRILL-2255",
        "DRILL-2183",
        "Flatten fails on a non-existing field. Running\r\n\r\n{code}\r\nselect flatten(i_dont_exist) from any.json\r\n{code}\r\n\r\nFlattening a non-existing column on JSON fails with the following:\r\n\r\n{code}\r\njava.lang.ClassCastException: org.apache.drill.exec.vector.NullableIntVector cannot be cast to org.apache.drill.exec.vector.RepeatedVector\r\n\tat org.apache.drill.exec.physical.impl.flatten.FlattenRecordBatch.getFlattenFieldTransferPair(FlattenRecordBatch.java:278) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.flatten.FlattenRecordBatch.setupNewSchema(FlattenRecordBatch.java:304) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:78) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.flatten.FlattenRecordBatch.innerNext(FlattenRecordBatch.java:122) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:89) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:134) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:97) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:116) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:303) [classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_65]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_65]\r\n\tat java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]\r\n{code}",
        "Flatten behavior not consistent with rest of drill when we use it on a non-existent field git.commit.id.abbrev=c54bd6a\r\n\r\nProjecting a non-existent column\r\n{code}\r\n0: jdbc:drill:schema=dfs_eea> select field_not_present from `data1.json`;\r\n+-------------------+\r\n| field_not_present |\r\n+-------------------+\r\n| null              |\r\n| null              |\r\n+-------------------+\r\n{code}\r\n\r\nUsing a non existent column in a function :\r\n{code}\r\n0: jdbc:drill:schema=dfs_eea> select MAX(field_not_present) from `data1.json`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| null       |\r\n+------------+\r\n1 row selected (0.07 seconds)\r\n{code}\r\n\r\nUsing a non-existent column in a filter\r\n{code}\r\n0: jdbc:drill:schema=dfs_eea> select MAX(uid) from `data1.json` where field_not_present > 1;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n+------------+\r\n{code}\r\n\r\nHowever when we use flatten things work differently\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs_eea> select uid , flatten(field_not_present) from `data1.json`;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.NullableIntVector cannot be cast to org.apache.drill.exec.vector.RepeatedVector [ afe463ac-2523-43b2-882f-814ae4a116e1 on qa-node191.qa.lab:31010 ]\r\n[ afe463ac-2523-43b2-882f-814ae4a116e1 on qa-node191.qa.lab:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nEven if we do not make it consistent, we should at-least make the error message more clear\r\n"
    ],
    [
        "DRILL-2256",
        "DRILL-2217",
        "Flatten on an empty list fails. Running\r\n\r\n{code}\r\nselect flatten(empty) from data.json\r\n{code}\r\n\r\non\r\n\r\n{code:title=data.json}\r\n{\"empty\": [ [] ] }\r\n{code}\r\n\r\nfails with\r\n\r\n{code}\r\njava.lang.NullPointerException\r\n\tat org.apache.drill.exec.vector.complex.RepeatedListVector$RepeatedListTransferPair.<init>(RepeatedListVector.java:273) ~[classes/:na]\r\n\tat org.apache.drill.exec.vector.complex.RepeatedListVector$RepeatedListTransferPair.<init>(RepeatedListVector.java:265) ~[classes/:na]\r\n\tat org.apache.drill.exec.vector.complex.RepeatedListVector.makeTransferPair(RepeatedListVector.java:322) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.setupNewSchema(ProjectRecordBatch.java:406) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:78) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:134) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:89) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.flatten.FlattenRecordBatch.innerNext(FlattenRecordBatch.java:122) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:89) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:134) ~[classes/:na]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:97) ~[classes/:na]\r\n\tat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:116) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:303) [classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_65]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_65]\r\n\tat java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]\r\n{code}\r\n\r\nI believe, flatten should have output as many nulls as the number of existing records/rows.",
        "Trying to flatten an empty list should return an empty result git.commit.id.abbrev=3d863b5\r\n\r\nData Set :\r\n{code}\r\n{\"empty\":[[],[[]]]}\r\n{code}\r\n\r\nQuery :\r\n{code}\r\nselect flatten(empty) from `data1.json`;\r\nQuery failed: RemoteRpcException: Failure while running fragment.[ 1b3123d9-92bc-45d5-bef8-b5f1be9def07 on qa-node191.qa.lab:31010 ]\r\n[ 1b3123d9-92bc-45d5-bef8-b5f1be9def07 on qa-node191.qa.lab:31010 ]\r\n{code}\r\n\r\nI also attached the error from the logs"
    ],
    [
        "DRILL-2262",
        "DRILL-2250",
        "selecting columns of certain datatypes from a dictionary encoded parquet file created by drill fails  After creating a parquet file containing all datatypes using CTAS from drill certain columns are not readable by doing a select in drill. \r\n\r\nThese datatypes are :\r\nDECIMAL18_col:       OPTIONAL INT64 O:DECIMAL R:0 D:1\r\nTIME_col:            OPTIONAL INT32 O:TIME R:0 D:1\r\nTIMESTAMP_col:       OPTIONAL INT64 O:TIMESTAMP R:0 D:1\r\n\r\nThe select from these columns fails with a similar error\r\n{code}\r\n0: jdbc:drill:> select DECIMAL18_col from parquet_all_default limit 1;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.NullableDecimal18Vector cannot be cast to org.apache.drill.exec.vector.NullableBigIntVector [ 26094858-356a-4128-ba2a-aa1473f74c93 on 10.10.30.167:31010 ]\r\n[ 26094858-356a-4128-ba2a-aa1473f74c93 on 10.10.30.167:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n0: jdbc:drill:> select TIME_col from parquet_all_default limit 1;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.NullableTimeVector cannot be cast to org.apache.drill.exec.vector.NullableIntVector [ 8938245b-a3a8-4bf2-8b6a-7e3860c90e8e on 10.10.30.167:31010 ]\r\n[ 8938245b-a3a8-4bf2-8b6a-7e3860c90e8e on 10.10.30.167:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n0: jdbc:drill:> select TIMESTAMP_col from parquet_all_default limit 1;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.NullableTimeStampVector cannot be cast to org.apache.drill.exec.vector.NullableBigIntVector [ 5dff5b38-ece7-4159-acfe-78dacb563e20 on 10.10.30.167:31010 ]\r\n[ 5dff5b38-ece7-4159-acfe-78dacb563e20 on 10.10.30.167:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nNote: Dictionary encoding is turned on by default as of commit : https://github.com/apache/drill/commit/00c08eff2a5de6e4334fb57e952aac9a852c3d37\r\n\r\nAfter setting session options to turn off dictionary encoding the same queries succeed successfully. \r\n",
        "Parquet reader hit ExecutionSetupException when reading decimal columns created using CTAS from another parquet files Not sure if this is related to DRILL-2249, since the error seems to be different.\r\n\r\nOn today's master commit:\r\n\r\nselect commit_id from sys.version;\r\n+------------+\r\n| commit_id  |\r\n+------------+\r\n| 4ed0a8d68ec5ef344fb54ff7c9d857f7f3f153aa |\r\n+------------+\r\n\r\nIf I create a parquet file containing decimal types, using the following CTAS\r\n{code}\r\ncreate table my_dec_table as select cast(l_quantity as decimal(10,2)) dec1, cast(l_discount as decimal(10,2)) dec2, cast(l_tax as decimal(10,2)) as dec3 from cp.`tpch/lineitem.parquet` ;\r\n\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 60175                     |\r\n+------------+---------------------------+\r\n1 row selected (1.71 seconds)\r\n\r\n{code}\r\n\r\nThen, if I want to query this new parquet file with :\r\n{code}\r\nselect * from my_dec_table;\r\n\r\nRemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.Decimal18Vector cannot be cast to org.apache.drill.exec.vector.BigIntVector [ de164ae4-2d2b-4257-bafb-3ca5165678f9 on 10.250.0.8:31010 ]\r\n[ de164ae4-2d2b-4257-bafb-3ca5165678f9 on 10.250.0.8:31010 ]\r\n\r\nselect dec1, dec2, dec3 from my_dec_table;\r\n\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.Decimal18Vector cannot be cast to org.apache.drill.exec.vector.BigIntVector [ 0f5b41c3-1ab3-43b4-9742-cea622d3f476 on 10.250.0.8:31010 ]\r\n[ 0f5b41c3-1ab3-43b4-9742-cea622d3f476 on 10.250.0.8:31010 ]\r\n\r\n{code}\r\n\r\nparquet-tools shows the following schema information for the generated parquet file:\r\n\r\n{code}\r\nmessage root {\r\n  required int64 dec1 (DECIMAL(10,2));\r\n  required int64 dec2 (DECIMAL(10,2));\r\n  required int64 dec3 (DECIMAL(10,2));\r\n}\r\n{code}\r\n\r\n"
    ],
    [
        "DRILL-2271",
        "DRILL-2246",
        "count(distinct ) against SystemTablePlugin will hit plan serialization/deserialization issue when executed in distributed mode. The following query run fine in single node. \r\n\r\n{code}\r\nselect count(distinct name) as cnt from `sys`.`options`;\r\n{code}\r\n\r\nHowever, if change the slice_target =1 and make it run in distributed mode, it hit error :\r\n\r\n{code}\r\n\r\nalter session set `planner.slice_target` = 1;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | planner.slice_target updated. |\r\n+------------+------------+\r\n\r\nselect count(distinct name) as cnt from `sys`.`options`;\r\nQuery failed: RemoteRpcException: Failure while trying to start remote fragment, No suitable constructor found for type [simple type, class org.apache.drill.exec.store.sys.SystemTablePlugin]: can not instantiate from JSON object (need to add/enable type information?)\r\n{code}\r\n\r\nSeems the serialization/deserialization of the plan for SystemTablePlugin does not work correctly.\r\n\r\n",
        "select from sys.options fails in a weird way when slice_target=1 Since this is not supposed to be customer visible option, it's a minor issue.\r\n\r\n{code}\r\nalter system set `planner.slice_target` =1;\r\nselect * from sys.options order by name;\r\n{code}\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from sys.options order by name;\r\nQuery failed: RemoteRpcException: Failure while trying to start remote fragment, No suitable constructor found for type [simple type, class org.apache.drill.exec.store.sys.SystemTablePlugin]: can not instantiate from JSON object (need to add/enable type information?)\r\n at [Source: {\r\n  \"pop\" : \"hash-partition-sender\",\r\n  \"@id\" : 0,\r\n  \"receiver-major-fragment\" : 1,\r\n  \"child\" : {\r\n    \"pop\" : \"sys\",\r\n    \"@id\" : 1,\r\n    \"table\" : \"OPTION\",\r\n    \"plugin\" : {\r\n      \"config\" : {\r\n        \"type\" : \"SystemTablePluginConfig\",\r\n        \"enabled\" : true\r\n      },\r\n      \"optimizerRules\" : [ ]\r\n    },\r\n    \"cost\" : 20.0\r\n  },\r\n  \"expr\" : \"hash(`name`) \",\r\n  \"destinations\" : [ \"ChFhdHNxYTQtMTMzLnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTM0LnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTMzLnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTM0LnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTMzLnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTM0LnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTMzLnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTM0LnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTMzLnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTM0LnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTMzLnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTM0LnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTMzLnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTM0LnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTMzLnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTM0LnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTMzLnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTM0LnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTMzLnFhLmxhYhCi8gEYo/IBIKTyAQ==\", \"ChFhdHNxYTQtMTM0LnFhLmxhYhCi8gEYo/IBIKTyAQ==\" ],\r\n  \"initialAllocation\" : 1000000,\r\n  \"maxAllocation\" : 10000000000,\r\n  \"cost\" : 0.0\r\n}; line: 10, column: 7] [ 3082d365-3bc6-4aeb-bb29-ddd133832cff on atsqa4-134.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}"
    ],
    [
        "DRILL-2274",
        "DRILL-2074",
        "Unable to allocate sv2 buffer after repeated attempts : JOIN, Order by used in query git.commit.id.abbrev=6676f2d\r\n\r\nThe below query fails :\r\n{code}\r\nselect sub1.uid from `data.json` sub1 inner join `data.json` sub2 on sub1.uid = sub2.uid order by sub1.uid;\r\n{code}\r\n\r\nError from the logs :\r\n{code}\r\n2015-02-20 00:24:08,431 [2b1981b0-149e-981b-f83f-512c587321d7:frag:1:2] ERROR o.a.d.e.w.f.AbstractStatusReporter - Error 66dba4ff-644c-4400-ab84-203256dc2600: Failure while running fragment.\r\n java.lang.RuntimeException: org.apache.drill.exec.memory.OutOfMemoryException: Unable to allocate sv2 buffer after repeated attempts\r\n \tat org.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:307) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:89) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \tat org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \tat org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:96) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \tat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \tat org.apache.drill.exec.physical.impl.SingleSenderCreator$SingleSenderRootExec.innerNext(SingleSenderCreator.java:97) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \tat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \tat org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:116) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:303) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_71]\r\n \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]\r\n \tat java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]\r\n Caused by: org.apache.drill.exec.memory.OutOfMemoryException: Unable to allocate sv2 buffer after repeated attempts\r\n \tat org.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.newSV2(ExternalSortBatch.java:516) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \tat org.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:305) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n \t... 16 common frames omitted\r\n{code}\r\n\r\nOn a different drillbit in the cluster, I found the below message for the same run\r\n{code}\r\n2015-02-20 00:24:08,435 [BitServer-6] WARN  o.a.d.exec.rpc.control.WorkEventBus - A fragment message arrived but there was no registered listener for that message: profile {\r\n   state: FAILED\r\n   error {\r\n     error_id: \"66dba4ff-644c-4400-ab84-203256dc2600\"\r\n     endpoint {\r\n       address: \"qa-node191.qa.lab\"\r\n       user_port: 31010\r\n       control_port: 31011\r\n       data_port: 31012\r\n     }\r\n     message: \"Failure while running fragment., Unable to allocate sv2 buffer after repeated attempts [ 66dba4ff-644c-4400-ab84-203256dc2600 on qa-node191.qa.lab:31010 ]\\n\"\r\n   }\r\n{code}\r\n\r\nI attached the data file which only has 2 records. I manually copied over the 2 records 50000 times and ran these queries on top of them.\r\n\r\nLet me know if you need anything else",
        "Queries fail with OutOfMemory Exception when Hash Join & Agg are turned off Query attached. \r\n\r\nHash Join and Hash Agg were turned off. And the following property was added to drill-override.conf:\r\nsort: {\r\n    purge.threshold : 100,\r\n    external: {\r\n      batch.size : 4000,\r\n      spill: {\r\n        batch.size : 4000,\r\n        group.size : 100,\r\n        threshold : 200,\r\n        directories : [ \"/drill_spill\" ],\r\n        fs : \"maprfs:///\"\r\n      }\r\n    }\r\n  }\r\n\r\nQuery failed with the below error message:\r\nQuery failed: RemoteRpcException: Failure while running fragment., Unable to allocate sv2 buffer after repeated attempts [ faf3044a-e14a-427b-b66d-7bcd7522ead5 on drone-42:31010 ]\r\n[ faf3044a-e14a-427b-b66d-7bcd7522ead5 on drone-42:31010 ]\r\n\r\nLog Snippets:\r\n2015-01-26 20:07:33,239 atsqa8c42.qa.lab [2b396307-2c1e-3486-90bc-fbaf09fbeb3e:frag:15:51] ERROR o.a.d.e.w.f.AbstractStatusReporter - Error faf3044a-e14a-427b-b66d-7bcd7522ead5: Failure while running fragment.\r\njava.lang.RuntimeException: org.apache.drill.exec.memory.OutOfMemoryException: Unable to allocate sv2 buffer after repeated attempts\r\n        at org.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:309) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:89) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:96) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.join.JoinStatus.nextRight(JoinStatus.java:80) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.join.JoinStatus.ensureInitial(JoinStatus.java:95) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.join.MergeJoinBatch.innerNext(MergeJoinBatch.java:147) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:89) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:96) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.SingleSenderCreator$SingleSenderRootExec.innerNext(SingleSenderCreator.java:105) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:116) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:303) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_65]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_65]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]\r\nCaused by: org.apache.drill.exec.memory.OutOfMemoryException: Unable to allocate sv2 buffer after repeated attempts\r\n        at org.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.newSV2(ExternalSortBatch.java:518) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:307) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        ... 24 common frames omitted\r\n\r\n\r\n2015-01-26 20:07:33,281 atsqa8c42.qa.lab [2b396307-2c1e-3486-90bc-fbaf09fbeb3e:frag:15:111] WARN  o.a.d.exec.memory.BufferAllocator - Unable to allocate buffer of size 4096 due to memory limit. Current allocation: 19998336\r\njava.lang.Exception: null\r\n        at org.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:206) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:217) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.selection.SelectionVector2.allocateNew(SelectionVector2.java:95) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.newSV2(ExternalSortBatch.java:514) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:307) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:89) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:96) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.join.JoinStatus.nextRight(JoinStatus.java:80) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.join.JoinStatus.ensureInitial(JoinStatus.java:95) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.join.MergeJoinBatch.innerNext(MergeJoinBatch.java:147) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:89) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:96) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.SingleSenderCreator$SingleSenderRootExec.innerNext(SingleSenderCreator.java:105) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:116) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:303) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_65]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_65]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]"
    ],
    [
        "DRILL-2286",
        "DRILL-2250",
        "Parquet compression causes read errors From what I can see, since compression has been added to the Parquet writer, reading errors can occur.\r\n\r\nBasically, things like timestamp and decimal are stored as int64 with some metadata.  It appears that when the column is compressed, it tries to read int64s into a vector of timestamp/decimal types, which causes a cast error.\r\n\r\nHere's the JSON file I'm using:\r\n\r\n{code}\r\n{ \"a\": 1.5 }\r\n{ \"a\": 3.5 }\r\n{ \"a\": 1.5 }\r\n{ \"a\": 2.5 }\r\n{ \"a\": 1.5 }\r\n{ \"a\": 5.5 }\r\n{ \"a\": 1.5 }\r\n{ \"a\": 6.0 }\r\n{ \"a\": 1.5 }\r\n{code}\r\n\r\nNow create a Parquet table like so:\r\n\r\ncreate table dfs.tmp.test as (select cast(a as decimal(18,8)) from dfs.tmp.`test.json`)\r\n\r\nNow when you try to query it like so:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> select * from dfs.tmp.test;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.NullableDecimal18Vector cannot be cast to org.apache.drill.exec.vector.NullableBigIntVector [ 91e23d42-fa06-4429-b78e-3ff32352e660 on ...:31010 ]\r\n[ 91e23d42-fa06-4429-b78e-3ff32352e660 on ...:31010 ]\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{noformat}\r\n\r\nThis is the same for timestamps, for example.\r\n\r\nThe relevant code is in ColumnReaderFactory whereby if the column chunk is encoded, it creates specific readers based on the type of the column (in this case int64, instead of timestamp/decimal).\r\n\r\nThis is pretty severe, as it looks like the compression is enabled by default now.  I do note that with only 1-2 records in the JSON file, it doesn't bother compressing and the queries then work fine.",
        "Parquet reader hit ExecutionSetupException when reading decimal columns created using CTAS from another parquet files Not sure if this is related to DRILL-2249, since the error seems to be different.\r\n\r\nOn today's master commit:\r\n\r\nselect commit_id from sys.version;\r\n+------------+\r\n| commit_id  |\r\n+------------+\r\n| 4ed0a8d68ec5ef344fb54ff7c9d857f7f3f153aa |\r\n+------------+\r\n\r\nIf I create a parquet file containing decimal types, using the following CTAS\r\n{code}\r\ncreate table my_dec_table as select cast(l_quantity as decimal(10,2)) dec1, cast(l_discount as decimal(10,2)) dec2, cast(l_tax as decimal(10,2)) as dec3 from cp.`tpch/lineitem.parquet` ;\r\n\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 60175                     |\r\n+------------+---------------------------+\r\n1 row selected (1.71 seconds)\r\n\r\n{code}\r\n\r\nThen, if I want to query this new parquet file with :\r\n{code}\r\nselect * from my_dec_table;\r\n\r\nRemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.Decimal18Vector cannot be cast to org.apache.drill.exec.vector.BigIntVector [ de164ae4-2d2b-4257-bafb-3ca5165678f9 on 10.250.0.8:31010 ]\r\n[ de164ae4-2d2b-4257-bafb-3ca5165678f9 on 10.250.0.8:31010 ]\r\n\r\nselect dec1, dec2, dec3 from my_dec_table;\r\n\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.Decimal18Vector cannot be cast to org.apache.drill.exec.vector.BigIntVector [ 0f5b41c3-1ab3-43b4-9742-cea622d3f476 on 10.250.0.8:31010 ]\r\n[ 0f5b41c3-1ab3-43b4-9742-cea622d3f476 on 10.250.0.8:31010 ]\r\n\r\n{code}\r\n\r\nparquet-tools shows the following schema information for the generated parquet file:\r\n\r\n{code}\r\nmessage root {\r\n  required int64 dec1 (DECIMAL(10,2));\r\n  required int64 dec2 (DECIMAL(10,2));\r\n  required int64 dec3 (DECIMAL(10,2));\r\n}\r\n{code}\r\n\r\n"
    ],
    [
        "DRILL-2286",
        "DRILL-2262",
        "Parquet compression causes read errors From what I can see, since compression has been added to the Parquet writer, reading errors can occur.\r\n\r\nBasically, things like timestamp and decimal are stored as int64 with some metadata.  It appears that when the column is compressed, it tries to read int64s into a vector of timestamp/decimal types, which causes a cast error.\r\n\r\nHere's the JSON file I'm using:\r\n\r\n{code}\r\n{ \"a\": 1.5 }\r\n{ \"a\": 3.5 }\r\n{ \"a\": 1.5 }\r\n{ \"a\": 2.5 }\r\n{ \"a\": 1.5 }\r\n{ \"a\": 5.5 }\r\n{ \"a\": 1.5 }\r\n{ \"a\": 6.0 }\r\n{ \"a\": 1.5 }\r\n{code}\r\n\r\nNow create a Parquet table like so:\r\n\r\ncreate table dfs.tmp.test as (select cast(a as decimal(18,8)) from dfs.tmp.`test.json`)\r\n\r\nNow when you try to query it like so:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> select * from dfs.tmp.test;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.NullableDecimal18Vector cannot be cast to org.apache.drill.exec.vector.NullableBigIntVector [ 91e23d42-fa06-4429-b78e-3ff32352e660 on ...:31010 ]\r\n[ 91e23d42-fa06-4429-b78e-3ff32352e660 on ...:31010 ]\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{noformat}\r\n\r\nThis is the same for timestamps, for example.\r\n\r\nThe relevant code is in ColumnReaderFactory whereby if the column chunk is encoded, it creates specific readers based on the type of the column (in this case int64, instead of timestamp/decimal).\r\n\r\nThis is pretty severe, as it looks like the compression is enabled by default now.  I do note that with only 1-2 records in the JSON file, it doesn't bother compressing and the queries then work fine.",
        "selecting columns of certain datatypes from a dictionary encoded parquet file created by drill fails  After creating a parquet file containing all datatypes using CTAS from drill certain columns are not readable by doing a select in drill. \r\n\r\nThese datatypes are :\r\nDECIMAL18_col:       OPTIONAL INT64 O:DECIMAL R:0 D:1\r\nTIME_col:            OPTIONAL INT32 O:TIME R:0 D:1\r\nTIMESTAMP_col:       OPTIONAL INT64 O:TIMESTAMP R:0 D:1\r\n\r\nThe select from these columns fails with a similar error\r\n{code}\r\n0: jdbc:drill:> select DECIMAL18_col from parquet_all_default limit 1;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.NullableDecimal18Vector cannot be cast to org.apache.drill.exec.vector.NullableBigIntVector [ 26094858-356a-4128-ba2a-aa1473f74c93 on 10.10.30.167:31010 ]\r\n[ 26094858-356a-4128-ba2a-aa1473f74c93 on 10.10.30.167:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n0: jdbc:drill:> select TIME_col from parquet_all_default limit 1;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.NullableTimeVector cannot be cast to org.apache.drill.exec.vector.NullableIntVector [ 8938245b-a3a8-4bf2-8b6a-7e3860c90e8e on 10.10.30.167:31010 ]\r\n[ 8938245b-a3a8-4bf2-8b6a-7e3860c90e8e on 10.10.30.167:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n0: jdbc:drill:> select TIMESTAMP_col from parquet_all_default limit 1;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.NullableTimeStampVector cannot be cast to org.apache.drill.exec.vector.NullableBigIntVector [ 5dff5b38-ece7-4159-acfe-78dacb563e20 on 10.10.30.167:31010 ]\r\n[ 5dff5b38-ece7-4159-acfe-78dacb563e20 on 10.10.30.167:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nNote: Dictionary encoding is turned on by default as of commit : https://github.com/apache/drill/commit/00c08eff2a5de6e4334fb57e952aac9a852c3d37\r\n\r\nAfter setting session options to turn off dictionary encoding the same queries succeed successfully. \r\n"
    ],
    [
        "DRILL-2291",
        "DRILL-2241",
        "CTAS broken when we have repeated lists git.commit.id.abbrev=6676f2d\r\n\r\nData Set :\r\n{code}\r\n{\r\n  \"id\" : 1,\r\n  \"lst_lst\" : [[1,2],[3,4]]\r\n}\r\n{code}\r\n\r\nThe below CTAS statement fails (The underlying query succeeds)\r\n{code}\r\ncreate table nested_list as select d.lst_lst from `temp.json` d;\r\nQuery failed: RemoteRpcException: Failure while running fragment.[ e5f3cdf2-d507-412e-a11a-24c9d2362975 on qa-node190.qa.lab:31010 ]\r\n[ e5f3cdf2-d507-412e-a11a-24c9d2362975 on qa-node190.qa.lab:31010 ]\r\n{code}\r\n\r\n\r\nBelow is the log file :\r\n{code}\r\n2015-02-24 18:45:56,332 [2b13391c-2000-e539-4adc-a661458b24c1:frag:0:0] ERROR o.a.d.e.w.f.AbstractStatusReporter - Error 3d91c456-3a24-4a8d-84b3-57a287aa2c54: Failure while running fragment.\r\njava.lang.NullPointerException: null\r\n        at org.apache.drill.exec.store.parquet.ParquetRecordWriter.cleanup(ParquetRecordWriter.java:318) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.WriterRecordBatch.cleanup(WriterRecordBatch.java:187) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.cleanup(IteratorValidatorBatchIterator.java:148) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.cleanup(AbstractSingleRecordBatch.java:121) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.cleanup(IteratorValidatorBatchIterator.java:148) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.internalStop(ScreenCreator.java:178) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:101) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:116) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:303) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_71]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]\r\n2015-02-24 18:45:56,334 [2b13391c-2000-e539-4adc-a661458b24c1:frag:0:0] INFO  o.a.drill.exec.work.foreman.Foreman - State change requested.  RUNNING --> FAILED\r\norg.apache.drill.exec.rpc.RemoteRpcException: Failure while running fragment.[ 3d91c456-3a24-4a8d-84b3-57a287aa2c54 on qa-node190.qa.lab:31010 ]\r\n[ 3d91c456-3a24-4a8d-84b3-57a287aa2c54 on qa-node190.qa.lab:31010 ]\r\n\r\n        at org.apache.drill.exec.work.foreman.QueryManager.statusUpdate(QueryManager.java:95) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.QueryManager$RootStatusReporter.statusChange(QueryManager.java:154) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.AbstractStatusReporter.fail(AbstractStatusReporter.java:114) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.AbstractStatusReporter.fail(AbstractStatusReporter.java:110) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.internalFail(FragmentExecutor.java:168) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:131) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:303) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_71]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]\r\n        at java.lang.Thread.run(Thread.java:745)\r\n{code}",
        "CTAS fails when writing a repeated list Drill can read the following JSON file with a repeated list:\r\n\r\n{\r\n  \"a\" : null\r\n  \"b\" : [ [\"B1\", \"B2\"] ],\r\n}\r\n\r\nWriting this to Parquet via a simple CTAS fails. \r\n> create table temp as select * from `replist.json`;\r\n\r\nLog indicates this to be unsupported (UnsupportedOperationException: Unsupported type LIST)\r\n\r\nLog attached. "
    ],
    [
        "DRILL-2298",
        "DRILL-1911",
        "Selecting same column multiple times with different case from a parquet file results in just one column returned When selecting the same column twice with the same case everything works as expected.\r\n\r\n{code}\r\n0: jdbc:drill:> select BIGINT_col,BIGINT_col from `parquet_storage/parquet_all_types.parquet` limit 1;\r\n+------------+-------------+\r\n| BIGINT_col | BIGINT_col0 |\r\n+------------+-------------+\r\n| 1          | 1           |\r\n+------------+-------------+\r\n1 row selected (0.086 seconds)\r\n{code}\r\n\r\nHowever, when different case is used for the same column only one column is returned.\r\n{code}\r\n0: jdbc:drill:> select BIGINT_col,bigint_col from `parquet_storage/parquet_all_types.parquet` limit 1;\r\n+------------+\r\n| BIGINT_col |\r\n+------------+\r\n| 1          |\r\n+------------+\r\n1 row selected (0.079 seconds)\r\n{code}",
        "Querying same field multiple times with different case would hit memory leak and return incorrect result.  git.commit.id.abbrev=309e1be\r\n\r\nIf query the same field twice, with different case, Drill will throw memory assertion error. \r\n\r\n select employee_id, Employee_id from cp.`employee.json` limit 2;\r\n+-------------+\r\n| employee_id |\r\n+-------------+\r\n| 1           |\r\n| 2           |\r\nQuery failed: Query failed: Failure while running fragment., Attempted to close accountor with 2 buffer(s) still allocatedfor QueryId: 2b5cc8eb-2817-aadb-e0fa-49272796592a, MajorFragmentId: 0, MinorFragmentId: 0.\r\n\r\n\r\n     Total 1 allocation(s) of byte size(s): 4096, at stack location:\r\n          org.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n          org.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n          org.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n          org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.doAlloc(ProjectRecordBatch.java:229)\r\n          org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.doWork(ProjectRecordBatch.java:167)\r\n          org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:93)\r\n          org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:132)\r\n          org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142)\r\n          org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118)\r\n          org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n          org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:97)\r\n          org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n          org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:114)\r\n          org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254)\r\n          java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n          java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n          java.lang.Thread.run(Thread.java:744)\r\n\r\n\r\nAlso, notice that the query result only contains one field; the second field is missing. \r\n\r\nThe plan looks fine.\r\n\r\nDrill Physical : \r\n00-00    Screen: rowcount = 463.0, cumulative cost = {1900.3 rows, 996.3 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 103\r\n00-01      Project(employee_id=[$0], Employee_id=[$1]): rowcount = 463.0, cumulative cost = {1854.0 rows, 950.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 102\r\n00-02        SelectionVectorRemover: rowcount = 463.0, cumulative cost = {1391.0 rows, 942.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 101\r\n00-03          Limit(fetch=[2]): rowcount = 463.0, cumulative cost = {928.0 rows, 479.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 100\r\n00-04            Project(employee_id=[$0], Employee_id=[$0]): rowcount = 463.0, cumulative cost = {926.0 rows, 471.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 99\r\n00-05              Scan(groupscan=[EasyGroupScan [selectionRoot=/employee.json, numFiles=1, columns=[`employee_id`], files=[/employee.json]]]): rowcount = 463.0, cumulative cost = {463.0 rows, 463.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 98\r\n"
    ],
    [
        "DRILL-2300",
        "DRILL-2220",
        "problems reading DECIMAL from parquet file There are several issues related to DECIMAL columns in parquet files, depending on which reader is used \"simple reader\" or \"complex reader\" and if the columns are OPTIONAL or REQUIRED.\r\n\r\nI have the following {{test.json}} file:\r\n{code}\r\n{ \"a\": \"1\" }\r\n{ \"a\": \"1\" }\r\n{ \"a\": \"1\" }\r\n{code}\r\n\r\nI created a parquet file using the following query:\r\n{noformat}\r\nCREATE TABLE dfs.tmp.`test_all_decimal` AS \r\n  SELECT \r\n    CAST(a AS DECIMAL(9,6)) decimal9_opt, \r\n    CAST('1' AS DECIMAL(9,6)) decimal9_req, \r\n    CAST(a AS DECIMAL(18, 8)) decimal18_opt, \r\n    CAST('1' AS DECIMAL(18, 8)) decimal18_req, \r\n    CAST(a AS DECIMAL(28,10)) decimal28_col, \r\n    CAST('1' AS DECIMAL(28,10)) decimal28_req, \r\n    CAST(a AS DECIMAL(38,10)) decimal38_col, \r\n    CAST('1' AS DECIMAL(38,10)) decimal38_req \r\n  FROM dfs.data.`test_char.json`;\r\n{noformat}\r\n\r\nThis creates a parquet file with the following metadata, retrieved using {{parquet tools}}:\r\n{noformat}\r\ncreator:       parquet-mr \r\n\r\nfile schema:   root \r\n-----------------------------------------------------------------------------------------------------\r\ndecimal9_opt:  OPTIONAL INT32 O:DECIMAL R:0 D:1\r\ndecimal9_req:  REQUIRED INT32 O:DECIMAL R:0 D:0\r\ndecimal18_opt: OPTIONAL INT64 O:DECIMAL R:0 D:1\r\ndecimal18_req: REQUIRED INT64 O:DECIMAL R:0 D:0\r\ndecimal28_col: OPTIONAL FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:1\r\ndecimal28_req: REQUIRED FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:0\r\ndecimal38_col: OPTIONAL FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:1\r\ndecimal38_req: REQUIRED FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:0\r\n\r\nrow group 1:   RC:3 TS:636 \r\n-----------------------------------------------------------------------------------------------------\r\ndecimal9_opt:   INT32 SNAPPY DO:0 FPO:4 SZ:62/58/0.94 VC:3 ENC:PLAIN_DICTIONARY,RLE,BIT_PACKED\r\ndecimal9_req:   INT32 SNAPPY DO:0 FPO:66 SZ:56/52/0.93 VC:3 ENC:PLAIN_DICTIONARY,BIT_PACKED\r\ndecimal18_opt:  INT64 SNAPPY DO:0 FPO:122 SZ:74/70/0.95 VC:3 ENC:PLAIN_DICTIONARY,RLE,BIT_PACKED\r\ndecimal18_req:  INT64 SNAPPY DO:0 FPO:196 SZ:68/64/0.94 VC:3 ENC:PLAIN_DICTIONARY,BIT_PACKED\r\ndecimal28_col:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:264 SZ:72/91/1.26 VC:3 ENC:RLE,BIT_PACKED,PLAIN\r\ndecimal28_req:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:336 SZ:66/85/1.29 VC:3 ENC:BIT_PACKED,PLAIN\r\ndecimal38_col:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:402 SZ:80/111/1.39 VC:3 ENC:RLE,BIT_PACKED,PLAIN\r\ndecimal38_req:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:482 SZ:77/105/1.36 VC:3 ENC:BIT_PACKED,PLAIN\r\n{noformat}\r\n\r\nIf we disable dictionary encoding:\r\n{noformat}\r\nalter session set `store.parquet.enable_dictionary_encoding` = false;\r\n{noformat}\r\n\r\nWe will get a simlar parquet file but with {{PLAIN}} encoding instead of {{PLAIN_DICTIONARY}} for {{DECIMAL9}} and {{DECIMAL18}} columns.\r\n\r\nWhen using the \"simple\" parquet reader, with dictionary encoding enabled,  \r\nThe following query returns wrong results for {{DECIMAL28/REQUIRED}} and {{DECIMAL38/REQUIRED}} (we can't read {{DECIMAL9}} nor {{DECIMAL18}} columns because of DRILL-2262):\r\n{noformat}\r\nselect decimal28_col, decimal28_req, decimal38_col, decimal38_req from dfs.tmp.`test_all_decimal`;\r\n+---------------+---------------+---------------+---------------+\r\n| decimal28_opt | decimal28_req | decimal38_opt | decimal38_req |\r\n+---------------+---------------+---------------+---------------+\r\n| 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n+---------------+---------------+---------------+---------------+\r\n{noformat}\r\n \r\nWhen dictionary encoding is disabled, the following query eturns wrong results for {{DECIMAL28/REQUIRED}} and {{DECIMAL38/REQUIRED}}:\r\n{noformat}\r\nselect decimal9_opt, decimal9_req, decimal18_opt, decimal18_req, decimal28_opt, decimal28_req, decimal38_opt, decimal38_req from dfs.tmp.`test_all_decimal_nodictionary`;\r\n+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+\r\n| decimal9_opt | decimal9_req | decimal18_opt | decimal18_req | decimal28_opt | decimal28_req | decimal38_opt | decimal38_req |\r\n+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+\r\n| 1.000000     | 1.000000     | 1.00000000    | 1.00000000    | 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.000000     | 1.000000     | 1.00000000    | 1.00000000    | 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.000000     | 1.000000     | 1.00000000    | 1.00000000    | 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+\r\n{noformat}\r\n\r\nWhen using the \"complex\" reader:\r\n{noformat}\r\nalter session set `store.parquet.use_new_reader` = true;\r\n{noformat}\r\n\r\nWe can't read {{DECIMAL28}} nor {{DECIMAL38}} because of DRILL-2220.\r\n\r\nthe following query gives wrong results for {DECIMAL9}} and {{DECIMAL18}} no matter if dictionary encoding is enabled or disabled:\r\n{noformat}\r\nselect decimal9_opt, decimal9_req, decimal18_opt, decimal18_req from dfs.tmp.`test_all_decimal`;\r\n+--------------+--------------+---------------+---------------+\r\n| decimal9_opt | decimal9_req | decimal18_opt | decimal18_req |\r\n+--------------+--------------+---------------+---------------+\r\n| 1000000      | 1000000      | 100000000     | 100000000     |\r\n| 1000000      | 1000000      | 100000000     | 100000000     |\r\n| 1000000      | 1000000      | 100000000     | 100000000     |\r\n+--------------+--------------+---------------+---------------+\r\n{noformat}",
        "Complex reader unable to read FIXED_LEN_BYTE_ARRAY types in parquet file File created using Create table as having all types.\r\nFile can be read fine using normal scalar parquet reader.\r\n\r\nSwitching to the complex reader results in \r\n\r\nQuery failed: RemoteRpcException: Failure while running fragment., Unsupported type: FIXED_LEN_BYTE_ARRAY [ bb9b5e4d-185c-40f9-998d-c32740273bf7 on 10.10.30.167:31010 ]\r\n[ bb9b5e4d-185c-40f9-998d-c32740273bf7 on 10.10.30.167:31010 ]\r\n\r\nAttached are the source parquet files and the query file."
    ],
    [
        "DRILL-2300",
        "DRILL-2243",
        "problems reading DECIMAL from parquet file There are several issues related to DECIMAL columns in parquet files, depending on which reader is used \"simple reader\" or \"complex reader\" and if the columns are OPTIONAL or REQUIRED.\r\n\r\nI have the following {{test.json}} file:\r\n{code}\r\n{ \"a\": \"1\" }\r\n{ \"a\": \"1\" }\r\n{ \"a\": \"1\" }\r\n{code}\r\n\r\nI created a parquet file using the following query:\r\n{noformat}\r\nCREATE TABLE dfs.tmp.`test_all_decimal` AS \r\n  SELECT \r\n    CAST(a AS DECIMAL(9,6)) decimal9_opt, \r\n    CAST('1' AS DECIMAL(9,6)) decimal9_req, \r\n    CAST(a AS DECIMAL(18, 8)) decimal18_opt, \r\n    CAST('1' AS DECIMAL(18, 8)) decimal18_req, \r\n    CAST(a AS DECIMAL(28,10)) decimal28_col, \r\n    CAST('1' AS DECIMAL(28,10)) decimal28_req, \r\n    CAST(a AS DECIMAL(38,10)) decimal38_col, \r\n    CAST('1' AS DECIMAL(38,10)) decimal38_req \r\n  FROM dfs.data.`test_char.json`;\r\n{noformat}\r\n\r\nThis creates a parquet file with the following metadata, retrieved using {{parquet tools}}:\r\n{noformat}\r\ncreator:       parquet-mr \r\n\r\nfile schema:   root \r\n-----------------------------------------------------------------------------------------------------\r\ndecimal9_opt:  OPTIONAL INT32 O:DECIMAL R:0 D:1\r\ndecimal9_req:  REQUIRED INT32 O:DECIMAL R:0 D:0\r\ndecimal18_opt: OPTIONAL INT64 O:DECIMAL R:0 D:1\r\ndecimal18_req: REQUIRED INT64 O:DECIMAL R:0 D:0\r\ndecimal28_col: OPTIONAL FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:1\r\ndecimal28_req: REQUIRED FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:0\r\ndecimal38_col: OPTIONAL FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:1\r\ndecimal38_req: REQUIRED FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:0\r\n\r\nrow group 1:   RC:3 TS:636 \r\n-----------------------------------------------------------------------------------------------------\r\ndecimal9_opt:   INT32 SNAPPY DO:0 FPO:4 SZ:62/58/0.94 VC:3 ENC:PLAIN_DICTIONARY,RLE,BIT_PACKED\r\ndecimal9_req:   INT32 SNAPPY DO:0 FPO:66 SZ:56/52/0.93 VC:3 ENC:PLAIN_DICTIONARY,BIT_PACKED\r\ndecimal18_opt:  INT64 SNAPPY DO:0 FPO:122 SZ:74/70/0.95 VC:3 ENC:PLAIN_DICTIONARY,RLE,BIT_PACKED\r\ndecimal18_req:  INT64 SNAPPY DO:0 FPO:196 SZ:68/64/0.94 VC:3 ENC:PLAIN_DICTIONARY,BIT_PACKED\r\ndecimal28_col:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:264 SZ:72/91/1.26 VC:3 ENC:RLE,BIT_PACKED,PLAIN\r\ndecimal28_req:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:336 SZ:66/85/1.29 VC:3 ENC:BIT_PACKED,PLAIN\r\ndecimal38_col:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:402 SZ:80/111/1.39 VC:3 ENC:RLE,BIT_PACKED,PLAIN\r\ndecimal38_req:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:482 SZ:77/105/1.36 VC:3 ENC:BIT_PACKED,PLAIN\r\n{noformat}\r\n\r\nIf we disable dictionary encoding:\r\n{noformat}\r\nalter session set `store.parquet.enable_dictionary_encoding` = false;\r\n{noformat}\r\n\r\nWe will get a simlar parquet file but with {{PLAIN}} encoding instead of {{PLAIN_DICTIONARY}} for {{DECIMAL9}} and {{DECIMAL18}} columns.\r\n\r\nWhen using the \"simple\" parquet reader, with dictionary encoding enabled,  \r\nThe following query returns wrong results for {{DECIMAL28/REQUIRED}} and {{DECIMAL38/REQUIRED}} (we can't read {{DECIMAL9}} nor {{DECIMAL18}} columns because of DRILL-2262):\r\n{noformat}\r\nselect decimal28_col, decimal28_req, decimal38_col, decimal38_req from dfs.tmp.`test_all_decimal`;\r\n+---------------+---------------+---------------+---------------+\r\n| decimal28_opt | decimal28_req | decimal38_opt | decimal38_req |\r\n+---------------+---------------+---------------+---------------+\r\n| 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n+---------------+---------------+---------------+---------------+\r\n{noformat}\r\n \r\nWhen dictionary encoding is disabled, the following query eturns wrong results for {{DECIMAL28/REQUIRED}} and {{DECIMAL38/REQUIRED}}:\r\n{noformat}\r\nselect decimal9_opt, decimal9_req, decimal18_opt, decimal18_req, decimal28_opt, decimal28_req, decimal38_opt, decimal38_req from dfs.tmp.`test_all_decimal_nodictionary`;\r\n+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+\r\n| decimal9_opt | decimal9_req | decimal18_opt | decimal18_req | decimal28_opt | decimal28_req | decimal38_opt | decimal38_req |\r\n+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+\r\n| 1.000000     | 1.000000     | 1.00000000    | 1.00000000    | 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.000000     | 1.000000     | 1.00000000    | 1.00000000    | 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.000000     | 1.000000     | 1.00000000    | 1.00000000    | 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+\r\n{noformat}\r\n\r\nWhen using the \"complex\" reader:\r\n{noformat}\r\nalter session set `store.parquet.use_new_reader` = true;\r\n{noformat}\r\n\r\nWe can't read {{DECIMAL28}} nor {{DECIMAL38}} because of DRILL-2220.\r\n\r\nthe following query gives wrong results for {DECIMAL9}} and {{DECIMAL18}} no matter if dictionary encoding is enabled or disabled:\r\n{noformat}\r\nselect decimal9_opt, decimal9_req, decimal18_opt, decimal18_req from dfs.tmp.`test_all_decimal`;\r\n+--------------+--------------+---------------+---------------+\r\n| decimal9_opt | decimal9_req | decimal18_opt | decimal18_req |\r\n+--------------+--------------+---------------+---------------+\r\n| 1000000      | 1000000      | 100000000     | 100000000     |\r\n| 1000000      | 1000000      | 100000000     | 100000000     |\r\n| 1000000      | 1000000      | 100000000     | 100000000     |\r\n+--------------+--------------+---------------+---------------+\r\n{noformat}",
        "Table created as cast of literal to any decimal type either can not be read back or produces incrorrect result This bug looks suspiciously similar to drill-2220, but assert is different + wrong result.\r\n\r\nDecimal9 : failure to read from the table\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select cast('1.2' as decimal(8,2)) from `test.json`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n+------------+\r\n6 rows selected (0.081 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> create table temp(c1) as select cast('1.2' as decimal(8,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.193 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.Decimal9Vector cannot be cast to org.apache.drill.exec.vector.IntVector [ 33d801e0-a1d4-4999-9aac-e35f445018bb on atsqa4-133.qa.lab:31010 ]\r\n[ 33d801e0-a1d4-4999-9aac-e35f445018bb on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nDecimal18 : failure to read from the table\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select cast('1.2' as decimal(18,2)) from `test.json`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n+------------+\r\n6 rows selected (0.064 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> create table temp1(c1) as select cast('1.2' as decimal(18,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.257 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp1;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.Decimal18Vector cannot be cast to org.apache.drill.exec.vector.BigIntVector [ 5a23c757-9723-43cc-874b-18aaf62640a4 on atsqa4-133.qa.lab:31010 ]\r\n[ 5a23c757-9723-43cc-874b-18aaf62640a4 on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nDecimal28 : wrong result\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> create table temp2(c1) as select cast('1.2' as decimal(28,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.194 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp2;\r\n+------------+\r\n|     c1     |\r\n+------------+\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n+------------+\r\n6 rows selected (0.057 seconds)\r\n{code}\r\n\r\nDecimal38: wrong result\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> create table temp4(c1) as select cast('1.2' as decimal(38,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.214 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp4;\r\n+------------+\r\n|     c1     |\r\n+------------+\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n+------------+\r\n6 rows selected (0.048 seconds)\r\n{code}"
    ],
    [
        "DRILL-2300",
        "DRILL-2249",
        "problems reading DECIMAL from parquet file There are several issues related to DECIMAL columns in parquet files, depending on which reader is used \"simple reader\" or \"complex reader\" and if the columns are OPTIONAL or REQUIRED.\r\n\r\nI have the following {{test.json}} file:\r\n{code}\r\n{ \"a\": \"1\" }\r\n{ \"a\": \"1\" }\r\n{ \"a\": \"1\" }\r\n{code}\r\n\r\nI created a parquet file using the following query:\r\n{noformat}\r\nCREATE TABLE dfs.tmp.`test_all_decimal` AS \r\n  SELECT \r\n    CAST(a AS DECIMAL(9,6)) decimal9_opt, \r\n    CAST('1' AS DECIMAL(9,6)) decimal9_req, \r\n    CAST(a AS DECIMAL(18, 8)) decimal18_opt, \r\n    CAST('1' AS DECIMAL(18, 8)) decimal18_req, \r\n    CAST(a AS DECIMAL(28,10)) decimal28_col, \r\n    CAST('1' AS DECIMAL(28,10)) decimal28_req, \r\n    CAST(a AS DECIMAL(38,10)) decimal38_col, \r\n    CAST('1' AS DECIMAL(38,10)) decimal38_req \r\n  FROM dfs.data.`test_char.json`;\r\n{noformat}\r\n\r\nThis creates a parquet file with the following metadata, retrieved using {{parquet tools}}:\r\n{noformat}\r\ncreator:       parquet-mr \r\n\r\nfile schema:   root \r\n-----------------------------------------------------------------------------------------------------\r\ndecimal9_opt:  OPTIONAL INT32 O:DECIMAL R:0 D:1\r\ndecimal9_req:  REQUIRED INT32 O:DECIMAL R:0 D:0\r\ndecimal18_opt: OPTIONAL INT64 O:DECIMAL R:0 D:1\r\ndecimal18_req: REQUIRED INT64 O:DECIMAL R:0 D:0\r\ndecimal28_col: OPTIONAL FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:1\r\ndecimal28_req: REQUIRED FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:0\r\ndecimal38_col: OPTIONAL FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:1\r\ndecimal38_req: REQUIRED FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:0\r\n\r\nrow group 1:   RC:3 TS:636 \r\n-----------------------------------------------------------------------------------------------------\r\ndecimal9_opt:   INT32 SNAPPY DO:0 FPO:4 SZ:62/58/0.94 VC:3 ENC:PLAIN_DICTIONARY,RLE,BIT_PACKED\r\ndecimal9_req:   INT32 SNAPPY DO:0 FPO:66 SZ:56/52/0.93 VC:3 ENC:PLAIN_DICTIONARY,BIT_PACKED\r\ndecimal18_opt:  INT64 SNAPPY DO:0 FPO:122 SZ:74/70/0.95 VC:3 ENC:PLAIN_DICTIONARY,RLE,BIT_PACKED\r\ndecimal18_req:  INT64 SNAPPY DO:0 FPO:196 SZ:68/64/0.94 VC:3 ENC:PLAIN_DICTIONARY,BIT_PACKED\r\ndecimal28_col:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:264 SZ:72/91/1.26 VC:3 ENC:RLE,BIT_PACKED,PLAIN\r\ndecimal28_req:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:336 SZ:66/85/1.29 VC:3 ENC:BIT_PACKED,PLAIN\r\ndecimal38_col:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:402 SZ:80/111/1.39 VC:3 ENC:RLE,BIT_PACKED,PLAIN\r\ndecimal38_req:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:482 SZ:77/105/1.36 VC:3 ENC:BIT_PACKED,PLAIN\r\n{noformat}\r\n\r\nIf we disable dictionary encoding:\r\n{noformat}\r\nalter session set `store.parquet.enable_dictionary_encoding` = false;\r\n{noformat}\r\n\r\nWe will get a simlar parquet file but with {{PLAIN}} encoding instead of {{PLAIN_DICTIONARY}} for {{DECIMAL9}} and {{DECIMAL18}} columns.\r\n\r\nWhen using the \"simple\" parquet reader, with dictionary encoding enabled,  \r\nThe following query returns wrong results for {{DECIMAL28/REQUIRED}} and {{DECIMAL38/REQUIRED}} (we can't read {{DECIMAL9}} nor {{DECIMAL18}} columns because of DRILL-2262):\r\n{noformat}\r\nselect decimal28_col, decimal28_req, decimal38_col, decimal38_req from dfs.tmp.`test_all_decimal`;\r\n+---------------+---------------+---------------+---------------+\r\n| decimal28_opt | decimal28_req | decimal38_opt | decimal38_req |\r\n+---------------+---------------+---------------+---------------+\r\n| 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n+---------------+---------------+---------------+---------------+\r\n{noformat}\r\n \r\nWhen dictionary encoding is disabled, the following query eturns wrong results for {{DECIMAL28/REQUIRED}} and {{DECIMAL38/REQUIRED}}:\r\n{noformat}\r\nselect decimal9_opt, decimal9_req, decimal18_opt, decimal18_req, decimal28_opt, decimal28_req, decimal38_opt, decimal38_req from dfs.tmp.`test_all_decimal_nodictionary`;\r\n+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+\r\n| decimal9_opt | decimal9_req | decimal18_opt | decimal18_req | decimal28_opt | decimal28_req | decimal38_opt | decimal38_req |\r\n+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+\r\n| 1.000000     | 1.000000     | 1.00000000    | 1.00000000    | 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.000000     | 1.000000     | 1.00000000    | 1.00000000    | 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.000000     | 1.000000     | 1.00000000    | 1.00000000    | 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+\r\n{noformat}\r\n\r\nWhen using the \"complex\" reader:\r\n{noformat}\r\nalter session set `store.parquet.use_new_reader` = true;\r\n{noformat}\r\n\r\nWe can't read {{DECIMAL28}} nor {{DECIMAL38}} because of DRILL-2220.\r\n\r\nthe following query gives wrong results for {DECIMAL9}} and {{DECIMAL18}} no matter if dictionary encoding is enabled or disabled:\r\n{noformat}\r\nselect decimal9_opt, decimal9_req, decimal18_opt, decimal18_req from dfs.tmp.`test_all_decimal`;\r\n+--------------+--------------+---------------+---------------+\r\n| decimal9_opt | decimal9_req | decimal18_opt | decimal18_req |\r\n+--------------+--------------+---------------+---------------+\r\n| 1000000      | 1000000      | 100000000     | 100000000     |\r\n| 1000000      | 1000000      | 100000000     | 100000000     |\r\n| 1000000      | 1000000      | 100000000     | 100000000     |\r\n+--------------+--------------+---------------+---------------+\r\n{noformat}",
        "Parquet reader hit IOBE when reading decimal type columns.  On today's master branch:\r\n\r\nselect commit_id from sys.version;\r\n+------------+\r\n| commit_id |\r\n+------------+\r\n| 4ed0a8d68ec5ef344fb54ff7c9d857f7f3f153aa |\r\n+------------+\r\n\r\nIf I create a parquet file containing two decimal(10,2) columns as:\r\n\r\n{code}\r\ncreate table my_dec_table as select *, cast(o_totalprice as decimal(10,2)) dec1, cast(o_totalprice as decimal(10,2)) dec2 from cp.`tpch/orders.parquet`;\r\n\r\n+------------+---------------------------+\r\n| Fragment | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0 | 15000 |\r\n+------------+---------------------------+\r\n1 row selected (1.977 seconds)\r\n{code}\r\n\r\nHowever, when I try to read from the new created parquet file, Drill report IOBE in parquet reader.\r\n\r\n{code}\r\nselect * from my_dec_table;\r\nQuery failed: Query stopped., index: 22531, length: 1 (expected: range(0, 22531)) [ ee35bc67-5c70-4677-bf7f-8db12e4a5491 on 10.250.0.8:31010 ]\r\n{code}\r\n\r\nThe plan looks fine to me for this query:\r\n\r\n{code}\r\nxplain plan for select * from my_dec_table;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=file:/Users/jni/work/data/tpcds/my_dec_table]], selectionRoot=/Users/jni/work/data/tpcds/my_dec_table, numFiles=1, columns=[`*`]]])\r\n{code}\r\n\r\nHere is part of the stack trace:\r\n\r\n{code}\r\njava.lang.IndexOutOfBoundsException: index: 22531, length: 1 (expected: range(0, 22531))\r\n\tat io.netty.buffer.DrillBuf.checkIndexD(DrillBuf.java:156) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n\tat io.netty.buffer.DrillBuf.chk(DrillBuf.java:178) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n\tat io.netty.buffer.DrillBuf.getByte(DrillBuf.java:673) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n\tat org.apache.drill.exec.store.parquet.columnreaders.FixedByteAlignedReader$DateReader.readIntLittleEndian(FixedByteAlignedReader.java:144) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.parquet.columnreaders.FixedByteAlignedReader.......\r\n{code}\r\n"
    ],
    [
        "DRILL-2300",
        "DRILL-2253",
        "problems reading DECIMAL from parquet file There are several issues related to DECIMAL columns in parquet files, depending on which reader is used \"simple reader\" or \"complex reader\" and if the columns are OPTIONAL or REQUIRED.\r\n\r\nI have the following {{test.json}} file:\r\n{code}\r\n{ \"a\": \"1\" }\r\n{ \"a\": \"1\" }\r\n{ \"a\": \"1\" }\r\n{code}\r\n\r\nI created a parquet file using the following query:\r\n{noformat}\r\nCREATE TABLE dfs.tmp.`test_all_decimal` AS \r\n  SELECT \r\n    CAST(a AS DECIMAL(9,6)) decimal9_opt, \r\n    CAST('1' AS DECIMAL(9,6)) decimal9_req, \r\n    CAST(a AS DECIMAL(18, 8)) decimal18_opt, \r\n    CAST('1' AS DECIMAL(18, 8)) decimal18_req, \r\n    CAST(a AS DECIMAL(28,10)) decimal28_col, \r\n    CAST('1' AS DECIMAL(28,10)) decimal28_req, \r\n    CAST(a AS DECIMAL(38,10)) decimal38_col, \r\n    CAST('1' AS DECIMAL(38,10)) decimal38_req \r\n  FROM dfs.data.`test_char.json`;\r\n{noformat}\r\n\r\nThis creates a parquet file with the following metadata, retrieved using {{parquet tools}}:\r\n{noformat}\r\ncreator:       parquet-mr \r\n\r\nfile schema:   root \r\n-----------------------------------------------------------------------------------------------------\r\ndecimal9_opt:  OPTIONAL INT32 O:DECIMAL R:0 D:1\r\ndecimal9_req:  REQUIRED INT32 O:DECIMAL R:0 D:0\r\ndecimal18_opt: OPTIONAL INT64 O:DECIMAL R:0 D:1\r\ndecimal18_req: REQUIRED INT64 O:DECIMAL R:0 D:0\r\ndecimal28_col: OPTIONAL FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:1\r\ndecimal28_req: REQUIRED FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:0\r\ndecimal38_col: OPTIONAL FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:1\r\ndecimal38_req: REQUIRED FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:0\r\n\r\nrow group 1:   RC:3 TS:636 \r\n-----------------------------------------------------------------------------------------------------\r\ndecimal9_opt:   INT32 SNAPPY DO:0 FPO:4 SZ:62/58/0.94 VC:3 ENC:PLAIN_DICTIONARY,RLE,BIT_PACKED\r\ndecimal9_req:   INT32 SNAPPY DO:0 FPO:66 SZ:56/52/0.93 VC:3 ENC:PLAIN_DICTIONARY,BIT_PACKED\r\ndecimal18_opt:  INT64 SNAPPY DO:0 FPO:122 SZ:74/70/0.95 VC:3 ENC:PLAIN_DICTIONARY,RLE,BIT_PACKED\r\ndecimal18_req:  INT64 SNAPPY DO:0 FPO:196 SZ:68/64/0.94 VC:3 ENC:PLAIN_DICTIONARY,BIT_PACKED\r\ndecimal28_col:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:264 SZ:72/91/1.26 VC:3 ENC:RLE,BIT_PACKED,PLAIN\r\ndecimal28_req:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:336 SZ:66/85/1.29 VC:3 ENC:BIT_PACKED,PLAIN\r\ndecimal38_col:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:402 SZ:80/111/1.39 VC:3 ENC:RLE,BIT_PACKED,PLAIN\r\ndecimal38_req:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:482 SZ:77/105/1.36 VC:3 ENC:BIT_PACKED,PLAIN\r\n{noformat}\r\n\r\nIf we disable dictionary encoding:\r\n{noformat}\r\nalter session set `store.parquet.enable_dictionary_encoding` = false;\r\n{noformat}\r\n\r\nWe will get a simlar parquet file but with {{PLAIN}} encoding instead of {{PLAIN_DICTIONARY}} for {{DECIMAL9}} and {{DECIMAL18}} columns.\r\n\r\nWhen using the \"simple\" parquet reader, with dictionary encoding enabled,  \r\nThe following query returns wrong results for {{DECIMAL28/REQUIRED}} and {{DECIMAL38/REQUIRED}} (we can't read {{DECIMAL9}} nor {{DECIMAL18}} columns because of DRILL-2262):\r\n{noformat}\r\nselect decimal28_col, decimal28_req, decimal38_col, decimal38_req from dfs.tmp.`test_all_decimal`;\r\n+---------------+---------------+---------------+---------------+\r\n| decimal28_opt | decimal28_req | decimal38_opt | decimal38_req |\r\n+---------------+---------------+---------------+---------------+\r\n| 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n+---------------+---------------+---------------+---------------+\r\n{noformat}\r\n \r\nWhen dictionary encoding is disabled, the following query eturns wrong results for {{DECIMAL28/REQUIRED}} and {{DECIMAL38/REQUIRED}}:\r\n{noformat}\r\nselect decimal9_opt, decimal9_req, decimal18_opt, decimal18_req, decimal28_opt, decimal28_req, decimal38_opt, decimal38_req from dfs.tmp.`test_all_decimal_nodictionary`;\r\n+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+\r\n| decimal9_opt | decimal9_req | decimal18_opt | decimal18_req | decimal28_opt | decimal28_req | decimal38_opt | decimal38_req |\r\n+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+\r\n| 1.000000     | 1.000000     | 1.00000000    | 1.00000000    | 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.000000     | 1.000000     | 1.00000000    | 1.00000000    | 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.000000     | 1.000000     | 1.00000000    | 1.00000000    | 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+\r\n{noformat}\r\n\r\nWhen using the \"complex\" reader:\r\n{noformat}\r\nalter session set `store.parquet.use_new_reader` = true;\r\n{noformat}\r\n\r\nWe can't read {{DECIMAL28}} nor {{DECIMAL38}} because of DRILL-2220.\r\n\r\nthe following query gives wrong results for {DECIMAL9}} and {{DECIMAL18}} no matter if dictionary encoding is enabled or disabled:\r\n{noformat}\r\nselect decimal9_opt, decimal9_req, decimal18_opt, decimal18_req from dfs.tmp.`test_all_decimal`;\r\n+--------------+--------------+---------------+---------------+\r\n| decimal9_opt | decimal9_req | decimal18_opt | decimal18_req |\r\n+--------------+--------------+---------------+---------------+\r\n| 1000000      | 1000000      | 100000000     | 100000000     |\r\n| 1000000      | 1000000      | 100000000     | 100000000     |\r\n| 1000000      | 1000000      | 100000000     | 100000000     |\r\n+--------------+--------------+---------------+---------------+\r\n{noformat}",
        "Vectorized Parquet reader fails to read correctly against RLE Dictionary encoded DATE column "
    ],
    [
        "DRILL-2318",
        "DRILL-2301",
        "Query fails when an ORDER BY clause is used with WITH-CLAUSE Adding a WITH clause with a simple CTE causes a query with an ORDER BY to fail. This happens even when the CTE is unrelated to the main query. \r\n\r\n*The following query fails to execute:*\r\n{code:sql}\r\nWITH \r\n     x\r\n     AS (SELECT  ss_sold_date_sk a1\r\n         FROM  store_sales) \r\nSELECT  x.a1\r\nFROM   x\r\nORDER  BY \r\n          x.a1;\r\n{code}\r\n\r\nError:\r\nQuery failed: SqlValidatorException: Table 'x' not found\r\n\r\nLog attached. \r\n\r\n*The following query executes fine:*\r\n{code:sql}\r\nWITH \r\n     x\r\n     AS (SELECT  ss_sold_date_sk a1\r\n         FROM  store_sales) \r\nSELECT  x.a1\r\nFROM   x\r\n{code}\r\n",
        "Query fails when multiple table aliases are provided for CTEs When a query contains a WITH clause which defines Common Table Expressions (CTEs), a second alias for the same the temporary table cannot be provided. \r\n\r\n*The following query fails to validate:*\r\nWITH C AS \r\n( \r\n         SELECT   c_customer_id          cid\r\n         FROM     customer\r\n         GROUP BY c_customer_id ) \r\nSELECT   c2.cid \r\nFROM     C c1, \r\n         C c2 \r\nWHERE    c2.cid = c1.cid \r\nORDER BY c2.cid limit 10;\r\n\r\nQuery failed: SqlValidatorException: Table 'c2' not found\r\n\r\nThe above query validates and executes successfully on Postgres. \r\n\r\nWhen the WITH clause is removed, the query succeeds.\r\n\r\n*The following query executes fine:*\r\nSELECT   c2.c_customer_id \r\nFROM     customer c1, \r\n         customer c2 \r\nWHERE    c2.c_customer_id = c1.c_customer_id \r\nORDER BY c2.c_customer_id limit 10;\r\n\r\nLogs attached. \r\n"
    ],
    [
        "DRILL-2334",
        "DRILL-2322",
        "Text record reader should fail gracefully when encountering bad records The attached file has 1 bad record.   Running a simple count(*) query on this file errors out with IOBE and/or possible schema change exception.\r\n\r\nThe hex dump of the file shows a bunch of 0's (the '*' below indicates more lines of 0's):\r\n{code}\r\n00001c0 3a 35 35 2e 35 30 35 35 30 00 00 00 00 00 00 00\r\n00001d0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n*\r\n02a01c0 00 00 00 00 00 00 00 00 00 35 35 35 0a 35 35 35\r\n{code}\r\n\r\n{code}\r\n0: jdbc:drill:zk=local> select count(*) from `badRecords2.dat`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\nQuery failed: RemoteRpcException: Failure while running fragment., You tried to do a batch data read operation when you were in a state of STOP.  You can only do this type of operation when you are in a state of OK or OK_NEW_SCHEMA.\r\n{code}\r\n\r\nlog file also shows an IOBE related to this: \r\n\r\n{code}\r\n18:49:00.003 [2b1024e4-5639-b4ec-392e-8d5879c3d4db:frag:0:0] DEBUG o.a.d.exec.physical.impl.ScanBatch - Failed to read the batch. Stopping...\r\njava.lang.IndexOutOfBoundsException: index: 374, length: 2752540 (expected: range(0, 65536))\r\n        at io.netty.buffer.AbstractByteBuf.checkIndex(AbstractByteBuf.java:1143) ~[netty-buffer-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:272) ~[netty-buffer-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.buffer.WrappedByteBuf.setBytes(WrappedByteBuf.java:390) ~[netty-buffer-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.buffer.UnsafeDirectLittleEndian.setBytes(UnsafeDirectLittleEndian.java:25) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n        at io.netty.buffer.DrillBuf.setBytes(DrillBuf.java:651) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n        at org.apache.drill.exec.vector.VarCharVector$Mutator.setSafe(VarCharVector.java:481) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.vector.RepeatedVarCharVector$Mutator.addSafe(RepeatedVarCharVector.java:451) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.text.DrillTextRecordReader.next(DrillTextRecordReader.java:172) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:165) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n{code}\r\n\r\n",
        "CSV record reader should log which file and which record caused an error in the reader I believe the title is self exploratory.\r\nIf the text reader fails for any reason due to an offending record drill should log which file (if there are multiple files) and which line/record the error occurs at. This will improve debugging when dealing with large files/ large number of files.\r\n"
    ],
    [
        "DRILL-2338",
        "DRILL-2220",
        "DECIMAL 38 when stored as a required datatype results in wrong data Writing out a decimal 38 column by casting a literal results in a different value being written out.\r\n\r\n{code}\r\n0: jdbc:drill:> create table mehant_bug as select cast('1.2' as decimal(38,2)) from cp.`employee.json` limit 1;\r\n\r\n0: jdbc:drill:> select * from mehant_bug;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 12000000.00 |\r\n+------------+\r\n1 row selected (0.08 seconds)\r\n{code}\r\n\r\n[root@perfnode167 impala_parquet]# parquet-tools-1.5.1-SNAPSHOT/parquet-schema  ../mehant_bug/0_0_0.parquet \r\nmessage root {\r\n  required fixed_len_byte_array(16) EXPR$0 (DECIMAL(38,2));\r\n}\r\n\r\nIf the column is stored as optional we do not have this issue.\r\n",
        "Complex reader unable to read FIXED_LEN_BYTE_ARRAY types in parquet file File created using Create table as having all types.\r\nFile can be read fine using normal scalar parquet reader.\r\n\r\nSwitching to the complex reader results in \r\n\r\nQuery failed: RemoteRpcException: Failure while running fragment., Unsupported type: FIXED_LEN_BYTE_ARRAY [ bb9b5e4d-185c-40f9-998d-c32740273bf7 on 10.10.30.167:31010 ]\r\n[ bb9b5e4d-185c-40f9-998d-c32740273bf7 on 10.10.30.167:31010 ]\r\n\r\nAttached are the source parquet files and the query file."
    ],
    [
        "DRILL-2338",
        "DRILL-2243",
        "DECIMAL 38 when stored as a required datatype results in wrong data Writing out a decimal 38 column by casting a literal results in a different value being written out.\r\n\r\n{code}\r\n0: jdbc:drill:> create table mehant_bug as select cast('1.2' as decimal(38,2)) from cp.`employee.json` limit 1;\r\n\r\n0: jdbc:drill:> select * from mehant_bug;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 12000000.00 |\r\n+------------+\r\n1 row selected (0.08 seconds)\r\n{code}\r\n\r\n[root@perfnode167 impala_parquet]# parquet-tools-1.5.1-SNAPSHOT/parquet-schema  ../mehant_bug/0_0_0.parquet \r\nmessage root {\r\n  required fixed_len_byte_array(16) EXPR$0 (DECIMAL(38,2));\r\n}\r\n\r\nIf the column is stored as optional we do not have this issue.\r\n",
        "Table created as cast of literal to any decimal type either can not be read back or produces incrorrect result This bug looks suspiciously similar to drill-2220, but assert is different + wrong result.\r\n\r\nDecimal9 : failure to read from the table\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select cast('1.2' as decimal(8,2)) from `test.json`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n+------------+\r\n6 rows selected (0.081 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> create table temp(c1) as select cast('1.2' as decimal(8,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.193 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.Decimal9Vector cannot be cast to org.apache.drill.exec.vector.IntVector [ 33d801e0-a1d4-4999-9aac-e35f445018bb on atsqa4-133.qa.lab:31010 ]\r\n[ 33d801e0-a1d4-4999-9aac-e35f445018bb on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nDecimal18 : failure to read from the table\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select cast('1.2' as decimal(18,2)) from `test.json`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n| 1.20       |\r\n+------------+\r\n6 rows selected (0.064 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> create table temp1(c1) as select cast('1.2' as decimal(18,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.257 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp1;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.Decimal18Vector cannot be cast to org.apache.drill.exec.vector.BigIntVector [ 5a23c757-9723-43cc-874b-18aaf62640a4 on atsqa4-133.qa.lab:31010 ]\r\n[ 5a23c757-9723-43cc-874b-18aaf62640a4 on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nDecimal28 : wrong result\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> create table temp2(c1) as select cast('1.2' as decimal(28,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.194 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp2;\r\n+------------+\r\n|     c1     |\r\n+------------+\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n+------------+\r\n6 rows selected (0.057 seconds)\r\n{code}\r\n\r\nDecimal38: wrong result\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> create table temp4(c1) as select cast('1.2' as decimal(38,2)) from `test.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 6                         |\r\n+------------+---------------------------+\r\n1 row selected (0.214 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from temp4;\r\n+------------+\r\n|     c1     |\r\n+------------+\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n| 12000000.00 |\r\n+------------+\r\n6 rows selected (0.048 seconds)\r\n{code}"
    ],
    [
        "DRILL-2338",
        "DRILL-2249",
        "DECIMAL 38 when stored as a required datatype results in wrong data Writing out a decimal 38 column by casting a literal results in a different value being written out.\r\n\r\n{code}\r\n0: jdbc:drill:> create table mehant_bug as select cast('1.2' as decimal(38,2)) from cp.`employee.json` limit 1;\r\n\r\n0: jdbc:drill:> select * from mehant_bug;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 12000000.00 |\r\n+------------+\r\n1 row selected (0.08 seconds)\r\n{code}\r\n\r\n[root@perfnode167 impala_parquet]# parquet-tools-1.5.1-SNAPSHOT/parquet-schema  ../mehant_bug/0_0_0.parquet \r\nmessage root {\r\n  required fixed_len_byte_array(16) EXPR$0 (DECIMAL(38,2));\r\n}\r\n\r\nIf the column is stored as optional we do not have this issue.\r\n",
        "Parquet reader hit IOBE when reading decimal type columns.  On today's master branch:\r\n\r\nselect commit_id from sys.version;\r\n+------------+\r\n| commit_id |\r\n+------------+\r\n| 4ed0a8d68ec5ef344fb54ff7c9d857f7f3f153aa |\r\n+------------+\r\n\r\nIf I create a parquet file containing two decimal(10,2) columns as:\r\n\r\n{code}\r\ncreate table my_dec_table as select *, cast(o_totalprice as decimal(10,2)) dec1, cast(o_totalprice as decimal(10,2)) dec2 from cp.`tpch/orders.parquet`;\r\n\r\n+------------+---------------------------+\r\n| Fragment | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0 | 15000 |\r\n+------------+---------------------------+\r\n1 row selected (1.977 seconds)\r\n{code}\r\n\r\nHowever, when I try to read from the new created parquet file, Drill report IOBE in parquet reader.\r\n\r\n{code}\r\nselect * from my_dec_table;\r\nQuery failed: Query stopped., index: 22531, length: 1 (expected: range(0, 22531)) [ ee35bc67-5c70-4677-bf7f-8db12e4a5491 on 10.250.0.8:31010 ]\r\n{code}\r\n\r\nThe plan looks fine to me for this query:\r\n\r\n{code}\r\nxplain plan for select * from my_dec_table;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=file:/Users/jni/work/data/tpcds/my_dec_table]], selectionRoot=/Users/jni/work/data/tpcds/my_dec_table, numFiles=1, columns=[`*`]]])\r\n{code}\r\n\r\nHere is part of the stack trace:\r\n\r\n{code}\r\njava.lang.IndexOutOfBoundsException: index: 22531, length: 1 (expected: range(0, 22531))\r\n\tat io.netty.buffer.DrillBuf.checkIndexD(DrillBuf.java:156) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n\tat io.netty.buffer.DrillBuf.chk(DrillBuf.java:178) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n\tat io.netty.buffer.DrillBuf.getByte(DrillBuf.java:673) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n\tat org.apache.drill.exec.store.parquet.columnreaders.FixedByteAlignedReader$DateReader.readIntLittleEndian(FixedByteAlignedReader.java:144) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.parquet.columnreaders.FixedByteAlignedReader.......\r\n{code}\r\n"
    ],
    [
        "DRILL-2338",
        "DRILL-2253",
        "DECIMAL 38 when stored as a required datatype results in wrong data Writing out a decimal 38 column by casting a literal results in a different value being written out.\r\n\r\n{code}\r\n0: jdbc:drill:> create table mehant_bug as select cast('1.2' as decimal(38,2)) from cp.`employee.json` limit 1;\r\n\r\n0: jdbc:drill:> select * from mehant_bug;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 12000000.00 |\r\n+------------+\r\n1 row selected (0.08 seconds)\r\n{code}\r\n\r\n[root@perfnode167 impala_parquet]# parquet-tools-1.5.1-SNAPSHOT/parquet-schema  ../mehant_bug/0_0_0.parquet \r\nmessage root {\r\n  required fixed_len_byte_array(16) EXPR$0 (DECIMAL(38,2));\r\n}\r\n\r\nIf the column is stored as optional we do not have this issue.\r\n",
        "Vectorized Parquet reader fails to read correctly against RLE Dictionary encoded DATE column "
    ],
    [
        "DRILL-2338",
        "DRILL-2300",
        "DECIMAL 38 when stored as a required datatype results in wrong data Writing out a decimal 38 column by casting a literal results in a different value being written out.\r\n\r\n{code}\r\n0: jdbc:drill:> create table mehant_bug as select cast('1.2' as decimal(38,2)) from cp.`employee.json` limit 1;\r\n\r\n0: jdbc:drill:> select * from mehant_bug;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 12000000.00 |\r\n+------------+\r\n1 row selected (0.08 seconds)\r\n{code}\r\n\r\n[root@perfnode167 impala_parquet]# parquet-tools-1.5.1-SNAPSHOT/parquet-schema  ../mehant_bug/0_0_0.parquet \r\nmessage root {\r\n  required fixed_len_byte_array(16) EXPR$0 (DECIMAL(38,2));\r\n}\r\n\r\nIf the column is stored as optional we do not have this issue.\r\n",
        "problems reading DECIMAL from parquet file There are several issues related to DECIMAL columns in parquet files, depending on which reader is used \"simple reader\" or \"complex reader\" and if the columns are OPTIONAL or REQUIRED.\r\n\r\nI have the following {{test.json}} file:\r\n{code}\r\n{ \"a\": \"1\" }\r\n{ \"a\": \"1\" }\r\n{ \"a\": \"1\" }\r\n{code}\r\n\r\nI created a parquet file using the following query:\r\n{noformat}\r\nCREATE TABLE dfs.tmp.`test_all_decimal` AS \r\n  SELECT \r\n    CAST(a AS DECIMAL(9,6)) decimal9_opt, \r\n    CAST('1' AS DECIMAL(9,6)) decimal9_req, \r\n    CAST(a AS DECIMAL(18, 8)) decimal18_opt, \r\n    CAST('1' AS DECIMAL(18, 8)) decimal18_req, \r\n    CAST(a AS DECIMAL(28,10)) decimal28_col, \r\n    CAST('1' AS DECIMAL(28,10)) decimal28_req, \r\n    CAST(a AS DECIMAL(38,10)) decimal38_col, \r\n    CAST('1' AS DECIMAL(38,10)) decimal38_req \r\n  FROM dfs.data.`test_char.json`;\r\n{noformat}\r\n\r\nThis creates a parquet file with the following metadata, retrieved using {{parquet tools}}:\r\n{noformat}\r\ncreator:       parquet-mr \r\n\r\nfile schema:   root \r\n-----------------------------------------------------------------------------------------------------\r\ndecimal9_opt:  OPTIONAL INT32 O:DECIMAL R:0 D:1\r\ndecimal9_req:  REQUIRED INT32 O:DECIMAL R:0 D:0\r\ndecimal18_opt: OPTIONAL INT64 O:DECIMAL R:0 D:1\r\ndecimal18_req: REQUIRED INT64 O:DECIMAL R:0 D:0\r\ndecimal28_col: OPTIONAL FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:1\r\ndecimal28_req: REQUIRED FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:0\r\ndecimal38_col: OPTIONAL FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:1\r\ndecimal38_req: REQUIRED FIXED_LEN_BYTE_ARRAY O:DECIMAL R:0 D:0\r\n\r\nrow group 1:   RC:3 TS:636 \r\n-----------------------------------------------------------------------------------------------------\r\ndecimal9_opt:   INT32 SNAPPY DO:0 FPO:4 SZ:62/58/0.94 VC:3 ENC:PLAIN_DICTIONARY,RLE,BIT_PACKED\r\ndecimal9_req:   INT32 SNAPPY DO:0 FPO:66 SZ:56/52/0.93 VC:3 ENC:PLAIN_DICTIONARY,BIT_PACKED\r\ndecimal18_opt:  INT64 SNAPPY DO:0 FPO:122 SZ:74/70/0.95 VC:3 ENC:PLAIN_DICTIONARY,RLE,BIT_PACKED\r\ndecimal18_req:  INT64 SNAPPY DO:0 FPO:196 SZ:68/64/0.94 VC:3 ENC:PLAIN_DICTIONARY,BIT_PACKED\r\ndecimal28_col:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:264 SZ:72/91/1.26 VC:3 ENC:RLE,BIT_PACKED,PLAIN\r\ndecimal28_req:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:336 SZ:66/85/1.29 VC:3 ENC:BIT_PACKED,PLAIN\r\ndecimal38_col:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:402 SZ:80/111/1.39 VC:3 ENC:RLE,BIT_PACKED,PLAIN\r\ndecimal38_req:  FIXED_LEN_BYTE_ARRAY SNAPPY DO:0 FPO:482 SZ:77/105/1.36 VC:3 ENC:BIT_PACKED,PLAIN\r\n{noformat}\r\n\r\nIf we disable dictionary encoding:\r\n{noformat}\r\nalter session set `store.parquet.enable_dictionary_encoding` = false;\r\n{noformat}\r\n\r\nWe will get a simlar parquet file but with {{PLAIN}} encoding instead of {{PLAIN_DICTIONARY}} for {{DECIMAL9}} and {{DECIMAL18}} columns.\r\n\r\nWhen using the \"simple\" parquet reader, with dictionary encoding enabled,  \r\nThe following query returns wrong results for {{DECIMAL28/REQUIRED}} and {{DECIMAL38/REQUIRED}} (we can't read {{DECIMAL9}} nor {{DECIMAL18}} columns because of DRILL-2262):\r\n{noformat}\r\nselect decimal28_col, decimal28_req, decimal38_col, decimal38_req from dfs.tmp.`test_all_decimal`;\r\n+---------------+---------------+---------------+---------------+\r\n| decimal28_opt | decimal28_req | decimal38_opt | decimal38_req |\r\n+---------------+---------------+---------------+---------------+\r\n| 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n+---------------+---------------+---------------+---------------+\r\n{noformat}\r\n \r\nWhen dictionary encoding is disabled, the following query eturns wrong results for {{DECIMAL28/REQUIRED}} and {{DECIMAL38/REQUIRED}}:\r\n{noformat}\r\nselect decimal9_opt, decimal9_req, decimal18_opt, decimal18_req, decimal28_opt, decimal28_req, decimal38_opt, decimal38_req from dfs.tmp.`test_all_decimal_nodictionary`;\r\n+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+\r\n| decimal9_opt | decimal9_req | decimal18_opt | decimal18_req | decimal28_opt | decimal28_req | decimal38_opt | decimal38_req |\r\n+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+\r\n| 1.000000     | 1.000000     | 1.00000000    | 1.00000000    | 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.000000     | 1.000000     | 1.00000000    | 1.00000000    | 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n| 1.000000     | 1.000000     | 1.00000000    | 1.00000000    | 1.0000000000  | 100000000.0000000000 | 1.0000000000  | 100000000.0000000000 |\r\n+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+\r\n{noformat}\r\n\r\nWhen using the \"complex\" reader:\r\n{noformat}\r\nalter session set `store.parquet.use_new_reader` = true;\r\n{noformat}\r\n\r\nWe can't read {{DECIMAL28}} nor {{DECIMAL38}} because of DRILL-2220.\r\n\r\nthe following query gives wrong results for {DECIMAL9}} and {{DECIMAL18}} no matter if dictionary encoding is enabled or disabled:\r\n{noformat}\r\nselect decimal9_opt, decimal9_req, decimal18_opt, decimal18_req from dfs.tmp.`test_all_decimal`;\r\n+--------------+--------------+---------------+---------------+\r\n| decimal9_opt | decimal9_req | decimal18_opt | decimal18_req |\r\n+--------------+--------------+---------------+---------------+\r\n| 1000000      | 1000000      | 100000000     | 100000000     |\r\n| 1000000      | 1000000      | 100000000     | 100000000     |\r\n| 1000000      | 1000000      | 100000000     | 100000000     |\r\n+--------------+--------------+---------------+---------------+\r\n{noformat}"
    ],
    [
        "DRILL-2385",
        "DRILL-1650",
        "count on complex objects failed with missing function implementation #Wed Mar 04 01:23:42 EST 2015\r\ngit.commit.id.abbrev=71b6bfe\r\n\r\nHave a complex type looks like the following:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> select t.sia from `complex.json` t limit 1;\r\n+------------+\r\n|    sia     |\r\n+------------+\r\n| [1,11,101,1001] |\r\n+------------+\r\n{code}\r\n\r\nA count on the complex type will fail with missing function implementation:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> select t.gbyi, count(t.sia) countsia from `complex.json` t group by t.gbyi;\r\nQuery failed: RemoteRpcException: Failure while running fragment., Schema is currently null.  You must call buildSchema(SelectionVectorMode) before this container can return a schema. [ 12856530-3133-45be-bdf4-ef8cc784f7b3 on qa-node119.qa.lab:31010 ]\r\n[ 12856530-3133-45be-bdf4-ef8cc784f7b3 on qa-node119.qa.lab:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\ndrillbit.log\r\n\r\n{code}\r\n2015-03-04 13:44:51,383 [2b08832b-9247-e90c-785d-751f02fc1548:frag:2:0] ERROR o.a.drill.exec.ops.FragmentContext - Fragment Context received failure.\r\norg.apache.drill.exec.exception.SchemaChangeException: Failure while materializing expression.\r\nError in expression at index 0.  Error: Missing function implementation: [count(BIGINT-REPEATED)].  Full expression: null.\r\n        at org.apache.drill.exec.physical.impl.aggregate.HashAggBatch.createAggregatorInternal(HashAggBatch.java:210) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.aggregate.HashAggBatch.createAggregator(HashAggBatch.java:158) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.aggregate.HashAggBatch.buildSchema(HashAggBatch.java:101) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:130) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.partitionsender.PartitionSenderRootExec.innerNext(PartitionSenderRootExec.java:114) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:121) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:303) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n2015-03-04 13:44:51,383 [2b08832b-9247-e90c-785d-751f02fc1548:frag:2:0] WARN  o.a.d.e.w.fragment.FragmentExecutor - Error while initializing or executing fragment\r\njava.lang.NullPointerException: Schema is currently null.  You must call buildSchema(SelectionVectorMode) before this container can return a schema.\r\n        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:208) ~[guava-14.0.1.jar:na]\r\n        at org.apache.drill.exec.record.VectorContainer.getSchema(VectorContainer.java:261) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.getSchema(AbstractRecordBatch.java:155) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.getSchema(IteratorValidatorBatchIterator.java:75) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.partitionsender.PartitionSenderRootExec.sendEmptyBatch(PartitionSenderRootExec.java:276) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.partitionsender.PartitionSenderRootExec.innerNext(PartitionSenderRootExec.java:131) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:121) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:303) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n{code}\r\n\r\nphysical plan:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> explain plan for select t.gbyi, count(t.sia) countsia from `complex.json` t group by t.gbyi;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(gbyi=[$0], countsia=[$1])\r\n00-02        UnionExchange\r\n01-01          HashAgg(group=[{0}], countsia=[$SUM0($1)])\r\n01-02            HashToRandomExchange(dist0=[[$0]])\r\n02-01              HashAgg(group=[{0}], countsia=[COUNT($1)])\r\n02-02                Project(gbyi=[$1], sia=[$0])\r\n02-03                  Scan(groupscan=[EasyGroupScan [selectionRoot=/drill/testdata/complex_type/json/complex.json, numFiles=1, columns=[`gbyi`, `sia`], files=[maprfs:/drill/testdata/complex_type/json/complex.json]]])\r\n{code}\r\n",
        "Add support for repeated_count multi-level array data type? 1. \"n1.json\" is :\r\n{code}\r\n{\"test\":[1,2,3,4,4,5]}\r\n{code}\r\n\r\nThen the function works fine:\r\n{code}\r\n0: jdbc:drill:> select repeated_count(t.test) from `n1.json` as t;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 6          |\r\n+------------+\r\n{code}\r\n\r\n2. However if the json has 2-level or more arrays:\r\n{code}\r\n{\"test\":[[1,2,3,4,5,6]]}\r\n{code}\r\n\r\nThe function failed:\r\n{code}\r\n0: jdbc:drill:> select t.test[0][0],t.test[0][1] from `n4.json` as t;\r\n+------------+------------+\r\n|   EXPR$0   |   EXPR$1   |\r\n+------------+------------+\r\n| 1          | 2          |\r\n+------------+------------+\r\n1 row selected (0.126 seconds)\r\n0: jdbc:drill:> select repeated_count(t.test[0]) from `n4.json` as t;\r\nQuery failed: Screen received stop request sent. Line 54, Column 22: \"end\" is neither a method, a field, nor a member class of \"org.apache.drill.exec.vector.complex.reader.FieldReader\" [11c5ff5b-c00f-4754-9bda-c3bec9471f72]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n{code}\r\n\r\nAm I missing something or is this a bug?"
    ],
    [
        "DRILL-2391",
        "DRILL-1967",
        "NPE during cleanup in parquet record writer when query fails during execution on CTAS Query below fails during execution due to the user error:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select\r\n. . . . . . . . . . . . >         case when columns[0] = '' then cast(null as varchar(255)) else cast(columns[0] as varchar(255)) end,\r\n. . . . . . . . . . . . >         case when columns[1] = '' then cast(null as integer) else cast(columns[1] as integer) end,\r\n. . . . . . . . . . . . >         case when columns[2] = '' then cast(null as bigint) else cast(columns[2] as bigint) end,\r\n. . . . . . . . . . . . >         case when columns[3] = '' then cast(null as float) else cast(columns[3] as float) end,\r\n. . . . . . . . . . . . >         case when columns[4] = '' then cast(null as double) else cast(columns[4] as double) end,\r\n. . . . . . . . . . . . >         case when columns[5] = '' then cast(null as date) else cast(columns[6] as date) end,\r\n. . . . . . . . . . . . >         case when columns[6] = '' then cast(null as time) else cast(columns[7] as time) end,\r\n. . . . . . . . . . . . >         case when columns[7] = '' then cast(null as timestamp) else cast(columns[8] as timestamp) end,\r\n. . . . . . . . . . . . >         case when columns[8] = '' then cast(null as boolean) else cast(columns[9] as boolean) end,\r\n. . . . . . . . . . . . >         case when columns[9] = '' then cast(null as decimal(8,2)) else cast(columns[9] as decimal(8,2)) end,\r\n. . . . . . . . . . . . >         case when columns[10] = '' then cast(null as decimal(18,4)) else cast(columns[10] as decimal(18,4)) end,\r\n. . . . . . . . . . . . >         case when columns[11] = '' then cast(null as decimal(28,4)) else cast(columns[11] as decimal(28,4)) end,\r\n. . . . . . . . . . . . >         case when columns[12] = '' then cast(null as decimal(38,6)) else cast(columns[12] as decimal(38,6)) end\r\n. . . . . . . . . . . . > from `t5.csv`;\r\nQuery failed: RemoteRpcException: Failure while running fragment., Value 0 for monthOfYear must be in the range [1,12] [ 5a56453c-304d-430a-b4b2-fbc48c9c2766 on atsqa4-133.qa.lab:31010 ]\r\n[ 5a56453c-304d-430a-b4b2-fbc48c9c2766 on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nIf I run the same query in CTAS, I get NPE during cleanup in parquet writer.\r\n\r\n{code}\r\n2015-03-05 22:31:05,212 [2b0726d7-4127-2a83-8c83-2376b767d800:frag:0:0] ERROR o.a.d.e.w.f.AbstractStatusReporter - Error 50633e23-7e6f-48b8-82ec-a395c5c596e4: Failure while running fragment.\r\njava.lang.NullPointerException: null\r\n        at org.apache.drill.exec.store.parquet.ParquetRecordWriter.cleanup(ParquetRecordWriter.java:318) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.WriterRecordBatch.cleanup(WriterRecordBatch.java:187) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.cleanup(IteratorValidatorBatchIterator.java:148) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.cleanup(AbstractSingleRecordBatch.java:121) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.cleanup(IteratorValidatorBatchIterator.java:148) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.internalStop(ScreenCreator.java:178) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:101) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:121) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:303) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_71]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]\r\n2015-03-05 22:31:05,213 [2b0726d7-4127-2a83-8c83-2376b767d800:frag:0:0] INFO  o.a.drill.exec.work.foreman.Foreman - State change requested.  RUNNING --> FAILED\r\norg.apache.drill.exec.rpc.RemoteRpcException: Failure while running fragment.[ 50633e23-7e6f-48b8-82ec-a395c5c596e4 on atsqa4-133.qa.lab:31010 ]\r\n[ 50633e23-7e6f-48b8-82ec-a395c5c596e4 on atsqa4-133.qa.lab:31010 ]\r\n{code}\r\n",
        "Null pointer exception in ParquetRecordWriter when caneled before data arrives To reproduce:\r\n\r\ncreate table my_table select a from json_table.json;\r\n\r\njson_table.json:\r\n{\r\n   \"a\" : \"a string\"\r\n}\r\n{\r\n  \"a\" : 1\r\n}\r\n\r\nThe schema change will cause an error before a batch is ever sent to the writer. This causes a null pointer in the cleanup method.\r\n\r\njava.lang.NullPointerException\r\n\torg.apache.drill.exec.store.parquet.ParquetRecordWriter.cleanup(ParquetRecordWriter.java:298) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.WriterRecordBatch.cleanup(WriterRecordBatch.java:187) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.cleanup(IteratorValidatorBatchIterator.java:148) ~[classes/:na]\r\n\torg.apache.drill.exec.record.AbstractSingleRecordBatch.cleanup(AbstractSingleRecordBatch.java:121) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.cleanup(IteratorValidatorBatchIterator.java:148) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.internalStop(ScreenCreator.java:178) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:101) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) ~[classes/:na]\r\n\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:114) ~[classes/:na]\r\n\torg.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) [classes/:na]\r\n\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_21]\r\n\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_21]\r\n\tjava.lang.Thread.run(Thread.java:722) [na:1.7.0_21]\r\n"
    ],
    [
        "DRILL-2398",
        "DRILL-2277",
        "IS NOT DISTINCT FROM predicate returns incorrect result when used as a join filter count(*) should return 0 and not NULL\r\n{code}\r\n0: jdbc:drill:schema=dfs> select\r\n. . . . . . . . . . . . >         count(*)\r\n. . . . . . . . . . . . > from\r\n. . . . . . . . . . . . >         j1 INNER JOIN j2 ON\r\n. . . . . . . . . . . . >         ( j1.c_double = j2.c_double)\r\n. . . . . . . . . . . . > where\r\n. . . . . . . . . . . . >         j1.c_bigint IS NOT DISTINCT FROM j2.c_bigint\r\n. . . . . . . . . . . . > ;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n+------------+\r\n{code}\r\nThese are the values in the table\r\n{code}\r\n0: jdbc:drill:schema=dfs> select j1.c_bigint, j2.c_bigint, count(*) from j1 INNER JOIN j2 ON (j1.c_double = j2.c_double) group by j1.c_bigint, j2.c_bigint;\r\n+------------+------------+------------+\r\n|  c_bigint  | c_bigint1  |   EXPR$1   |\r\n+------------+------------+------------+\r\n| 460194667  | -498749284 | 1          |\r\n| 464547172  | -498828740 | 1          |\r\n| 467451850  | -498966611 | 2          |\r\n| 471050029  | -499154096 | 3          |\r\n| 472873799  | -499233550 | 3          |\r\n| 475698977  | -499395929 | 2          |\r\n| 478986584  | -499564607 | 1          |\r\n| 488139464  | -499763274 | 3          |\r\n| 498214699  | -499871720 | 2          |\r\n+------------+------------+------------+\r\n9 rows selected (0.339 seconds)\r\n{code}\r\nIS DISTINCT FROM predicate returns correct result\r\n{code}\r\nselect\r\n        count(*)\r\nfrom\r\n        j1 INNER JOIN j2 ON\r\n        ( j1.c_double = j2.c_double)\r\nwhere\r\n        j1.c_bigint IS DISTINCT FROM j2.c_bigint\r\n{code}\r\n\r\nExplain plan for query that returns incorrect result:\r\n{code}\r\n00-01      StreamAgg(group=[{}], EXPR$0=[COUNT()])\r\n00-02        Project($f0=[0])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[CAST(CASE(IS NULL($1), IS NULL($3), IS NULL($3), IS NULL($1), =($1, $3))):BOOLEAN NOT NULL])\r\n00-05              HashJoin(condition=[=($0, $2)], joinType=[inner])\r\n00-07                Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/joins/j1]], selectionRoot=/joins/j1, numFiles=1, columns=[`c_double`, `c_bigint`]]])\r\n00-06                Project(c_double0=[$0], c_bigint0=[$1])\r\n00-08                  Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/joins/j2]], selectionRoot=/joins/j2, numFiles=1, columns=[`c_double`, `c_bigint`]]])\r\n{code}\r\n",
        "COUNT(*) should return 0 instead of an empty result set when there are no records git.commit.id.abbrev=6676f2d\r\n\r\nData Set :\r\n{code}\r\n{\r\n \"id\":1\r\n}\r\n{code}\r\n\r\nQuery :\r\n{code}\r\nselect count(*) from `temp.json` where uid < 1;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n+------------+\r\n{code}\r\n\r\nPostgres returns 0 in this case.\r\n\r\nMarking it as critical since we return an incorrect result"
    ],
    [
        "DRILL-2410",
        "DRILL-2408",
        "CTAS has issues when the underlying query returns 0 results git.commit.id.abbrev=e92db23\r\n\r\nThe below CTAS statement succeeds. Here the query in CTAS returns 0 results.\r\n{code}\r\ncreate table empty_table as select columns[0] id from `rankings` where columns[0] < 0;\r\n{code}\r\n\r\nHowever when I try to run any queries on top of it drill reports a failure\r\n{code}\r\nselect * from temp1;\r\nQuery failed: RuntimeException: maprfs:/drill/empty_table/0_0_0.parquet is not a Parquet file (too small)\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nAttached the error log file\r\n\r\n",
        "CTAS should not create empty folders when underlying query returns no results {noformat}\r\n0: jdbc:drill:schema=dfs> select c_integer, c_bigint, c_date, c_time, c_varchar from j4 where c_bigint is null;\r\n+------------+------------+------------+------------+------------+\r\n| c_integer  |  c_bigint  |   c_date   |   c_time   | c_varchar  |\r\n+------------+------------+------------+------------+------------+\r\n+------------+------------+------------+------------+------------+\r\nNo rows selected (0.126 seconds)\r\n0: jdbc:drill:schema=dfs> create table ctas_t6(c1,c2,c3,c4,c5) as select c_integer, c_bigint, c_date, c_time, c_varchar from j4 where c_bigint is null;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 0                         |\r\n+------------+---------------------------+\r\n1 row selected (0.214 seconds)\r\n0: jdbc:drill:schema=dfs> select * from ctas_t6;\r\nQuery failed: IndexOutOfBoundsException: Index: 0, Size: 0\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{noformat}\r\n\r\nparquet file was not created, but directory was:\r\n{noformat}\r\n[Mon Apr 06 09:03:41 root@/mapr/vmarkman.cluster.com/drill/testdata/joins/ctas_t6 ] # pwd\r\n/mapr/vmarkman.cluster.com/drill/testdata/joins/ctas_t6\r\n[Mon Apr 06 09:03:45 root@/mapr/vmarkman.cluster.com/drill/testdata/joins/ctas_t6 ] # ls -l\r\ntotal 0\r\n{noformat}"
    ],
    [
        "DRILL-2411",
        "DRILL-2277",
        "Scalar SUM/AVG over empty result set returns no rows instead of NULL Queries below should return NULL:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select sum(a2) from t2 where 1=0;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n+------------+\r\nNo rows selected (0.08 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select avg(a2) from t2 where 1=0;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n+------------+\r\nNo rows selected (0.074 seconds)\r\n{code}\r\n\r\nWhen grouped, result is correct:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select a2, sum(a2) from t2 where 1=0 group by a2;\r\n+------------+------------+\r\n|     a2     |   EXPR$1   |\r\n+------------+------------+\r\n+------------+------------+\r\nNo rows selected (0.11 seconds)\r\n{code}\r\n\r\nI'm not convinced and it is not very intuitive that correct result should be NULL, but this is what postgres returns and Aman thinks NULL is the correct behavior :)",
        "COUNT(*) should return 0 instead of an empty result set when there are no records git.commit.id.abbrev=6676f2d\r\n\r\nData Set :\r\n{code}\r\n{\r\n \"id\":1\r\n}\r\n{code}\r\n\r\nQuery :\r\n{code}\r\nselect count(*) from `temp.json` where uid < 1;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n+------------+\r\n{code}\r\n\r\nPostgres returns 0 in this case.\r\n\r\nMarking it as critical since we return an incorrect result"
    ],
    [
        "DRILL-2411",
        "DRILL-2398",
        "Scalar SUM/AVG over empty result set returns no rows instead of NULL Queries below should return NULL:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select sum(a2) from t2 where 1=0;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n+------------+\r\nNo rows selected (0.08 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select avg(a2) from t2 where 1=0;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n+------------+\r\nNo rows selected (0.074 seconds)\r\n{code}\r\n\r\nWhen grouped, result is correct:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select a2, sum(a2) from t2 where 1=0 group by a2;\r\n+------------+------------+\r\n|     a2     |   EXPR$1   |\r\n+------------+------------+\r\n+------------+------------+\r\nNo rows selected (0.11 seconds)\r\n{code}\r\n\r\nI'm not convinced and it is not very intuitive that correct result should be NULL, but this is what postgres returns and Aman thinks NULL is the correct behavior :)",
        "IS NOT DISTINCT FROM predicate returns incorrect result when used as a join filter count(*) should return 0 and not NULL\r\n{code}\r\n0: jdbc:drill:schema=dfs> select\r\n. . . . . . . . . . . . >         count(*)\r\n. . . . . . . . . . . . > from\r\n. . . . . . . . . . . . >         j1 INNER JOIN j2 ON\r\n. . . . . . . . . . . . >         ( j1.c_double = j2.c_double)\r\n. . . . . . . . . . . . > where\r\n. . . . . . . . . . . . >         j1.c_bigint IS NOT DISTINCT FROM j2.c_bigint\r\n. . . . . . . . . . . . > ;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n+------------+\r\n{code}\r\nThese are the values in the table\r\n{code}\r\n0: jdbc:drill:schema=dfs> select j1.c_bigint, j2.c_bigint, count(*) from j1 INNER JOIN j2 ON (j1.c_double = j2.c_double) group by j1.c_bigint, j2.c_bigint;\r\n+------------+------------+------------+\r\n|  c_bigint  | c_bigint1  |   EXPR$1   |\r\n+------------+------------+------------+\r\n| 460194667  | -498749284 | 1          |\r\n| 464547172  | -498828740 | 1          |\r\n| 467451850  | -498966611 | 2          |\r\n| 471050029  | -499154096 | 3          |\r\n| 472873799  | -499233550 | 3          |\r\n| 475698977  | -499395929 | 2          |\r\n| 478986584  | -499564607 | 1          |\r\n| 488139464  | -499763274 | 3          |\r\n| 498214699  | -499871720 | 2          |\r\n+------------+------------+------------+\r\n9 rows selected (0.339 seconds)\r\n{code}\r\nIS DISTINCT FROM predicate returns correct result\r\n{code}\r\nselect\r\n        count(*)\r\nfrom\r\n        j1 INNER JOIN j2 ON\r\n        ( j1.c_double = j2.c_double)\r\nwhere\r\n        j1.c_bigint IS DISTINCT FROM j2.c_bigint\r\n{code}\r\n\r\nExplain plan for query that returns incorrect result:\r\n{code}\r\n00-01      StreamAgg(group=[{}], EXPR$0=[COUNT()])\r\n00-02        Project($f0=[0])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[CAST(CASE(IS NULL($1), IS NULL($3), IS NULL($3), IS NULL($1), =($1, $3))):BOOLEAN NOT NULL])\r\n00-05              HashJoin(condition=[=($0, $2)], joinType=[inner])\r\n00-07                Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/joins/j1]], selectionRoot=/joins/j1, numFiles=1, columns=[`c_double`, `c_bigint`]]])\r\n00-06                Project(c_double0=[$0], c_bigint0=[$1])\r\n00-08                  Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/joins/j2]], selectionRoot=/joins/j2, numFiles=1, columns=[`c_double`, `c_bigint`]]])\r\n{code}\r\n"
    ],
    [
        "DRILL-2425",
        "DRILL-2036",
        "Wrong results when identifier change cases within the same data file #Fri Mar 06 16:51:10 EST 2015\r\ngit.commit.id.abbrev=fb293ba\r\n\r\nI have the following JSON file that one of the identifier change cases:\r\n\r\n{code}\r\n[root@qa-node120 md-83]# hadoop fs -cat /drill/testdata/complex_type/json/schema/a.json\r\n{\"SOURCE\": \"ebm\",\"msAddressIpv6Array\": null}\r\n{\"SOURCE\": \"ebm\",\"msAddressIpv6Array\": {\"msAddressIpv6_1\":\"99.111.222.0\", \"msAddressIpv6_2\":\"88.222.333.0\"}}\r\n{\"SOURCE\": \"ebm\",\"msAddressIpv6Array\": {\"msAddressIpv6_1\":\"99.111.222.1\", \"msAddressIpv6_2\":\"88.222.333.1\"}}\r\n{\"SOURCE\": \"ebm\",\"msAddressIpv6Array\": {\"msaddressipv6_1\":\"99.111.222.2\", \"msAddressIpv6_2\":\"88.222.333.2\"}}\r\n{code}\r\n\r\nQuery this file through drill gives wrong results:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> select t.msAddressIpv6Array.msAddressIpv6_1 as msAddressIpv6_1 from `schema/a.json` t;\r\n+-----------------+\r\n| msAddressIpv6_1 |\r\n+-----------------+\r\n| null            |\r\n| null            |\r\n| null            |\r\n| 99.111.222.2    |\r\n+-----------------+\r\n{code}\r\n\r\nplan:\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> explain plan for select t.msAddressIpv6Array.msAddressIpv6_1 as msAddressIpv6_1 from `schema/a.json` t;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(msAddressIpv6_1=[ITEM($0, 'msAddressIpv6_1')])\r\n00-02        Scan(groupscan=[EasyGroupScan [selectionRoot=/drill/testdata/complex_type/json/schema/a.json, numFiles=1, columns=[`msAddressIpv6Array`.`msAddressIpv6_1`], files=[maprfs:/drill/testdata/complex_type/json/schema/a.json]]])\r\n{code}",
        "select * query returns wrong result when column name in json file changes case {code}\r\n#Sun Jan 18 21:24:57 EST 2015\r\ngit.commit.id.abbrev=a418af1\r\n{code}\r\n\r\ntest.json - column \"city\" has upper case C in the second row\r\n\r\n{code}\r\n{ \"CustomerId\": \"100\", \"city\": 10 }\r\n{ \"CustomerId\": \"101\", \"City\": 20 }\r\n{code}\r\n\r\nWrong result:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from `test.json`;\r\n+------------+------------+\r\n| CustomerId |    city    |\r\n+------------+------------+\r\n| 100        | null       |\r\n| 101        | 20         |\r\n+------------+------------+\r\n2 rows selected (0.077 seconds)\r\n{code}\r\n\r\n\r\n"
    ],
    [
        "DRILL-2507",
        "DRILL-2351",
        "most TestParquetWriter tests aren't actually launching the test queries  Many tests in TestParquetWriter call runTestAndValidate to build and run the test. This method omits to run the test query",
        "Fix TestParquetWriter  In TestParquetWriter we have a utility method runTestAndValidate() to create a parquet table and validate the results with a subsequent query. However we  only seem to be creating the test builder required for the validation and never actually run it. I tried modifying runTestAndValidate() to run validation and it causes memory leaks and other exceptions. \r\n\r\nNeed to fix this class so that we actually perform the validation of the CTAS."
    ],
    [
        "DRILL-2513",
        "DRILL-2093",
        "JsonWriter writes a timestamp values for 'time' datatype git.commit.id.abbrev=9c9ee8c\r\n\r\nQuery :\r\n{code}\r\ncreate table temp_fromcsv as select cast(columns[3] as time) time_col from dfs.`cross-sources`.`fewtypes.tbl`;\r\n{code}\r\n\r\n\r\nThe newly create json file has the below values\r\n{code}\r\nselect * from dfs.`cross-sources`.`temp_fromcsv`;\r\n+------------+\r\n|  time_col  |\r\n+------------+\r\n| 1970-01-01T00:00:00.000Z |\r\n| 1970-01-01T01:00:00.000Z |\r\n| 1970-01-01T02:03:00.000Z |\r\n| 1970-01-01T11:59:00.000Z |\r\n| 1970-01-01T12:00:00.000Z |\r\n| 1970-01-01T12:01:00.000Z |\r\n| 1970-01-01T23:59:00.000Z |\r\n| 1970-01-01T23:59:59.990Z |\r\n| 1970-01-01T15:36:39.000Z |\r\n| 1970-01-01T15:36:39.000Z |\r\n| 1970-01-01T00:01:00.000Z |\r\n| 1970-01-01T02:33:00.000Z |\r\n| 1970-01-01T23:59:00.000Z |\r\n| 1970-01-01T12:03:00.000Z |\r\n| 1970-01-01T12:31:00.000Z |\r\n| 1970-01-01T19:59:00.000Z |\r\n| 1970-01-01T11:59:59.990Z |\r\n| 1970-01-01T15:37:39.000Z |\r\n| 1970-01-01T15:36:39.000Z |\r\n| 1970-01-01T11:59:59.990Z |\r\n| 1970-01-01T15:37:39.000Z |\r\n{code}\r\n\r\nThis issue does not happen with ParquetWriter\r\nI attached the data file used.",
        "Columns of time and timestamp data type are not stored correctly in json format on CTAS I have a csv file and am trying to create matching file in json format.\r\n\r\n{code}\r\nalter session set `store.format` = 'json';\r\n\r\ncreate table test_json(c_varchar, c_integer, c_bigint, c_smalldecimal, c_bigdecimal, c_float, c_date, c_time, c_timestamp, c_boolean) as\r\nselect\r\n        case when columns[0] = '' then cast(null as varchar(255)) else cast(columns[0] as varchar(255)) end,\r\n        case when columns[1] = '' then cast(null as integer) else cast(columns[1] as integer) end,\r\n        case when columns[2] = '' then cast(null as bigint) else cast(columns[2] as bigint) end,\r\n        case when columns[3] = '' then cast(null as decimal(18,4)) else cast(columns[3] as decimal(18, 4)) end,\r\n        case when columns[4] = '' then cast(null as decimal(38,4)) else cast(columns[4] as decimal(38, 4)) end,\r\n        case when columns[5] = '' then cast(null as float) else cast(columns[5] as float) end,\r\n        case when columns[6] = '' then cast(null as date) else cast(columns[6] as date) end,\r\n        case when columns[7] = '' then cast(null as time) else cast(columns[7] as time) end,\r\n        case when columns[8] = '' then cast(null as timestamp) else cast(columns[8] as timestamp) end,\r\n        case when columns[9] = '' then cast(null as boolean) else cast(columns[9] as boolean) end\r\nfrom `t1.csv`;\r\n{code}\r\n\r\nCreate table succeeds, but I can't read back time or timestamp:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select cast(c_time as time) from test_json;\r\nQuery failed: RemoteRpcException: Failure while running fragment., Invalid format: \"1970-01-01T08:13:16.000Z\" is malformed at \"70-01-01T08:13:16.000Z\" [ b3f6c0c9-01e4-410f-919d-a899ede35ed9 on atsqa4-133.qa.lab:31010 ]\r\n[ b3f6c0c9-01e4-410f-919d-a899ede35ed9 on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n0: jdbc:drill:schema=dfs> select c_time from test_json;\r\n+------------+\r\n|   c_time   |\r\n+------------+\r\n| 1970-01-01T08:13:16.000Z |\r\n| 1970-01-01T04:15:45.000Z |\r\n| 1970-01-01T18:21:06.000Z |\r\n| 1970-01-01T13:35:54.000Z |\r\n| 1970-01-01T05:17:11.000Z |\r\n+------------+\r\n5 rows selected (0.055 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select cast(c_timestamp as timestamp) from test_json;\r\nQuery failed: RemoteRpcException: Failure while running fragment., Invalid format: \"2014-03-16T03:55:21.000Z\" is malformed at \"T03:55:21.000Z\" [ 527a42e5-2eb7-41d7-a55b-c6cee0779db6 on atsqa4-133.qa.lab:31010 ]\r\n[ 527a42e5-2eb7-41d7-a55b-c6cee0779db6 on atsqa4-133.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}"
    ],
    [
        "DRILL-2574",
        "DRILL-1909",
        "SendingAccountor can suffer from lost updates In SendingAccountor.waitForSendToComplete():\r\n{code}\r\npublic synchronized void waitForSendComplete() {\r\n  try {\r\n    wait.acquire(batchesSent.get());\r\n    batchesSent.set(0);\r\n  } catch (InterruptedException e) {\r\n    logger.warn(\"Failure while waiting for send complete.\", e);\r\n    // TODO InterruptedException\r\n  }\r\n}\r\n{code}\r\nIt's possible that between the time batchesSent.get() returns and that batchesSent.set(0) are executed, that additional batches could have been sent. If that happens, then the set(0) overwrites the count, losing those. This needs to be better protected against that.",
        "Fix SendingAccountor thread safety SendingAccountor's increment() method is not synchronized, but both it and (already synchronized) method waitForSendComplete() both modify the same variable.\r\n\r\n\r\n"
    ],
    [
        "DRILL-2606",
        "DRILL-2605",
        "Casting a column from hbase to boolean after a join with parquet makes DRILL unresponsive Small scale repro without HBase:\r\n\r\n{code}\r\nalter session set `planner.slice_target`=1;\r\nselect cast(cast(bool_col  as varchar(100)) as boolean) from dfs.`/Users/hadoop/Downloads/fewtypes_null.parquet` order by int_col;\r\n{code}\r\n\r\ngit.commit.id.abbrev=4d398ed\r\n\r\nThe below query never returns and causes Sqlline to hang. DRILL also becomes unresponsive after executing this query. So I have to restart drill for sqlline to even come up.\r\n{code}\r\nselect cast(cast(o.types.bool_col as varchar(100)) as boolean) bool_col\r\nfrom dfs.`cross-sources`.`fewtypes_null.parquet` p\r\ninner join hbase.fewtypes_null o\r\n    on p.int_col = cast(cast(o.types.int_col as varchar(100)) as int)\r\n{code}\r\n\r\nThis issue is similar to DRILL-2605. However in this case I did not use a view and some this causes drill/sqlline to become unresponsive where in DRILL-2605 we just get back an error.\r\n\r\nI attached the data files, data loading script, and the error log. Let me know if you need anything else.",
        "Projecting a boolean column from a hbase view after a join is causing an ExpressionParsingException git.commit.id.abbrev=4d398ed\r\n\r\nI created a view on top of hbase table using the below DDL :\r\n{code}\r\ncreate or replace view dfs.`cross-sources`.fewtypes_null_hbase_booleanbug_view as\r\nselect\r\n    cast(cast(a.types.int_col as varchar(100)) as int) int_col,\r\n    cast(cast(a.types.bool_col as varchar(100)) as boolean) bool_col\r\nfrom hbase.fewtypes_null a;\r\n{code}\r\n\r\nThe below query fails :\r\n{code}\r\nselect o.bool_col\r\nfrom dfs.`cross-sources`.`fewtypes_null.parquet` p\r\ninner join dfs.`cross-sources`.fewtypes_null_hbase_booleanbug_view o\r\n    on p.int_col=o.int_col;\r\nQuery failed: RemoteRpcException: Failure while trying to start remote fragment, Expression has syntax error! line 1:65:no viable alternative at input 'BIT' [ 6e771b83-e3e3-405d-b738-1cd7ee8d5cb5 on qa-node114.qa.lab:31010 ]\r\n{code}\r\n\r\nJust projecting the boolean column from hbase succeeds\r\n{code}\r\nselect o.bool_col from dfs.`cross-sources`.fewtypes_null_hbase_booleanbug_view o;\r\n+------------+\r\n|  bool_col  |\r\n+------------+\r\n| false      |\r\n| false      |\r\n| true       |\r\n| false      |\r\n| false      |\r\n| true       |\r\n| false      |\r\n| true       |\r\n| true       |\r\n| false      |\r\n| true       |\r\n| false      |\r\n| false      |\r\n| false      |\r\n| true       |\r\n| false      |\r\n| true       |\r\n+------------+\r\n17 rows selected (1.366 seconds)\r\n{code}\r\n\r\nI attached the data files, hive script to load data into hbase, and the error log. Let me know if you need anything"
    ],
    [
        "DRILL-2615",
        "DRILL-1709",
        "'DESC' should be short for 'DESCRIBE' ",
        "desc => describe command There is no desc command, can you please add that shortcut to describe.\r\n\r\nRegards,\r\n\r\nHari Sekhon\r\nhttp://www.linkedin.com/in/harisekhon"
    ],
    [
        "DRILL-2616",
        "DRILL-2109",
        "strings loaded incorrectly from parquet files When loading string columns from parquet data sources, some rows have their string values replaced with the value from other rows.\r\n\r\nExample parquet for which the problem occurs:\r\nhttps://drive.google.com/file/d/0B2JGBdceNMxdeFlJcW1FUElOdXc/view?usp=sharing",
        "Parquet reader does not return correct data for required dictionary encoded fields The reader returns all the data for the column but the records are in the wrong order. If you read more than one column, then the values do not match up.\r\n\r\n"
    ],
    [
        "DRILL-2648",
        "DRILL-2609",
        "CONVERT_FROM gives no specific message with called with bad type value Invoking CONVERT_FROM like this:\r\n\r\n  SELECT CONVERT_FROM(CAST(NULL AS INTEGER), '') FROM  INFORMATION_SCHEMA.CATALOGS;\r\n\r\nyields errors with no useful information (nothing specific to the problem):\r\n\r\n  0: jdbc:drill:zk=local> SELECT CONVERT_FROM(CAST(NULL AS INTEGER), '') FROM  INFORMATION_SCHEMA.CATALOGS;\r\n  Query failed: AssertionError: \r\n\r\n  Error: exception while executing query: Failure while executing query. (state=,code=0)\r\n  0: jdbc:drill:zk=local> \r\n\r\n\r\nInvoking CONFIRM_FROM with a bad type string should yield a message saying that the type string was unrecognized  (and eventually should list all valid type strings that Drill knows).\r\n",
        "convert_from should throw an appropriate error when an invalid data type is used as the second argument git.commit.id.abbrev=4d398ed\r\n\r\nThe below query fails with an assertion error. It would be nice if it returns a proper error message indicating that the datatype used is invalid\r\n{code}\r\n0: jdbc:drill:schema=dfs.hbase> select convert_from(a.types.int_col, 'abc') from hbase.fewtypes_null a;\r\nQuery failed: AssertionError:\r\n{code}\r\n\r\nBelow is the error from the logs :\r\n{code}\r\norg.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: null\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:211) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\tat org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_67]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_67]\r\n\tat java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]\r\nCaused by: java.lang.AssertionError: null\r\n\tat org.apache.drill.exec.planner.logical.PreProcessLogicalRel.visit(PreProcessLogicalRel.java:117) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\tat org.eigenbase.rel.ProjectRel.accept(ProjectRel.java:106) ~[optiq-core-0.9-drill-r20.jar:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.preprocessNode(DefaultSqlHandler.java:195) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:135) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:735) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:202) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\t... 4 common frames omitted\r\n2015-03-27 13:29:13,363 [2aea4265-bbbb-54b2-ef91-32544876cad1:foreman] INFO  o.a.drill.exec.work.foreman.Foreman - foreman cleaning up - status: []\r\n2015-03-27 13:29:13,364 [2aea4265-bbbb-54b2-ef91-32544876cad1:foreman] ERROR o.a.drill.exec.work.foreman.Foreman - Error 172a64eb-10b3-4df1-84d0-8a4eba97c147: AssertionError: \r\norg.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: null\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:211) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\tat org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_67]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_67]\r\n\tat java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]\r\nCaused by: java.lang.AssertionError: null\r\n\tat org.apache.drill.exec.planner.logical.PreProcessLogicalRel.visit(PreProcessLogicalRel.java:117) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\tat org.eigenbase.rel.ProjectRel.accept(ProjectRel.java:106) ~[optiq-core-0.9-drill-r20.jar:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.preprocessNode(DefaultSqlHandler.java:195) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:135) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:735) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:202) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n\t... 4 common frames omitted\r\n{code}\r\n\r\nLet me know if you need anything else."
    ],
    [
        "DRILL-2716",
        "DRILL-1460",
        "Casting integer values from JSON file to float/double datatype fails. Casting integer values from JSON file to float/double datatype fails.\r\n\r\n{code}\r\n\r\nproject key without any casting.\r\n\r\n0: jdbc:drill:> select key from `bgint_f.json`;\r\nQuery failed: Query stopped., You tried to write a BigInt type when you are using a ValueWriter of type NullableFloat8WriterImpl. [ 9f8610ea-6d99-44ee-80b6-8869e544d10c on centos-02.qa.lab:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nproject key, cast it to float, fails.\r\n\r\n0: jdbc:drill:> select cast(key as float) from `bgint_f.json`;\r\nQuery failed: Query stopped., You tried to write a BigInt type when you are using a ValueWriter of type NullableFloat8WriterImpl. [ 8e6200a3-b02e-44fe-81e3-ae3bcd241c01 on centos-02.qa.lab:31010 ]\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nproject key, cast it double, fails.\r\n\r\n0: jdbc:drill:> select cast(key as double) from `bgint_f.json`;\r\nQuery failed: Query stopped., You tried to write a BigInt type when you are using a ValueWriter of type NullableFloat8WriterImpl. [ eb124f63-5665-4f64-bd77-d2bb2d040fe6 on centos-02.qa.lab:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nall_text_mode was set to false.\r\n\r\n0: jdbc:drill:> select * from sys.options where name like '%json%';\r\n+------------+------------+------------+------------+------------+------------+------------+\r\n|    name    |    kind    |    type    |  num_val   | string_val |  bool_val  | float_val  |\r\n+------------+------------+------------+------------+------------+------------+------------+\r\n| store.json.all_text_mode | BOOLEAN    | SYSTEM     | null       | null       | false      | null       |\r\n+------------+------------+------------+------------+------------+------------+------------+\r\n1 row selected (0.211 seconds)\r\n\r\nDetails of version\r\n\r\n| 9d92b8e319f2d46e8659d903d355450e15946533 | DRILL-2580: Exit early from HashJoinBatch if build side is empty | 26.03.2015 @ 16:13:53 EDT \r\n\r\nStack trace from drillbit.log \r\n\r\n2015-04-07 23:19:08,826 [2adb9a12-fbf9-29fe-416a-17a4829e6470:frag:0:0] ERROR o.a.drill.exec.ops.FragmentContext - Fragment Context received failure -- Fragment: 0:0\r\njava.lang.RuntimeException: Error closing fragment context.\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.closeOutResources(FragmentExecutor.java:224) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:166) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]\r\nCaused by: java.lang.IllegalArgumentException: You tried to write a BigInt type when you are using a ValueWriter of type NullableFloat8WriterImpl.\r\n        at org.apache.drill.exec.vector.complex.impl.AbstractFieldWriter.fail(AbstractFieldWriter.java:607) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.vector.complex.impl.AbstractFieldWriter.writeBigInt(AbstractFieldWriter.java:189) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.vector.complex.impl.NullableFloat8WriterImpl.writeBigInt(NullableFloat8WriterImpl.java:88) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:276) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.vector.complex.fn.JsonReader.writeDataSwitch(JsonReader.java:197) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.vector.complex.fn.JsonReader.writeToVector(JsonReader.java:171) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:145) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.easy.json.JSONRecordReader.next(JSONRecordReader.java:123) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:170) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:89) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:134) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:68) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:96) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:58) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:163) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        ... 4 common frames omitted\r\n\r\nContents from bgint_f.json\r\n\r\n{\"key\":9.223372e+18}\r\n{\"key\":100000000}\r\n{\"key\":-1}\r\n{\"key\":0}\r\n{\"key\":99999999}\r\n{\"key\":4294967296}\r\n{\"key\":-100000}\r\n{\"key\":100}\r\n{\"key\":536870912}\r\n\r\n{code}",
        "JsonReader fails reading files with decimal numbers and integers in the same field Used the following dataset : http://thecodebarbarian.wordpress.com/2014/03/28/plugging-usda-nutrition-data-into-mongodb\r\n\r\nExecuted the following query\r\n\r\n{noformat}select t.nutrients from dfs.usda.`usda.json` t limit 1;{noformat}\r\nand it failed with following exception\r\n{noformat}\r\n2014-09-27 17:48:39,421 [b9dfbb9b-29a9-425d-801c-2e418533525f:frag:0:0] ERROR o.a.d.e.p.i.ScreenCreator$ScreenRoot - Error 0568d90a-d7df-4a5d-87e9-8b9f718dffa4: Screen received stop request sent.\r\njava.lang.IllegalArgumentException: You tried to write a BigInt type when you are using a ValueWriter of type NullableFloat8WriterImpl.\r\n\tat org.apache.drill.exec.vector.complex.impl.AbstractFieldWriter.fail(AbstractFieldWriter.java:513) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.vector.complex.impl.AbstractFieldWriter.write(AbstractFieldWriter.java:145) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.vector.complex.impl.NullableFloat8WriterImpl.write(NullableFloat8WriterImpl.java:88) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:257) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:310) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:204) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:134) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:65) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n{noformat}\r\n\r\n{noformat}select t.nutrients[0].units from dfs.usda.`usda.json` t limit 1;{noformat}\r\nand it failed with following exception\r\n{noformat}\r\n2014-09-27 17:50:04,394 [9ee8a529-17fd-492f-9cba-2d1f5842eae1:frag:0:0] ERROR o.a.d.e.p.i.ScreenCreator$ScreenRoot - Error c4c6bffd-b62b-4878-af1e-58db64453307: Screen received stop request sent.\r\njava.lang.IllegalArgumentException: You tried to write a BigInt type when you are using a ValueWriter of type NullableFloat8WriterImpl.\r\n\tat org.apache.drill.exec.vector.complex.impl.AbstractFieldWriter.fail(AbstractFieldWriter.java:513) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.vector.complex.impl.AbstractFieldWriter.write(AbstractFieldWriter.java:145) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.vector.complex.impl.NullableFloat8WriterImpl.write(NullableFloat8WriterImpl.java:88) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:257) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:310) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.vector.complex.fn.JsonReader.writeData(JsonReader.java:204) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.vector.complex.fn.JsonReader.write(JsonReader.java:134) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.vector.complex.fn.JsonReaderWithState.write(JsonReaderWithState.java:65) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:111) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:158) ~[drill-java-exec-0.6.0-incubating-SNAPSHOT-rebuffed.jar:0.6.0-incubating-SNAPSHOT]\r\n{noformat}"
    ],
    [
        "DRILL-2727",
        "DRILL-1954",
        "CTAS select * from CSV file results in Exception CREATE TABLE csv_tbl as SELECT * FROM `input.csv`\r\nresults in Exception, with message Repeated types are not supported\r\n\r\n{code}\r\n\r\n0: jdbc:drill:> create table newCSV_Int_tbl12 as select * from `int_f.csv`;\r\nQuery failed: Query stopped., Repeated types are not supported. [ 84ab8d15-6aac-42bb-908a-d663e7453abf on centos-02.qa.lab:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n0: jdbc:drill:> create table newCSV_Int_tbl12 as select * from `int_f.csv`;\r\nQuery failed: Query stopped., Repeated types are not supported. [ 84ab8d15-6aac-42bb-908a-d663e7453abf on centos-02.qa.lab:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n0: jdbc:drill:> create table newCSV_Int_tbl13 as select columns[0] from `int_f.csv`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 12                        |\r\n+------------+---------------------------+\r\n1 row selected (0.146 seconds)\r\n0: jdbc:drill:> select * from newCSV_Int_tbl13;\r\n+------------+\r\n|  columns   |\r\n+------------+\r\n| [\"EXPR$0\"] |\r\n| [\"1\"]      |\r\n| [\"0\"]      |\r\n| [\"-1\"]     |\r\n| [\"65535\"]  |\r\n| [\"1234567\"] |\r\n| [\"1000000\"] |\r\n| [\"101010\"] |\r\n| [\"11111\"]  |\r\n| [\"100\"]    |\r\n| [\"13\"]     |\r\n| [\"19\"]     |\r\n| [\"17\"]     |\r\n+------------+\r\n13 rows selected (0.117 seconds)\r\n\r\nStack trace from drillbit.log\r\n\r\norg.apache.drill.exec.rpc.RemoteRpcException: Failure while running fragment., Repeated types are not supported. [ 0b36b8c1-3d1e-40a4-9937-aade9ead445b on centos-02.qa.lab:31010 ]\r\n[ 0b36b8c1-3d1e-40a4-9937-aade9ead445b on centos-02.qa.lab:31010 ]\r\n\r\n        at org.apache.drill.exec.work.foreman.QueryManager.statusUpdate(QueryManager.java:163) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.QueryManager$RootStatusReporter.statusChange(QueryManager.java:281) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.AbstractStatusReporter.fail(AbstractStatusReporter.java:114) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.AbstractStatusReporter.fail(AbstractStatusReporter.java:110) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.internalFail(FragmentExecutor.java:230) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:165) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]\r\n2015-04-09 00:37:48,977 [2ada3623-63c8-3c52-31cd-45c219efd25f:frag:0:0] WARN  o.a.d.e.w.fragment.FragmentExecutor - Error while initializing or executing fragment\r\njava.lang.RuntimeException: Error closing fragment context.\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.closeOutResources(FragmentExecutor.java:224) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:166) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]\r\nCaused by: java.lang.UnsupportedOperationException: Repeated types are not supported.\r\n        at org.apache.drill.exec.store.StringOutputRecordWriter$RepeatedVarCharStringFieldConverter.writeField(StringOutputRecordWriter.java:1556) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.EventBasedRecordWriter.write(EventBasedRecordWriter.java:58) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.WriterRecordBatch.innerNext(WriterRecordBatch.java:116) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:89) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:134) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:68) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:96) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:58) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:163) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        ... 4 common frames omitted\r\n2015-04-09 00:37:48,978 [2ada3623-63c8-3c52-31cd-45c219efd25f:frag:0:0] ERROR o.a.drill.exec.ops.FragmentContext - Fragment Context received failure -- Fragment: 0:0\r\njava.lang.RuntimeException: Error closing fragment context.\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.closeOutResources(FragmentExecutor.java:224) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:166) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]\r\nCaused by: java.lang.UnsupportedOperationException: Repeated types are not supported.\r\n        at org.apache.drill.exec.store.StringOutputRecordWriter$RepeatedVarCharStringFieldConverter.writeField(StringOutputRecordWriter.java:1556) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.EventBasedRecordWriter.write(EventBasedRecordWriter.java:58) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.WriterRecordBatch.innerNext(WriterRecordBatch.java:116) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:89) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:134) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:68) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:96) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:58) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:163) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        ... 4 common frames omitted\r\n\r\ndata used in the CSV file was\r\n\r\n[root@centos-01 csv_dir]# cat int_f.csv \r\n1\r\n0\r\n-1\r\n65535\r\n1234567\r\n1000000\r\n101010\r\n11111\r\n100\r\n13\r\n19\r\n17\r\n{code}",
        "Update CSV Writer to treat a single RepeatedVarChar column as separate columns on output rather than Erroring I am experiencing this error when attempting to create a table.  When the same query is executed using SELECT * the results display fine.\r\n\r\n0: jdbc:drill:zk=local> CREATE TABLE fcc_cell AS select * from dfs.`/Users/Documents/fcc/fcc_lic_vw.csv` WHERE columns[10] IN ('\"Mobile/Fixed Broadband\"', '\"Fixed Wireless\"') AND columns[17] = '\"A\"';\r\nQuery failed: Query stopped., Repeated types are not supported. [ cd76d8fd-b00e-47d5-962c-6566381b17fb on 192.168.1.4:31010 ]\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)"
    ],
    [
        "DRILL-2731",
        "DRILL-2198",
        "Need to repeat realloc in vectors until there is sufficient space When calling setSafe() on a valuevector, we check that the vectors capacity is sufficient to hold the new value, and it is not, we call reAlloc(). However, it is possible that there is still insufficient space after this. The call to reAlloc() should be repeated until there is sufficient space.",
        "Vector re-allocation fails for large records In many places we reallocate vectors without making sure that there is enough space after reallocation. This causes problem for large records that won't fit into buffer even after reallocating once. We should keep re-allocating until there is enough space for writing the record. "
    ],
    [
        "DRILL-2775",
        "DRILL-2618",
        "message error not clear when doing a select or ctas on empty folder if you have an empty folder \"emptyfolder\", you get a cryptic error message when you try to query the folder or CTAS a table that has the same name of the empty folder:\r\n{noformat}\r\n0: jdbc:drill:zk=local> select * from emptyfolder;\r\nError: PARSE ERROR: Index: 0, Size: 0\r\n\r\n[Error Id: ef86154b-8219-4b48-84bf-cb318f7d4ae4 on abdel-11.qa.lab:31010] (state=,code=0)\r\n{noformat}\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> create table emptyfolder as select * from `test.json`;\r\nError: SYSTEM ERROR: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n\r\n[Error Id: 1c3965f4-1566-4df6-9bb6-a91211771976 on abdel-11.qa.lab:31010] (state=,code=0)\r\n{noformat}",
        "BasicFormatMatcher calls getFirstPath(...) without checking # of paths is not zero {{BasicFormatMatcher.isReadable(...)}} calls {{getFirstPath(...)}} without checking that there is at least one path.  This can cause an IndexOutOfBoundsException.\r\n\r\nTo reproduce, create an empty directory {{/tmp/CaseInsensitiveColumnNames}} and run {{exec/java-exec/src/test/java/org/apache/drill/TestExampleQueries.java}}."
    ],
    [
        "DRILL-2787",
        "DRILL-2786",
        "NPE when first argument to maxdir UDF is invalid \"dfs.\" is not a valid workspace\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from bigtable where dir0 = maxdir('dfs.','bigtable'); \r\nQuery failed: NullPointerException: \r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\ndfs.test1 does not exist\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from bigtable where dir0 = maxdir('dfs.test1','bigtable');\r\nQuery failed: NullPointerException: \r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nEmpty string as a first argument:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from bigtable where dir0 = maxdir('','bigtable');\r\nQuery failed: NullPointerException: \r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n",
        "NPE when first argument to maxdir is invalid \"dfs.\" is not a valid workspace\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from bigtable where dir0 = maxdir('dfs.','bigtable'); \r\nQuery failed: NullPointerException: \r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\ndfs.test1 does not exist\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from bigtable where dir0 = maxdir('dfs.test1','bigtable');\r\nQuery failed: NullPointerException: \r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nEmpty string as a first argument:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from bigtable where dir0 = maxdir('','bigtable');\r\nQuery failed: NullPointerException: \r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n"
    ],
    [
        "DRILL-2794",
        "DRILL-2553",
        "Partition pruning is not happening correctly (results in a full table scan) when maxdir/mindir is used in the filter condition Directory structure:\r\n{code}\r\n[Tue Apr 14 13:43:54 root@/mapr/vmarkman.cluster.com/test/smalltable ] # ls -R\r\n.:\r\n2014  2015  2016\r\n\r\n./2014:\r\n\r\n./2015:\r\n01  02\r\n\r\n./2015/01:\r\nt1.csv\r\n\r\n./2015/02:\r\nt2.csv\r\n\r\n./2016:\r\nt1.csv\r\n\r\n[Tue Apr 14 13:44:26 root@/mapr/vmarkman.cluster.com/test/bigtable ] # ls -R\r\n.:\r\n2015  2016\r\n\r\n./2015:\r\n01  02  03  04\r\n\r\n./2015/01:\r\n0_0_0.parquet  1_0_0.parquet  2_0_0.parquet  3_0_0.parquet  4_0_0.parquet  5_0_0.parquet\r\n\r\n./2015/02:\r\n0_0_0.parquet\r\n\r\n./2015/03:\r\n0_0_0.parquet\r\n\r\n./2015/04:\r\n0_0_0.parquet\r\n\r\n./2016:\r\n01  parquet.file\r\n\r\n./2016/01:\r\n0_0_0.parquet\r\n{code}\r\n\r\nSimple case, partition pruning is happening correctly: only 2016 directory is scanned from 'smalltable'.\r\n{code}\r\n0: jdbc:drill:schema=dfs> explain plan for select * from smalltable where dir0 = maxdir('dfs.test', 'bigtable');\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(*=[$0])\r\n00-02        Project(*=[$0])\r\n00-03          Scan(groupscan=[EasyGroupScan [selectionRoot=/test/smalltable, numFiles=1, columns=[`*`], files=[maprfs:/test/smalltable/2016/t1.csv]]])\r\n | {\r\n  \"head\" : {\r\n    \"version\" : 1,\r\n    \"generator\" : {\r\n      \"type\" : \"ExplainHandler\",\r\n      \"info\" : \"\"\r\n    },\r\n    \"type\" : \"APACHE_DRILL_PHYSICAL\",\r\n    \"options\" : [ ],\r\n    \"queue\" : 0,\r\n    \"resultMode\" : \"EXEC\"\r\n  },\r\n  \"graph\" : [ {\r\n    \"pop\" : \"fs-scan\",\r\n    \"@id\" : 3,\r\n    \"files\" : [ \"maprfs:/test/smalltable/2016/t1.csv\" ],\r\n    \"storage\" : {\r\n      \"type\" : \"file\",\r\n      \"enabled\" : true,\r\n      \"connection\" : \"maprfs:///\",\r\n      \"workspaces\" : {\r\n        \"root\" : {\r\n          \"location\" : \"/\",\r\n          \"writable\" : false,\r\n          \"defaultInputFormat\" : null\r\n        },\r\n...\r\n...\r\n{code}\r\nWith added second predicate (dir1 = mindir('dfs.test', 'bigtable/2016') which evaluates to false (there is no directory '01' in smalltable)\r\nwe end up scanning everything in the smalltable. This does not look right to me and I think this is a bug.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> explain plan for select * from smalltable where dir0 = maxdir('dfs.test', 'bigtable') and dir1 = mindir('dfs.test', 'bigtable/2016');\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(*=[$0])\r\n00-02        Project(T15\u00a6\u00a6*=[$0])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[AND(=($1, '2016'), =($2, '01'))])\r\n00-05              Project(T15\u00a6\u00a6*=[$0], dir0=[$1], dir1=[$2])\r\n00-06                Scan(groupscan=[EasyGroupScan [selectionRoot=/test/smalltable, numFiles=3, columns=[`*`], files=[maprfs:/test/smalltable/2015/01/t1.csv, maprfs:/test/smalltable/2015/02/t2.csv, maprfs:/test/smalltable/2016/t1.csv]]])\r\n | {\r\n  \"head\" : {\r\n    \"version\" : 1,\r\n    \"generator\" : {\r\n      \"type\" : \"ExplainHandler\",\r\n      \"info\" : \"\"\r\n    },\r\n    \"type\" : \"APACHE_DRILL_PHYSICAL\",\r\n    \"options\" : [ ],\r\n    \"queue\" : 0,\r\n    \"resultMode\" : \"EXEC\"\r\n  },\r\n  \"graph\" : [ {\r\n    \"pop\" : \"fs-scan\",\r\n    \"@id\" : 6,\r\n    \"files\" : [ \"maprfs:/test/smalltable/2015/01/t1.csv\", \"maprfs:/test/smalltable/2015/02/t2.csv\", \"maprfs:/test/smalltable/2016/t1.csv\" ],\r\n    \"storage\" : {\r\n      \"type\" : \"file\",\r\n      \"enabled\" : true,\r\n      \"connection\" : \"maprfs:///\",\r\n      \"workspaces\" : {\r\n        \"root\" : {\r\n          \"location\" : \"/\",\r\n          \"writable\" : false,\r\n          \"defaultInputFormat\" : null\r\n        },\r\n...\r\n...\r\n{code}\r\n\r\nHere is a similar example with parquet file where predicate \"a1=11\" evaluates to false.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> explain plan for select * from bigtable where dir0=maxdir('dfs.test','bigtable') and a1 = 11;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(*=[$0])\r\n00-02        Project(T25\u00a6\u00a6*=[$0])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[AND(=($1, '2016'), =($2, 11))])\r\n00-05              Project(T25\u00a6\u00a6*=[$0], dir0=[$1], a1=[$2])\r\n00-06                Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/test/bigtable/2016/01/0_0_0.parquet], ReadEntryWithPath [path=maprfs:/test/bigtable/2016/parquet.file]], selectionRoot=/test/bigtable, numFiles=2, columns=[`*`]]])\r\n | {\r\n  \"head\" : {\r\n    \"version\" : 1,\r\n    \"generator\" : {\r\n      \"type\" : \"ExplainHandler\",\r\n      \"info\" : \"\"\r\n    },\r\n    \"type\" : \"APACHE_DRILL_PHYSICAL\",\r\n    \"options\" : [ ],\r\n    \"queue\" : 0,\r\n    \"resultMode\" : \"EXEC\"\r\n  },\r\n  \"graph\" : [ {\r\n    \"pop\" : \"parquet-scan\",\r\n    \"@id\" : 6,\r\n    \"entries\" : [ {\r\n      \"path\" : \"maprfs:/test/bigtable/2016/01/0_0_0.parquet\"\r\n    }, {\r\n      \"path\" : \"maprfs:/test/bigtable/2016/parquet.file\"\r\n    } ],\r\n{code}\r\n\r\nAnd finally, when we use the same table in the from clause and in maxdir/mindir, we scan only one file (to return schema):\r\nI would think that the same should happen in the bug case above ...\r\n{code}\r\n0: jdbc:drill:schema=dfs> explain plan for select * from bigtable where dir0 = maxdir('dfs.test', 'bigtable') and dir1 = mindir('dfs.test', 'bigtable/2016');\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(*=[$0])\r\n00-02        Project(T29\u00a6\u00a6*=[$0])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[AND(=($1, '2016'), =($2, 'parquet.file'))])\r\n00-05              Project(T29\u00a6\u00a6*=[$0], dir0=[$1], dir1=[$2])\r\n00-06                Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/test/bigtable/2015/01/4_0_0.parquet]], selectionRoot=/test/bigtable, numFiles=1, columns=[`*`]]])\r\n | {\r\n  \"head\" : {\r\n    \"version\" : 1,\r\n    \"generator\" : {\r\n      \"type\" : \"ExplainHandler\",\r\n      \"info\" : \"\"\r\n    },\r\n    \"type\" : \"APACHE_DRILL_PHYSICAL\",\r\n    \"options\" : [ ],\r\n    \"queue\" : 0,\r\n    \"resultMode\" : \"EXEC\"\r\n  },\r\n  \"graph\" : [ {\r\n    \"pop\" : \"parquet-scan\",\r\n    \"@id\" : 6,\r\n    \"entries\" : [ {\r\n      \"path\" : \"maprfs:/test/bigtable/2015/01/4_0_0.parquet\"\r\n    } ],\r\n{code}\r\n\r\n\r\n\r\n\r\n",
        "Cost calculation fails to properly choose single file scan in favor of a multi-file scan when files are small There is a failing test case in the patch for constant folding that should be checked in soon. The test attempts to prune out one directory of a scan after a constant expression returning the name of a directory is folded, but the files being read from both directories are very small. Our current method of calculating cost makes the pruned and unpruned plans report the same cost. This could be fixed in a few different locations, EasyGroupScan.getScanStats() being used here could factor the file count into its calculation of the total row count. We also could move to a two part metric to track the number of files, instead of just an estimated row count. This would require some changes in the cost calculation of the scan rels themselves which use the information from the scan stats. I think in general we should consider solving this as high up as possible, as we want to make as optimal cost estimates as possible, even if the information provided from storage plugins is not completely accurate. For example, even disregarding the row count reported by EasyGroupScan, the rel nodes have knowledge of the number of partitions. It seems like at this level we should be able to avoid picking the plan that has a superset of the partitions of the other possible plan."
    ],
    [
        "DRILL-2801",
        "DRILL-2083",
        "ORDER BY produces extra records Running in embedded mode on my mac.\r\n{code}\r\n$ wc -w data.csv\r\n   50000 data.csv\r\n{code}\r\nHere's the query:\r\n{code}\r\n0: jdbc:drill:zk=local> SELECT count(*) FROM dfs.`data.csv`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 50000      |\r\n+------------+\r\n1 row selected (0.223 seconds)\r\n0: jdbc:drill:zk=local> SELECT columns[0] FROM dfs.`data.csv` ORDER BY columns[0];\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n...\r\n| 6          |\r\n+------------+\r\n50,001 rows selected (0.928 seconds)\r\n0: jdbc:drill:zk=local> SELECT tab.col, COUNT(tab.col) FROM (SELECT columns[0] col FROM dfs.`data.csv` ORDER BY columns[0]) tab GROUP BY tab.col;\r\n+------------+------------+\r\n|     col      |   EXPR$1   |\r\n+------------+------------+\r\n| 2          | 10000      |\r\n| 3          | 10000      |\r\n| 4          | 10000      |\r\n| 5          | 10001      |\r\n| 6          | 10000      |\r\n+------------+------------+\r\n5 rows selected (0.704 seconds)\r\n{code}",
        "order by on large dataset returns wrong results #Mon Jan 26 14:10:51 PST 2015\r\ngit.commit.id.abbrev=3c6d0ef\r\n\r\nTest data has 1 million rows and can be accessed at \r\n\r\nhttp://apache-drill.s3.amazonaws.com/files/complex.json.gz\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> select count (t.id) from `complex.json` t;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 1000000    |\r\n+------------+\r\n{code}\r\n\r\nBut order by returned 30 more rows.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> select t.id from `complex.json` t order by t.id;\r\n....\r\n| 999997     |\r\n| 999998     |\r\n| 999999     |\r\n| 1000000    |\r\n+------------+\r\n1,000,030 rows selected (19.449 seconds)\r\n{code}\r\n\r\nphysical plan\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> explain plan for select t.id from `complex.json` t order by t.id;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      SingleMergeExchange(sort0=[0 ASC])\r\n01-01        SelectionVectorRemover\r\n01-02          Sort(sort0=[$0], dir0=[ASC])\r\n01-03            HashToRandomExchange(dist0=[[$0]])\r\n02-01              Scan(groupscan=[EasyGroupScan [selectionRoot=/drill/testdata/complex_type/json/complex.json, numFiles=1, columns=[`id`], files=[maprfs:/drill/testdata/complex_type/json/complex.json]]])\r\n{code}"
    ],
    [
        "DRILL-2830",
        "DRILL-2345",
        "Self-Join via view gives wrong results Create a view by:\r\n{code}\r\ncreate view v1(x, y) as select n_regionkey,  n_nationkey from cp.`tpch/nation.parquet`;\r\n{code}\r\n\r\nAnd join this view with the file which created the view with condition t.n_nationkey  = v1.y (where 'y' is n_nationkey)\r\n\r\n{code}\r\nselect t.n_nationkey from cp.`tpch/nation.parquet` t inner join v1 on t.n_nationkey  = v1.y;\r\n\r\n+-------------+\r\n| n_nationkey |\r\n+-------------+\r\n| 0           |\r\n| 1           |\r\n| 1           |\r\n| 1           |\r\n| 4           |\r\n| 0           |\r\n| 3           |\r\n| 3           |\r\n| 2           |\r\n| 2           |\r\n| 4           |\r\n| 4           |\r\n| 2           |\r\n| 4           |\r\n| 0           |\r\n| 0           |\r\n| 0           |\r\n| 1           |\r\n| 2           |\r\n| 3           |\r\n| 4           |\r\n| 2           |\r\n| 3           |\r\n| 3           |\r\n| 1           |\r\n+-------------+\r\n25 rows selected (0.153 seconds)\r\n{code}\r\n\r\nAfter investigating the plan, I found out that the result was produced as if the join condition was t.n_nationkey  = v1.x (where x is 'n_regionkey')",
        "Join between drill table and view created on the same table throws unsupported exception {code}\r\n0: jdbc:drill:schema=dfs> select * from t1 limit 1;\r\n+------------+------------+------------+\r\n|     a1     |     b1     |     c1     |\r\n+------------+------------+------------+\r\n| 1          | aaaaa      | 2015-01-01 |\r\n+------------+------------+------------+\r\n1 row selected (0.068 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> create view v1(x,y,z) as select a1, b1, c1 from t1;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | View 'v1' created successfully in 'dfs.aggregation' schema |\r\n+------------+------------+\r\n1 row selected (0.084 seconds)\r\n{code}\r\n\r\nInner join between table and a view on the same table should work:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from t1 inner join v1 on t1.a1 = v1.x;\r\nQuery failed: UnsupportedRelOperatorException: This query cannot be planned possibly due to either a cartesian join or an inequality join\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nSelf join works:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select t1.a1, t1_alias.a1 from t1 inner join t1 t1_alias on t1.a1 = t1_alias.a1;\r\n+------------+------------+\r\n|     a1     |    a10     |\r\n+------------+------------+\r\n| 1          | 1          |\r\n| 2          | 2          |\r\n| 3          | 3          |\r\n| 4          | 4          |\r\n| 5          | 5          |\r\n| 6          | 6          |\r\n| 7          | 7          |\r\n| 9          | 9          |\r\n| 10         | 10         |\r\n+------------+------------+\r\n9 rows selected (0.09 seconds)\r\n{code}"
    ],
    [
        "DRILL-2838",
        "DRILL-2167",
        "Applying flatten after joining 2 sub-queries returns empty maps git.commit.id.abbrev=5cd36c5\r\n\r\nThe below query applies flatten after joining 2 subqueries. It generates empty maps which is wrong\r\n{code}\r\nselect v1.uid, flatten(events), flatten(transactions) from \r\n    (select uid, events from `data.json`) v1\r\ninner join\r\n    (select uid, transactions from `data.json`) v2\r\non v1.uid = v2.uid;\r\n+------------+------------+------------+\r\n|    uid     |   EXPR$1   |   EXPR$2   |\r\n+------------+------------+------------+\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 1          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n| 2          | {}         | {}         |\r\n+------------+------------+------------+\r\n36 rows selected (0.244 seconds)\r\n{code}\r\n\r\nI attached the data set. Let me know if you have any questions.",
        "Order by on a repeated index from the output of a flatten on large no of records results in incorrect results git.commit.id.abbrev=3e33880\r\n\r\nThe below query results in 200006 records. Based on the data set we should only receive 200000 records. \r\n{code}\r\nselect s.uid from (select d.uid, flatten(d.map.rm) rms from `data.json` d) s order by s.rms.rptd[1].d;\r\n{code}\r\n\r\nWhen I removed the order by part, drill correctly reported 200000 records.\r\n{code}\r\nselect s.uid from (select d.uid, flatten(d.map.rm) rms from `data.json` d) s;\r\n{code}\r\n\r\n\r\nI attached the data set with 2 records. I copied over the data set 50000 times and ran the queries on top of it. Let me know if you have any other questions"
    ],
    [
        "DRILL-3006",
        "DRILL-1980",
        "CTAS with interval data type creates invalid parquet file Used the below CTAS statement:\r\ncreate table t6 as select interval '10' day  interval_day_col from cp.`employee.json` limit 1;\r\n\r\nWhen I query the table 't6'  the following exception is encountered:\r\n\r\nCaused by: java.io.IOException: Failure while trying to get footer for file file:/tmp/t6/0_0_0.parquet\r\n        at org.apache.drill.exec.store.parquet.FooterGatherer$FooterReader.convertToIOException(FooterGatherer.java:120) ~[drill-java-exec-1.0.0-SNAPSHOT-rebuffed.jar:1.0.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.TimedRunnable.getValue(TimedRunnable.java:67) ~[drill-java-exec-1.0.0-SNAPSHOT-rebuffed.jar:1.0.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.TimedRunnable.run(TimedRunnable.java:136) ~[drill-java-exec-1.0.0-SNAPSHOT-rebuffed.jar:1.0.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.parquet.FooterGatherer.getFooters(FooterGatherer.java:95) ~[drill-java-exec-1.0.0-SNAPSHOT-rebuffed.jar:1.0.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.parquet.ParquetGroupScan.readFooterHelper(ParquetGroupScan.java:229) ~[drill-java-exec-1.0.0-SNAPSHOT-rebuffed.jar:1.0.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.parquet.ParquetGroupScan.access$000(ParquetGroupScan.java:79) ~[drill-java-exec-1.0.0-SNAPSHOT-rebuffed.jar:1.0.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.parquet.ParquetGroupScan$1.run(ParquetGroupScan.java:206) ~[drill-java-exec-1.0.0-SNAPSHOT-rebuffed.jar:1.0.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.parquet.ParquetGroupScan$1.run(ParquetGroupScan.java:204) ~[drill-java-exec-1.0.0-SNAPSHOT-rebuffed.jar:1.0.0-SNAPSHOT]\r\n        at java.security.AccessController.doPrivileged(Native Method) ~[na:1.7.0_67]\r\n        at javax.security.auth.Subject.doAs(Subject.java:415) ~[na:1.7.0_67]\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556) ~[hadoop-common-2.4.1.jar:na]\r\n        at org.apache.drill.exec.store.parquet.ParquetGroupScan.readFooter(ParquetGroupScan.java:204) ~[drill-java-exec-1.0.0-SNAPSHOT-rebuffed.jar:1.0.0-SNAPSHOT]\r\n        ... 21 common frames omitted\r\nCaused by: java.lang.IllegalArgumentException: Invalid FIXED_LEN_BYTE_ARRAY length: 0\r\n        at parquet.Preconditions.checkArgument(Preconditions.java:50) ~[parquet-common-1.6.0rc3-drill-r0.3.jar:1.6.0rc3-drill-r0.3]\r\n        at parquet.schema.Types$PrimitiveBuilder.build(Types.java:320) ~[parquet-column-1.6.0rc3-drill-r0.3.jar:1.6.0rc3-drill-r0.3]\r\n        at parquet.schema.Types$PrimitiveBuilder.build(Types.java:250) ~[parquet-column-1.6.0rc3-drill-r0.3.jar:1.6.0rc3-drill-r0.3]\r\n        at parquet.schema.Types$Builder.named(Types.java:228) ~[parquet-column-1.6.0rc3-drill-r0.3.jar:1.6.0rc3-drill-r0.3]\r\n        at parquet.format.converter.ParquetMetadataConverter.buildChildren(ParquetMetadataConverter.java:640) ~[parquet-hadoop-1.6.0rc3-drill-r0.3.jar:1.6.0rc3-drill-r0.3]\r\n        at parquet.format.converter.ParquetMetadataConverter.fromParquetSchema(ParquetMetadataConverter.java:601) ~[parquet-hadoop-1.6.0rc3-drill-r0.3.jar:1.6.0rc3-drill-r0.3]\r\n        at parquet.format.converter.ParquetMetadataConverter.fromParquetMetadata(ParquetMetadataConverter.java:543) ~[parquet-hadoop-1.6.0rc3-drill-r0.3.jar:1.6.0rc3-drill-r0.3]\r\n        at parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:529) ~[parquet-hadoop-1.6.0rc3-drill-r0.3.jar:1.6.0rc3-drill-r0.3]\r\n        at parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:480) ~[parquet-hadoop-1.6.0rc3-drill-r0.3.jar:1.6.0rc3-drill-r0.3]\r\n        at org.apache.drill.exec.store.parquet.FooterGatherer.readFooter(FooterGatherer.java:161) ~[drill-java-exec-1.0.0-SNAPSHOT-rebuffed.jar:1.0.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.parquet.FooterGatherer$FooterReader.runInner(FooterGatherer.java:115) ~[drill-java-exec-1.0.0-SNAPSHOT-rebuffed.jar:1.0.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.parquet.FooterGatherer$FooterReader.runInner(FooterGatherer.java:102) ~[drill-java-exec-1.0.0-SNAPSHOT-rebuffed.jar:1.0.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.TimedRunnable.run(TimedRunnable.java:47) ~[drill-java-exec-1.0.0-SNAPSHOT-rebuffed.jar:1.0.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.TimedRunnable.run(TimedRunnable.java:107) ~[drill-java-exec-1.0.0-SNAPSHOT-rebuffed.jar:1.0.0-SNAPSHOT]\r\n        ... 30 common frames omitted\r\n\r\nWhen I run parquet tools (parquet-schema or parquet-meta) on the parquet file I get a similar error: \"Invalid FIXED_LEN_BYTE_ARRAY length: 0\"\r\n",
        "Create table with a Cast to interval day results in a file which cannot be read Created a parquet file from a json file with all types listed in it.\r\n{code}\r\n0: jdbc:drill:> CREATE TABLE parquet_all_types AS SELECT cast( INT_col as int) INT_col,cast( BIGINT_col as bigint) BIGINT_col,cast( DECIMAL9_col as decimal) DECIMAL9_col,cast( DECIMAL18_col as decimal(18,9)) DECIMAL18_col,cast( DECIMAL28SPARSE_col as decimal(28, 14)) DECIMAL28SPARSE_col,cast( DECIMAL38SPARSE_col as decimal(38, 19)) DECIMAL38SPARSE_col,cast( DATE_col as date) DATE_col,cast( TIME_col as time) TIME_col,cast( TIMESTAMP_col as timestamp) TIMESTAMP_col,cast( FLOAT4_col as float) FLOAT4_col,cast( FLOAT8_col as double) FLOAT8_col,cast( BIT_col as boolean) BIT_col,cast( VARCHAR_col as varchar(65000)) VARCHAR_col,cast( VAR16CHAR_col as varchar(65000)) VAR16CHAR_col,cast( VARBINARY_col as varbinary(65000)) VARBINARY_col,cast( INTERVALYEAR_col as interval year) INTERVALYEAR_col,cast( INTERVALDAY_col as interval day) INTERVALDAY_col FROM `/user/root/alltypes.json`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 8                         |\r\n+------------+---------------------------+\r\n1 row selected (0.595 seconds)\r\n{code}\r\nTried reading created parquet file from drill. Fails with\r\n{code}\r\n0: jdbc:drill:> explain plan for select * from `/parquet_all_types/0_0_0.parquet`;\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: Internal error: Error while applying rule DrillTableRule, args [rel#6060:EnumerableTableAccessRel.ENUMERABLE.ANY([]).[](table=[dfs, root, /parquet_all_types/0_0_0.parquet])]\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}"
    ],
    [
        "DRILL-3101",
        "DRILL-1499",
        "Setting \"slice_target\" to 1 changes the order of the columns in a \"select *\" query with order by git.commit.id.abbrev=d8b1975\r\n\r\nWith Default Settings :\r\n{code}\r\nselect * from region order by length(r_name);\r\n+-------------+--------+-----------+\r\n| r_regionkey | r_name | r_comment |\r\n+-------------+--------+-----------+\r\n| 2 | ASIA | ges. thinly even pinto beans ca |\r\n| 0 | AFRICA | lar deposits. blithely final packages cajole. regular waters are final requests. regular accounts are according to  |\r\n| 3 | EUROPE | ly final courts cajole furiously final excuse |\r\n| 1 | AMERICA | hs use ironic, even requests. s |\r\n| 4 | MIDDLE EAST | uickly special accounts cajole carefully blithely close requests. carefully final asymptotes haggle furiousl |\r\n{code}\r\n\r\nNow after setting the slice target to 1, the order of the columns changed\r\n{code}\r\n0: jdbc:drill:schema=dfs_eea> alter session set `planner.slice_target` = 1;\r\n+-------+--------------------------------+\r\n|  ok   |            summary             |\r\n+-------+--------------------------------+\r\n| true  | planner.slice_target updated.  |\r\n+-------+--------------------------------+\r\n1 row selected (0.11 seconds)\r\n0: jdbc:drill:schema=dfs_eea> select * from region order by length(r_name);\r\n+-----------+--------+-------------+\r\n| r_comment | r_name | r_regionkey |\r\n+-----------+--------+-------------+\r\n| ges. thinly even pinto beans ca | ASIA | 2 |\r\n| lar deposits. blithely final packages cajole. regular waters are final requests. regular accounts are according to  | AFRICA | 0 |\r\n| ly final courts cajole furiously final excuse | EUROPE | 3 |\r\n| hs use ironic, even requests. s | AMERICA | 1 |\r\n| uickly special accounts cajole carefully blithely close requests. carefully final asymptotes haggle furiousl | MIDDLE EAST | 4 |\r\n+-----------+--------+-------------+\r\n5 rows selected (0.796 seconds)\r\n{code}\r\n\r\nThis does not happen when we do not use an \"order by\" in query\r\n",
        "Different column order could appear in the result set for a schema-less select * query, even there are no changing schemas. For a select * query referring to a schema-less table, Drill could return different column, depending on the physical operators the query involves:\r\n\r\nQ1:\r\n\r\n{code}\r\nselect * from cp.`employee.json` limit 3;\r\n+-------------+------------+------------+------------+-------------+----------------+------------+---------------+------------+------------+------------+---------------+-----------------+----------------+------------+-----------------+\r\n| employee_id | full_name  | first_name | last_name  | position_id | position_title |  store_id  | department_id | birth_date | hire_date  |   salary   | supervisor_id | education_level | marital_status |   gender   | management_role |\r\n+-------------+------------+------------+------------+-------------+----------------+------------+---------------+------------+------------+------------+---------------+-----------------+----------------+------------+-----------------+\r\n{code}\r\n\r\nQ2:\r\n\r\n{code}\r\nselect * from cp.`employee.json` order by last_name limit 3;\r\n+------------+---------------+-----------------+-------------+------------+------------+------------+------------+------------+-----------------+----------------+-------------+----------------+------------+------------+---------------+\r\n| birth_date | department_id | education_level | employee_id | first_name | full_name  |   gender   | hire_date  | last_name  | management_role | marital_status | position_id | position_title |   salary   |  store_id  | supervisor_id |\r\n+------------+---------------+-----------------+-------------+------------+------------+------------+------------+------------+-----------------+----------------+-------------+----------------+------------+------------+---------------+\r\n{code}\r\n\r\nThe difference between Q1 and Q2 is the order by clause.  With order by clause in Q2, Drill will sort the column names alphabetically, while for Q1, the column names are in the same order as in the data source. \r\n\r\nThe underlying cause for such difference is that the sort or sort-based merger operator would require canonicalization, since the incoming batches could contain different schemas. \r\n\r\n However, it would be better that such canonicalization is used only when the incoming batches have changing schemas. If all the incoming batches have identical schemas, no need to sort the column orders.  With this fix, Drill will present the same column order in the result set, for a schema-less select * query,  if there is no changing schemas from incoming data sources. \r\n\r\n\r\n\r\n"
    ],
    [
        "DRILL-3163",
        "DRILL-2699",
        "Fix hang/ leak issue exposed by TestDrillbitResilience#foreman_runTryEnd When the test is run multiple times, the IDE hangs sometimes and fails sometimes.",
        "Collect all cleanup errors before reporting a failure to the client If a query fails, the fragments and foreman should make sure to collect all failures and report them back to the client. Some known places where this isn't respected:\r\n\r\n- If a fragment fails, it will report the failure to the foreman before cleaning up. Any failure that happens in the cleanup process will be dropped by the foreman.\r\n- If multiple fragments fail, the Foreman will only report to the user the first failure it received and close immediately. All other failures will be dropped.\r\n"
    ],
    [
        "DRILL-3164",
        "DRILL-1491",
        "Compilation fails with Java 8 I just got this:\r\n{code}\r\nted:drill[1.0.0*]$ mvn package -DskipTests\r\n...\r\nDetected JDK Version: 1.8.0-40 is not in the allowed range [1.7,1.8).\r\n...\r\n{code}\r\nClearly there is an overly restrictive pattern at work.\r\n",
        "Support for JDK 8 This will be the umbrella JIRA used to track and fix issues with JDK 8 support."
    ],
    [
        "DRILL-3167",
        "DRILL-2699",
        "When a query fails, Foreman should wait for all fragments to finish cleaning up before sending a FAILED state to the client TestDrillbitResilience.foreman_runTryEnd() exposes this problem intermittently\r\n\r\nThe query fails and the Foreman reports the failure to the client which removes the results listener associated to the failed query. \r\nSometimes, a data batch reaches the client after the FAILED state already arrived, the client doesn't handle this properly and the corresponding buffer is never released.\r\n\r\nMaking the Foreman wait for all fragments to finish before sending the final state should help avoid such scenarios.",
        "Collect all cleanup errors before reporting a failure to the client If a query fails, the fragments and foreman should make sure to collect all failures and report them back to the client. Some known places where this isn't respected:\r\n\r\n- If a fragment fails, it will report the failure to the foreman before cleaning up. Any failure that happens in the cleanup process will be dropped by the foreman.\r\n- If multiple fragments fail, the Foreman will only report to the user the first failure it received and close immediately. All other failures will be dropped.\r\n"
    ],
    [
        "DRILL-3167",
        "DRILL-3163",
        "When a query fails, Foreman should wait for all fragments to finish cleaning up before sending a FAILED state to the client TestDrillbitResilience.foreman_runTryEnd() exposes this problem intermittently\r\n\r\nThe query fails and the Foreman reports the failure to the client which removes the results listener associated to the failed query. \r\nSometimes, a data batch reaches the client after the FAILED state already arrived, the client doesn't handle this properly and the corresponding buffer is never released.\r\n\r\nMaking the Foreman wait for all fragments to finish before sending the final state should help avoid such scenarios.",
        "Fix hang/ leak issue exposed by TestDrillbitResilience#foreman_runTryEnd When the test is run multiple times, the IDE hangs sometimes and fails sometimes."
    ],
    [
        "DRILL-3172",
        "DRILL-1863",
        "Can not plan exception when over clause is empty  With all currently supported window functions:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select c_integer, max(c_integer) over() from j7;\r\nError: SYSTEM ERROR: org.apache.calcite.plan.RelOptPlanner$CannotPlanException: Node [rel#4414:Subset#7.PHYSICAL.SINGLETON([]).[]] could not be implemented; planner state:\r\n\r\nRoot: rel#4414:Subset#7.PHYSICAL.SINGLETON([]).[]\r\nOriginal rel:\r\nAbstractConverter(subset=[rel#4414:Subset#7.PHYSICAL.SINGLETON([]).[]], convention=[PHYSICAL], DrillDistributionTraitDef=[SINGLETON([])], sort=[[]]): rowcount = 7680.0, cumulative cost = {inf}, id = 4416\r\n  DrillScreenRel(subset=[rel#4413:Subset#7.LOGICAL.ANY([]).[]]): rowcount = 7680.0, cumulative cost = {768.0 rows, 768.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 4412\r\n    DrillProjectRel(subset=[rel#4411:Subset#6.LOGICAL.ANY([]).[]], c_integer=[$0], EXPR$1=[$1]): rowcount = 7680.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 4410\r\n      DrillProjectRel(subset=[rel#4409:Subset#5.LOGICAL.ANY([]).[]], c_integer=[$1], $1=[$2]): rowcount = 7680.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 4408\r\n        DrillWindowRel(subset=[rel#4407:Subset#4.LOGICAL.ANY([]).[]], window#0=[window(partition {} order by [] range between UNBOUNDED PRECEDING and UNBOUNDED FOLLOWING aggs [MAX($1)])]): rowcount = 7680.0, cumulative cost = {7680.0 rows, 15360.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 4406\r\n          DrillScanRel(subset=[rel#4405:Subset#3.LOGICAL.ANY([]).[]], table=[[dfs, subqueries, j7]], groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:///drill/testdata/subqueries/j7]], selectionRoot=/drill/testdata/subqueries/j7, numFiles=1, columns=[`*`]]]): rowcount = 7680.0, cumulative cost = {7680.0 rows, 7.68E7 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 4395\r\n{code}",
        "query with empty over clause throws an exception Using an over clause without a \"partition by\" nor a \"order by\" clauses throws a _ForemanException_ (complete message attached)\r\n\r\nFor example the following query throws an exception:\r\n{code}\r\nSELECT employee_id, position_id, salary, SUM(salary) OVER() FROM cp.`employee.json`;\r\n{code}"
    ],
    [
        "DRILL-3174",
        "DRILL-2724",
        "Calcite blocks queries whose type-missmatch can be resolved by Drill's Implicit casting {code}\r\nselect a from ... union all select b from ...\r\n{code}\r\n\r\nwhere a is int, and b is a bunch of integers in varchar types. Drill-Calcite interrupts this query by the column types. Since Drill has its own ways of handling type-mismatch, can we let Calcite ignore type checking?\r\n\r\n",
        "Implicit cast test fails in Union All query (reports type mismatch) Test that performs implicit cast in a Union All query fails. Each CSV file has only numeric data. Test was performed on 4 node cluster.\r\n\r\n{code}\r\nWith casting, assuming implicit casting would work. Data in each of the csv files is numeric.\r\n\r\n0: jdbc:drill:> select * from (select cast(columns[0] as bigint) from `bgint_f.csv` union all select cast(columns[0] as char(2)) from `char_f.csv` union all select cast(columns[0] as double) from `dbl_f.csv` union all select cast(columns[0] as float) from `float_f.csv` union all select cast(columns[0] as int) from `int_f.csv` union all select cast(columns[0] as varchar(100)) from `vchar_f.csv`) order by EXPR$0;\r\nQuery failed: SqlValidatorException: Type mismatch in column 1 of UNION ALL\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nexplain plan did not return the physical plan\r\n\r\n0: jdbc:drill:> explain plan for select * from (select cast(columns[0] as bigint) from `bgint_f.csv` union all select cast(columns[0] as char(2)) from `char_f.csv` union all select cast(columns[0] as double) from `dbl_f.csv` union all select cast(columns[0] as float) from `float_f.csv` union all select cast(columns[0] as int) from `int_f.csv` union all select cast(columns[0] as varchar(100)) from `vchar_f.csv`) order by EXPR$0;\r\nQuery failed: SqlValidatorException: Type mismatch in column 1 of UNION ALL\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nStack trace from drillbit.log\r\n\r\n2015-04-08 21:30:44,553 [2ada61fa-8207-279a-97fd-b40631cdb151:foreman] ERROR o.a.drill.exec.work.foreman.Foreman - Error 00ab460b-d2be-441e-8baa-0a8ff474769e: SqlValidatorException: Type mismatch in column 1 of UNION ALL\r\norg.apache.drill.exec.planner.sql.QueryInputException: Failure validating SQL. org.eigenbase.util.EigenbaseContextException: From line 1, column 23 to line 1, column 48: Type mismatch in column 1 of UNION ALL\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:147) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:773) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:204) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]\r\nCaused by: net.hydromatic.optiq.tools.ValidationException: org.eigenbase.util.EigenbaseContextException: From line 1, column 23 to line 1, column 48: Type mismatch in column 1 of UNION ALL\r\n        at net.hydromatic.optiq.prepare.PlannerImpl.validate(PlannerImpl.java:176) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateNode(DefaultSqlHandler.java:157) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:133) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        ... 5 common frames omitted\r\nCaused by: org.eigenbase.util.EigenbaseContextException: From line 1, column 23 to line 1, column 48: Type mismatch in column 1 of UNION ALL\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.7.0_75]\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[na:1.7.0_75]\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.7.0_75]\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526) ~[na:1.7.0_75]\r\n        at org.eigenbase.resource.Resources$ExInstWithCause.ex(Resources.java:348) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.SqlUtil.newContextException(SqlUtil.java:673) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.SqlUtil.newContextException(SqlUtil.java:661) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:3588) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.type.SetopOperandTypeChecker.checkOperandTypes(SetopOperandTypeChecker.java:100) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.SqlOperator.checkOperandTypes(SqlOperator.java:533) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.SqlOperator.validateOperands(SqlOperator.java:412) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SetopNamespace.validateImpl(SetopNamespace.java:70) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:85) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:785) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:774) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SetopNamespace.validateImpl(SetopNamespace.java:68) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:85) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:785) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:774) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SetopNamespace.validateImpl(SetopNamespace.java:68) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:85) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:785) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:774) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SetopNamespace.validateImpl(SetopNamespace.java:68) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:85) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:785) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:774) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SetopNamespace.validateImpl(SetopNamespace.java:68) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:85) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:785) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:774) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:2605) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:2590) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:2813) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:85) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:785) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:774) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.SqlSelect.validate(SqlSelect.java:211) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:748) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:464) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at net.hydromatic.optiq.prepare.PlannerImpl.validate(PlannerImpl.java:174) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        ... 8 common frames omitted\r\nCaused by: org.eigenbase.sql.validate.SqlValidatorException: Type mismatch in column 1 of UNION ALL\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.7.0_75]\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[na:1.7.0_75]\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.7.0_75]\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526) ~[na:1.7.0_75]\r\n        at org.eigenbase.resource.Resources$ExInstWithCause.ex(Resources.java:348) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        at org.eigenbase.resource.Resources$ExInst.ex(Resources.java:457) ~[optiq-core-0.9-drill-r20.jar:na]\r\n        ... 45 common frames omitted\r\n\r\nThis is how my data looks like in each of the CSV files\r\n\r\n[root@centos-01 csv_dir]# cat bgint_f.csv \r\n1\r\n2\r\n0\r\n-1\r\n1000000\r\n65535\r\n100\r\n13\r\n19\r\n17\r\n111111\r\n1010101\r\n9999999\r\n[root@centos-01 csv_dir]# cat int_f.csv \r\n1\r\n0\r\n-1\r\n65535\r\n1234567\r\n1000000\r\n101010\r\n11111\r\n100\r\n13\r\n19\r\n17\r\n[root@centos-01 csv_dir]# cat dbl_f.csv \r\n123.45\r\n11.98\r\n12345.39\r\n1.1\r\n1.0\r\n0.0\r\n-1.0\r\n11111.99\r\n99999.99\r\n[root@centos-01 csv_dir]# cat float_f.csv \r\n1.1\r\n1.234\r\n1234.19\r\n13.19\r\n1.0\r\n-1.0\r\n0.0\r\n1111.98\r\n9999.99\r\n[root@centos-01 csv_dir]# cat char_f.csv \r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n0\r\n[root@centos-01 csv_dir]# cat vchar_f.csv\r\n12345\r\n1\r\n0\r\n-1\r\n200000\r\n1000000\r\n65535\r\n13\r\n19\r\n17\r\n11111\r\n10101\r\n{code}"
    ],
    [
        "DRILL-3175",
        "DRILL-1862",
        "Missing partition by key in OVER clause causes RelOptPlanner$CannotPlanException Since the two inputs to OVER clause are optional. Test that uses only order by clause with order by column results in CannotPlanException\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select count(columns[3]) over ( order by columns[13]) from `airports.csv`;\r\nError: SYSTEM ERROR: org.apache.calcite.plan.RelOptPlanner$CannotPlanException: Node [rel#6046:Subset#10.PHYSICAL.SINGLETON([]).[]] could not be implemented; planner state:\r\n\r\nRoot: rel#6046:Subset#10.PHYSICAL.SINGLETON([]).[]\r\nOriginal rel:\r\nAbstractConverter(subset=[rel#6046:Subset#10.PHYSICAL.SINGLETON([]).[]], convention=[PHYSICAL], DrillDistributionTraitDef=[SINGLETON([])], sort=[[]]): rowcount = 69890.0, cumulative cost = {inf}, id = 6048\r\n  DrillScreenRel(subset=[rel#6045:Subset#10.LOGICAL.ANY([]).[]]): rowcount = 69890.0, cumulative cost = {6989.0 rows, 6989.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 6044\r\n    DrillProjectRel(subset=[rel#6043:Subset#9.LOGICAL.ANY([]).[]], EXPR$0=[$0]): rowcount = 69890.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 6042\r\n      DrillProjectRel(subset=[rel#6041:Subset#8.LOGICAL.ANY([]).[]], $0=[$2]): rowcount = 69890.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 6040\r\n        DrillWindowRel(subset=[rel#6039:Subset#7.LOGICAL.ANY([]).[]], window#0=[window(partition {} order by [1] range between UNBOUNDED PRECEDING and CURRENT ROW aggs [COUNT($0)])]): rowcount = 69890.0, cumulative cost = {69890.0 rows, 139780.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 6038\r\n          DrillProjectRel(subset=[rel#6037:Subset#6.LOGICAL.ANY([]).[]], $0=[ITEM($0, 3)], $1=[ITEM($0, 13)]): rowcount = 69890.0, cumulative cost = {69890.0 rows, 559120.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 6036\r\n            DrillScanRel(subset=[rel#6035:Subset#5.LOGICAL.ANY([]).[]], table=[[dfs, tmp, airports.csv]], groupscan=[EasyGroupScan [selectionRoot=/tmp/airports.csv, numFiles=1, columns=[`columns`[3], `columns`[13]], files=[maprfs:///tmp/airports.csv]]]): rowcount = 69890.0, cumulative cost = {69890.0 rows, 69890.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 6014\r\n\r\n{code}",
        "over clause with only order by clause throws an exception When using over clause without a \"partition by\" clause but with an \"order by\" clause, an _ForemanException_ is thrown (complete error message attached).\r\n\r\nFor example the following query throws and exception:\r\n{code}\r\nSELECT employee_id, position_id, salary, SUM(salary) OVER(ORDER BY salary) FROM cp.`employee.json` LIMIT 20;\r\n{code}"
    ],
    [
        "DRILL-3217",
        "DRILL-3208",
        "System error not being propogated to sqlline in multi-node cluster git.commit.id.abbrev=6f54223\r\n\r\nI have a hive table on top of a parquet file. When we have a type mis-macth for any column in the hive ddl, the queries from drill fail. However there seems to be an issue with error propogation\r\n\r\nIn embedded mode the error seems to be propogated as expected\r\n{code}\r\n0: jdbc:drill:zk=local> select * from customer;\r\nError: SYSTEM ERROR: java.lang.ClassCastException: org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable cannot be cast to org.apache.hadoop.io.IntWritable\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 459c71ea-c66e-459b-9d66-8408b4bf0954 on qa-node190.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nIn a multi-node setup, there is not message on sqlline\r\n{code}\r\n0: jdbc:drill:schema=dfs_eea> select count(*) from customer;\r\nError: SYSTEM ERROR: \r\n\r\nFragment 0:0\r\n\r\n[Error Id: e5ac4048-73fa-441a-b6a7-fb1e25838d04 on qa-node191.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nLog files in a multi-node setup also do not contain the message. I attached the logs and embedded and multi-node setup along with the data. Below is hive ddl (wrong type for phone)\r\n{code}\r\ncreate external table if not exists tpch01_parquet_nodate.customer (\r\n    c_custkey int,\r\n    c_name string,\r\n    c_address string,\r\n    c_nationkey int,\r\n    c_phone int,\r\n    c_acctbal double,\r\n    c_mktsegment string,\r\n    c_comment string\r\n)\r\nSTORED AS PARQUET\r\nLOCATION '/tpch/customer/customer.parquet';\r\n{code}\r\n\r\nLet me know if you need anything",
        "Hive : Tpch (SF 0.01) query 10 fails with a system error when the data is backed by hive tables git.commit.id.abbrev=6f54223\r\n\r\nI created hive tables on top of tpch parquet data. (Attached the hive ddl script). Since hive does not support date in parquet serde, I regenerated the parquet files for orders and lineitem to use string for the date fields. Remaining files do not have a date column.\r\n\r\nWhen I executed query 10 in the tpch suite, it failed with a system error.\r\n{code}\r\n0: jdbc:drill:schema=dfs_eea> use hive.tpch01_parquet_nodate;\r\n+-------+---------------------------------------------------------+\r\n|  ok   |                         summary                         |\r\n+-------+---------------------------------------------------------+\r\n| true  | Default schema changed to [hive.tpch01_parquet_nodate]  |\r\n+-------+---------------------------------------------------------+\r\n1 row selected (0.091 seconds)\r\n0: jdbc:drill:schema=dfs_eea>\r\nselect\r\n  c.c_custkey,\r\n  c.c_name,\r\n  sum(l.l_extendedprice * (1 - l.l_discount)) as revenue,\r\n  c.c_acctbal,\r\n  n.n_name,\r\n  c.c_address,\r\n  c.c_phone,\r\n  c.c_comment\r\nfrom\r\n  customer c,\r\n  orders o,\r\n  lineitem l,\r\n  nation n\r\nwhere\r\n  c.c_custkey = o.o_custkey\r\n  and l.l_orderkey = o.o_orderkey\r\n  and cast(o.o_orderdate as date) >= date '1994-03-01'\r\n  and cast(o.o_orderdate as date) < date '1994-03-01' + interval '3' month\r\n  and l.l_returnflag = 'R'\r\n  and c.c_nationkey = n.n_nationkey\r\ngroup by\r\n  c.c_custkey,\r\n  c.c_name,\r\n  c.c_acctbal,\r\n  c.c_phone,\r\n  n.n_name,\r\n  c.c_address,\r\n  c.c_comment\r\norder by\r\n  revenue desc\r\nlimit 20;\r\n\r\nError: SYSTEM ERROR: \r\n\r\nFragment 0:0\r\n\r\n[Error Id: 1d327ae0-1cf2-4776-acd3-8eef6cca4b6a on qa-node191.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nI tried running the above query using dfs instead of hive and it worked as expected.\r\n\r\nI attached the newly generated parquet files and the hive ddl for creating hive tables. Let me know if you need anything\r\n"
    ],
    [
        "DRILL-3224",
        "DRILL-3205",
        "DrillTestWrapper test-failure message doesn't show actual values For at least one of the test-failure (validation) error messages from DrillTestWrapper, the message does not include the actual values.\r\n\r\nNote how the message below does tell you what was expected (redundant with what's in the test source code, but still convenient), but doesn't tell you what the test actually got (not available elsewhere (before/without debugging)).\r\n\r\n\r\n{noformat}\r\njava.lang.Exception: Did not find expected record in result set: `CHARACTER_MAXIMUM_LENGTH` : -1, `COLUMN_NAME` : inttype, `NUMERIC_SCALE` : -1, `DATA_TYPE` : INTEGER, `NUMERIC_PRECISION` : -1, \r\n\r\n\tat org.apache.drill.DrillTestWrapper.compareResults(DrillTestWrapper.java:541)\r\n\tat org.apache.drill.DrillTestWrapper.compareUnorderedResults(DrillTestWrapper.java:295)\r\n\tat org.apache.drill.DrillTestWrapper.run(DrillTestWrapper.java:119)\r\n\tat org.apache.drill.TestBuilder.go(TestBuilder.java:125)\r\n\tat org.apache.drill.exec.hive.TestInfoSchemaOnHiveStorage.varCharMaxLengthAndDecimalPrecisionInInfoSchema(TestInfoSchemaOnHiveStorage.java:94)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n{noformat}\r\n\r\n",
        "Test framework should report actual values in case of failures Test framework seems to report expected results alone. This makes it particularly hard to investigate and reason about the problem. Even in case where test result matcher is un-ordered we can report actual results if returned number of rows & columns is manageable (say r x c \u2264 100)\r\n\r\n{code:title=Sample test failure that reports expected result but not the actual value}\r\nTests run: 43, Failures: 0, Errors: 1, Skipped: 3, Time elapsed: 30.168 sec <<< FAILURE! - in org.apache.drill.TestFunctionsQuery\r\ntestToCharFunction(org.apache.drill.TestFunctionsQuery)  Time elapsed: 0.112 sec  <<< ERROR!\r\njava.lang.Exception: Did not find expected record in result set: `DEC28_1` : 12,345,678,912,345,678,912.5567, `DEC38_1` : 999999999999999999999999999.5, `DEC9_1` : 1,234.56, `DEC18_1` : 99999912399.9567, `FLOAT8_1` : 1,234.56, `FLOAT8_2` : $1,234.50, \r\n\r\n\tat org.apache.drill.DrillTestWrapper.compareResults(DrillTestWrapper.java:541)\r\n\tat org.apache.drill.DrillTestWrapper.compareUnorderedResults(DrillTestWrapper.java:295)\r\n\tat org.apache.drill.DrillTestWrapper.run(DrillTestWrapper.java:119)\r\n\tat org.apache.drill.TestBuilder.go(TestBuilder.java:125)\r\n\tat org.apache.drill.TestFunctionsQuery.testToCharFunction(TestFunctionsQuery.java:517)\r\n{code}"
    ],
    [
        "DRILL-3247",
        "DRILL-3218",
        "Query without casting results in CompileException Need a better error message, currently we report CompileException, when columns are not casted to correct datatypes.\r\n\r\nPlease see DRILL-3218 for more details.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> SELECT MAX(columns[0]) OVER (PARTITION BY columns[6] ORDER BY columns[4]) FROM `allTypData2.csv`;\r\nError: SYSTEM ERROR: org.codehaus.commons.compiler.CompileException: Line 330, Column 31: Unknown variable or type \"incoming\"\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 285af8f1-ddb4-4d3e-a2d7-bfaef20df5e0 on centos-02.qa.lab:31010] (state=,code=0)\r\n{code}",
        "Window function usage throws CompileException \r\nPARTITION BY date ORDER BY timestamp\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> SELECT MAX(columns[0]) OVER (PARTITION BY columns[6] ORDER BY columns[4]) FROM `allTypData2.csv`;\r\nError: SYSTEM ERROR: org.codehaus.commons.compiler.CompileException: Line 330, Column 31: Unknown variable or type \"incoming\"\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 285af8f1-ddb4-4d3e-a2d7-bfaef20df5e0 on centos-02.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nI will add more details in a bit."
    ],
    [
        "DRILL-3263",
        "DRILL-3234",
        "Read smallint and tinyint data from hive as integer until these types are well supported throughout Drill ",
        "Drill fails to implicit cast hive tinyint and smallint data as int I have the following hive table:\r\n describe `hive.default`.voter_hive;\r\n+----------------+------------+--------------+\r\n|  COLUMN_NAME   | DATA_TYPE  | IS_NULLABLE  |\r\n+----------------+------------+--------------+\r\n| voter_id       | SMALLINT   | YES          |\r\n| name           | VARCHAR    | YES          |\r\n| age            | TINYINT    | YES          |\r\n| registration   | VARCHAR    | YES          |\r\n| contributions  | DECIMAL    | YES          |\r\n| voterzone      | INTEGER    | YES          |\r\n| create_time    | TIMESTAMP  | YES          |\r\n+----------------+------------+--------------+\r\n\r\nIf just include the voter_id and age fields in the select, then the query works fine.  However if I include them in the where clause, the query would fail. For example:\r\n\r\nselect voter_id, name, age from voter_hive where age < 30;\r\nError: SYSTEM ERROR: org.apache.drill.exec.exception.SchemaChangeException: Failure while trying to materialize incoming schema.  Errors:\r\n \r\nError in expression at index -1.  Error: Missing function implementation: [castINT(TINYINT-OPTIONAL)].  Full expression: --UNKNOWN EXPRESSION--..\r\n"
    ],
    [
        "DRILL-3287",
        "DRILL-3122",
        "Changing session level parameter back to the default value does not change it's status back to DEFAULT Initial state:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from sys.options where status like '%CHANGED%';\r\n+-----------------------------------+----------+---------+----------+----------+-------------+-----------+------------+\r\n|               name                |   kind   |  type   |  status  | num_val  | string_val  | bool_val  | float_val  |\r\n+-----------------------------------+----------+---------+----------+----------+-------------+-----------+------------+\r\n| planner.enable_decimal_data_type  | BOOLEAN  | SYSTEM  | CHANGED  | null     | null        | true      | null       |\r\n+-----------------------------------+----------+---------+----------+----------+-------------+-----------+------------+\r\n1 row selected (0.247 seconds)\r\n{code}\r\n\r\nI changed session parameter:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> alter session set `planner.enable_hashjoin` = false;\r\n+-------+-----------------------------------+\r\n|  ok   |              summary              |\r\n+-------+-----------------------------------+\r\n| true  | planner.enable_hashjoin updated.  |\r\n+-------+-----------------------------------+\r\n1 row selected (0.1 seconds)\r\n{code}\r\n\r\nSo far, so good: it appears on changed options list: \r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from sys.options where status like '%CHANGED%';\r\n+-----------------------------------+----------+----------+----------+----------+-------------+-----------+------------+\r\n|               name                |   kind   |   type   |  status  | num_val  | string_val  | bool_val  | float_val  |\r\n+-----------------------------------+----------+----------+----------+----------+-------------+-----------+------------+\r\n| planner.enable_decimal_data_type  | BOOLEAN  | SYSTEM   | CHANGED  | null     | null        | true      | null       |\r\n| planner.enable_hashjoin           | BOOLEAN  | SESSION  | CHANGED  | null     | null        | false     | null       |\r\n+-----------------------------------+----------+----------+----------+----------+-------------+-----------+------------+\r\n2 rows selected (0.133 seconds)\r\n{code}\r\n\r\nI changed session parameter back to it's default value:\r\n{code}\r\n0: jdbc:drill:schema=dfs> alter session set `planner.enable_hashjoin` = true;\r\n+-------+-----------------------------------+\r\n|  ok   |              summary              |\r\n+-------+-----------------------------------+\r\n| true  | planner.enable_hashjoin updated.  |\r\n+-------+-----------------------------------+\r\n1 row selected (0.096 seconds)\r\n{code}\r\n\r\n{color:red} It still appears on changed list, even though it has default value:{color}\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from sys.options where status like '%CHANGED%';\r\n+-----------------------------------+----------+----------+----------+----------+-------------+-----------+------------+\r\n|               name                |   kind   |   type   |  status  | num_val  | string_val  | bool_val  | float_val  |\r\n+-----------------------------------+----------+----------+----------+----------+-------------+-----------+------------+\r\n| planner.enable_decimal_data_type  | BOOLEAN  | SYSTEM   | CHANGED  | null     | null        | true      | null       |\r\n| planner.enable_hashjoin           | BOOLEAN  | SESSION  | CHANGED  | null     | null        | true      | null       |\r\n+-----------------------------------+----------+----------+----------+----------+-------------+-----------+------------+\r\n2 rows selected (0.124 seconds)\r\n{code}",
        "Changing a session option to default value results in status as changed Alter session option hash join to true(which is the default) and the following query shows that the option has changed which could be misleading to users relying on the status field to see if an option has changed or not. Especially in the case of a boolean value. \r\n{code}\r\n0: jdbc:drill:zk=10.10.100.171:5181> select * from sys.options where name like '%hash%';\r\n+--------------------------------------------+----------+---------+----------+-------------+-------------+-----------+------------+\r\n|                    name                    |   kind   |  type   |  status  |   num_val   | string_val  | bool_val  | float_val  |\r\n+--------------------------------------------+----------+---------+----------+-------------+-------------+-----------+------------+\r\n| exec.max_hash_table_size                   | LONG     | SYSTEM  | DEFAULT  | 1073741824  | null        | null      | null       |\r\n| exec.min_hash_table_size                   | LONG     | SYSTEM  | DEFAULT  | 65536       | null        | null      | null       |\r\n| planner.enable_hash_single_key             | BOOLEAN  | SYSTEM  | DEFAULT  | null        | null        | true      | null       |\r\n| planner.enable_hashagg                     | BOOLEAN  | SYSTEM  | DEFAULT  | null        | null        | true      | null       |\r\n| planner.enable_hashjoin                    | BOOLEAN  | SYSTEM  | DEFAULT  | null        | null        | true      | null       |\r\n| planner.enable_hashjoin_swap               | BOOLEAN  | SYSTEM  | DEFAULT  | null        | null        | true      | null       |\r\n| planner.join.hash_join_swap_margin_factor  | DOUBLE   | SYSTEM  | DEFAULT  | null        | null        | null      | 10.0       |\r\n| planner.memory.hash_agg_table_factor       | DOUBLE   | SYSTEM  | DEFAULT  | null        | null        | null      | 1.1        |\r\n| planner.memory.hash_join_table_factor      | DOUBLE   | SYSTEM  | DEFAULT  | null        | null        | null      | 1.1        |\r\n+--------------------------------------------+----------+---------+----------+-------------+-------------+-----------+------------+\r\n9 rows selected (0.191 seconds)\r\n0: jdbc:drill:zk=10.10.100.171:5181> alter session set `planner.enable_hashjoin`=true;\r\n+-------+-----------------------------------+\r\n|  ok   |              summary              |\r\n+-------+-----------------------------------+\r\n| true  | planner.enable_hashjoin updated.  |\r\n+-------+-----------------------------------+\r\n1 row selected (0.083 seconds)\r\n0: jdbc:drill:zk=10.10.100.171:5181> select * from sys.options where name like '%hash%';\r\n+--------------------------------------------+----------+----------+----------+-------------+-------------+-----------+------------+\r\n|                    name                    |   kind   |   type   |  status  |   num_val   | string_val  | bool_val  | float_val  |\r\n+--------------------------------------------+----------+----------+----------+-------------+-------------+-----------+------------+\r\n| exec.max_hash_table_size                   | LONG     | SYSTEM   | DEFAULT  | 1073741824  | null        | null      | null       |\r\n| exec.min_hash_table_size                   | LONG     | SYSTEM   | DEFAULT  | 65536       | null        | null      | null       |\r\n| planner.enable_hash_single_key             | BOOLEAN  | SYSTEM   | DEFAULT  | null        | null        | true      | null       |\r\n| planner.enable_hashagg                     | BOOLEAN  | SYSTEM   | DEFAULT  | null        | null        | true      | null       |\r\n| planner.enable_hashjoin                    | BOOLEAN  | SYSTEM   | DEFAULT  | null        | null        | true      | null       |\r\n| planner.enable_hashjoin                    | BOOLEAN  | SESSION  | CHANGED  | null        | null        | true      | null       |\r\n| planner.enable_hashjoin_swap               | BOOLEAN  | SYSTEM   | DEFAULT  | null        | null        | true      | null       |\r\n| planner.join.hash_join_swap_margin_factor  | DOUBLE   | SYSTEM   | DEFAULT  | null        | null        | null      | 10.0       |\r\n| planner.memory.hash_agg_table_factor       | DOUBLE   | SYSTEM   | DEFAULT  | null        | null        | null      | 1.1        |\r\n| planner.memory.hash_join_table_factor      | DOUBLE   | SYSTEM   | DEFAULT  | null        | null        | null      | 1.1        |\r\n+--------------------------------------------+----------+----------+----------+-------------+-------------+-----------+------------+\r\n{code}"
    ],
    [
        "DRILL-3303",
        "DRILL-1491",
        "Build doesn't support recent version of Java I am unable to build drill. I get the following error:\r\nFailed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.3.1:enforce (validate_java_and_maven_version) on project drill-root: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -> [Help 1]\r\n\r\nIt looks like my java version is too recent. Do I need to install an old version or should this be updated in the build configuration? \r\n[WARNING] Rule 1: org.apache.maven.plugins.enforcer.RequireJavaVersion failed with message:\r\nDetected JDK Version: 1.8.0-45 is not in the allowed range [1.7,1.8).\r\n\r\nI followed these steps:\r\ngit clone https://github.com/apache/drill.git\r\nmvn clean install",
        "Support for JDK 8 This will be the umbrella JIRA used to track and fix issues with JDK 8 support."
    ],
    [
        "DRILL-3303",
        "DRILL-3164",
        "Build doesn't support recent version of Java I am unable to build drill. I get the following error:\r\nFailed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.3.1:enforce (validate_java_and_maven_version) on project drill-root: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -> [Help 1]\r\n\r\nIt looks like my java version is too recent. Do I need to install an old version or should this be updated in the build configuration? \r\n[WARNING] Rule 1: org.apache.maven.plugins.enforcer.RequireJavaVersion failed with message:\r\nDetected JDK Version: 1.8.0-45 is not in the allowed range [1.7,1.8).\r\n\r\nI followed these steps:\r\ngit clone https://github.com/apache/drill.git\r\nmvn clean install",
        "Compilation fails with Java 8 I just got this:\r\n{code}\r\nted:drill[1.0.0*]$ mvn package -DskipTests\r\n...\r\nDetected JDK Version: 1.8.0-40 is not in the allowed range [1.7,1.8).\r\n...\r\n{code}\r\nClearly there is an overly restrictive pattern at work.\r\n"
    ],
    [
        "DRILL-3338",
        "DRILL-3279",
        "Error message must be modified when PERCENT_RANK window functions are used without ORDER BY  The following message is thrown when a query containing PERCENT_RANK window function fails:\r\n\r\n{code:sql}\r\nSELECT PERCENT_RANK() OVER (PARTITION BY ss.ss_store_sk) FROM store_sales ss LIMIT 20;\r\nError: PARSE ERROR: From line 1, column 28 to line 1, column 56: RANK or DENSE_RANK functions require ORDER BY clause in window specification\r\n{code}\r\n\r\nThe error message must be modified to include PERCENT_RANK function",
        "Window functions without ORDER BY - Error message needs to be fixed We need to add these two window function names CUME_DIST and PERCENT_RANK to the error message, when user issues window function query without an order by in window definition.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select cume_dist() over (partition by col_chr) from `allDataInPrq/0_0_0.parquet`;\r\nError: PARSE ERROR: From line 1, column 25 to line 1, column 46: RANK or DENSE_RANK functions require ORDER BY clause in window specification\r\n\r\n\r\n[Error Id: 330100e4-d90e-43db-8893-8e9ad3783874 on centos-03.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select row_number() over (partition by col_chr) from `allDataInPrq/0_0_0.parquet`;\r\nError: PARSE ERROR: From line 1, column 26 to line 1, column 47: RANK or DENSE_RANK functions require ORDER BY clause in window specification\r\n\r\n\r\n[Error Id: 4a95813f-592c-45e4-a74c-1cd34a9067c9 on centos-03.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select rank() over (partition by col_chr) from `allDataInPrq/0_0_0.parquet`;\r\nError: PARSE ERROR: From line 1, column 20 to line 1, column 41: RANK or DENSE_RANK functions require ORDER BY clause in window specification\r\n\r\n\r\n[Error Id: 19bbd577-b653-4ebb-8f62-b9aee2bd6be5 on centos-03.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select dense_rank() over (partition by col_chr) from `allDataInPrq/0_0_0.parquet`;\r\nError: PARSE ERROR: From line 1, column 26 to line 1, column 47: RANK or DENSE_RANK functions require ORDER BY clause in window specification\r\n\r\n\r\n[Error Id: f9a438db-097c-441e-94a4-1d07835fcca7 on centos-03.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select percent_rank() over (partition by col_chr) from `allDataInPrq/0_0_0.parquet`;\r\nError: PARSE ERROR: From line 1, column 28 to line 1, column 49: RANK or DENSE_RANK functions require ORDER BY clause in window specification\r\n\r\n\r\n[Error Id: 897f32ea-4da5-4b1d-8864-f504bfe5ab6f on centos-03.qa.lab:31010] (state=,code=0)\r\n{code}"
    ],
    [
        "DRILL-3344",
        "DRILL-3211",
        "When Group By clause is present, the argument in window function should not refer to any column outside Group By CTAS\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> create table tblForView(col_int, col_bigint, col_char_2, col_vchar_52, col_tmstmp, col_dt, col_booln, col_dbl, col_tm) as select cast(columns[0] as INT), cast(columns[1] as BIGINT),cast(columns[2] as CHAR(2)), cast(columns[3] as VARCHAR(52)), cast(columns[4] as TIMESTAMP), cast(columns[5] as DATE), cast(columns[6] as BOOLEAN),cast(columns[7] as DOUBLE),cast(columns[8] as TIME) from `forPrqView.csv`;\r\n+-----------+----------------------------+\r\n| Fragment  | Number of records written  |\r\n+-----------+----------------------------+\r\n| 0_0       | 30                         |\r\n+-----------+----------------------------+\r\n1 row selected (0.586 seconds)\r\n{code}\r\n\r\nFailing query\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select max(col_tm) over(), col_char_2 from tblForView group by col_char_2;\r\nError: SYSTEM ERROR: java.lang.AssertionError: Internal error: while converting MAX(`tblForView`.`col_tm`)\r\n\r\n\r\n[Error Id: 11afbdc9-d47a-4a52-aa77-40c20ffd2bc6 on centos-03.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nStack trace\r\n\r\n{code}\r\n[Error Id: 11afbdc9-d47a-4a52-aa77-40c20ffd2bc6 on centos-03.qa.lab:31010]\r\norg.apache.drill.common.exceptions.UserException: SYSTEM ERROR: java.lang.AssertionError: Internal error: while converting MAX(`tblForView`.`col_tm`)\r\n\r\n\r\n[Error Id: 11afbdc9-d47a-4a52-aa77-40c20ffd2bc6 on centos-03.qa.lab:31010]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:522) ~[drill-common-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$ForemanResult.close(Foreman.java:738) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:840) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:782) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.common.EventProcessor.sendEvent(EventProcessor.java:73) [drill-common-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.moveToState(Foreman.java:784) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.moveToState(Foreman.java:893) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:253) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\nCaused by: org.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: Internal error: while converting MAX(`tblForView`.`col_tm`)\r\n        ... 4 common frames omitted\r\nCaused by: java.lang.AssertionError: Internal error: while converting MAX(`tblForView`.`col_tm`)\r\n        at org.apache.calcite.util.Util.newInternal(Util.java:790) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.ReflectiveConvertletTable$2.convertCall(ReflectiveConvertletTable.java:152) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:60) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertOver(SqlToRelConverter.java:1762) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.access$1000(SqlToRelConverter.java:180) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:3938) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.createAggImpl(SqlToRelConverter.java:2521) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertAgg(SqlToRelConverter.java:2342) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:604) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:564) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:2741) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:522) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.prepare.PlannerImpl.convert(PlannerImpl.java:198) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToRel(DefaultSqlHandler.java:246) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:182) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:178) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:904) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:242) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        ... 3 common frames omitted\r\nCaused by: java.lang.reflect.InvocationTargetException: null\r\n        at sun.reflect.GeneratedMethodAccessor129.invoke(Unknown Source) ~[na:na]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_45]\r\n        at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_45]\r\n        at org.apache.calcite.sql2rel.ReflectiveConvertletTable$2.convertCall(ReflectiveConvertletTable.java:142) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        ... 19 common frames omitted\r\nCaused by: java.lang.AssertionError: null\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.getRootField(SqlToRelConverter.java:3811) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.adjustInputRef(SqlToRelConverter.java:3139) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:3114) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.access$1400(SqlToRelConverter.java:180) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4062) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:3490) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:274) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:3945) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.StandardConvertletTable.convertExpressionList(StandardConvertletTable.java:833) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.StandardConvertletTable.convertAggregateFunction(StandardConvertletTable.java:706) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        ... 23 common frames omitted\r\n{code}",
        "Assert in a query with window function and group by clause  {code}\r\n0: jdbc:drill:schema=dfs> select sum(a1) over (partition by b1)  from t1 group by b1;\r\nError: SYSTEM ERROR: java.lang.AssertionError: Internal error: while converting SUM(`t1`.`a1`)\r\n[Error Id: 21872cfa-6f09-4e92-aee6-5dd8698cf9e7 on atsqa4-133.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\ndrillbit.log\r\n{code}\r\nCaused by: java.lang.AssertionError: Internal error: while converting SUM(`t1`.`a1`)\r\n        at org.apache.calcite.util.Util.newInternal(Util.java:790) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.ReflectiveConvertletTable$2.convertCall(ReflectiveConvertletTable.java:152) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:60) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertOver(SqlToRelConverter.java:1762) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.access$1000(SqlToRelConverter.java:180) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:3937) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.createAggImpl(SqlToRelConverter.java:2521) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertAgg(SqlToRelConverter.java:2342) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:604) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:564) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:2741) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:522) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.prepare.PlannerImpl.convert(PlannerImpl.java:198) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToRel(DefaultSqlHandler.java:246) ~[drill-java-exec-1.0.0-mapr-r1-rebuffed.jar:1.0.0-mapr-r1]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:182) ~[drill-java-exec-1.0.0-mapr-r1-rebuffed.jar:1.0.0-mapr-r1]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:177) ~[drill-java-exec-1.0.0-mapr-r1-rebuffed.jar:1.0.0-mapr-r1]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:902) [drill-java-exec-1.0.0-mapr-r1-rebuffed.jar:1.0.0-mapr-r1]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:240) [drill-java-exec-1.0.0-mapr-r1-rebuffed.jar:1.0.0-mapr-r1]\r\n        ... 3 common frames omitted\r\nCaused by: java.lang.reflect.InvocationTargetException: null\r\n        at sun.reflect.GeneratedMethodAccessor120.invoke(Unknown Source) ~[na:na]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_71]\r\n        at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_71]\r\n        at org.apache.calcite.sql2rel.ReflectiveConvertletTable$2.convertCall(ReflectiveConvertletTable.java:142) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        ... 19 common frames omitted\r\nCaused by: java.lang.AssertionError: null\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.getRootField(SqlToRelConverter.java:3810) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.adjustInputRef(SqlToRelConverter.java:3139) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:3114) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.access$1400(SqlToRelConverter.java:180) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4061) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:3489) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:274) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:3944) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.StandardConvertletTable.convertExpressionList(StandardConvertletTable.java:833) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.StandardConvertletTable.convertAggregateFunction(StandardConvertletTable.java:706) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        ... 23 common frames omitted\r\n{code}"
    ],
    [
        "DRILL-3351",
        "DRILL-3211",
        "Invalid query must be caught earlier The below query is not valid and we must report an error instead of returning results. Postgres doe not support this kind of a query.\r\n\r\nDrill returns some results, we must instead report an error to user.\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> SELECT MIN(col_int) OVER() FROM vwOnParq group by col_char_2;\r\n+---------+\r\n| EXPR$0  |\r\n+---------+\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n+---------+\r\n18 rows selected (0.27 seconds)\r\n{code}\r\n\r\nOutput from Postgres\r\n{code}\r\npostgres=# select min(col_int) over() from all_typs_tbl group by col_char_2;\r\nERROR:  column \"all_typs_tbl.col_int\" must appear in the GROUP BY clause or be used in an aggregate function\r\nLINE 1: select min(col_int) over() from all_typs_tbl group by col_ch...\r\n{code}\r\n\r\nQuerying the original parquet file that was used to create the view, returns an assertion error\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> SELECT MIN(col_int) OVER() FROM `tblForView/0_0_0.parquet` group by col_char_2;\r\nError: SYSTEM ERROR: java.lang.AssertionError: Internal error: while converting MIN(`tblForView/0_0_0.parquet`.`col_int`)\r\n\r\n\r\n[Error Id: e8ed279d-aa8c-4db1-9906-5dd7fdecaac2 on centos-02.qa.lab:31010] (state=,code=0)\r\n{code}",
        "Assert in a query with window function and group by clause  {code}\r\n0: jdbc:drill:schema=dfs> select sum(a1) over (partition by b1)  from t1 group by b1;\r\nError: SYSTEM ERROR: java.lang.AssertionError: Internal error: while converting SUM(`t1`.`a1`)\r\n[Error Id: 21872cfa-6f09-4e92-aee6-5dd8698cf9e7 on atsqa4-133.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\ndrillbit.log\r\n{code}\r\nCaused by: java.lang.AssertionError: Internal error: while converting SUM(`t1`.`a1`)\r\n        at org.apache.calcite.util.Util.newInternal(Util.java:790) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.ReflectiveConvertletTable$2.convertCall(ReflectiveConvertletTable.java:152) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:60) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertOver(SqlToRelConverter.java:1762) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.access$1000(SqlToRelConverter.java:180) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:3937) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.createAggImpl(SqlToRelConverter.java:2521) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertAgg(SqlToRelConverter.java:2342) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:604) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:564) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:2741) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:522) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.prepare.PlannerImpl.convert(PlannerImpl.java:198) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToRel(DefaultSqlHandler.java:246) ~[drill-java-exec-1.0.0-mapr-r1-rebuffed.jar:1.0.0-mapr-r1]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:182) ~[drill-java-exec-1.0.0-mapr-r1-rebuffed.jar:1.0.0-mapr-r1]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:177) ~[drill-java-exec-1.0.0-mapr-r1-rebuffed.jar:1.0.0-mapr-r1]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:902) [drill-java-exec-1.0.0-mapr-r1-rebuffed.jar:1.0.0-mapr-r1]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:240) [drill-java-exec-1.0.0-mapr-r1-rebuffed.jar:1.0.0-mapr-r1]\r\n        ... 3 common frames omitted\r\nCaused by: java.lang.reflect.InvocationTargetException: null\r\n        at sun.reflect.GeneratedMethodAccessor120.invoke(Unknown Source) ~[na:na]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_71]\r\n        at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_71]\r\n        at org.apache.calcite.sql2rel.ReflectiveConvertletTable$2.convertCall(ReflectiveConvertletTable.java:142) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        ... 19 common frames omitted\r\nCaused by: java.lang.AssertionError: null\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.getRootField(SqlToRelConverter.java:3810) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.adjustInputRef(SqlToRelConverter.java:3139) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:3114) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.access$1400(SqlToRelConverter.java:180) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4061) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:3489) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:274) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:3944) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.StandardConvertletTable.convertExpressionList(StandardConvertletTable.java:833) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.sql2rel.StandardConvertletTable.convertAggregateFunction(StandardConvertletTable.java:706) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        ... 23 common frames omitted\r\n{code}"
    ],
    [
        "DRILL-3351",
        "DRILL-3344",
        "Invalid query must be caught earlier The below query is not valid and we must report an error instead of returning results. Postgres doe not support this kind of a query.\r\n\r\nDrill returns some results, we must instead report an error to user.\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> SELECT MIN(col_int) OVER() FROM vwOnParq group by col_char_2;\r\n+---------+\r\n| EXPR$0  |\r\n+---------+\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n| AZ      |\r\n+---------+\r\n18 rows selected (0.27 seconds)\r\n{code}\r\n\r\nOutput from Postgres\r\n{code}\r\npostgres=# select min(col_int) over() from all_typs_tbl group by col_char_2;\r\nERROR:  column \"all_typs_tbl.col_int\" must appear in the GROUP BY clause or be used in an aggregate function\r\nLINE 1: select min(col_int) over() from all_typs_tbl group by col_ch...\r\n{code}\r\n\r\nQuerying the original parquet file that was used to create the view, returns an assertion error\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> SELECT MIN(col_int) OVER() FROM `tblForView/0_0_0.parquet` group by col_char_2;\r\nError: SYSTEM ERROR: java.lang.AssertionError: Internal error: while converting MIN(`tblForView/0_0_0.parquet`.`col_int`)\r\n\r\n\r\n[Error Id: e8ed279d-aa8c-4db1-9906-5dd7fdecaac2 on centos-02.qa.lab:31010] (state=,code=0)\r\n{code}",
        "When Group By clause is present, the argument in window function should not refer to any column outside Group By CTAS\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> create table tblForView(col_int, col_bigint, col_char_2, col_vchar_52, col_tmstmp, col_dt, col_booln, col_dbl, col_tm) as select cast(columns[0] as INT), cast(columns[1] as BIGINT),cast(columns[2] as CHAR(2)), cast(columns[3] as VARCHAR(52)), cast(columns[4] as TIMESTAMP), cast(columns[5] as DATE), cast(columns[6] as BOOLEAN),cast(columns[7] as DOUBLE),cast(columns[8] as TIME) from `forPrqView.csv`;\r\n+-----------+----------------------------+\r\n| Fragment  | Number of records written  |\r\n+-----------+----------------------------+\r\n| 0_0       | 30                         |\r\n+-----------+----------------------------+\r\n1 row selected (0.586 seconds)\r\n{code}\r\n\r\nFailing query\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select max(col_tm) over(), col_char_2 from tblForView group by col_char_2;\r\nError: SYSTEM ERROR: java.lang.AssertionError: Internal error: while converting MAX(`tblForView`.`col_tm`)\r\n\r\n\r\n[Error Id: 11afbdc9-d47a-4a52-aa77-40c20ffd2bc6 on centos-03.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nStack trace\r\n\r\n{code}\r\n[Error Id: 11afbdc9-d47a-4a52-aa77-40c20ffd2bc6 on centos-03.qa.lab:31010]\r\norg.apache.drill.common.exceptions.UserException: SYSTEM ERROR: java.lang.AssertionError: Internal error: while converting MAX(`tblForView`.`col_tm`)\r\n\r\n\r\n[Error Id: 11afbdc9-d47a-4a52-aa77-40c20ffd2bc6 on centos-03.qa.lab:31010]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:522) ~[drill-common-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$ForemanResult.close(Foreman.java:738) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:840) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:782) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.common.EventProcessor.sendEvent(EventProcessor.java:73) [drill-common-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.moveToState(Foreman.java:784) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.moveToState(Foreman.java:893) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:253) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\nCaused by: org.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: Internal error: while converting MAX(`tblForView`.`col_tm`)\r\n        ... 4 common frames omitted\r\nCaused by: java.lang.AssertionError: Internal error: while converting MAX(`tblForView`.`col_tm`)\r\n        at org.apache.calcite.util.Util.newInternal(Util.java:790) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.ReflectiveConvertletTable$2.convertCall(ReflectiveConvertletTable.java:152) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:60) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertOver(SqlToRelConverter.java:1762) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.access$1000(SqlToRelConverter.java:180) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:3938) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.createAggImpl(SqlToRelConverter.java:2521) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertAgg(SqlToRelConverter.java:2342) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:604) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:564) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:2741) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:522) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.prepare.PlannerImpl.convert(PlannerImpl.java:198) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToRel(DefaultSqlHandler.java:246) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:182) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:178) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:904) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:242) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        ... 3 common frames omitted\r\nCaused by: java.lang.reflect.InvocationTargetException: null\r\n        at sun.reflect.GeneratedMethodAccessor129.invoke(Unknown Source) ~[na:na]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_45]\r\n        at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_45]\r\n        at org.apache.calcite.sql2rel.ReflectiveConvertletTable$2.convertCall(ReflectiveConvertletTable.java:142) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        ... 19 common frames omitted\r\nCaused by: java.lang.AssertionError: null\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.getRootField(SqlToRelConverter.java:3811) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.adjustInputRef(SqlToRelConverter.java:3139) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:3114) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.access$1400(SqlToRelConverter.java:180) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4062) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:3490) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:274) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:3945) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.StandardConvertletTable.convertExpressionList(StandardConvertletTable.java:833) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.StandardConvertletTable.convertAggregateFunction(StandardConvertletTable.java:706) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        ... 23 common frames omitted\r\n{code}"
    ],
    [
        "DRILL-3358",
        "DRILL-3327",
        "CUME_DIST window function provides wrong result when only ORDER BY clause is specified *Drill:*\r\n{code:sql}\r\n> SELECT CUME_DIST() OVER (ORDER BY ss.ss_store_sk) FROM store_sales ss ORDER BY 1 LIMIT 20;\r\n+---------------------+\r\n|       EXPR$0        |\r\n+---------------------+\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n| 0.9923989432198661  |\r\n+---------------------+\r\n20 rows selected (17.317 seconds)\r\n{code}\r\n\r\n*Postgres*\r\n{code:sql}\r\n# SELECT CUME_DIST() OVER (ORDER BY ss.ss_store_sk) FROM store_sales ss ORDER BY 1 LIMIT 20;\r\n     cume_dist    \r\n\r\n-------------------\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n 0.158622193275665\r\n(20 rows)\r\n{code}",
        "row_number function returns incorrect result when only order by clause is specified Queries returning wrong result:\r\n{code}\r\nselect c_integer, row_number() over(order by c_integer) from j1 order by 2;\r\nselect c_integer, row_number() over(order by c_integer) from j1 order by 2 desc;\r\nselect c_integer, row_number() over(order by c_integer desc) from j1 order by 2;\r\nselect c_integer, row_number() over(order by c_integer desc) from j1 order by 2;\r\nselect c_integer, row_number() over(order by c_integer nulls first) from j1 order by c_integer nulls last,  row_number() over(order by c_integer nulls first);\r\nselect c_integer, row_number() over(order by c_integer nulls last) from j1 order by c_integer nulls first, row_number() over(order by c_integer nulls last);\r\n{code}\r\n\r\n\r\nIn attached tar file (row_number_wrong_result.tar) you will find:\r\n\r\n1. *.sql - file that contains query\r\n2. *.res - expected result (generated from Postgres)\r\n3. *.out - result returned by drill"
    ],
    [
        "DRILL-3380",
        "DRILL-3379",
        "CTAS Auto Partitioning : We are not pruning when we use functions in the select list git.commit.id.abbrev=5a34d81\r\n\r\nI used the below query to create a paritioned data set\r\n{code}\r\ncreate table `lineitem` partition by (l_moddate) as select l.*, l_shipdate - extract(day from l_shipdate) + 1 l_moddate from cp.`tpch/lineitem.parquet` l;\r\n{code}\r\n\r\nThe plan for the below query only scans one file\r\n{code}\r\nexplain plan for select * from `lineitem` where l_moddate = date '1994-07-01';\r\n 00-00    Screen\r\n00-01      Project(*=[$0])\r\n00-02        Project(*=[$0])\r\n00-03          Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=/drill/testdata/ctas_auto_partition/tpch_single_partition/lineitem/0_0_31.parquet]], selectionRoot=/drill/testdata/ctas_auto_partition/tpch_single_partition/lineitem, numFiles=1, columns=[`*`]]])\r\n{code}\r\n\r\nHowever the below plan indicates a full table scan\r\n{code}\r\nexplain plan for select count(*) from `tpch_single_partition/lineitem` where l_moddate = date '1994-07-01';\r\n00-00    Screen\r\n00-01      StreamAgg(group=[{}], EXPR$0=[COUNT()])\r\n00-02        Project($f0=[0])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[=($0, 1994-07-01)])\r\n00-05              Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:///drill/testdata/ctas_auto_partition/tpch_single_partition/lineitem]], selectionRoot=/drill/testdata/ctas_auto_partition/tpch_single_partition/lineitem, numFiles=1, columns=[`l_moddate`]]])\r\n{code}",
        "Passing references when cloning ParquetGroupScan causes incorrect planning This is causing the planner to sometimes choose the plan without pruning, even though the pruning rule was successful."
    ],
    [
        "DRILL-3424",
        "DRILL-540",
        "Hive Views are not accessible through Drill Query Hive Views are not accessible through Drill Query. \r\nError Message: Hive Views are Not Supported in Current Version",
        "Allow querying hive views in drill Currently hive views cannot be queried from drill.\r\n"
    ],
    [
        "DRILL-3445",
        "DRILL-3312",
        "BufferAllocator.buffer() implementations should throw an OutOfMemoryRuntimeException current implementations of BufferAllocator.buffer() return null if it can't allocate the buffer because of direct memory or fragment limits., but many places in the code don't actually check if the buffer is null before trying to access it, this will result in confusing NullPointerException(s) when we are in fact running out of memory.\r\n\r\nWe should change the implementations to throw an OutOfMemoryRuntimeException instead. Drill already handles this exception properly in most cases and displays a proper error message to the user.",
        "PageReader.allocatePageData() calls BufferAllocator.buffer(int) but doesn't check if the result is null Trying to reproduce DRILL-3241, Drill returns the following error:\r\n{noformat}\r\nSYSTEM ERROR: org.apache.drill.common.exceptions.DrillRuntimeException: Error in parquet record reader.\r\nMessage: \r\nHadoop path: /Users/hakim/MapR/data/tpcds100/parquet/store_sales/1_10_1.parquet\r\nTotal records read: 0\r\nMock records read: 0\r\nRecords to read: 21845\r\nRow group index: 0\r\nRecords in row group: 3348801\r\nParquet Metadata: ParquetMetaData{FileMetaData{schema: message root {\r\n   ...omitted text\r\n}\r\n, metadata: {}}, blocks: [...omitted text]}\r\n\r\nFragment 3:0\r\n\r\n[Error Id: 0f9ba088-3091-4166-b46f-53ab60d364f8 on 10.250.50.54:31010]{noformat}\r\n\r\nThe logs contain more information about the cause:\r\n{noformat}\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.drill.exec.store.parquet.DirectCodecFactory$FullDirectDecompressor.decompress(DirectCodecFactory.java:238) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.parquet.columnreaders.PageReader.next(PageReader.java:230) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.parquet.columnreaders.NullableColumnReader.processPages(NullableColumnReader.java:76) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.parquet.columnreaders.ParquetRecordReader.readAllFixedFields(ParquetRecordReader.java:380) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n\tat org.apache.drill.exec.store.parquet.columnreaders.ParquetRecordReader.next(ParquetRecordReader.java:423) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n   ...\r\n{noformat}\r\n\r\nJust before that line, PageReader.next() calls the following method:\r\n{noformat}\r\n  private void allocatePageData(int size) {\r\n    Preconditions.checkArgument(pageData == null);\r\n    pageData = parentColumnReader.parentReader.getOperatorContext().getAllocator().buffer(size);\r\n  }\r\n{noformat}\r\n\r\nthis method should check the output of BufferAllocator.buffer(int) to make sure the buffer was successfully allocated."
    ],
    [
        "DRILL-3495",
        "DRILL-3479",
        "drill-embedded shows an older version number on startup {code}\r\n[apache-drill-1.1.0]$ bin/drill-embedded \r\nJul 13, 2015 4:53:30 PM org.glassfish.jersey.server.ApplicationHandler initialize\r\nINFO: Initiating Jersey application, version Jersey: 2.8 2014-04-29 01:25:26...\r\napache drill 1.0.0 \r\n\"json ain't no thang\"\r\n0: jdbc:drill:zk=local> \r\n{code}\r\n\r\nThe version number displayed in the interactive mode is \"1.0.0\" instead of \"1.1.0\"",
        "Sqlline from drill v1.1.0 displays version as 1.0.0 Sqlline from drill 1.1.0 displays drill version as 1.0.0\r\n\r\n/opt/drill/bin/sqlline\r\napache drill 1.0.0 \r\n\"start your sql engine\""
    ],
    [
        "DRILL-3508",
        "DRILL-2618",
        "IOOB raised if FROM points at an empty directory I union select where the right hand side points to an empty directory files with the following error code:\r\n\r\nError: PARSE ERROR: Index: 0, Size: 0\r\n[Error Id: fed7527a-9d7a-4980-80f5-ff3f0b92d49b on localhost:31010] (state=,code=0)\r\n\r\nWe use this to merge tenant based information and sometimes there are no files on the right hand side of the union. It would be ideal that that scenario would return an empty list rather than an error.",
        "BasicFormatMatcher calls getFirstPath(...) without checking # of paths is not zero {{BasicFormatMatcher.isReadable(...)}} calls {{getFirstPath(...)}} without checking that there is at least one path.  This can cause an IndexOutOfBoundsException.\r\n\r\nTo reproduce, create an empty directory {{/tmp/CaseInsensitiveColumnNames}} and run {{exec/java-exec/src/test/java/org/apache/drill/TestExampleQueries.java}}."
    ],
    [
        "DRILL-3508",
        "DRILL-2775",
        "IOOB raised if FROM points at an empty directory I union select where the right hand side points to an empty directory files with the following error code:\r\n\r\nError: PARSE ERROR: Index: 0, Size: 0\r\n[Error Id: fed7527a-9d7a-4980-80f5-ff3f0b92d49b on localhost:31010] (state=,code=0)\r\n\r\nWe use this to merge tenant based information and sometimes there are no files on the right hand side of the union. It would be ideal that that scenario would return an empty list rather than an error.",
        "message error not clear when doing a select or ctas on empty folder if you have an empty folder \"emptyfolder\", you get a cryptic error message when you try to query the folder or CTAS a table that has the same name of the empty folder:\r\n{noformat}\r\n0: jdbc:drill:zk=local> select * from emptyfolder;\r\nError: PARSE ERROR: Index: 0, Size: 0\r\n\r\n[Error Id: ef86154b-8219-4b48-84bf-cb318f7d4ae4 on abdel-11.qa.lab:31010] (state=,code=0)\r\n{noformat}\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> create table emptyfolder as select * from `test.json`;\r\nError: SYSTEM ERROR: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n\r\n[Error Id: 1c3965f4-1566-4df6-9bb6-a91211771976 on abdel-11.qa.lab:31010] (state=,code=0)\r\n{noformat}"
    ],
    [
        "DRILL-3544",
        "DRILL-2862",
        "Need better error messages when convert_to is given a bad type The first query below fails because I used UTF-8 instead of UTF8.  This should have a decent error message.\r\n\r\n{code}\r\n0: jdbc:drill:zk=local> SELECT CONVERT_TO('[ [1, 2], [3, 4], [5]]' ,'UTF-8') AS MYCOL1 FROM sys.version;\r\nError: SYSTEM ERROR: org.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: null\r\n\r\n[Error Id: 899207da-2338-4b09-bdc8-8e12e320b661 on 172.16.0.61:31010] (state=,code=0)\r\n0: jdbc:drill:zk=local> SELECT CONVERT_TO('[ [1, 2], [3, 4], [5]]' ,'UTF8') AS MYCOL1 FROM sys.version;\r\n+-------------+\r\n|   MYCOL1    |\r\n+-------------+\r\n| [B@71f3d3a  |\r\n+-------------+\r\n1 row selected (0.108 seconds)\r\n0: jdbc:drill:zk=local> \r\n{code}",
        "Convert_to/Convert_From throw assertion when an incorrect encoding type is specified Below is the error from SQLLine. Replacing UTF-8 to UTF8 works fine.\r\nThe error message need to accurately represent the problem.\r\n\r\n\r\n0: jdbc:drill:> select Convert_from(t.address.state,'UTF-8') from customers t limit 10;\r\nQuery failed: AssertionError: \r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)"
    ],
    [
        "DRILL-3591",
        "DRILL-2852",
        "Partition pruning hit AssertionError when a cast function is used in comparison operand In Drill unit test  TestPartitionFilter.testPartitionFilter6_Parquet(), the query is :\r\n\r\n{code}\r\nselect * from dfs_test.tmp.parquet where (yr=1995 and o_totalprice < 40000) or yr=1996\r\n{code}\r\n\r\nIf I slightly modify the filter, by adding a cast function: {\r\n{code}\r\nselect * from dfs_test.`%s/multilevel/parquet` where (dir0=cast(1995 as varchar(10)) and o_totalprice < 40000) or dir0=1996\r\n{code}\r\n\r\nIt will hit Assertion Error, when PruneScanRule calls interpreter to evaluate the filter condition.\r\n\r\n{code}\r\norg.apache.drill.common.exceptions.UserException: SYSTEM ERROR: AssertionError\r\n...\r\nCaused by: java.lang.AssertionError\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.isBitOn(InterpreterEvaluator.java:490) ~[classes/:na]\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanAnd(InterpreterEvaluator.java:434) ~[classes/:na]\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator(InterpreterEvaluator.java:332) ~[classes/:na]\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator(InterpreterEvaluator.java:147) ~[classes/:na]\r\n\tat org.apache.drill.common.expression.BooleanOperator.accept(BooleanOperator.java:36) ~[classes/:na]\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOr(InterpreterEvaluator.java:463) ~[classes/:na]\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator(InterpreterEvaluator.java:334) ~[classes/:na]\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator(InterpreterEvaluator.java:147) ~[classes/:na]\r\n\tat org.apache.drill.common.expression.BooleanOperator.accept(BooleanOperator.java:36) ~[classes/:na]\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator.evaluate(InterpreterEvaluator.java:80) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.logical.partition.PruneScanRule.doOnMatch(PruneScanRule.java:420) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.logical.partition.PruneScanRule$2.onMatch(PruneScanRule.java:156) ~[classes/:na]\r\n\tat org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:228) ~[calcite-core-1.1.0-drill-r15.jar:1.1.0-drill-r15]\r\n\tat org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:795) ~[calcite-core-1.1.0-drill-r15.jar:1.1.0-drill-r15]\r\n\tat org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:303) ~[calcite-core-1.1.0-drill-r15.jar:1.1.0-drill-r15]\r\n\tat org.apache.calcite.prepare.PlannerImpl.transform(PlannerImpl.java:316) ~[calcite-core-1.1.0-drill-r15.jar:1.1.0-drill-r15]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.logicalPlanningVolcanoAndLopt(DefaultSqlHandler.java:528) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToDrel(DefaultSqlHandler.java:213) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToDrel(DefaultSqlHandler.java:248) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:164) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:178) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:903) [classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:242) [classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n\tat java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n\t... 13 more\r\n{code}\r\n\r\nI debug a bit, seems in PartitionPruneRule, the condition is rewrote to the following, which seems not right, since CAST function becomes one operand of \"AND\".\r\n\r\n{code}\r\nOR(AND(CAST(1995):VARCHAR(10) CHARACTER SET \"ISO-8859-1\" COLLATE \"ISO-8859-1$en_US$primary\" NOT NULL, =($1, CAST(1995):VARCHAR(10) CHARACTER SET \"ISO-8859-1\" COLLATE \"ISO-8859-1$en_US$primary\" NOT NULL)), =($1, 1996))\r\n{code}\r\n\r\n",
        "CASTing the column 'dir0' in view causes partition pruning to fail  If the partition column 'dir0' is CAST inside a view and the query has a filter on that column, then partition pruning interpreter evaluator encounters an AssertionError. \r\n\r\nTable data in the example below is from test/resources/multilevel/parquet.  \r\n\r\n{code}\r\ncreate view dfs.tmp.myview2 as select cast(dir0 as varchar(100)) as myyear, dir1 as myquarter, o_totalprice from `multilevel/parquet` ;\r\n\r\nselect * from dfs.tmp.myview2 where myyear = '1995' and myquarter = 'Q2' and o_totalprice < 40000.0;\r\n{code}\r\n\r\nFailure stack trace: \r\n{code}\r\nCaused by: java.lang.AssertionError: Internal error: Error while applying rule PruneScanRule:Filter_On_Project,\r\n...\r\n<skip>\r\n\r\nCaused by: java.lang.AssertionError: null\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.isBitOn(InterpreterEvaluator.java:490) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanAnd(InterpreterEvaluator.java:434) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator(InterpreterEvaluator.java:332) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator(InterpreterEvaluator.java:147) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.expression.BooleanOperator.accept(BooleanOperator.java:36) ~[drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator.evaluate(InterpreterEvaluator.java:80) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.logical.partition.PruneScanRule.doOnMatch(PruneScanRule.java:224) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n{code}"
    ],
    [
        "DRILL-3620",
        "DRILL-3479",
        "Drill 1.1 reports version 1.0 upon start in cmd line  On Mon, Aug 10, 2015 at 11:21 AM, Ellen Friedman <efriedman@maprtech.com> wrote:\r\n\r\nSaw this on Twitter:\r\n\r\nquick bug report: @apacheDrill 1.1 reports version 1.0 upon start in cmd line mode\r\napache drill 1.0.0 \r\n\"drill baby drill\"",
        "Sqlline from drill v1.1.0 displays version as 1.0.0 Sqlline from drill 1.1.0 displays drill version as 1.0.0\r\n\r\n/opt/drill/bin/sqlline\r\napache drill 1.0.0 \r\n\"start your sql engine\""
    ],
    [
        "DRILL-3620",
        "DRILL-3495",
        "Drill 1.1 reports version 1.0 upon start in cmd line  On Mon, Aug 10, 2015 at 11:21 AM, Ellen Friedman <efriedman@maprtech.com> wrote:\r\n\r\nSaw this on Twitter:\r\n\r\nquick bug report: @apacheDrill 1.1 reports version 1.0 upon start in cmd line mode\r\napache drill 1.0.0 \r\n\"drill baby drill\"",
        "drill-embedded shows an older version number on startup {code}\r\n[apache-drill-1.1.0]$ bin/drill-embedded \r\nJul 13, 2015 4:53:30 PM org.glassfish.jersey.server.ApplicationHandler initialize\r\nINFO: Initiating Jersey application, version Jersey: 2.8 2014-04-29 01:25:26...\r\napache drill 1.0.0 \r\n\"json ain't no thang\"\r\n0: jdbc:drill:zk=local> \r\n{code}\r\n\r\nThe version number displayed in the interactive mode is \"1.0.0\" instead of \"1.1.0\""
    ],
    [
        "DRILL-3632",
        "DRILL-3566",
        "org.apache.drill.jdbc.impl.DrillJdbc41Factory$DrillJdbc41PreparedStatement cannot be cast to org.apache.drill.jdbc.impl.DrillStatementImpl when we use PreparedStatement for a query with jdbc, drill throws:\r\nCaused by: java.lang.ClassCastException: org.apache.drill.jdbc.impl.DrillJdbc41Factory$DrillJdbc41PreparedStatement cannot be cast to org.apache.drill.jdbc.impl.DrillStatementImpl\r\n\tat org.apache.drill.jdbc.impl.DrillJdbc41Factory.newResultSet(DrillJdbc41Factory.java:106) ~[drill-jdbc-1.1.0.jar:1.1.0]\r\n\tat org.apache.drill.jdbc.impl.DrillJdbc41Factory.newResultSet(DrillJdbc41Factory.java:46) ~[drill-jdbc-1.1.0.jar:1.1.0]\r\n\tat net.hydromatic.avatica.AvaticaConnection.executeQueryInternal(AvaticaConnection.java:397) ~[optiq-avatica-0.9-drill-r20.jar:na]",
        "Calling Connection.prepareStatement throws a ClassCastException Git Commit # : 65935db8d01b95a7a3107835d7cd5e61220e2f84\r\n\r\nI am hitting the below exception when using Connection.prepareStatement without binding any parameters\r\n{code}\r\nPreparedStatement stmt = con.prepareStatement(DRILL_SAMPLE_QUERY);\r\n\r\nException in thread \"main\" java.lang.ClassCastException: org.apache.drill.jdbc.impl.DrillJdbc41Factory$DrillJdbc41PreparedStatement cannot be cast to org.apache.drill.jdbc.impl.DrillStatementImpl\r\nat org.apache.drill.jdbc.impl.DrillJdbc41Factory.newResultSet(DrillJdbc41Factory.java:106)\r\nat org.apache.drill.jdbc.impl.DrillJdbc41Factory.newResultSet(DrillJdbc41Factory.java:1)\r\nat net.hydromatic.avatica.AvaticaConnection.executeQueryInternal(AvaticaConnection.java:397)\r\nat net.hydromatic.avatica.AvaticaPreparedStatement.executeQuery(AvaticaPreparedStatement.java:77)\r\nat com.incorta.trails.DrillTest.query(DrillTest.java:33)\r\nat com.incorta.trails.DrillTest.main(DrillTest.java:12)\r\n{code}"
    ],
    [
        "DRILL-3639",
        "DRILL-2852",
        "Internal error: Error while applying rule PruneScanRule:Filter_On_Scan_Parquet exception:\r\n(org.apache.drill.exec.work.foreman.ForemanException) Unexpected exception during fragment initialization: Internal error: Error while applying rule PruneScanRule:Filter_On_Scan_Parquet\r\nCaused By (java.lang.AssertionError) null\r\n    org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.isBitOn():490\r\n    org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanAnd():434\r\n    org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator():332\r\n    org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator():147\r\n    org.apache.drill.common.expression.BooleanOperator.accept():36\r\n\r\nmy sql:\r\nselect col1 from hdfs.root.`/dir/table1` where col2<cast('222222222.0' as double) and col3=101 limit 10\r\n\r\ncol2 is not a partition column and col3 is a partition column, so the FindPartitionConditions get a wrong condition like 'AND(CAST('222222222.0'):DOUBLE NOT NULL, =($1, 101))'",
        "CASTing the column 'dir0' in view causes partition pruning to fail  If the partition column 'dir0' is CAST inside a view and the query has a filter on that column, then partition pruning interpreter evaluator encounters an AssertionError. \r\n\r\nTable data in the example below is from test/resources/multilevel/parquet.  \r\n\r\n{code}\r\ncreate view dfs.tmp.myview2 as select cast(dir0 as varchar(100)) as myyear, dir1 as myquarter, o_totalprice from `multilevel/parquet` ;\r\n\r\nselect * from dfs.tmp.myview2 where myyear = '1995' and myquarter = 'Q2' and o_totalprice < 40000.0;\r\n{code}\r\n\r\nFailure stack trace: \r\n{code}\r\nCaused by: java.lang.AssertionError: Internal error: Error while applying rule PruneScanRule:Filter_On_Project,\r\n...\r\n<skip>\r\n\r\nCaused by: java.lang.AssertionError: null\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.isBitOn(InterpreterEvaluator.java:490) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanAnd(InterpreterEvaluator.java:434) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator(InterpreterEvaluator.java:332) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator(InterpreterEvaluator.java:147) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.expression.BooleanOperator.accept(BooleanOperator.java:36) ~[drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator.evaluate(InterpreterEvaluator.java:80) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.logical.partition.PruneScanRule.doOnMatch(PruneScanRule.java:224) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n{code}"
    ],
    [
        "DRILL-3639",
        "DRILL-3591",
        "Internal error: Error while applying rule PruneScanRule:Filter_On_Scan_Parquet exception:\r\n(org.apache.drill.exec.work.foreman.ForemanException) Unexpected exception during fragment initialization: Internal error: Error while applying rule PruneScanRule:Filter_On_Scan_Parquet\r\nCaused By (java.lang.AssertionError) null\r\n    org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.isBitOn():490\r\n    org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanAnd():434\r\n    org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator():332\r\n    org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator():147\r\n    org.apache.drill.common.expression.BooleanOperator.accept():36\r\n\r\nmy sql:\r\nselect col1 from hdfs.root.`/dir/table1` where col2<cast('222222222.0' as double) and col3=101 limit 10\r\n\r\ncol2 is not a partition column and col3 is a partition column, so the FindPartitionConditions get a wrong condition like 'AND(CAST('222222222.0'):DOUBLE NOT NULL, =($1, 101))'",
        "Partition pruning hit AssertionError when a cast function is used in comparison operand In Drill unit test  TestPartitionFilter.testPartitionFilter6_Parquet(), the query is :\r\n\r\n{code}\r\nselect * from dfs_test.tmp.parquet where (yr=1995 and o_totalprice < 40000) or yr=1996\r\n{code}\r\n\r\nIf I slightly modify the filter, by adding a cast function: {\r\n{code}\r\nselect * from dfs_test.`%s/multilevel/parquet` where (dir0=cast(1995 as varchar(10)) and o_totalprice < 40000) or dir0=1996\r\n{code}\r\n\r\nIt will hit Assertion Error, when PruneScanRule calls interpreter to evaluate the filter condition.\r\n\r\n{code}\r\norg.apache.drill.common.exceptions.UserException: SYSTEM ERROR: AssertionError\r\n...\r\nCaused by: java.lang.AssertionError\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.isBitOn(InterpreterEvaluator.java:490) ~[classes/:na]\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanAnd(InterpreterEvaluator.java:434) ~[classes/:na]\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator(InterpreterEvaluator.java:332) ~[classes/:na]\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator(InterpreterEvaluator.java:147) ~[classes/:na]\r\n\tat org.apache.drill.common.expression.BooleanOperator.accept(BooleanOperator.java:36) ~[classes/:na]\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOr(InterpreterEvaluator.java:463) ~[classes/:na]\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator(InterpreterEvaluator.java:334) ~[classes/:na]\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitBooleanOperator(InterpreterEvaluator.java:147) ~[classes/:na]\r\n\tat org.apache.drill.common.expression.BooleanOperator.accept(BooleanOperator.java:36) ~[classes/:na]\r\n\tat org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator.evaluate(InterpreterEvaluator.java:80) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.logical.partition.PruneScanRule.doOnMatch(PruneScanRule.java:420) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.logical.partition.PruneScanRule$2.onMatch(PruneScanRule.java:156) ~[classes/:na]\r\n\tat org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:228) ~[calcite-core-1.1.0-drill-r15.jar:1.1.0-drill-r15]\r\n\tat org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:795) ~[calcite-core-1.1.0-drill-r15.jar:1.1.0-drill-r15]\r\n\tat org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:303) ~[calcite-core-1.1.0-drill-r15.jar:1.1.0-drill-r15]\r\n\tat org.apache.calcite.prepare.PlannerImpl.transform(PlannerImpl.java:316) ~[calcite-core-1.1.0-drill-r15.jar:1.1.0-drill-r15]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.logicalPlanningVolcanoAndLopt(DefaultSqlHandler.java:528) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToDrel(DefaultSqlHandler.java:213) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToDrel(DefaultSqlHandler.java:248) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:164) ~[classes/:na]\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:178) ~[classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:903) [classes/:na]\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:242) [classes/:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n\tat java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n\t... 13 more\r\n{code}\r\n\r\nI debug a bit, seems in PartitionPruneRule, the condition is rewrote to the following, which seems not right, since CAST function becomes one operand of \"AND\".\r\n\r\n{code}\r\nOR(AND(CAST(1995):VARCHAR(10) CHARACTER SET \"ISO-8859-1\" COLLATE \"ISO-8859-1$en_US$primary\" NOT NULL, =($1, CAST(1995):VARCHAR(10) CHARACTER SET \"ISO-8859-1\" COLLATE \"ISO-8859-1$en_US$primary\" NOT NULL)), =($1, 1996))\r\n{code}\r\n\r\n"
    ],
    [
        "DRILL-3642",
        "DRILL-3616",
        "External Sort will leak memory if query is cancelled while it's spilling to disk When we cancel a query that was spilling to disk we \"may\" get memory leaks.",
        "Memory leak in a cleanup code after canceling queries with window functions spilling to disk Bunch of concurrent queries with window functions were cancelled.\r\nGot an error in drillbit.log that might indicate that we have a memory leak in in cleanup code after cancellation.\r\n\r\nAssigning to myself for creation of a reproducible test case.\r\n\r\n{code}\r\n2015-08-05 22:43:56,475 [2a3d6e54-12c2-2519-3ea1-736cb1e39e2a:frag:0:0] INFO  o.a.d.e.w.fragment.FragmentExecutor - 2a3d6e54-12c2-2519-3ea1-736cb1e39e2a:0:0: State change requested from CANCELLATION_REQUESTED --> FAILED\r\n2015-08-05 22:43:56,475 [2a3d6e54-12c2-2519-3ea1-736cb1e39e2a:frag:0:0] INFO  o.a.d.e.w.fragment.FragmentExecutor - 2a3d6e54-12c2-2519-3ea1-736cb1e39e2a:0:0: State change requested from FAILED --> FAILED\r\n2015-08-05 22:43:56,475 [2a3d6e54-12c2-2519-3ea1-736cb1e39e2a:frag:0:0] INFO  o.a.d.e.w.fragment.FragmentExecutor - 2a3d6e54-12c2-2519-3ea1-736cb1e39e2a:0:0: State change requested from FAILED --> FINISHED\r\n2015-08-05 22:43:56,476 [2a3d6e54-12c2-2519-3ea1-736cb1e39e2a:frag:0:0] ERROR o.a.d.e.w.fragment.FragmentExecutor - SYSTEM ERROR: IllegalStateException: Unaccounted for outstanding allocation (902492)\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 1b9714b9-5a39-48ec-80e7-c49c79825cda on atsqa4-133.qa.lab:31010]\r\norg.apache.drill.common.exceptions.UserException: SYSTEM ERROR: IllegalStateException: Unaccounted for outstanding allocation (902492)\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 1b9714b9-5a39-48ec-80e7-c49c79825cda on atsqa4-133.qa.lab:31010]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:523) ~[drill-common-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.sendFinalState(FragmentExecutor.java:323) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.cleanup(FragmentExecutor.java:178) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:292) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_71]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]\r\nCaused by: java.lang.RuntimeException: Exception while closing\r\n        at org.apache.drill.common.DrillAutoCloseables.closeNoChecked(DrillAutoCloseables.java:46) ~[drill-common-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.ops.OperatorContextImpl.close(OperatorContextImpl.java:139) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.ops.FragmentContext.suppressingClose(FragmentContext.java:439) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.ops.FragmentContext.close(FragmentContext.java:424) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.closeOutResources(FragmentExecutor.java:352) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.cleanup(FragmentExecutor.java:173) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        ... 5 common frames omitted\r\nCaused by: java.lang.IllegalStateException: Unaccounted for outstanding allocation (902492)\r\n        at org.apache.drill.exec.memory.BaseAllocator.close(BaseAllocator.java:1278) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.common.DrillAutoCloseables.closeNoChecked(DrillAutoCloseables.java:44) ~[drill-common-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        ... 10 common frames omitted\r\n2015-08-05 22:43:56,477 [2a3d6e54-f2c1-4682-9121-73c9110e3dd7:frag:0:0] ERROR o.a.d.e.w.fragment.FragmentExecutor - SYSTEM ERROR: ChannelClosedException: Channel closed /10.10.88.133:31010 <--> /10.10.88.133:45492.\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 16ee1618-f062-4a7b-af14-32a60e14d78f on atsqa4-133.qa.lab:31010]\r\norg.apache.drill.common.exceptions.UserException: SYSTEM ERROR: ChannelClosedException: Channel closed /10.10.88.133:31010 <--> /10.10.88.133:45492.\r\n{code}"
    ],
    [
        "DRILL-3674",
        "DRILL-3583",
        "RuntimeException - Unknown variable or type \"logger\" I execute a query (details removed given the project nature) with a wrong placement of distinct statement :\r\n\r\nselect distinct columns[1] as payer, sum(columns[4]) as charge, sum(columns[5]) as payment from dfs.`<PATH TO VALID FILE>` group by columns[1];\r\n\r\nI get an exception:\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: SYSTEM ERROR: CompileException: Line 75, Column 177: Unknown variable or type \"logger\"\r\n\r\nFragment 3:0\r\n\r\n[Error Id: 952f78ef-9a30-4060-a857-40960648fc24 on 192.168.1.16:31010]\r\n\tat sqlline.IncrementalRows.hasNext(IncrementalRows.java:73)\r\n\tat sqlline.TableOutputFormat$ResizingRowsProvider.next(TableOutputFormat.java:87)\r\n\tat sqlline.TableOutputFormat.print(TableOutputFormat.java:118)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1583)\r\n\tat sqlline.Commands.execute(Commands.java:852)\r\n\tat sqlline.Commands.sql(Commands.java:751)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:738)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:612)\r\n\tat sqlline.SqlLine.start(SqlLine.java:366)\r\n\tat sqlline.SqlLine.main(SqlLine.java:259)\r\n\r\nIn some previous logs, I have seen that the Log4J might not have been properly configured by the system (out of the box).",
        "SUM on varchar column produces incorrect error With the implementation of DRILL-3319, a bug was introduced whereby the codegen for an aggregate when SUMing a varchar column fails:\r\n\r\n{code}\r\n0: jdbc:drill:zk=local> select sum(full_name) from cp.`employee.json`;\r\njava.lang.RuntimeException: java.sql.SQLException: SYSTEM ERROR: CompileException: Line 57, Column 177: Unknown variable or type \"logger\"\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 8d5585c4-620c-4275-b0c5-8bc4cbc2da90 on pharma-lap14.ad.pharmadata.net.au:31010]\r\n        at sqlline.IncrementalRows.hasNext(IncrementalRows.java:73)\r\n        at sqlline.TableOutputFormat$ResizingRowsProvider.next(TableOutputFormat.java:87)\r\n        at sqlline.TableOutputFormat.print(TableOutputFormat.java:118)\r\n        at sqlline.SqlLine.print(SqlLine.java:1583)\r\n        at sqlline.Commands.execute(Commands.java:852)\r\n        at sqlline.Commands.sql(Commands.java:751)\r\n        at sqlline.SqlLine.dispatch(SqlLine.java:738)\r\n        at sqlline.SqlLine.begin(SqlLine.java:612)\r\n        at sqlline.SqlLine.start(SqlLine.java:366)\r\n        at sqlline.SqlLine.main(SqlLine.java:259)\r\n{code}\r\n\r\nThis is due to the fact AggregateErrorFunctions now builds its errors with a \"logger\" static field, which does not exist in the codegenned code.\r\n\r\nWe either need to include a static logger in codegen aggregates, or revert back to simpler exceptions for these functions."
    ],
    [
        "DRILL-3680",
        "DRILL-3679",
        "window function query returns Incorrect results  \r\nQuery plan from Drill for the query that returns wrong results\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> explain plan for select c1 , c2 , lead(c2) OVER ( PARTITION BY c2 ORDER BY c1) lead_c2 FROM (SELECT c1 , c2, ntile(3) over(PARTITION BY c2 ORDER BY c1) FROM `tblWnulls.parquet`);\r\n+------+------+\r\n| text | json |\r\n+------+------+\r\n| 00-00    Screen\r\n00-01      Project(c1=[$0], c2=[$1], lead_c2=[$2])\r\n00-02        Project(c1=[$0], c2=[$1], lead_c2=[$2])\r\n00-03          Project(c1=[$0], c2=[$1], $2=[$3])\r\n00-04            Window(window#0=[window(partition {1} order by [0] range between UNBOUNDED PRECEDING and CURRENT ROW aggs [LEAD($1)])])\r\n00-05              Window(window#0=[window(partition {1} order by [0] range between UNBOUNDED PRECEDING and CURRENT ROW aggs [NTILE($2)])])\r\n00-06                SelectionVectorRemover\r\n00-07                  Sort(sort0=[$1], sort1=[$0], dir0=[ASC], dir1=[ASC])\r\n00-08                    Project(c1=[$1], c2=[$0])\r\n00-09                      Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:///tmp/tblWnulls.parquet]], selectionRoot=maprfs:/tmp/tblWnulls.parquet, numFiles=1, columns=[`c1`, `c2`]]])\r\n{code}\r\n\r\nResults returned by Drill.\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select c1 , c2 , lead(c2) OVER ( PARTITION BY c2 ORDER BY c1) lead_c2 FROM (SELECT c1 , c2, ntile(3) over(PARTITION BY c2 ORDER BY c1) FROM `tblWnulls.parquet`);\r\n+-------------+-------+----------+\r\n|     c1      |  c2   | lead_c2  |\r\n+-------------+-------+----------+\r\n| 0           | a     | null     |\r\n| 1           | a     | null     |\r\n| 5           | a     | null     |\r\n| 10          | a     | null     |\r\n| 11          | a     | null     |\r\n| 14          | a     | null     |\r\n| 11111       | a     | null     |\r\n| 2           | b     | null     |\r\n| 9           | b     | null     |\r\n| 13          | b     | null     |\r\n| 17          | b     | null     |\r\n| 4           | c     | null     |\r\n| 6           | c     | null     |\r\n| 8           | c     | null     |\r\n| 12          | c     | null     |\r\n| 13          | c     | null     |\r\n| 13          | c     | null     |\r\n| null        | c     | null     |\r\n| 10          | d     | null     |\r\n| 11          | d     | null     |\r\n| 2147483647  | d     | null     |\r\n| 2147483647  | d     | null     |\r\n| null        | d     | null     |\r\n| null        | d     | null     |\r\n| -1          | e     | null     |\r\n| 15          | e     | null     |\r\n| 19          | null  | null     |\r\n| 65536       | null  | null     |\r\n| 1000000     | null  | null     |\r\n| null        | null  | null     |\r\n+-------------+-------+----------+\r\n30 rows selected (0.339 seconds)\r\n{code}\r\n\r\nResults returned by Postgres\r\n\r\n{code}\r\npostgres=# select c1 , c2 , lead(c2) OVER ( PARTITION BY c2 ORDER BY c1) lead_c2 FROM (SELECT c1 , c2, ntile(3) over(PARTITION BY c2 ORDER BY c1) FROM t222) sub_query;\r\n     c1     | c2 | lead_c2 \r\n------------+----+---------\r\n          0 | a  | a\r\n          1 | a  | a\r\n          5 | a  | a\r\n         10 | a  | a\r\n         11 | a  | a\r\n         14 | a  | a\r\n      11111 | a  | \r\n          2 | b  | b\r\n          9 | b  | b\r\n         13 | b  | b\r\n         17 | b  | \r\n          4 | c  | c\r\n          6 | c  | c\r\n          8 | c  | c\r\n         12 | c  | c\r\n         13 | c  | c\r\n         13 | c  | c\r\n            | c  | \r\n         10 | d  | d\r\n         11 | d  | d\r\n 2147483647 | d  | d\r\n 2147483647 | d  | d\r\n            | d  | d\r\n            | d  | \r\n         -1 | e  | e\r\n         15 | e  | \r\n         19 |    | \r\n      65536 |    | \r\n    1000000 |    | \r\n            |    | \r\n(30 rows)\r\n{code}",
        "IOB Exception : when window functions used in outer and inner query IOB Exception seen when two different window functions are used in inner and outer queries.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select rnum, position_id, ntile(4) over(order by position_id) from (select position_id, row_number() over(order by position_id) as rnum from cp.`employee.json`);\r\njava.lang.RuntimeException: java.sql.SQLException: SYSTEM ERROR: IndexOutOfBoundsException: index: 0, length: 4 (expected: range(0, 0))\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 8e0cbf82-842d-4fa7-ab0d-1d982a3d6c24 on centos-03.qa.lab:31010]\r\n\tat sqlline.IncrementalRows.hasNext(IncrementalRows.java:73)\r\n\tat sqlline.TableOutputFormat$ResizingRowsProvider.next(TableOutputFormat.java:87)\r\n\tat sqlline.TableOutputFormat.print(TableOutputFormat.java:118)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1583)\r\n\tat sqlline.Commands.execute(Commands.java:852)\r\n\tat sqlline.Commands.sql(Commands.java:751)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:738)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:612)\r\n\tat sqlline.SqlLine.start(SqlLine.java:366)\r\n\tat sqlline.SqlLine.main(SqlLine.java:259)\r\n{code}"
    ],
    [
        "DRILL-3690",
        "DRILL-3667",
        "Partitioning pruning produces wrong results when there are nested expressions in the filter Consider the following query:\r\nselect 1 from foo where dir0 not in (1994) and col1 not in ('bar');\r\n\r\nThe filter condition is: AND(NOT(=($1, 1994)), NOT(=($2, 'bar')))\r\nIn FindPartitionCondition we rewrite the filter to cherry pick the partition column conditions so the interpreter can evaluate it, however when the expression contains more than two levels of nesting (in this case AND(NOT(=))) ) the expression does not get rewritten correctly. In this case the expression gets rewritten as: AND(=($1, 1994)). NOT is missing from the rewritten expression producing wrong results.\r\n\r\n",
        "Random Assertion Error while planning git.commit.id.abbrev=b55e232\r\n\r\nWhen assertions are enabled, the below query produces an Assertion Error randomly.\r\n{code}\r\nexplain plan for select l_orderkey, l_partkey, l_quantity, l_shipdate, l_shipinstruct, `year`, `month` from hive.lineitem_text_partitioned_hive_hier_intint where (`year` IN (negative(-1993)) and `month`=sqrt(100)) or (`year` IN (cast(abs(-1994.0) as int)) and `month`=cast('5' as int));\r\n{code}\r\n\r\nWhen Assertions are disabled, we randomly get a wrong plan where we prune away things which we should not have pruned resulting in wrong results. Again this is random.  \r\n\r\nWrong Plan\r\n{code}\r\n00-00    Screen\r\n00-01      Project(l_orderkey=[$0], l_partkey=[$1], l_quantity=[$2], l_shipdate=[$3], l_shipinstruct=[$4], year=[$5], month=[$6])\r\n00-02        Project(l_orderkey=[$0], l_partkey=[$1], l_quantity=[$2], l_shipdate=[$3], l_shipinstruct=[$4], year=[$5], month=[$6])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[OR(AND(=($5, NEGATIVE(-1993)), =(CAST($6):DOUBLE, 1E1)), AND(=($5, 1994), =($6, CAST('5'):INTEGER NOT NULL)))])\r\n00-05              Project(l_orderkey=[$0], l_partkey=[$2], l_quantity=[$6], l_shipdate=[$4], l_shipinstruct=[$1], year=[$3], month=[$5])\r\n00-06                Scan(groupscan=[HiveScan [table=Table(dbName:default, tableName:lineitem_text_partitioned_hive_hier_intint), inputSplits=[maprfs:///drill/testdata/partition_pruning/hive/text/lineitem_hierarchical_intint/1991/10/lineitemaj.tbl:0+106646, maprfs:///drill/testdata/partition_pruning/hive/text/lineitem_hierarchical_intint/1992/10/lineitemaj.tbl:0+107653, maprfs:///drill/testdata/partition_pruning/hive/text/lineitem_hierarchical_intint/1993/10/lineitemaj.tbl:0+107386, maprfs:///drill/testdata/partition_pruning/hive/text/lineitem_hierarchical_intint/1994/10/lineitemaj.tbl:0+107846, maprfs:///drill/testdata/partition_pruning/hive/text/lineitem_hierarchical_intint/1995/10/lineitemaj.tbl:0+107581, maprfs:///drill/testdata/partition_pruning/hive/text/lineitem_hierarchical_intint/1996/10/lineitemaj.tbl:0+107072, maprfs:///drill/testdata/partition_pruning/hive/text/lineitem_hierarchical_intint/1997/10/lineitemaj.tbl:0+1786], columns=[`l_orderkey`, `l_partkey`, `l_quantity`, `l_shipdate`, `l_shipinstruct`, `year`, `month`], partitions= [Partition(values:[1991, 10]), Partition(values:[1992, 10]), Partition(values:[1993, 10]), Partition(values:[1994, 10]), Partition(values:[1995, 10]), Partition(values:[1996, 10]), Partition(values:[1997, 10])]]])\r\n{code}\r\n\r\nRight Plan\r\n{code}\r\n00-00    Screen\r\n00-01      Project(l_orderkey=[$0], l_partkey=[$1], l_quantity=[$2], l_shipdate=[$3], l_shipinstruct=[$4], year=[$5], month=[$6])\r\n00-02        Project(l_orderkey=[$0], l_partkey=[$1], l_quantity=[$2], l_shipdate=[$3], l_shipinstruct=[$4], year=[$5], month=[$6])\r\n00-03          Project(l_orderkey=[$0], l_partkey=[$2], l_quantity=[$6], l_shipdate=[$4], l_shipinstruct=[$1], year=[$3], month=[$5])\r\n00-04            Scan(groupscan=[HiveScan [table=Table(dbName:default, tableName:lineitem_text_partitioned_hive_hier_intint), inputSplits=[maprfs:///drill/testdata/partition_pruning/hive/text/lineitem_hierarchical_intint/1993/10/lineitemaj.tbl:0+107386, maprfs:///drill/testdata/partition_pruning/hive/text/lineitem_hierarchical_intint/1994/5/lineitemae.tbl:0+107451], columns=[`l_orderkey`, `l_partkey`, `l_quantity`, `l_shipdate`, `l_shipinstruct`, `year`, `month`], partitions= [Partition(values:[1993, 10]), Partition(values:[1994, 5])]]])\r\n{code}\r\n\r\nI attached the data and the stack trace when we got an assertion error. Let me know if you need something."
    ],
    [
        "DRILL-3716",
        "DRILL-2748",
        "Drill should push filter past aggregate in order to improve query performance. For the following query which has a filter on top of an aggregation, Drill's currently push the filter pass through the aggregation. As a result, we may miss some optimization opportunity. For instance, such filter could potentially been pushed into scan if it qualifies for partition pruning.\r\n\r\nFor the following query:\r\n\r\n{code}\r\nselect n_regionkey, cnt from \r\n     (select n_regionkey, count(*) cnt \r\n      from (select n.n_nationkey, n.n_regionkey, n.n_name \r\n               from cp.`tpch/nation.parquet` n \r\n                  left join \r\n                       cp.`tpch/region.parquet` r \r\n                on n.n_regionkey = r.r_regionkey) \r\n       group by n_regionkey) \r\nwhere n_regionkey = 2;\r\n{code}\r\n\r\nThe current plan shows a filter (00-04) on top of aggregation(00-05). The better plan would have the filter pushed pass the aggregation. \r\n\r\nThe root cause of this problem is Drill's ruleset does not include  FilterAggregateTransoposeRule from Calcite library.\r\n\r\n{code}\r\n00-01      Project(n_regionkey=[$0], cnt=[$1])\r\n00-02        Project(n_regionkey=[$0], cnt=[$1])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[=($0, 2)])\r\n00-05              StreamAgg(group=[{0}], cnt=[COUNT()])\r\n00-06                Project(n_regionkey=[$0])\r\n00-07                  MergeJoin(condition=[=($0, $1)], joinType=[left])\r\n00-09                    SelectionVectorRemover\r\n00-11                      Sort(sort0=[$0], dir0=[ASC])\r\n00-13                        Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=classpath:/tpch/nation.parquet]], selectionRoot=classpath:/tpch/nation.parquet, numFiles=1, columns=[`n_regionkey`]]])\r\n00-08                    SelectionVectorRemover\r\n00-10                      Sort(sort0=[$0], dir0=[ASC])\r\n00-12                        Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=classpath:/tpch/region.parquet]], selectionRoot=classpath:/tpch/region.parquet, numFiles=1, columns=[`r_regionkey`]]])\r\n{code}\r\n\r\n",
        "Filter is not pushed down into subquery with the group by I'm not sure about this one, theoretically filter could have been pushed into the subquery.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> explain plan for select x, y, z from (select a1, b1, avg(a1) from t1 group by a1, b1) as sq(x, y, z) where x = 10;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(x=[$0], y=[$1], z=[$2])\r\n00-02        Project(x=[$0], y=[$1], z=[CAST(/(CastHigh(CASE(=($3, 0), null, $2)), $3)):ANY NOT NULL])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[=($0, 10)])\r\n00-05              HashAgg(group=[{0, 1}], agg#0=[$SUM0($0)], agg#1=[COUNT($0)])\r\n00-06                Project(a1=[$1], b1=[$0])\r\n00-07                  Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/predicates/t1]], selectionRoot=/drill/testdata/predicates/t1, numFiles=1, columns=[`a1`, `b1`]]])\r\n{code}\r\n\r\nSame with distinct in subquery:\r\n{code}\r\n0: jdbc:drill:schema=dfs> explain plan for select x, y, z from ( select distinct a1, b1, c1 from t1 ) as sq(x, y, z) where x = 10;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(x=[$0], y=[$1], z=[$2])\r\n00-02        Project(x=[$0], y=[$1], z=[$2])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[=($0, 10)])\r\n00-05              HashAgg(group=[{0, 1, 2}])\r\n00-06                Project(a1=[$2], b1=[$1], c1=[$0])\r\n00-07                  Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/predicates/t1]], selectionRoot=/drill/testdata/predicates/t1, numFiles=1, columns=[`a1`, `b1`, `c1`]]])\r\n{code}"
    ],
    [
        "DRILL-3737",
        "DRILL-3539",
        "CTAS from empty text file fails with NPE {code}\r\ncreate table a(aa) as select columns[0] from `empty.csv`;\r\n{code}\r\n\r\nshows:\r\n\r\nError: SYSTEM ERROR: NullPointerException\r\nFragment 0:0",
        "CTAS over empty json file throws NPE CTAS over empty JSON file results in NPE.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> create table t45645 as select * from `empty.json`;\r\nError: SYSTEM ERROR: NullPointerException\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 79039288-5402-4b0a-b32d-5bf5024f3b71 on centos-02.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nStack trace from drillbit.log\r\n\r\n{code}\r\n2015-07-22 00:34:03,788 [2a511b03-90b3-1d39-f4e3-cfd754aa085f:frag:0:0] ERROR o.a.d.e.w.fragment.FragmentExecutor - SYSTEM ERROR: NullPointerException\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 79039288-5402-4b0a-b32d-5bf5024f3b71 on centos-02.qa.lab:31010]\r\norg.apache.drill.common.exceptions.UserException: SYSTEM ERROR: NullPointerException\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 79039288-5402-4b0a-b32d-5bf5024f3b71 on centos-02.qa.lab:31010]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:523) ~[drill-common-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.sendFinalState(FragmentExecutor.java:323) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.cleanup(FragmentExecutor.java:178) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:292) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\nCaused by: java.lang.NullPointerException: null\r\n        at org.apache.drill.exec.physical.impl.WriterRecordBatch.addOutputContainerData(WriterRecordBatch.java:133) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.WriterRecordBatch.innerNext(WriterRecordBatch.java:126) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:147) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:105) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:95) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:147) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:83) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:79) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:73) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor$1.run(FragmentExecutor.java:258) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor$1.run(FragmentExecutor.java:252) ~[drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at java.security.AccessController.doPrivileged(Native Method) ~[na:1.7.0_45]\r\n        at javax.security.auth.Subject.doAs(Subject.java:415) ~[na:1.7.0_45]\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1566) ~[hadoop-common-2.5.1-mapr-1503.jar:na]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:252) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        ... 4 common frames omitted\r\n2015-07-22 00:34:03,792 [BitServer-4] INFO  o.a.drill.exec.work.foreman.Foreman - State change requested.  RUNNING --> FAILED\r\norg.apache.drill.common.exceptions.UserRemoteException: SYSTEM ERROR: NullPointerException\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 79039288-5402-4b0a-b32d-5bf5024f3b71 on centos-02.qa.lab:31010]\r\n        at org.apache.drill.exec.work.foreman.QueryManager$1.statusUpdate(QueryManager.java:466) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.rpc.control.WorkEventBus.statusUpdate(WorkEventBus.java:71) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.batch.ControlMessageHandler.handle(ControlMessageHandler.java:79) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.rpc.control.ControlServer.handle(ControlServer.java:61) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.rpc.control.ControlServer.handle(ControlServer.java:38) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.rpc.RpcBus.handle(RpcBus.java:61) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:233) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:205) [drill-java-exec-1.2.0-SNAPSHOT.jar:1.2.0-SNAPSHOT]\r\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89) [netty-codec-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.handler.timeout.ReadTimeoutHandler.channelRead(ReadTimeoutHandler.java:150) [netty-handler-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242) [netty-codec-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n        at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:618) [netty-transport-native-epoll-4.0.27.Final-linux-x86_64.jar:na]\r\n        at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:329) [netty-transport-native-epoll-4.0.27.Final-linux-x86_64.jar:na]\r\n        at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:250) [netty-transport-native-epoll-4.0.27.Final-linux-x86_64.jar:na]\r\n        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) [netty-common-4.0.27.Final.jar:4.0.27.Final]\r\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n{code}\r\n"
    ],
    [
        "DRILL-3798",
        "DRILL-3781",
        "Cannot group by the functions without () Drill can not group-by the function without ().\r\neg:\r\n{code}\r\nSELECT CURRENT_DATE \r\nFROM hive.h1db.testdate\r\ngroup  by CURRENT_DATE;\r\n\r\n\r\n  Caused By (org.apache.calcite.sql.validate.SqlValidatorException) Column 'CURRENT_DATE' not found in any table\r\n{code}\r\n\r\nBad ones:\r\n{code}\r\nSELECT CURRENT_TIME \r\nFROM hive.h1db.testdate\r\ngroup  by CURRENT_TIME;\r\n\r\nSELECT CURRENT_TIMESTAMP \r\nFROM hive.h1db.testdate\r\ngroup  by CURRENT_TIMESTAMP;\r\n\r\nSELECT LOCALTIME \r\nFROM hive.h1db.testdate\r\ngroup  by LOCALTIME;\r\n\r\nSELECT LOCALTIMESTAMP \r\nFROM hive.h1db.testdate\r\ngroup  by LOCALTIMESTAMP;\r\n{code}\r\n\r\nGood ones:\r\n{code}\r\nSELECT NOW()\r\nFROM hive.h1db.testdate\r\ngroup  by NOW();\r\n\r\nSELECT TIMEOFDAY()\r\nFROM hive.h1db.testdate\r\ngroup  by TIMEOFDAY();\r\n\r\nSELECT UNIX_TIMESTAMP()\r\nFROM hive.h1db.testdate\r\ngroup  by UNIX_TIMESTAMP();\r\n\r\nSELECT PI()\r\nFROM hive.h1db.testdate\r\ngroup  by  PI();\r\n{code}",
        "Using CURRENT_DATE in a group by throws a column not found error for hive tables and csv files Commit # : e43155d8eabb6fc2d0fa4c68c25d6e7c59bf4521\r\n\r\nUsing CURRENT_DATE in a group by seems to failing against hive and csv files. With parquet and json, there seems to be no issues.\r\n\r\nQuery against a hive table :\r\n{code}\r\nselect CURRENT_DATE from student_hive group by CURRENT_DATE;\r\nError: PARSE ERROR: From line 1, column 48 to line 1, column 59: Column 'CURRENT_DATE' not found in any table\r\n\r\n\r\n[Error Id: e7d7df50-c5e8-4eda-990a-050b9a2b188e on qa-node190.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nQuery against csv files :\r\n{code}\r\nselect CURRENT_DATE  from `temp.tbl` group by CURRENT_DATE;\r\nError: DATA_READ ERROR: Selected column 'CURRENT_DATE' must have name 'columns' or must be plain '*'\r\n\r\nFile Path maprfs:///drill/testdata/temp.tbl\r\nFragment 0:0\r\n\r\n[Error Id: 1856f171-966e-4078-bbea-7ff3e9e22e15 on qa-node190.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nA similar query against json and parquet seems to be working fine\r\n{code}\r\nselect current_date from `a.json` group by current_date;\r\n+---------------+\r\n| current_date  |\r\n+---------------+\r\n| 2015-09-14    |\r\n+---------------+\r\n\r\nselect CURRENT_DATE from cp.`tpch/lineitem.parquet` group by CURRENT_DATE;\r\n+---------------+\r\n| CURRENT_DATE  |\r\n+---------------+\r\n| 2015-09-14    |\r\n+---------------+\r\n{code}\r\n\r\nI attached the log files for the failing conditions. Let me know if you need anything"
    ],
    [
        "DRILL-3813",
        "DRILL-2618",
        "table from directory subtree having no descendent files fails with index error Trying to use as a table a directory subtree that has no descendent files (but zero or more descendent directories) yields what seems to be a partially handled index out-of-bounds condition.  \r\n\r\nFor example, with {{/tmp/empty_directory}} being an empty directory:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> SELECT * FROM `dfs`.`tmp`.`empty_directory`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 747425c9-5350-4813-9f0d-ecf580e15101 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=local> \r\n{noformat}\r\n\r\n\r\nAlso, with {{/tmp/no_child_files_subtree}} having two child directories and a grandchild directory, but not descendent files:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> SELECT * FROM `dfs`.`tmp`.`no_child_files_subtree`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: abc90424-8434-4403-b44b-0ba69ef43151 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=local> \r\n{noformat}\r\n\r\n\r\nA directory subtree having no files was expected to be taken as a table with no rows (and a null schema).\r\n",
        "BasicFormatMatcher calls getFirstPath(...) without checking # of paths is not zero {{BasicFormatMatcher.isReadable(...)}} calls {{getFirstPath(...)}} without checking that there is at least one path.  This can cause an IndexOutOfBoundsException.\r\n\r\nTo reproduce, create an empty directory {{/tmp/CaseInsensitiveColumnNames}} and run {{exec/java-exec/src/test/java/org/apache/drill/TestExampleQueries.java}}."
    ],
    [
        "DRILL-3813",
        "DRILL-2775",
        "table from directory subtree having no descendent files fails with index error Trying to use as a table a directory subtree that has no descendent files (but zero or more descendent directories) yields what seems to be a partially handled index out-of-bounds condition.  \r\n\r\nFor example, with {{/tmp/empty_directory}} being an empty directory:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> SELECT * FROM `dfs`.`tmp`.`empty_directory`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 747425c9-5350-4813-9f0d-ecf580e15101 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=local> \r\n{noformat}\r\n\r\n\r\nAlso, with {{/tmp/no_child_files_subtree}} having two child directories and a grandchild directory, but not descendent files:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> SELECT * FROM `dfs`.`tmp`.`no_child_files_subtree`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: abc90424-8434-4403-b44b-0ba69ef43151 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=local> \r\n{noformat}\r\n\r\n\r\nA directory subtree having no files was expected to be taken as a table with no rows (and a null schema).\r\n",
        "message error not clear when doing a select or ctas on empty folder if you have an empty folder \"emptyfolder\", you get a cryptic error message when you try to query the folder or CTAS a table that has the same name of the empty folder:\r\n{noformat}\r\n0: jdbc:drill:zk=local> select * from emptyfolder;\r\nError: PARSE ERROR: Index: 0, Size: 0\r\n\r\n[Error Id: ef86154b-8219-4b48-84bf-cb318f7d4ae4 on abdel-11.qa.lab:31010] (state=,code=0)\r\n{noformat}\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> create table emptyfolder as select * from `test.json`;\r\nError: SYSTEM ERROR: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n\r\n[Error Id: 1c3965f4-1566-4df6-9bb6-a91211771976 on abdel-11.qa.lab:31010] (state=,code=0)\r\n{noformat}"
    ],
    [
        "DRILL-3813",
        "DRILL-3508",
        "table from directory subtree having no descendent files fails with index error Trying to use as a table a directory subtree that has no descendent files (but zero or more descendent directories) yields what seems to be a partially handled index out-of-bounds condition.  \r\n\r\nFor example, with {{/tmp/empty_directory}} being an empty directory:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> SELECT * FROM `dfs`.`tmp`.`empty_directory`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 747425c9-5350-4813-9f0d-ecf580e15101 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=local> \r\n{noformat}\r\n\r\n\r\nAlso, with {{/tmp/no_child_files_subtree}} having two child directories and a grandchild directory, but not descendent files:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> SELECT * FROM `dfs`.`tmp`.`no_child_files_subtree`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: abc90424-8434-4403-b44b-0ba69ef43151 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=local> \r\n{noformat}\r\n\r\n\r\nA directory subtree having no files was expected to be taken as a table with no rows (and a null schema).\r\n",
        "IOOB raised if FROM points at an empty directory I union select where the right hand side points to an empty directory files with the following error code:\r\n\r\nError: PARSE ERROR: Index: 0, Size: 0\r\n[Error Id: fed7527a-9d7a-4980-80f5-ff3f0b92d49b on localhost:31010] (state=,code=0)\r\n\r\nWe use this to merge tenant based information and sometimes there are no files on the right hand side of the union. It would be ideal that that scenario would return an empty list rather than an error."
    ],
    [
        "DRILL-3829",
        "DRILL-3827",
        "Metadata Caching : Drill should ignore a corrupted cache file git.commit.id.abbrev=3c89b30\r\n\r\nDrill should validate the cache file structure and ignore it if it detects any corruption to its contents.\r\n\r\nI placed an empty cache file in the directory and executed a count(*) query on top of the directory. Below is what I got\r\n{code}\r\nselect count(*) from dfs.`/drill/testdata/metadata_caching/lineitem`;\r\nError: SYSTEM ERROR: JsonMappingException: No content to map due to end-of-input\r\n at [Source: com.mapr.fs.MapRFsDataInputStream@293240cd; line: 1, column: 1]\r\n\r\n\r\n[Error Id: 88f77d37-aff3-4adc-bb0e-6c13b49e7776 on qa-node190.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nAt the very least we should inform that the cache file has been corrupted.\r\n",
        "Empty metadata file causes queries on the table to fail I ran into a situation where drill created an empty metadata file (which is a separate issue and I will try to narrow it down. Suspicion is that this happens when \"refresh table metada x\" fails with \"permission denied\" error).\r\n\r\nHowever, we need to guard against situation where metadata file is empty or corrupted. We probably should skip reading it if we encounter unexpected result and continue with query planning without that information. In the same fashion as partition pruning failure. It's also important to log this information somewhere, drillbit.log as a start. It would be really nice to have a flag in the query profile that tells a user if we used metadata file for planning or not. Will help in debugging performance issues.\r\n\r\nVery confusing exception is thrown if you have zero length meta data file in the directory:\r\n{code}\r\n[Wed Sep 23 07:45:28] # ls -la\r\ntotal 2\r\ndrwxr-xr-x  2 root root   2 Sep 10 14:55 .\r\ndrwxr-xr-x 16 root root  35 Sep 15 12:54 ..\r\n-rwxr-xr-x  1 root root 483 Jul  1 11:29 0_0_0.parquet\r\n-rwxr-xr-x  1 root root   0 Sep 10 14:55 .drill.parquet_metadata\r\n\r\n0: jdbc:drill:schema=dfs> select * from t1;\r\nError: SYSTEM ERROR: JsonMappingException: No content to map due to end-of-input\r\n at [Source: com.mapr.fs.MapRFsDataInputStream@342bd88d; line: 1, column: 1]\r\n[Error Id: c97574f6-b3e8-4183-8557-c30df6ca675f on atsqa4-133.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nWorkaround is trivial, remove the file. Marking it as critical, since we don't have any concurrency control in place and this file can get corrupted as well.\r\n"
    ],
    [
        "DRILL-3903",
        "DRILL-2618",
        "Querying empty directory yield internal index-out-of-bounds error Trying to use an empty directory as a table results in an internal IndexOutOfBounds error:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=localhost:2181> SELECT *   FROM `dfs`.`root`.`/tmp/empty_directory`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 66ff61ed-ea41-4af9-87c5-f91480ef1b21 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=localhost:2181> \r\n{noformat}\r\n\r\n\r\n",
        "BasicFormatMatcher calls getFirstPath(...) without checking # of paths is not zero {{BasicFormatMatcher.isReadable(...)}} calls {{getFirstPath(...)}} without checking that there is at least one path.  This can cause an IndexOutOfBoundsException.\r\n\r\nTo reproduce, create an empty directory {{/tmp/CaseInsensitiveColumnNames}} and run {{exec/java-exec/src/test/java/org/apache/drill/TestExampleQueries.java}}."
    ],
    [
        "DRILL-3903",
        "DRILL-2775",
        "Querying empty directory yield internal index-out-of-bounds error Trying to use an empty directory as a table results in an internal IndexOutOfBounds error:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=localhost:2181> SELECT *   FROM `dfs`.`root`.`/tmp/empty_directory`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 66ff61ed-ea41-4af9-87c5-f91480ef1b21 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=localhost:2181> \r\n{noformat}\r\n\r\n\r\n",
        "message error not clear when doing a select or ctas on empty folder if you have an empty folder \"emptyfolder\", you get a cryptic error message when you try to query the folder or CTAS a table that has the same name of the empty folder:\r\n{noformat}\r\n0: jdbc:drill:zk=local> select * from emptyfolder;\r\nError: PARSE ERROR: Index: 0, Size: 0\r\n\r\n[Error Id: ef86154b-8219-4b48-84bf-cb318f7d4ae4 on abdel-11.qa.lab:31010] (state=,code=0)\r\n{noformat}\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> create table emptyfolder as select * from `test.json`;\r\nError: SYSTEM ERROR: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n\r\n[Error Id: 1c3965f4-1566-4df6-9bb6-a91211771976 on abdel-11.qa.lab:31010] (state=,code=0)\r\n{noformat}"
    ],
    [
        "DRILL-3903",
        "DRILL-3508",
        "Querying empty directory yield internal index-out-of-bounds error Trying to use an empty directory as a table results in an internal IndexOutOfBounds error:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=localhost:2181> SELECT *   FROM `dfs`.`root`.`/tmp/empty_directory`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 66ff61ed-ea41-4af9-87c5-f91480ef1b21 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=localhost:2181> \r\n{noformat}\r\n\r\n\r\n",
        "IOOB raised if FROM points at an empty directory I union select where the right hand side points to an empty directory files with the following error code:\r\n\r\nError: PARSE ERROR: Index: 0, Size: 0\r\n[Error Id: fed7527a-9d7a-4980-80f5-ff3f0b92d49b on localhost:31010] (state=,code=0)\r\n\r\nWe use this to merge tenant based information and sometimes there are no files on the right hand side of the union. It would be ideal that that scenario would return an empty list rather than an error."
    ],
    [
        "DRILL-3903",
        "DRILL-3813",
        "Querying empty directory yield internal index-out-of-bounds error Trying to use an empty directory as a table results in an internal IndexOutOfBounds error:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=localhost:2181> SELECT *   FROM `dfs`.`root`.`/tmp/empty_directory`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 66ff61ed-ea41-4af9-87c5-f91480ef1b21 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=localhost:2181> \r\n{noformat}\r\n\r\n\r\n",
        "table from directory subtree having no descendent files fails with index error Trying to use as a table a directory subtree that has no descendent files (but zero or more descendent directories) yields what seems to be a partially handled index out-of-bounds condition.  \r\n\r\nFor example, with {{/tmp/empty_directory}} being an empty directory:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> SELECT * FROM `dfs`.`tmp`.`empty_directory`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 747425c9-5350-4813-9f0d-ecf580e15101 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=local> \r\n{noformat}\r\n\r\n\r\nAlso, with {{/tmp/no_child_files_subtree}} having two child directories and a grandchild directory, but not descendent files:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> SELECT * FROM `dfs`.`tmp`.`no_child_files_subtree`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: abc90424-8434-4403-b44b-0ba69ef43151 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=local> \r\n{noformat}\r\n\r\n\r\nA directory subtree having no files was expected to be taken as a table with no rows (and a null schema).\r\n"
    ],
    [
        "DRILL-3908",
        "DRILL-3355",
        "ResultSetMetadata.getColumnDisplaySize() does not return correct column length value ResultSetMetadata.getColumnDisplaySize() does not return correct column length value. In fact, invocation of this method seems to always return value 10.  This causes our JDBC application to process data incorrectly.  I see that this problem was referenced in jira DRILL-3151, but the this bug has apparently not been addressed, although status of DRILL-3151 is \"resolved\". This is a critical bug for us. ",
        "Implement ResultSetMetadata's getPrecision, getScale, getColumnDisplaySize (need RPC-level data) JDBC ResultSetMetadata methods getPrecision(...), getScale(...), and getColumnDisplaySize() are not implemented, currently because required data is not available in the RPC-level data.\r\n\r\nThe unavailable data includes:\r\n- string type lengths (N in VARCHAR(N), BINARY(N))\r\n- interval qualifier information (which units, leading digit precision, fractional seconds precision)\r\n- datetime type fractional seconds precision\r\n\r\n(Whether an interval is a YEAR/MONTH interval or is a DAY/HOUR/MINUTE/SECOND interval is available.)\r\n\r\n"
    ],
    [
        "DRILL-3939",
        "DRILL-3228",
        "Drill fails to parse valid JSON object The following valid JSON object queried from DRILL using various clients:\r\n--- t.json start---\r\n{\r\n    \"l1\": {\r\n        \"f1\": \"text1\",\r\n        \"f2\": {\r\n            \"command\": \"list\",\r\n            \"StorageArray\": [\r\n                {\r\n                    \"array1\": \"Array1\",\r\n                    \"Pool\": {\r\n                        \"myPool\": \"PoolName\"\r\n                    }\r\n                },\r\n                {\r\n                    \"array2\": \"Arrays2\",\r\n                    \"Pool\": [\r\n                        {\r\n                            \"myPool\": \"PoolName1\"\r\n                        },\r\n                        {\r\n                            \"myPool\": \"PoolName2\"\r\n                        }\r\n                    ]\r\n                }\r\n            ]\r\n        }\r\n    }\r\n}\r\n--- t.json end ---\r\n\r\nGenerates the following error:\r\n----\r\nERROR [HY000] [MapR][Drill] (1040) Drill failed to execute the query: SELECT * FROM `dfs`.`hdvm`.`./t.json` LIMIT 100\r\n[30027]Query execution error. Details:[ \r\nDATA_READ ERROR: You tried to write a Map type when you are using a ValueWriter of type SingleMapWriter.\r\n\r\nFile  /mapr/demo.mapr.com/data/hcs/hdvm/t.json\r\nRecord  1\r\nLine  16\r\nColumn  39\r\nField  Pool\r\nLine  16\r\nColumn  39\r\nField  Pool\r\nFragment 0:0\r\n\r\n[Error Id: 13e7a786-1135-410f-a4f0-877eab9222d6 on maprdemo:31010]\r\n]----",
        "Implement Embedded Type An Umbrella for the implementation of Embedded types within Drill."
    ],
    [
        "DRILL-3965",
        "DRILL-3376",
        "Index out of bounds exception in partition pruning Hit IOOB while trying to perform partition pruning on a table that was created using CTAS auto partitioning with the below stack trace.\r\n\r\nCaused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -8\r\n\tat java.lang.String.substring(String.java:1875) ~[na:1.7.0_79]\r\n\tat org.apache.drill.exec.planner.DFSPartitionLocation.<init>(DFSPartitionLocation.java:31) ~[drill-java-exec-1.2.0.jar:1.2.0]\r\n\tat org.apache.drill.exec.planner.ParquetPartitionDescriptor.createPartitionSublists(ParquetPartitionDescriptor.java:126) ~[drill-java-exec-1.2.0.jar:1.2.0]\r\n\tat org.apache.drill.exec.planner.AbstractPartitionDescriptor.iterator(AbstractPartitionDescriptor.java:53) ~[drill-java-exec-1.2.0.jar:1.2.0]\r\n\tat org.apache.drill.exec.planner.logical.partition.PruneScanRule.doOnMatch(PruneScanRule.java:190) ~[drill-java-exec-1.2.0.jar:1.2.0]\r\n\tat org.apache.drill.exec.planner.logical.partition.ParquetPruneScanRule$2.onMatch(ParquetPruneScanRule.java:87) ~[drill-java-exec-1.2.0.jar:1.2.0]\r\n\tat org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:228) ~[calcite-core-1.4.0-drill-r5.jar:1.4.0-drill-r5]",
        "Reading individual files created by CTAS with partition causes an exception Create a table using CTAS with partitioning:\r\n\r\n{code}\r\ncreate table `lineitem_part` partition by (l_moddate) as select l.*, l_shipdate - extract(day from l_shipdate) + 1 l_moddate from cp.`tpch/lineitem.parquet` l\r\n{code}\r\n\r\nThen the following query causes an exception\r\n{code}\r\nselect distinct l_moddate from `lineitem_part/0_0_1.parquet` where l_moddate = date '1992-01-01';\r\n{code}\r\n\r\n\r\nTrace in the log file - \r\n{panel}\r\nCaused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 0\r\n    at java.lang.String.charAt(String.java:658) ~[na:1.7.0_65]\r\n    at org.apache.drill.exec.planner.logical.partition.PruneScanRule$PathPartition.<init>(PruneScanRule.java:493) ~[drill-java-exec-1.1.0-SNAPSHOT.jar:1.1.0-SNAPSHOT]\r\n    at org.apache.drill.exec.planner.logical.partition.PruneScanRule.doOnMatch(PruneScanRule.java:385) ~[drill-java-exec-1.1.0-SNAPSHOT.jar:1.1.0-SNAPSHOT]\r\n    at org.apache.drill.exec.planner.logical.partition.PruneScanRule$4.onMatch(PruneScanRule.java:278) ~[drill-java-exec-1.1.0-SNAPSHOT.jar:1.1.0-SNAPSHOT]\r\n    at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:228) ~[calcite-core-1.1.0-drill-r9.jar:1.1.0-drill-r9]\r\n    ... 13 common frames omitted\r\n{panel}"
    ],
    [
        "DRILL-4044",
        "DRILL-4035",
        "NPE in partition pruning test on (JDK8 + Drill 1.3) NPE reported in drillbit.log for parquet partition pruning test with Drill 1.3 and JDK 8\r\n\r\nFailing query is from test file Functional/partition_pruning/hive/parquet/dynamic_int_partition/data/parquetSelectPartOrMultipleWithDirIN.q\r\n\r\n{code}\r\nselect l_orderkey, l_partkey, l_quantity, cast(l_shipdate as date) l_shipdate, l_shipinstruct, `year` from hive.dynamic_partitions.lineitem_parquet_partitioned_hive where (`year` IN (1993) and l_orderkey>29600) or (`year` IN (1994) and l_orderkey>29700);\r\n{code}\r\n\r\nFrom test output file - dynamicPartitionDirectoryHive-IntPartitionData_parquetSelectPartOrMultipleWithDirIN.output_Fri_Nov_06_00:44:16_UTC_2015\r\n{code}\r\n0       SYSTEM ERROR: NullPointerException\r\n\r\nFragment 0:0\r\n\r\n[Error Id: c6f424ce-10b9-48e1-8783-b0dd281b6fc3 on centos-01.qa.lab:31010]\r\n{code}\r\n\r\nFrom the drillbit.log\r\n\r\n{code}\r\n2015-11-06 00:44:17,222 [29c4081f-628a-fda7-05c5-a70aa9aa148b:foreman] INFO  o.a.d.e.p.l.partition.PruneScanRule - Pruned 7 partitions down to 2\r\n2015-11-06 00:44:17,256 [29c4081f-628a-fda7-05c5-a70aa9aa148b:foreman] INFO  o.a.d.e.p.l.partition.PruneScanRule - No partitions were eligible for pruning\r\n2015-11-06 00:44:17,323 [29c4081f-628a-fda7-05c5-a70aa9aa148b:frag:0:0] INFO  o.a.d.e.w.fragment.FragmentExecutor - 29c4081f-628a-fda7-05c5-a70aa9aa148b:0:0: State change requested AWAITING_ALLOCATION --> FAILED\r\n2015-11-06 00:44:17,323 [29c4081f-628a-fda7-05c5-a70aa9aa148b:frag:0:0] INFO  o.a.d.e.w.fragment.FragmentExecutor - 29c4081f-628a-fda7-05c5-a70aa9aa148b:0:0: State change requested FAILED --> FINISHED\r\n2015-11-06 00:44:17,324 [29c4081f-628a-fda7-05c5-a70aa9aa148b:frag:0:0] ERROR o.a.d.e.w.fragment.FragmentExecutor - SYSTEM ERROR: NullPointerException\r\n\r\nFragment 0:0\r\n\r\n[Error Id: c6f424ce-10b9-48e1-8783-b0dd281b6fc3 on centos-01.qa.lab:31010]\r\norg.apache.drill.common.exceptions.UserException: SYSTEM ERROR: NullPointerException\r\n\r\nFragment 0:0\r\n\r\n[Error Id: c6f424ce-10b9-48e1-8783-b0dd281b6fc3 on centos-01.qa.lab:31010]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:534) ~[drill-common-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.sendFinalState(FragmentExecutor.java:323) [drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.cleanup(FragmentExecutor.java:178) [drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:292) [drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_65]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_65]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_65]\r\n{code}",
        "NPE seen on Functional test run using JDK 8 I am seeing an NPE in the Functional test run using JDK8 and Drill 1.3\r\n\r\nFailing test is : Functional/partition_pruning/hive/parquet/dynamic_hier_intint/data/parquetCount1.q\r\nselect count(*) from hive.dynamic_partitions.lineitem_parquet_partitioned_hive_hier_intint;\r\n\r\n{code}\r\nDrill version was, git.commit.id=e4b94a78\r\n\r\nroot@centos drill-1.3.0]# java -version\r\nopenjdk version \"1.8.0_65\"\r\nOpenJDK Runtime Environment (build 1.8.0_65-b17)\r\nOpenJDK 64-Bit Server VM (build 25.65-b01, mixed mode)\r\n[root@centos drill-1.3.0]# javac -version\r\njavac 1.8.0_65\r\n{code}\r\n\r\n{code}\r\n2015-11-05 01:37:45 INFO  DrillTestJdbc:76 - running test /root/public_framework/drill-test-framework/framework/resources/Functional/window_functions/last_val/lastValFn_9.q 981260622\r\n2015-11-05 01:37:45 INFO  DrillResultSetImpl$ResultsListener:1470 - [#137] Query failed:\r\noadd.org.apache.drill.common.exceptions.UserRemoteException: SYSTEM ERROR: NullPointerException\r\n\r\nFragment 0:0\r\n\r\n[Error Id: cefc7238-a646-4f9a-b4f2-0bd102efe393 on centos-01.qa.lab:31010]\r\n        at oadd.org.apache.drill.exec.rpc.user.QueryResultHandler.resultArrived(QueryResultHandler.java:118)\r\n        at oadd.org.apache.drill.exec.rpc.user.UserClient.handleReponse(UserClient.java:110)\r\n        at oadd.org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:47)\r\n        at oadd.org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:32)\r\n        at oadd.org.apache.drill.exec.rpc.RpcBus.handle(RpcBus.java:61)\r\n        at oadd.org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:233)\r\n        at oadd.org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:205)\r\n        at oadd.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n        at oadd.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n        at oadd.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n        at oadd.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n        at oadd.io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n        at oadd.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)\r\n        at oadd.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)\r\n        at oadd.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)\r\n        at oadd.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\r\n        at oadd.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\r\n        at oadd.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\r\n        at oadd.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\r\n        at java.lang.Thread.run(Thread.java:745)\r\n{code}"
    ],
    [
        "DRILL-4055",
        "DRILL-2618",
        "Query over nested empty directory results in IOB Exception SELECT * over a nested empty directory results in IOB Exception. We need a better error message that states that the directory being queried is empty.\r\n\r\ngit.commit.id=3a73f098\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select * from `nested_dirs/data/parquet`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 18b21dac-199d-4e5f-8a28-7c91723e1b63 on centos-04.qa.lab:31010] (state=,code=0)\r\n\r\nThe below command does not return any results, which is expected since there are no files in the directory, it is empty.\r\n\r\n[root@centos-01 ~]# hadoop fs -ls /tmp/nested_dirs/data/parquet\r\n[root@centos-01 ~]#\r\n\r\nStack trace from drillbit.log\r\n\r\norg.apache.drill.common.exceptions.UserException: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 18b21dac-199d-4e5f-8a28-7c91723e1b63 ]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:534) ~[drill-common-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:187) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:905) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:244) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_85]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_85]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_85]\r\nCaused by: org.apache.calcite.tools.ValidationException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n        at org.apache.calcite.prepare.PlannerImpl.validate(PlannerImpl.java:179) ~[calcite-core-1.4.0-drill-r8.jar:1.4.0-drill-r8]\r\n        at org.apache.calcite.prepare.PlannerImpl.validateAndGetType(PlannerImpl.java:188) ~[calcite-core-1.4.0-drill-r8.jar:1.4.0-drill-r8]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateNode(DefaultSqlHandler.java:447) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateAndConvert(DefaultSqlHandler.java:190) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:159) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:184) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        ... 5 common frames omitted\r\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n        at java.util.ArrayList.rangeCheck(ArrayList.java:635) ~[na:1.7.0_85]\r\n        at java.util.ArrayList.get(ArrayList.java:411) ~[na:1.7.0_85]\r\n        at org.apache.drill.exec.store.dfs.FileSelection.getFirstPath(FileSelection.java:126) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.BasicFormatMatcher.isReadable(BasicFormatMatcher.java:79) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.create(WorkspaceSchemaFactory.java:340) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.create(WorkspaceSchemaFactory.java:155) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.getNewEntry(ExpandingConcurrentMap.java:96) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.get(ExpandingConcurrentMap.java:90) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.getTable(WorkspaceSchemaFactory.java:278) ~[drill-java-exec-1.3.0.jar:1.3.0\r\n{code}",
        "BasicFormatMatcher calls getFirstPath(...) without checking # of paths is not zero {{BasicFormatMatcher.isReadable(...)}} calls {{getFirstPath(...)}} without checking that there is at least one path.  This can cause an IndexOutOfBoundsException.\r\n\r\nTo reproduce, create an empty directory {{/tmp/CaseInsensitiveColumnNames}} and run {{exec/java-exec/src/test/java/org/apache/drill/TestExampleQueries.java}}."
    ],
    [
        "DRILL-4055",
        "DRILL-2775",
        "Query over nested empty directory results in IOB Exception SELECT * over a nested empty directory results in IOB Exception. We need a better error message that states that the directory being queried is empty.\r\n\r\ngit.commit.id=3a73f098\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select * from `nested_dirs/data/parquet`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 18b21dac-199d-4e5f-8a28-7c91723e1b63 on centos-04.qa.lab:31010] (state=,code=0)\r\n\r\nThe below command does not return any results, which is expected since there are no files in the directory, it is empty.\r\n\r\n[root@centos-01 ~]# hadoop fs -ls /tmp/nested_dirs/data/parquet\r\n[root@centos-01 ~]#\r\n\r\nStack trace from drillbit.log\r\n\r\norg.apache.drill.common.exceptions.UserException: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 18b21dac-199d-4e5f-8a28-7c91723e1b63 ]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:534) ~[drill-common-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:187) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:905) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:244) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_85]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_85]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_85]\r\nCaused by: org.apache.calcite.tools.ValidationException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n        at org.apache.calcite.prepare.PlannerImpl.validate(PlannerImpl.java:179) ~[calcite-core-1.4.0-drill-r8.jar:1.4.0-drill-r8]\r\n        at org.apache.calcite.prepare.PlannerImpl.validateAndGetType(PlannerImpl.java:188) ~[calcite-core-1.4.0-drill-r8.jar:1.4.0-drill-r8]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateNode(DefaultSqlHandler.java:447) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateAndConvert(DefaultSqlHandler.java:190) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:159) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:184) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        ... 5 common frames omitted\r\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n        at java.util.ArrayList.rangeCheck(ArrayList.java:635) ~[na:1.7.0_85]\r\n        at java.util.ArrayList.get(ArrayList.java:411) ~[na:1.7.0_85]\r\n        at org.apache.drill.exec.store.dfs.FileSelection.getFirstPath(FileSelection.java:126) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.BasicFormatMatcher.isReadable(BasicFormatMatcher.java:79) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.create(WorkspaceSchemaFactory.java:340) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.create(WorkspaceSchemaFactory.java:155) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.getNewEntry(ExpandingConcurrentMap.java:96) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.get(ExpandingConcurrentMap.java:90) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.getTable(WorkspaceSchemaFactory.java:278) ~[drill-java-exec-1.3.0.jar:1.3.0\r\n{code}",
        "message error not clear when doing a select or ctas on empty folder if you have an empty folder \"emptyfolder\", you get a cryptic error message when you try to query the folder or CTAS a table that has the same name of the empty folder:\r\n{noformat}\r\n0: jdbc:drill:zk=local> select * from emptyfolder;\r\nError: PARSE ERROR: Index: 0, Size: 0\r\n\r\n[Error Id: ef86154b-8219-4b48-84bf-cb318f7d4ae4 on abdel-11.qa.lab:31010] (state=,code=0)\r\n{noformat}\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> create table emptyfolder as select * from `test.json`;\r\nError: SYSTEM ERROR: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n\r\n[Error Id: 1c3965f4-1566-4df6-9bb6-a91211771976 on abdel-11.qa.lab:31010] (state=,code=0)\r\n{noformat}"
    ],
    [
        "DRILL-4055",
        "DRILL-3508",
        "Query over nested empty directory results in IOB Exception SELECT * over a nested empty directory results in IOB Exception. We need a better error message that states that the directory being queried is empty.\r\n\r\ngit.commit.id=3a73f098\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select * from `nested_dirs/data/parquet`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 18b21dac-199d-4e5f-8a28-7c91723e1b63 on centos-04.qa.lab:31010] (state=,code=0)\r\n\r\nThe below command does not return any results, which is expected since there are no files in the directory, it is empty.\r\n\r\n[root@centos-01 ~]# hadoop fs -ls /tmp/nested_dirs/data/parquet\r\n[root@centos-01 ~]#\r\n\r\nStack trace from drillbit.log\r\n\r\norg.apache.drill.common.exceptions.UserException: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 18b21dac-199d-4e5f-8a28-7c91723e1b63 ]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:534) ~[drill-common-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:187) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:905) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:244) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_85]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_85]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_85]\r\nCaused by: org.apache.calcite.tools.ValidationException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n        at org.apache.calcite.prepare.PlannerImpl.validate(PlannerImpl.java:179) ~[calcite-core-1.4.0-drill-r8.jar:1.4.0-drill-r8]\r\n        at org.apache.calcite.prepare.PlannerImpl.validateAndGetType(PlannerImpl.java:188) ~[calcite-core-1.4.0-drill-r8.jar:1.4.0-drill-r8]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateNode(DefaultSqlHandler.java:447) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateAndConvert(DefaultSqlHandler.java:190) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:159) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:184) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        ... 5 common frames omitted\r\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n        at java.util.ArrayList.rangeCheck(ArrayList.java:635) ~[na:1.7.0_85]\r\n        at java.util.ArrayList.get(ArrayList.java:411) ~[na:1.7.0_85]\r\n        at org.apache.drill.exec.store.dfs.FileSelection.getFirstPath(FileSelection.java:126) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.BasicFormatMatcher.isReadable(BasicFormatMatcher.java:79) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.create(WorkspaceSchemaFactory.java:340) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.create(WorkspaceSchemaFactory.java:155) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.getNewEntry(ExpandingConcurrentMap.java:96) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.get(ExpandingConcurrentMap.java:90) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.getTable(WorkspaceSchemaFactory.java:278) ~[drill-java-exec-1.3.0.jar:1.3.0\r\n{code}",
        "IOOB raised if FROM points at an empty directory I union select where the right hand side points to an empty directory files with the following error code:\r\n\r\nError: PARSE ERROR: Index: 0, Size: 0\r\n[Error Id: fed7527a-9d7a-4980-80f5-ff3f0b92d49b on localhost:31010] (state=,code=0)\r\n\r\nWe use this to merge tenant based information and sometimes there are no files on the right hand side of the union. It would be ideal that that scenario would return an empty list rather than an error."
    ],
    [
        "DRILL-4055",
        "DRILL-3813",
        "Query over nested empty directory results in IOB Exception SELECT * over a nested empty directory results in IOB Exception. We need a better error message that states that the directory being queried is empty.\r\n\r\ngit.commit.id=3a73f098\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select * from `nested_dirs/data/parquet`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 18b21dac-199d-4e5f-8a28-7c91723e1b63 on centos-04.qa.lab:31010] (state=,code=0)\r\n\r\nThe below command does not return any results, which is expected since there are no files in the directory, it is empty.\r\n\r\n[root@centos-01 ~]# hadoop fs -ls /tmp/nested_dirs/data/parquet\r\n[root@centos-01 ~]#\r\n\r\nStack trace from drillbit.log\r\n\r\norg.apache.drill.common.exceptions.UserException: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 18b21dac-199d-4e5f-8a28-7c91723e1b63 ]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:534) ~[drill-common-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:187) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:905) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:244) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_85]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_85]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_85]\r\nCaused by: org.apache.calcite.tools.ValidationException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n        at org.apache.calcite.prepare.PlannerImpl.validate(PlannerImpl.java:179) ~[calcite-core-1.4.0-drill-r8.jar:1.4.0-drill-r8]\r\n        at org.apache.calcite.prepare.PlannerImpl.validateAndGetType(PlannerImpl.java:188) ~[calcite-core-1.4.0-drill-r8.jar:1.4.0-drill-r8]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateNode(DefaultSqlHandler.java:447) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateAndConvert(DefaultSqlHandler.java:190) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:159) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:184) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        ... 5 common frames omitted\r\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n        at java.util.ArrayList.rangeCheck(ArrayList.java:635) ~[na:1.7.0_85]\r\n        at java.util.ArrayList.get(ArrayList.java:411) ~[na:1.7.0_85]\r\n        at org.apache.drill.exec.store.dfs.FileSelection.getFirstPath(FileSelection.java:126) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.BasicFormatMatcher.isReadable(BasicFormatMatcher.java:79) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.create(WorkspaceSchemaFactory.java:340) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.create(WorkspaceSchemaFactory.java:155) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.getNewEntry(ExpandingConcurrentMap.java:96) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.get(ExpandingConcurrentMap.java:90) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.getTable(WorkspaceSchemaFactory.java:278) ~[drill-java-exec-1.3.0.jar:1.3.0\r\n{code}",
        "table from directory subtree having no descendent files fails with index error Trying to use as a table a directory subtree that has no descendent files (but zero or more descendent directories) yields what seems to be a partially handled index out-of-bounds condition.  \r\n\r\nFor example, with {{/tmp/empty_directory}} being an empty directory:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> SELECT * FROM `dfs`.`tmp`.`empty_directory`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 747425c9-5350-4813-9f0d-ecf580e15101 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=local> \r\n{noformat}\r\n\r\n\r\nAlso, with {{/tmp/no_child_files_subtree}} having two child directories and a grandchild directory, but not descendent files:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> SELECT * FROM `dfs`.`tmp`.`no_child_files_subtree`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: abc90424-8434-4403-b44b-0ba69ef43151 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=local> \r\n{noformat}\r\n\r\n\r\nA directory subtree having no files was expected to be taken as a table with no rows (and a null schema).\r\n"
    ],
    [
        "DRILL-4055",
        "DRILL-3903",
        "Query over nested empty directory results in IOB Exception SELECT * over a nested empty directory results in IOB Exception. We need a better error message that states that the directory being queried is empty.\r\n\r\ngit.commit.id=3a73f098\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select * from `nested_dirs/data/parquet`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 18b21dac-199d-4e5f-8a28-7c91723e1b63 on centos-04.qa.lab:31010] (state=,code=0)\r\n\r\nThe below command does not return any results, which is expected since there are no files in the directory, it is empty.\r\n\r\n[root@centos-01 ~]# hadoop fs -ls /tmp/nested_dirs/data/parquet\r\n[root@centos-01 ~]#\r\n\r\nStack trace from drillbit.log\r\n\r\norg.apache.drill.common.exceptions.UserException: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 18b21dac-199d-4e5f-8a28-7c91723e1b63 ]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:534) ~[drill-common-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:187) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:905) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:244) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_85]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_85]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_85]\r\nCaused by: org.apache.calcite.tools.ValidationException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n        at org.apache.calcite.prepare.PlannerImpl.validate(PlannerImpl.java:179) ~[calcite-core-1.4.0-drill-r8.jar:1.4.0-drill-r8]\r\n        at org.apache.calcite.prepare.PlannerImpl.validateAndGetType(PlannerImpl.java:188) ~[calcite-core-1.4.0-drill-r8.jar:1.4.0-drill-r8]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateNode(DefaultSqlHandler.java:447) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateAndConvert(DefaultSqlHandler.java:190) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:159) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:184) [drill-java-exec-1.3.0.jar:1.3.0]\r\n        ... 5 common frames omitted\r\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n        at java.util.ArrayList.rangeCheck(ArrayList.java:635) ~[na:1.7.0_85]\r\n        at java.util.ArrayList.get(ArrayList.java:411) ~[na:1.7.0_85]\r\n        at org.apache.drill.exec.store.dfs.FileSelection.getFirstPath(FileSelection.java:126) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.BasicFormatMatcher.isReadable(BasicFormatMatcher.java:79) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.create(WorkspaceSchemaFactory.java:340) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.create(WorkspaceSchemaFactory.java:155) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.getNewEntry(ExpandingConcurrentMap.java:96) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.get(ExpandingConcurrentMap.java:90) ~[drill-java-exec-1.3.0.jar:1.3.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.getTable(WorkspaceSchemaFactory.java:278) ~[drill-java-exec-1.3.0.jar:1.3.0\r\n{code}",
        "Querying empty directory yield internal index-out-of-bounds error Trying to use an empty directory as a table results in an internal IndexOutOfBounds error:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=localhost:2181> SELECT *   FROM `dfs`.`root`.`/tmp/empty_directory`;\r\nError: VALIDATION ERROR: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 66ff61ed-ea41-4af9-87c5-f91480ef1b21 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=localhost:2181> \r\n{noformat}\r\n\r\n\r\n"
    ],
    [
        "DRILL-4102",
        "DRILL-2853",
        "Only one row found in a JSON document that contains multiple items. I tried to analyse a JSON file that had the following (sample) structure:\r\n\r\n{code:json}\r\n{\r\n    \"Key1\": {\r\n      \"htmltags\": \"<htmltag attr1='bravo' /><htmltag attr2='delta' /><htmltag attr3='charlie' />\"\r\n    },\r\n    \"Key2\": {\r\n      \"htmltags\": \"<htmltag attr1='kilo' /><htmltag attr2='lima' /><htmltag attr3='mike' />\"\r\n    },\r\n    \"Key3\": {\r\n      \"htmltags\": \"<htmltag attr1='november' /><htmltag attr2='foxtrot' /><htmltag attr3='sierra' />\"\r\n    }\r\n}\r\n{code}\r\n\r\n(Apologies for the obfuscation, I am unable to publish the original dataset. But the structure is exactly the same. Note especially how the keys and other data points *differ* in some places, and remain identical in others.)\r\n\r\nWhen I run a {code:sql}SELECT * FROM DataFile.json{code} what I get is a single row listed under three columns: {code:html}\"<htmltag attr1='bravo' /><htmltag attr2='delta' /><htmltag attr3='charlie' />\"{code} [i.e., only the entry `Key1.htmltags`] .\r\n\r\nIdeally, I should see three rows, each with entries from Key1..Key3, listed under the correct respective column.",
        "AssertionError: Internal error: while converting `KVGEN`(*) I am seeing an assertion error in the below query that uses KVGEN function. Data in the JSON file was,\r\n\r\n{code}\r\n0: jdbc:drill:> select * from `kvgenData.json`;\r\n+------------+------------+------------+------------+------------+------------+\r\n|    eid     |   ename    |    eage    |  empDept   | empSalary  |  empPhone  |\r\n+------------+------------+------------+------------+------------+------------+\r\n| 12345      | MR WRIGHT  | null       | null       | null       | null       |\r\n| null       | null       | 45         | HR         | null       | null       |\r\n| null       | null       | null       | null       | 50000      | 123-456-6789 |\r\n+------------+------------+------------+------------+------------+------------+\r\n3 rows selected (0.185 seconds)\r\n\r\n{code}\r\n\r\nFailing query\r\n\r\n{code}\r\n0: jdbc:drill:> select kvgen(*) from `kvgenData.json`;\r\nQuery failed: SYSTEM ERROR: Unexpected exception during fragment initialization: Internal error: while converting `KVGEN`(*)\r\n\r\n[13b9f79f-6f24-43d4-9119-2cdae441895f on centos-02.qa.lab:31010]\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nStack trace from drillbit.log \r\n\r\n{code}\r\n\r\n2015-04-23 06:42:15,807 [2ac76bb7-e392-bb0b-c5e3-91dfa98b2c94:foreman] INFO  o.a.drill.exec.work.foreman.Foreman - State change requested.  PENDING --> FAILED\r\norg.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: Internal error: while converting `KVGEN`(*)\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:211) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]\r\nCaused by: java.lang.AssertionError: Internal error: while converting `KVGEN`(*)\r\n        at org.eigenbase.util.Util.newInternal(Util.java:750) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.sql2rel.ReflectiveConvertletTable$2.convertCall(ReflectiveConvertletTable.java:149) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:52) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4099) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:3485) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.sql.SqlCall.accept(SqlCall.java:125) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:3994) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.sql2rel.SqlToRelConverter.convertSelectList(SqlToRelConverter.java:3301) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:519) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:474) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:2657) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:432) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at net.hydromatic.optiq.prepare.PlannerImpl.convert(PlannerImpl.java:190) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToRel(DefaultSqlHandler.java:166) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:133) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:155) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:770) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:202) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        ... 3 common frames omitted\r\nCaused by: java.lang.reflect.InvocationTargetException: null\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.7.0_75]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[na:1.7.0_75]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_75]\r\n        at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_75]\r\n        at org.eigenbase.sql2rel.ReflectiveConvertletTable$2.convertCall(ReflectiveConvertletTable.java:139) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        ... 19 common frames omitted\r\nCaused by: java.lang.AssertionError: Internal error: invariant violated: conversion result not null\r\n        at org.eigenbase.util.Util.newInternal(Util.java:735) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.util.Util.permAssert(Util.java:843) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:3995) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.sql2rel.StandardConvertletTable.convertExpressionList(StandardConvertletTable.java:731) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.sql2rel.StandardConvertletTable.convertFunction(StandardConvertletTable.java:593) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        ... 24 common frames omitted\r\n\r\n{code}"
    ],
    [
        "DRILL-4145",
        "DRILL-4140",
        "IndexOutOfBoundsException raised during select * query on S3 csv file When trying to query (via sqlline or WebUI) a .csv file I am getting an IndexOutofBoundsException:\r\n{noformat} 0: jdbc:drill:> select * from s3data.root.`staging/data/apps1-bad.csv` limit 1;\r\nError: SYSTEM ERROR: IndexOutOfBoundsException: index: 16384, length: 4 (expected: range(0, 16384))\r\n\r\nFragment 0:0\r\n\r\n[Error Id: be9856d2-0b80-4b9c-94a4-a1ca38ec5db0 on ip-XXXXX.compute.internal:31010] (state=,code=0)\r\n0: jdbc:drill:> select * from s3data.root.`staging/data/apps1.csv` limit 1;\r\n+----------+----------------------+----------+----------+----------+------------+----------+------------+----------+--------------+-----------+----------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+----------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\r\n| FIELD_1  |       FIELD_2        | FIELD_3  | FIELD_4  | FIELD_5  |  FIELD_6   | FIELD_7  |  FIELD_8   | FIELD_9  |   FIELD_10   | FIELD_11  |       FIELD_12       | FIELD_13  | FIELD_14  | FIELD_15  | FIELD_16  | FIELD_17  | FIELD_18  | FIELD_19  |       FIELD_20       | FIELD_21  | FIELD_22  | FIELD_23  | FIELD_24  | FIELD_25  | FIELD_26  | FIELD_27  | FIELD_28  | FIELD_29  | FIELD_30  | FIELD_31  | FIELD_32  | FIELD_33  | FIELD_34  | FIELD_35  |\r\n+----------+----------------------+----------+----------+----------+------------+----------+------------+----------+--------------+-----------+----------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+----------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\r\n| 489517   | 27/10/2015 02:05:27  | 261      | 1130232  | 0        | 925630488  | 0        | 925630488  | -1       | 19531580547  | 00000000  | 27/10/2015 02:00:00  |           | 30        | 300       | 0         | 0         | 00000000  | 00000000  | 27/10/2015 02:05:27  | 0         | 1         | 0         | 35.0      |           |           |           | 505       | 872.0     |           | aBc       |           |           |           |           |\r\n+----------+----------------------+----------+----------+----------+------------+----------+------------+----------+--------------+-----------+----------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+----------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\r\n1 row selected (1.094 seconds)\r\n0: jdbc:drill:>  {noformat}\r\n\r\nGood file: apps1.csv, and \r\nBad file: apps1-bad.csv  attached.\r\n",
        "csv extractHeader: fails on multiple empty columns When the extractHeader feature is enabled and several columns in a row are empty, an exception occurs.\r\n\r\n{code:title=csv_with_header.csvh|borderStyle=solid}\r\nname,num1,num2,num3\r\nhello,1,2,3\r\nhello,,,\r\nhello,1,2,3\r\nhello,1,2,3\r\n{code}\r\n\r\n{code:java}\r\nSELECT * FROM dfs.`/tmp/csv_with_header.csvh`;\r\njava.lang.NegativeArraySizeException\r\n        at org.apache.drill.exec.vector.VarCharVector$Accessor.get(VarCharVector.java:450)\r\n        at org.apache.drill.exec.vector.accessor.VarCharAccessor.getBytes(VarCharAccessor.java:125)\r\n        at org.apache.drill.exec.vector.accessor.VarCharAccessor.getString(VarCharAccessor.java:146)\r\n        at org.apache.drill.exec.vector.accessor.VarCharAccessor.getObject(VarCharAccessor.java:136)\r\n        at org.apache.drill.exec.vector.accessor.VarCharAccessor.getObject(VarCharAccessor.java:94)\r\n        at org.apache.drill.exec.vector.accessor.BoundCheckingAccessor.getObject(BoundCheckingAccessor.java:148)\r\n        at org.apache.drill.jdbc.impl.TypeConvertingSqlAccessor.getObject(TypeConvertingSqlAccessor.java:795)\r\n        at org.apache.drill.jdbc.impl.AvaticaDrillSqlAccessor.getObject(AvaticaDrillSqlAccessor.java:179)\r\n        at net.hydromatic.avatica.AvaticaResultSet.getObject(AvaticaResultSet.java:351)\r\n        at org.apache.drill.jdbc.impl.DrillResultSetImpl.getObject(DrillResultSetImpl.java:401)\r\n        at sqlline.Rows$Row.<init>(Rows.java:157)\r\n        at sqlline.IncrementalRows.hasNext(IncrementalRows.java:63)\r\n        at sqlline.TableOutputFormat$ResizingRowsProvider.next(TableOutputFormat.java:87)\r\n        at sqlline.TableOutputFormat.print(TableOutputFormat.java:118)\r\n        at sqlline.SqlLine.print(SqlLine.java:1593)\r\n        at sqlline.Commands.execute(Commands.java:852)\r\n        at sqlline.Commands.sql(Commands.java:751)\r\n        at sqlline.SqlLine.dispatch(SqlLine.java:746)\r\n        at sqlline.SqlLine.begin(SqlLine.java:621)\r\n        at sqlline.SqlLine.start(SqlLine.java:375)\r\n        at sqlline.SqlLine.main(SqlLine.java:268)\r\n{code}\r\n\r\nAs a workaround an empty column can be added to each line (inluding the header)."
    ]
]