[
    [
        "ACCUMULO-12",
        "ACCUMULO-11",
        "Unique server id not constructed consistently In LiveTserverSet.scanServers() the server address is read from zookeeper, then a session id id read from zookeeper.  These two pieces of info are combined to form a unique server id.  Both pieces of info are read from the minimum ephemeral node.  However, there is no guarantee the minimum node is the same node for the two reads.",
        "unique server id is not constructed consistently In LiveTServerSet.scanServers() the server address is read from zookeeper, then a session is read from zookeeper.  Both of these red data from the minimum ephemeral node. However, there is no guarantee the minimum node is the same node for the two reads.\r\n "
    ],
    [
        "ACCUMULO-109",
        "ACCUMULO-5",
        "KilledTabletServerSplit functional test fails Sometime the test times out.  Looking at the logs, I see\r\n\r\n{noformat}\r\njava.lang.RuntimeException: java.lang.IllegalStateException: Existing time 270 >= 270\r\n...\r\n{noformat}\r\n\r\n\r\nThis is just after the recovery of the root tablet.\r\n",
        "Log recovery fails with IllegalStateException Sometimes a minor compaction will finish successfully, but the process will die before the compaction finish event is written to the write ahead log.  Recovery attempts to handle this case by looking at what files the tablet has and comparing those with compaction start events.  This check is failing because compaction start events have absolute paths and the recovery code passes in table relative paths.  So the absolute paths and relative paths never match up.\r\n\r\nWhen this occurs, the logical time code will throw an illegal state exception because the recovered data was not newer than the existing data.  An exception like the following will occur and the tablet will fail to load.\r\n    \r\n    IllegalStateException: existing time 19867 >= 19866"
    ],
    [
        "ACCUMULO-171",
        "ACCUMULO-5",
        "java.lang.illegalStateException: existing time 14981 >= 14981 When trying to recover accumulo after a crash, we get the following exception when the server is trying to load the !METADATA tablet:\r\n\r\njava.lang.illegalStateException: existing time 14981 >= 14981\r\n\r\nAccumulo doesn't recover successfully and keeps trying to load the !METADATA table unsuccessfully with the same exception.",
        "Log recovery fails with IllegalStateException Sometimes a minor compaction will finish successfully, but the process will die before the compaction finish event is written to the write ahead log.  Recovery attempts to handle this case by looking at what files the tablet has and comparing those with compaction start events.  This check is failing because compaction start events have absolute paths and the recovery code passes in table relative paths.  So the absolute paths and relative paths never match up.\r\n\r\nWhen this occurs, the logical time code will throw an illegal state exception because the recovered data was not newer than the existing data.  An exception like the following will occur and the tablet will fail to load.\r\n    \r\n    IllegalStateException: existing time 19867 >= 19866"
    ],
    [
        "ACCUMULO-171",
        "ACCUMULO-109",
        "java.lang.illegalStateException: existing time 14981 >= 14981 When trying to recover accumulo after a crash, we get the following exception when the server is trying to load the !METADATA tablet:\r\n\r\njava.lang.illegalStateException: existing time 14981 >= 14981\r\n\r\nAccumulo doesn't recover successfully and keeps trying to load the !METADATA table unsuccessfully with the same exception.",
        "KilledTabletServerSplit functional test fails Sometime the test times out.  Looking at the logs, I see\r\n\r\n{noformat}\r\njava.lang.RuntimeException: java.lang.IllegalStateException: Existing time 270 >= 270\r\n...\r\n{noformat}\r\n\r\n\r\nThis is just after the recovery of the root tablet.\r\n"
    ],
    [
        "ACCUMULO-230",
        "ACCUMULO-208",
        "problems found in accumulo_sample I've been working on a presentation on the accumulo sample, which will show off some of the accumulo-specific features.  But when I ran it, I found very poor performance on some queries, and inconsistent results when I changed the user's authorizations.\r\n\r\nSome queries like \"three little pigs\" ran with sub-second response times.  Other queries such as \"old man sea\" would take as long as 8 minutes.\r\n\r\nIf I searched english-only articles, I would get more results than if I searched all the samples.  Again, this only happens on some queries: the majority ran fine.  \"old man sea\" would consistently show broken behavior.",
        "Fix accumulo_sample accumulo_sample has some compilation problems and its usage of the accumulo client API needs to be made consistent with the recommended usage."
    ],
    [
        "ACCUMULO-240",
        "ACCUMULO-225",
        "client code for TableConfiguration has problems There are a number of problems with accessing TableConfiguration object from client code, the first being that TableOperationImpl uses code that relies on a hidden HDFSZooInstance in client configuration. If the client is not running with access to the HDFS Accumulo directory, or if the HDFS setup on the client differs from the Accumulo instance, then this call will fail:\r\n\r\n{noformat}\r\nException in thread \"main\" java.lang.ExceptionInInitializerError\r\nCaused by: org.apache.accumulo.core.client.impl.HdfsZooInstance$AccumuloNotInitializedException: Accumulo not initialized, there is no instance id at /accumulo/instance_id\r\n\tat org.apache.accumulo.core.client.impl.HdfsZooInstance._getInstanceID(HdfsZooInstance.java:136)\r\n\tat org.apache.accumulo.core.client.impl.HdfsZooInstance.getInstanceID(HdfsZooInstance.java:123)\r\n\tat org.apache.accumulo.core.conf.ZooConfiguration.getInstanceId(ZooConfiguration.java:65)\r\n\tat org.apache.accumulo.core.conf.ZooConfiguration.iterator(ZooConfiguration.java:132)\r\n\tat org.apache.accumulo.core.conf.TableConfiguration.iterator(TableConfiguration.java:129)\r\n\tat org.apache.accumulo.core.conf.ConfigSanityCheck.validate(ConfigSanityCheck.java:29)\r\n\tat org.apache.accumulo.core.conf.AccumuloConfiguration.getTableConfiguration(AccumuloConfiguration.java:150)\r\n\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.getProperties(TableOperationsImpl.java:544)\r\n\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.setLocalityGroups(TableOperationsImpl.java:583)\r\n{noformat}\r\n\r\nAnother problem is that the ZooConfiguration object uses static instance information, negating the possibility of using multiple instances in the same JVM.",
        "Setting Locality Groups via the API throws an exception If a client attempts to set locality groups via the API and the client does not have core-site.xml on its classpath, the following exception is thrown after the groups are actually successfully set.\r\n\r\nException in thread \"main\" org.apache.accumulo.core.client.impl.HdfsZooInstance$AccumuloNotInitializedException: Accumulo not initialized, there is no instance id at /accumulo/instance_id\r\n\r\nThe exception is confusing because:\r\n1) the group(s) are actually created successfully\r\n2) the exception misleads the user to believe there is something wrong with Accumulo's configuration where the problem is that the client isn't actually connecting to HDFS properly because it is missing core-site.xml on its classpath.\r\n\r\n"
    ],
    [
        "ACCUMULO-298",
        "ACCUMULO-58",
        "bad table config prevented table deletion, safe shutdown create a table with a bad aggregator (did this by running the wikisearch ingester, but it had old package names) using zookeeper settings; attempt to use the tables, see the table errors.  Then try to delete the table (hangs) or shutdown (hangs).  Upon restart, the tables were successfully deleted.",
        "Misconfigured aggregator can block table deletion If you misconfigure an aggregator such that it can't be loaded, it will cause a tablet to indefinitely attempt to minor/major compact. If you attempt to delete the table with this problem the tablet will never break out of the compaction attempt to a point where it will detect it needs to delete. The only way to break out of the loop is kill any tservers who are hosting tablets attempts to be compacted. This definately exists in 1.3, fairly confident it affects the other versions as well. We would need to have the compaction loops check somehow to detect if the tablet needs to be deleted before reattempting.\r\n\r\nAdditionally, a functional test should be written to exercise this error to prevent regression."
    ],
    [
        "ACCUMULO-545",
        "ACCUMULO-258",
        "Write ahead assignment is not rack aware. When write ahead logs are assigned, its possible that all write ahead loggers for a tablet server could be on the same rack.  There is no way for a user to prefer off rack walogs.",
        "Improve logger distribution Current logger assignemnt is currently current node + proceeding node. We should support some form of rack awareness to help improve distribution."
    ],
    [
        "ACCUMULO-592",
        "ACCUMULO-364",
        "We lost the security policy example In our moving of the configuration to example folders we seem to have inadvertently blown away the security policy example file. We should have that back for those who want/need to use it.",
        "Security Policy file should be an example Currently there is a security policy file which has been tuned for a somewhat specific configuration. As of 1.4, we've designed it to fail open, such that no file means open security policy. We should change the default policy file to an example file, this way we can be more confident the system works out of the box. Then users can go about renaming to begin using the security policy should they chose to do so."
    ],
    [
        "ACCUMULO-651",
        "ACCUMULO-360",
        "Key constructor, Column Visibility can corrupt a table The Key constructor does not check the integrity of the column visibility field which can cause some corruption in a table.  For example the following code will generate a valid Key that can be written to a table\r\n\r\nKey k = new Key(new Text(\"BOB\"), new Text(\"CF\"), new Text(\"CQ\"), new Text(\"public,private\"), System.currentTimeMillis());\r\n\r\nand that key can be used as output (in my case a mapreduce job).\r\n\r\nDuring scan operations the following exception is thrown...\r\n\r\nexception while scanning tablet a<<\r\n\torg.apache.accumulo.core.util.BadArgumentException: bad character (44) near index 6\r\n\tpublic,private\r\n\t      ^\r\n\t\tat org.apache.accumulo.core.security.ColumnVisibility$ColumnVisibilityParser.parse_(ColumnVisibility.java:238)\r\n\t\tat org.apache.accumulo.core.security.ColumnVisibility$ColumnVisibilityParser.parse(ColumnVisibility.java:157)\r\n\t\tat org.apache.accumulo.core.security.ColumnVisibility.validate(ColumnVisibility.java:257)\r\n\t\tat org.apache.accumulo.core.security.ColumnVisibility.<init>(ColumnVisibility.java:311)\r\n\t\tat org.apache.accumulo.core.security.ColumnVisibility.<init>(ColumnVisibility.java:279)\r\n\t\tat org.apache.accumulo.core.iterators.system.VisibilityFilter.accept(VisibilityFilter.java:70)\r\n\t\tat org.apache.accumulo.core.iterators.Filter.findTop(Filter.java:72)\r\n\t\tat org.apache.accumulo.core.iterators.Filter.seek(Filter.java:65)\r\n\t\tat org.apache.accumulo.core.iterators.system.SourceSwitchingIterator.readNext(SourceSwitchingIterator.java:116)\r\n\t\tat org.apache.accumulo.core.iterators.system.SourceSwitchingIterator.seek(SourceSwitchingIterator.java:168)\r\n\t\tat org.apache.accumulo.server.tabletserver.Tablet.nextBatch(Tablet.java:1741)\r\n\t\tat org.apache.accumulo.server.tabletserver.Tablet.access$3200(Tablet.java:143)\r\n\t\tat org.apache.accumulo.server.tabletserver.Tablet$Scanner.read(Tablet.java:1883)\r\n\t\tat org.apache.accumulo.server.tabletserver.TabletServer$ThriftClientHandler$NextBatchTask.run(TabletServer.java:905)\r\n\t\tat org.apache.accumulo.cloudtrace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n\t\tat java.lang.Thread.run(Thread.java:662)\r\n",
        "Bulk importing Keys with invalid ColumnVisibility doesn't fail until scan A Key is allowed to have an invalid ColumnVisibility so it can be used in a Range (see ACCUMULO-193).  Also, we don't want Key to create a ColumnVisibility object to test the validity of a supplied Text, CharSequence, or byte[] visibility due to the large amount of overhead in doing so.  This isn't a problem when writing Mutations to Accumulo, but during bulk import Keys are written directly to files.  Thus the user doesn't receive an error for the invalid ColumnVisibility until scanning the table."
    ],
    [
        "ACCUMULO-719",
        "ACCUMULO-682",
        "Failure compiling native libraries on Mac OSX 10.7.4 Native libraries can't be compiled on Mac OSX 10.7.4, the Java paths have changed. Recently newer versions of xcode starting at 4.3 have removed the need for /Developer, which the current make file looks for Java header files.",
        "Native library compiling fails on OS X 10.7 (Lion) The makefiles for building the native libraries require the Java headers (jni.h, etc) The Makefile in src/server/src/main/c++/nativeMap points to \r\n\r\n/Developer/SDKs/MacOSX10.6.sdk/System/Library/Frameworks/JavaVM.framework/Versions/A/Headers\r\n\r\nThe correct directory to include that works for both 10.6 and 10.7 is\r\n\r\n/System/Library/Frameworks/JavaVM.framework/Headers\r\n\r\nthe Makefile in the mlock directory is including this directory correctly.\r\n\r\n"
    ],
    [
        "ACCUMULO-773",
        "ACCUMULO-453",
        "Server Activity visualization is broken The visualization servlet is broken.",
        "Profile and optimize server activity visualization The script reads the monitor xml periodically.  I created a fake xml page with a lot of servers to see how it would work, and it slowed down considerably.  At the very least, it should read the xml less often (perhaps every 5 seconds) and I'd like to lower the memory usage as well.  JSON may be a better alternative."
    ],
    [
        "ACCUMULO-816",
        "ACCUMULO-750",
        "Make listscans output act like an Accumulo table The listscans command produces a lot of useful output that may be hard for a human to process. The command should have a --csv option that produces csv that could be read by spread sheets and tools like sqlite for post processing.  I think there are at least two csv standards.  An Excel one and a standard one. Not sure which format we should produce, maybe both.  \r\n\r\nSee discussion on ACCUMULO-511",
        "Clean up listscans output This needs to be made more readable.\r\nSome options might include making the output span multiple lines, paginate, truncate, use indentation or column headers, ASCII borders, etc."
    ],
    [
        "ACCUMULO-828",
        "ACCUMULO-827",
        "CloudStone5 succeeds improprerly CloudStone5 asserts nothing, which means that when the createtable command returns a non-zero command because it does nothing, the test succeeds when it should fail.",
        "CloudStone Code Has Issues A couple of things:\r\n\r\n# ClassNotFound on AccumuloOutputFormat http://mail-archives.apache.org/mod_mbox/accumulo-user/201210.mbox/%3CCAM+ZXRmJkpK9Piako+nRfyYfP5JyKX7qgQCJRcNe+2eNC2uJkw@mail.gmail.com%3E\r\n# ClassNotFound for cloudtrace for me (probably the same thing as above)\r\n# Test succeeded for me despite the mapreduce job failing."
    ],
    [
        "ACCUMULO-888",
        "ACCUMULO-734",
        "NPE writing to the WAL \r\nI missconfigured the WAL max size to \"4\" and got this strange error:\r\n\r\n{noformat}\r\n2012-11-29 12:12:55,610 [log.TabletServerLogger] ERROR: Unexpected error writing to log, retrying attempt 5\r\njava.lang.NullPointerException\r\n        at org.apache.accumulo.server.tabletserver.log.RemoteLogger.logManyTablets(RemoteLogger.java:145)\r\n        at org.apache.accumulo.server.tabletserver.log.TabletServerLogger$6.write(TabletServerLogger.java:393)\r\n        at org.apache.accumulo.server.tabletserver.log.TabletServerLogger.write(TabletServerLogger.java:298)\r\n        at org.apache.accumulo.server.tabletserver.log.TabletServerLogger.logManyTablets(TabletServerLogger.java:382)\r\n        at org.apache.accumulo.server.tabletserver.TabletServer$ThriftClientHandler.flush(TabletServer.java:1539)\r\n        at org.apache.accumulo.server.tabletserver.TabletServer$ThriftClientHandler.closeUpdate(TabletServer.java:1610)\r\n{noformat}",
        "look into merging cloudtrace/htrace See [HBASE-6524|http://issues.apache.org/jira/browse/hbase-6524]"
    ],
    [
        "ACCUMULO-911",
        "ACCUMULO-889",
        "Get ClassNotFoundException when running Accumulo 1.4.0 Bulk Ingest Example Trying to follow the steps in the README.bulkIngest for Accumulo 1.4.0 but get a ClassNotFound Exception.  Has anyone else seen this?",
        "Update jar regex in examples and scripts I tried running the example in README.mapred for 1.4.2 and got an error on the command\r\n{noformat}\r\n$ bin/tool.sh lib/examples-simple*[^c].jar ...\r\nException in thread \"main\" java.lang.ClassNotFoundException: lib.examples-simple-1.4.2-sources.jar\r\n{noformat}\r\n\r\nThe jar regex need to be updated to exclude the sources jar (currently it just excludes the javadoc jar).  We should check other jar regex to make sure they're up to date as well."
    ],
    [
        "ACCUMULO-912",
        "ACCUMULO-761",
        "getConnector in MockAccumulo clobber's existing user authorizations If a user exists in MockAccumulo with a set of authorizations, and you do a getConnector for the user, it will clobber that user's authorizations and remake them. It should do a check before creating the internal user object. On top of that, it should also check the password if the user does exist.",
        "Authorizations lost on MockInstance Steps to reproduce: \r\n1) Create a MockInstance variable. \r\n2) change the current user auths using securityOperations().\r\n3) Pass the reference to an interfaced-declared 'Instance' variable type in another class.\r\n4) calling getAuthorizations for the same user returns a blank string \"\".\r\n\r\nI noticed within testNG and stepping through the IntellJ debugger."
    ],
    [
        "ACCUMULO-959",
        "ACCUMULO-97",
        "compactions starving METADATA table scans A large tablet had many files, and the compaction was taking a long time.  During this time, it reserved so many readers that the scan of a metadata  tablet on the same host was starved of readers, causing scans to back up.\r\n",
        "!METADATA scans could be delayed by regular scans The tablet server only allows scans to open a max number of files.  Once the max is reached, scans wait.  Its possible that a !METADATA scan could get stuck waiting for a long running non !METADATA scan to release files.  This could slow down the whole system.  !METADATA scans wold not starve because a fair semaphore is used.  However with client timeouts, starvation may be possible."
    ],
    [
        "ACCUMULO-1062",
        "ACCUMULO-893",
        "large numbers of threads make in-memory updates slow Using hundreds of ingest programs, each running several batch writers, ingest would eventually become slow.  Testing (attached) proved that performance fell off as the number of threads increased.\r\n",
        "splitting and hold time are interacting badly Testing on a medium sized cluster using the continuous ingest test.\r\n\r\nI pre-split the table and started ingest.  I got tired of waiting for the tablets to split again so I lowered the split threshold to 100M.\r\n\r\nThe tablet servers started minor compaction in preparation for the split; memory became held.. and then nothing would happen until this error came up:\r\n\r\n{noformat}\r\n2012-12-06 12:57:02,025 [thrift.TabletClientService$Processor] ERROR: tserver:r01n16 Internal error processing closeUpdate\r\norg.apache.accumulo.server.tabletserver.HoldTimeoutException: Commits are held\r\n        at org.apache.accumulo.server.tabletserver.TabletServerResourceManager.waitUntilCommitsAreEnabled(TabletServerResourceManager.java:386)\r\n        at org.apache.accumulo.server.tabletserver.TabletServer$ThriftClientHandler.flush(TabletServer.java:1469)\r\n        at org.apache.accumulo.server.tabletserver.TabletServer$ThriftClientHandler.closeUpdate(TabletServer.java:1610)\r\n        at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n        at java.lang.reflect.Method.invoke(Method.java:597)\r\n        at org.apache.accumulo.cloudtrace.instrument.thrift.TraceWrap$1.invoke(TraceWrap.java:59)\r\n        at $Proxy2.closeUpdate(Unknown Source)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor$closeUpdate.process(TabletClientService.java:2338)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor.process(TabletClientService.java:2037)\r\n        at org.apache.accumulo.server.util.TServerUtils$TimedProcessor.process(TServerUtils.java:154)\r\n        at org.apache.thrift.server.TNonblockingServer$FrameBuffer.invoke(TNonblockingServer.java:631)\r\n        at org.apache.accumulo.server.util.TServerUtils$THsHaServer$Invocation.run(TServerUtils.java:202)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        at java.lang.Thread.run(Thread.java:662)\r\n{noformat}\r\n\r\nThe tservers appeared to make no progress on splitting until I stopped the ingesters.\r\n\r\n"
    ],
    [
        "ACCUMULO-1071",
        "ACCUMULO-429",
        "Finding ACCUMULO_HOME and programs in scripts is broken for symbolic links Finding the home $bin location for the accumulo/bin scripts needs to be iterative to traverse symbolic links.  Also, using 'locationByProgram' within bin/accumulo doesn't work for symbolic links.",
        "Clean up startup scripts We do a fair amount of craziness to resolve paths, symlinks, etc. in our startup scripts. And we lack consistancy in doing so. We should go through our startup scripts and have some consistancy with out we resolve paths and make sure things work properly with relative paths, absolute paths, symlinks, and any other path craziness people can think of."
    ],
    [
        "ACCUMULO-1105",
        "ACCUMULO-3659",
        "random walk test is failing All random walk tests are failing:\r\n\r\n{noformat}\r\n by: ThriftSecurityException(user:root, code:BAD_CREDENTIALS)\r\n        at org.apache.accumulo.core.client.impl.thrift.ClientService$authenticateUser_result$authenticateUser_resultStandardScheme.read(ClientService.java:8039)\r\n        at org.apache.accumulo.core.client.impl.thrift.ClientService$authenticateUser_result$authenticateUser_resultStandardScheme.read(ClientService.java:8017)\r\n        at org.apache.accumulo.core.client.impl.thrift.ClientService$authenticateUser_result.read(ClientService.java:7961)\r\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)\r\n        at org.apache.accumulo.core.client.impl.thrift.ClientService$Client.recv_authenticateUser(ClientService.java:333)\r\n\r\n{noformat}\r\n",
        "random walk test fails with authentication problems None of the tests run. They die with \r\n\r\n{noformat}\r\n by: ThriftSecurityException(user:root, code:BAD_CREDENTIALS)\r\n        at org.apache.accumulo.core.client.impl.thrift.ClientService$authenticateUser_result$authenticateUser_resultStandardScheme.read(ClientService.java:8039)\r\n        at org.apache.accumulo.core.client.impl.thrift.ClientService$authenticateUser_result$authenticateUser_resultStandardScheme.read(ClientService.java:8017)\r\n        at org.apache.accumulo.core.client.impl.thrift.ClientService$authenticateUser_result.read(ClientService.java:7961)\r\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)\r\n        at org.apache.accumulo.core.client.impl.thrift.ClientService$Client.recv_authenticateUser(ClientService.java:333)\r\n{noformat}\r\n"
    ],
    [
        "ACCUMULO-1260",
        "ACCUMULO-114",
        "config shell command exposes overridden trace.password This may affect multiple versions. I've only looked on 1.4.1. When I run the 'config' shell command, the overridden trace.password value is displayed instead of displaying asterisks.",
        "hide passwords when logging the configuration at start-up Hide the system password from the log.  It's not that much help in debugging anyhow."
    ],
    [
        "ACCUMULO-1278",
        "ACCUMULO-1268",
        "ZooKeeperInstance sessionTimeout > 63 blocks forever When I construct a ZooKeeperInstance with valid ZK config but no running Accumulo servers, and provide the sessionTimeout parameter that the docs say is in millis, for timeouts <= 63, it fails quickly, but any number >= 64 apparently blocks forever.  Certainly more than about 30 seconds.",
        "add client wide timeout setting Currently if Accumulo is down, getConnector will sit indefinitely. This is designed to be tolerant in several processes which wait for Accumulo to come up into a stable state.\r\n\r\nHowever, for writing client applications, it's horrendous. It provides no ability for the developer to provide feedback to the client. There should be some method to allow ServerClient.executeRaw() to eventually kick out of the infinite loop it currently has. If the developer wants the code to continue infinitely, they can either disregard the flag or wrap the getConnector() code in a loop themselves.\r\n\r\nMore generally it would be nice if the user could configure a timeout that would be honored by all client API calls."
    ],
    [
        "ACCUMULO-1473",
        "ACCUMULO-1432",
        "FindOfflineTablets is broken org.apache.accumulo.server.util.FindOfflineTablets does not work.  ",
        "FindOfflineTablets requires 4 arguments but only uses 2 org.apache.accumulo.server.util.FindOfflineTablets checks for 4 arguments in its main, but only uses 2."
    ],
    [
        "ACCUMULO-1508",
        "ACCUMULO-305",
        "Improve MajC logging MajC logging prints messages when a compaction starts and finishes, but it doesn't provide any mechanism to match the start/end. It would be useful to include a table extent or fate ID or both or even just a one-up counter in the logs to add some value.",
        "Log context before starting major compaction Need to log the tablet (and possible files that will be compacted) before starting a major compaction"
    ],
    [
        "ACCUMULO-1568",
        "ACCUMULO-1067",
        "Make configuration changes Atomic I have not seen this issue, but its something I thought of.  Assume the following happens.\r\n\r\n * iterators I1 ... I10 are configured to Table T1 at time 1\r\n * iterators NI1 ... NI10 are configured to Table T1 at time 2\r\n * compaction is reading iterator config and reads I1 ... I5  and NI4 ... NI9, a combination of the two iterator configurations w/o NI10, this could be really bad.\r\n\r\nSeems like this could be solved by use of Zookeeper transactions.   The Accumulo API will need to change to allow setting many iterators or config settings at once.\r\n \r\n\r\n ",
        "InstanceOperations.setProperty / getSystemConfiguration inconsistent state Performing an InstanceOperations.setProperty followed by an InstanceOperations.getSystemConfiguration can show that the property set wasn't actually set. This is most likely due to the fact that getSystemConfiguration contacts a random tserver to get the configuration, and that request could be received before the pub/sub event from Zookeeper is received.  Potential solution is to have this method contact the Master directly."
    ],
    [
        "ACCUMULO-1615",
        "ACCUMULO-1610",
        "'service accumulo-tserver stop' does not work Started accumulo with 'service accumulo-tserver start'\r\n\r\nTried to stop accumulo with 'service accumulo-tserver stop'\r\n\r\nFails with org.apache.hadoop.security.AccessControlException as it tries to \r\nstop with user 'root' and not 'ACCUMULO_USER'. \r\n",
        "service accumulo-tserver stop runs as the wrong user if you're running accumulo as its own user and have hdfs permissions for /accumulo locked down to only that user, `service accumulo stop` throws an error that root can't touch /accumulo.  it looks like the cause is that $TARGET_USER isn't set to anything in the init.d script's stop) path."
    ],
    [
        "ACCUMULO-1619",
        "ACCUMULO-1485",
        "Provide a way to override parent configuration with a delete Per-table configuration options are inherited from a hierarchy of configuration scopes. In some cases, it would be useful to denote a key as \"do not inherit from the parent\" (treat it as though it doesn't exist, possibly falling back on the default if the property is defined in the default configuration).\r\n\r\nIn such cases, a property key could be prefixed with a \"!\" character (as in \"NOT\") to \"unset\" a key that is set with a value in a parent scope.\r\n\r\nThis ticket removes the previously undocumented restriction that empty string is the same as a property not being set at all.",
        "Allow overriding system and per-table configuration with a delete Currently, there's no way to override a per-table configuration property stored in the site.xml file on a per-table basis.\r\n\r\nThis could be done with the syntax:\r\n!configurationkey\r\n"
    ],
    [
        "ACCUMULO-1647",
        "ACCUMULO-1519",
        "CloudStone tests broken for 1.4.3 1.4.3 doesn't care for the -f flag in deletetable calls, appears related to ACCUMULO-897\r\n\r\nQuick review of 1.4.x Jenkins workspace suggests -f flag has been removed, but downloaded src and dist tarballs still contain it.\r\n",
        "CloudStone tests broken for < 1.5 Summary: \r\nforce option for deletetable shell operation added in 1.5, CloudStone python benchmark change ported to 1.4.3 which tries to call the -f option, but results in error.\r\n\r\n\r\n\r\nLonger:\r\nForce command (\"f\" was added to the deletetable shell command @ \r\nhttps://github.com/apache/accumulo/commit/6abb3e5ccb533adff0ae715dc3eb0c4a56ed4874#L9L67\r\n1.4.3 tagged version is \r\nhttps://github.com/apache/accumulo/blob/1.4.3/src/core/src/main/java/org/apache/accumulo/core/util/shell/commands/DeleteTableCommand.java\r\n\r\nThe CloudStone python scripts in versions < 1.5 send the force command, which results in [shell.Shell] ERROR: org.apache.commons.cli.UnrecognizedOptionException: Unrecognized option: -f\r\n\r\nThis appears to have been the result of Accumulo-897 - https://github.com/apache/accumulo/commit/736c230ad1f3e106fa1713323545ee568251033f#test/system/bench/lib/IngestBenchmark.py\r\n\r\n"
    ],
    [
        "ACCUMULO-1649",
        "ACCUMULO-734",
        "investigate replacing cloudtrace with Zipkin [Zipkin|http://twitter.github.io/zipkin/architecture.html] has better documentation, visualization and is more likely to be used by other projects.",
        "look into merging cloudtrace/htrace See [HBASE-6524|http://issues.apache.org/jira/browse/hbase-6524]"
    ],
    [
        "ACCUMULO-1649",
        "ACCUMULO-888",
        "investigate replacing cloudtrace with Zipkin [Zipkin|http://twitter.github.io/zipkin/architecture.html] has better documentation, visualization and is more likely to be used by other projects.",
        "NPE writing to the WAL \r\nI missconfigured the WAL max size to \"4\" and got this strange error:\r\n\r\n{noformat}\r\n2012-11-29 12:12:55,610 [log.TabletServerLogger] ERROR: Unexpected error writing to log, retrying attempt 5\r\njava.lang.NullPointerException\r\n        at org.apache.accumulo.server.tabletserver.log.RemoteLogger.logManyTablets(RemoteLogger.java:145)\r\n        at org.apache.accumulo.server.tabletserver.log.TabletServerLogger$6.write(TabletServerLogger.java:393)\r\n        at org.apache.accumulo.server.tabletserver.log.TabletServerLogger.write(TabletServerLogger.java:298)\r\n        at org.apache.accumulo.server.tabletserver.log.TabletServerLogger.logManyTablets(TabletServerLogger.java:382)\r\n        at org.apache.accumulo.server.tabletserver.TabletServer$ThriftClientHandler.flush(TabletServer.java:1539)\r\n        at org.apache.accumulo.server.tabletserver.TabletServer$ThriftClientHandler.closeUpdate(TabletServer.java:1610)\r\n{noformat}"
    ],
    [
        "ACCUMULO-1649",
        "ACCUMULO-898",
        "investigate replacing cloudtrace with Zipkin [Zipkin|http://twitter.github.io/zipkin/architecture.html] has better documentation, visualization and is more likely to be used by other projects.",
        "look into replacing cloudtrace HBase has created their own distributed tracing library, and today I bumped into zipkin.  zipkin has a reasonable visualization, and seems to work with thrift.  We should look into replacing our tracing with one of these."
    ],
    [
        "ACCUMULO-1659",
        "ACCUMULO-1658",
        "test/system/continuous/start-stats.sh refers to undefined ACCUMULO_CONF_DIR test/system/continuous/start-stats.sh refers to ACCUMULO_CONF_DIR environment variable, but this can fail if the value is not set",
        "System integration tests should default ACCUMULO_CONF_DIR to ACCUMULO_HOME/conf The various system integration tests need to make use of ACCUMULO_CONF_DIR and when not present, default it to ACCUMULO_HOME/conf.\r\n\r\n* Functional Tests - adds ACCUMULO_CONF_DIR to the classpath, but grabs all configuration files out of ACCUMULO_HOME/conf/\r\n* Continuous Ingest Test - start-stats.sh, mapred-setup.sh, agitator.pl, and magitator.pl all presume continuous-env.sh will define it\r\n* Random Walk Tests - reset-cluster.sh requires it, start-*.sh will only load accumulo-env.sh files that are within it.\r\n* Scalability Tests - requires it to be defined.\r\n\r\n"
    ],
    [
        "ACCUMULO-1676",
        "ACCUMULO-1362",
        "Potential resource leak in RBlockState ctor due to un-closed streams In RBlockState ctor,\r\n{code}\r\n          DataInputStream tempDataInputStream = new DataInputStream(boundedRangeFileInputStream);\r\n          // Read the init vector from the front of the stream before initializing the cipher stream\r\n          \r\n          int ivLength = tempDataInputStream.readInt();\r\n          byte[] initVector = new byte[ivLength];\r\n          tempDataInputStream.readFully(initVector);\r\n{code}\r\ntempDataInputStream should be closed.\r\n\r\nClosing boundedRangeFileInputStream and inputStreamToBeCompressed should be considered as well.",
        "Eclipse warns about unclosed resources Eclipse gives several warnings about unclosed java.util.Scanner objects and potential leaks in the shell."
    ],
    [
        "ACCUMULO-1697",
        "ACCUMULO-1379",
        "Thread leaks in Tomcat on hot-redeploy Testing on 1.3.7 cluster with Tomcat and redeploying my web app by 'touch'ing the .war file.\r\n\r\nIn catalina.out:\r\nSep 06, 2013 4:46:38 PM org.apache.catalina.startup.HostConfig deleteRedeployResources\r\nINFO: Undeploying context [/myapp]\r\nSep 06, 2013 4:46:48 PM org.jboss.modcluster.ModClusterService drainSessions\r\nWARN: Failed to drain pending requests from context [/myapp] within specified timeout: 10 seconds\r\nSep 06, 2013 4:46:48 PM org.apache.catalina.loader.WebappClassLoader clearReferencesThreads\r\nSEVERE: The web application [/myapp] appears to have started a thread named [localhost-startStop-6-SendThread(r03sv04:2181)] but has failed to stop it. This is very likely to create a memory leak.\r\nSep 06, 2013 4:46:48 PM org.apache.catalina.loader.WebappClassLoader clearReferencesThreads\r\nSEVERE: The web application [/myapp] appears to have started a thread named [localhost-startStop-6-EventThread] but has failed to stop it. This is very likely to create a memory leak.\r\nSep 06, 2013 4:46:48 PM org.apache.catalina.loader.WebappClassLoader clearReferencesThreads\r\nSEVERE: The web application [/myapp] appears to have started a thread named [Thrift Connection Pool Checker] but has failed to stop it. This is very likely to create a memory leak.\r\n\r\nI can't easily re-test this on a 1.4 or 1.5 cluster at this time, but I examined the current code in ThriftTransportPool regarding the third message. No reference is kept to the Daemon instance, so it's never stopped. This is normally not a problem in JVMs because daemon threads are interrupted automatically at JVM shutdown, but this is a problem for hot-redeploys in Tomcat.\r\n\r\nIt would be nice to have a call to shut down the Thrift checker thread. For the ZooKeeper threads, we should be able to call ClientCnxn.close() by calling ZooKeeper.close(), but the ZooKeepers are locked up in ZooSession.sessions and can't be closed directly.",
        "PermGen leak Under version 1.3.7 we are using the following code to initialize a cloudbase connection during initialization of our web app:\r\n\r\n                        ZooKeeperInstance instance = new ZooKeeperInstance(instanceName, zooKeepers);\r\n                        connector = instance.getConnector(userId, password.getBytes());\r\n\r\nThe problem is that under the hood, this call creates several threads that are not cleaned up when the app is undeployed in JBoss. This is occurring without performing any scans or interacting with cloudbase in any other way. After relatively few redeploys of the app, the PermGen Space is OOM.\r\n\r\nI can't find any reference in the cloudbase API akin to a close() method for the Connector object. This is a classloader leak effecting any webapp that is accessing cloudbase directly. The result of this leak is not simply orphaned threads, but thousands of classes not gc'd because the classloader itself can't be gc'd. This is what is filling up PermGen.\r\n"
    ],
    [
        "ACCUMULO-1745",
        "ACCUMULO-1653",
        "Merge broken on cloned tables This is with a recent version of the 1.6 build. I have tested and it does NOT occur in 1.5.0.\r\n\r\n\r\nin the shell-\r\ncreatetable testtable\r\naddsplits a b c d\r\nclonetable testtable clonetable\r\ntable clonetable\r\nmerge -e b\r\n\r\n\r\nThe master will get stuck in an infinite loop attempting to find the non-existant c-00000000 directory.",
        "Unable to merge during RandomWalk I was running RandomWalk and ran into an issue where one of my walkers got stuck doing a merge (on table id=20). The only related error that showed up was that a file in HDFS couldn't be found (and continues to not be found, it keeps checking regularly) while trying to merge the metadata records. Here's the error:\r\n{noformat}\r\n2013-08-14 11:54:27,246 [master.Master] ERROR: Unable merge metadata table records\r\norg.apache.accumulo.core.client.AccumuloException: java.io.IOException: Could not find file /20//c-00000000 in [hdfs://localhost:9000/accumulo/tables]\r\n        at org.apache.accumulo.server.master.TabletGroupWatcher.mergeMetadataRecords(TabletGroupWatcher.java:548)\r\n        at org.apache.accumulo.server.master.TabletGroupWatcher.updateMergeState(TabletGroupWatcher.java:368)\r\n        at org.apache.accumulo.server.master.TabletGroupWatcher.run(TabletGroupWatcher.java:263)\r\nCaused by: java.io.IOException: Could not find file /20//c-00000000 in [hdfs://localhost:9000/accumulo/tables]\r\n        at org.apache.accumulo.server.fs.VolumeManagerImpl.getFullPath(VolumeManagerImpl.java:446)\r\n        at org.apache.accumulo.server.util.MetadataTableUtil.createDeleteMutation(MetadataTableUtil.java:501)\r\n        at org.apache.accumulo.server.master.TabletGroupWatcher.mergeMetadataRecords(TabletGroupWatcher.java:502)\r\n        ... 2 more\r\n{noformat}\r\n\r\nAnd the metadata table had c-00000000 listed here:\r\n{noformat}\r\nroot@802 !METADATA> grep c-00000\r\n20;00b581d7479230dd srv:dir []    /c-00000000\r\n20< srv:dir []    /c-00000001\r\n{noformat}\r\n\r\nI was testing my table namespaces code but this seemed like an unrelated issue."
    ],
    [
        "ACCUMULO-1815",
        "ACCUMULO-814",
        "egrep does not respect -f option ",
        "show few option in shell does not work with grep and does not use user formatter There is a scan option called show few.  Commands that extend the scan commmand have this option, but it does not work.  \r\n\r\nAlso when the show few option is used, the users formatter is not used.  If formatters had a set limit option, then user formatters could be used with this option."
    ],
    [
        "ACCUMULO-1827",
        "ACCUMULO-1481",
        "Broken Hadoop1 compatibility From https://builds.apache.org/job/Accumulo-Master-Hadoop-1/1135/\r\n\r\n{noformat}\r\n[ERROR] /home/jenkins/jenkins-slave/workspace/Accumulo-Master-Hadoop-1/server/src/main/java/org/apache/accumulo/server/master/Master.java:[308,24] cannot find symbol\r\nsymbol  : method isFile()\r\nlocation: class org.apache.hadoop.fs.FileStatus\r\n[ERROR] /home/jenkins/jenkins-slave/workspace/Accumulo-Master-Hadoop-1/server/src/main/java/org/apache/accumulo/server/util/Initialize.java:[240,18] cannot find symbol\r\nsymbol  : method isDirectory()\r\nlocation: class org.apache.hadoop.fs.FileStatus\r\n[ERROR] /home/jenkins/jenkins-slave/workspace/Accumulo-Master-Hadoop-1/server/src/main/java/org/apache/accumulo/server/util/Initialize.java:[255,16] cannot find symbol\r\nsymbol  : method isDirectory()\r\nlocation: class org.apache.hadoop.fs.FileStatus\r\n[ERROR] /home/jenkins/jenkins-slave/workspace/Accumulo-Master-Hadoop-1/server/src/main/java/org/apache/accumulo/server/util/Initialize.java:[316,18] cannot find symbol\r\nsymbol  : method isDirectory()\r\nlocation: class org.apache.hadoop.fs.FileStatus\r\n[ERROR] /home/jenkins/jenkins-slave/workspace/Accumulo-Master-Hadoop-1/server/src/main/java/org/apache/accumulo/server/util/Initialize.java:[323,20] cannot find symbol\r\nsymbol  : method isDirectory()\r\nlocation: class org.apache.hadoop.fs.FileStatus\r\n[INFO] 5 errors \r\n{noformat}",
        "Root tablet in its own table This will help simplify the design of a lot of our internal code. The root tablet is really a special table, and should be treated specially."
    ],
    [
        "ACCUMULO-1840",
        "ACCUMULO-1830",
        "Deleted WAL still referenced in !METADATA Running b0da55 locally with hadoop2.2.0.\r\n\r\nI wrote some data to a number of tables. Killed the tabletserver. The tabletserver failed to complete recovery because it ran out of memory.\r\n\r\nI upped the tserver's heap, and restarted it. !METADATA and !!ROOT are online and have all tablets assigned, but none of the other tables are coming online. There's a log entry for each of the tables that aren't online. Each of those tables have only one tablet.\r\n\r\n{noformat}\r\nlog:localhost+9997/hdfs://localhost:8020/accumulo1.6/wal/localhost+9997/3f941aa2-0cf8-42f5-8135-fb51a07c5c1a\r\n{noformat}\r\n\r\nLooking for that WAL in the GC's log results in: \r\n\r\n{noformat}\r\n2013-10-31 21:03:26,805 [util.MetadataTableUtil] INFO : Returning logs [!!R<< hdfs://localhost:8020/accumulo1.6/wal/localhost+9997/56c7a37f-06bd-49ad-ad18-72dd787f8ae3 (1), !!R<< hdfs://localhost:8020/accumulo1.6/wal/localhost+9997/3f941aa2-0cf8-42f5-8135-fb51a07c5c1a (1)] for extent !!R<<\r\n2013-10-31 21:03:26,815 [gc.GarbageCollectWriteAheadLogs] DEBUG: deleted [hdfs://localhost:8020/accumulo1.6/wal/localhost+9997/6f1facae-7d5f-4c74-8a41-8621921af85f, hdfs://localhost:8020/accumulo1.6/wal/localhost+9997/56c7a37f-06bd-49ad-ad18-72dd787f8ae3, hdfs://localhost:8020/accumulo1.6/wal/localhost+9997/3f941aa2-0cf8-42f5-8135-fb51a07c5c1a] from localhost+9997\r\n2013-10-31 21:08:27,105 [util.MetadataTableUtil] INFO : Returning logs [!!R<< hdfs://localhost:8020/accumulo1.6/wal/localhost+9997/3f941aa2-0cf8-42f5-8135-fb51a07c5c1a (1)] for extent !!R<<\r\n2013-10-31 21:08:27,115 [gc.GarbageCollectWriteAheadLogs] DEBUG: deleted [hdfs://localhost:8020/accumulo1.6/wal/localhost+9997/56c7a37f-06bd-49ad-ad18-72dd787f8ae3, hdfs://localhost:8020/accumulo1.6/wal/localhost+9997/3f941aa2-0cf8-42f5-8135-fb51a07c5c1a] from localhost+9997\r\n2013-10-31 21:14:25,667 [gc.GarbageCollectWriteAheadLogs] DEBUG: deleted [hdfs://localhost:8020/accumulo1.6/wal/localhost+9997/20b1adb0-9c5f-4027-82d2-c8e1a1be9f87, hdfs://localhost:8020/accumulo1.6/wal/localhost+9997/3f941aa2-0cf8-42f5-8135-fb51a07c5c1a] from localhost+9997\r\n2013-10-31 21:14:25,667 [gc.GarbageCollectWriteAheadLogs] DEBUG: Removing sorted WAL hdfs://localhost:8020/accumulo1.6/recovery/3f941aa2-0cf8-42f5-8135-fb51a07c5c1a\r\n{noformat}",
        "illegal state in RestartStressIT {noformat}\r\n2013-10-29 15:20:11,125 [state.MetaDataTableScanner] ERROR: java.lang.RuntimeException: org.apache.accumulo.server.master.state.TabletLocationState$BadLocationStateException: found two locations for the same extent 1<: host:50867[14205a7c2a90003] and host:41255[14205a7c2a9000a]\r\njava.lang.RuntimeException: org.apache.accumulo.server.master.state.TabletLocationState$BadLocationStateException: found two locations for the same extent 1<: host[14205a7c2a90003] and host:41255[14205a7c2a9000a]\r\n        at org.apache.accumulo.server.master.state.MetaDataTableScanner.fetch(MetaDataTableScanner.java:189)\r\n        at org.apache.accumulo.server.master.state.MetaDataTableScanner.next(MetaDataTableScanner.java:124)\r\n        at org.apache.accumulo.server.master.state.MetaDataTableScanner.next(MetaDataTableScanner.java:1)\r\n        at org.apache.accumulo.server.master.TabletGroupWatcher.run(TabletGroupWatcher.java:143)\r\nCaused by: org.apache.accumulo.server.master.state.TabletLocationState$BadLocationStateException: found two locations for the same extent 1<: host:50867[14205a7c2a90003] and host:41255[14205a7c2a9000a]\r\n        at org.apache.accumulo.server.master.state.MetaDataTableScanner.createTabletLocationState(MetaDataTableScanner.java:157)\r\n        at org.apache.accumulo.server.master.state.MetaDataTableScanner.fetch(MetaDataTableScanner.java:185)\r\n        ... 3 more\r\n{noformat}\r\n\r\nHere's where the test stopped \r\n{noformat}\r\njava.lang.IllegalStateException: Tablet has multiple locations : 1<\r\n\tat org.apache.accumulo.core.metadata.MetadataLocationObtainer.getMetadataLocationEntries(MetadataLocationObtainer.java:233)\r\n\tat org.apache.accumulo.core.metadata.MetadataLocationObtainer.lookupTablet(MetadataLocationObtainer.java:118)\r\n\tat org.apache.accumulo.core.client.impl.TabletLocatorImpl.lookupTabletLocation(TabletLocatorImpl.java:462)\r\n\tat org.apache.accumulo.core.client.impl.TabletLocatorImpl._locateTablet(TabletLocatorImpl.java:619)\r\n\tat org.apache.accumulo.core.client.impl.TabletLocatorImpl.locateTablet(TabletLocatorImpl.java:437)\r\n\tat org.apache.accumulo.core.client.impl.ThriftScanner.scan(ThriftScanner.java:226)\r\n\tat org.apache.accumulo.core.client.impl.ScannerIterator$Reader.run(ScannerIterator.java:84)\r\n\tat org.apache.accumulo.core.client.impl.ScannerIterator.hasNext(ScannerIterator.java:177)\r\n\tat org.apache.accumulo.test.VerifyIngest.verifyIngest(VerifyIngest.java:162)\r\n\tat org.apache.accumulo.test.functional.RestartStressIT.test(RestartStressIT.java:73)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n\tat java.lang.reflect.Method.invoke(Method.java:597)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n{noformat}\r\n"
    ],
    [
        "ACCUMULO-1852",
        "ACCUMULO-1845",
        "nativeMap test doesn't run on OS X The Makefile has bad includes for the test run, but even when I fix that, it still fails looking like it's not linking stdc++ properly.  [~elserj] and I both looked at it for a bit and couldn't make it work.\r\n\r\nSince I last saw it succeed, I have both updated to Mavericks and merged in the change to have the pom manage make.\r\n\r\nI also haven't seen NativeMapIT pass since the updates, so I'm wondering if it's not just the test execution that's broken, but also the actual nativemap build.",
        "Can't test nativeMap code `make test` is still broken on OSX\r\n\r\n{noformat}\r\n% make clean test \r\nrm -f libNativeMap-Mac_OS_X-x86_64-64.dylib runTests\r\ng++ -m64 -dynamiclib -undefined dynamic_lookup -O3 -I/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/include/darwin -I/usr/include/c++/4.2.1 -Ijavah  -o libNativeMap-Mac_OS_X-x86_64-64.dylib nativeMap/org_apache_accumulo_tserver_NativeMap.cc\r\nJAVA_HOME is /Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home\r\ng++ -g -Wall -undefined dynamic_lookup -I/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/include/linux -I/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/include/darwin -I/usr/include/c++/4.2.1 -InativeMap -o runTests testNativeMap/test.cc testNativeMap/util.cc libNativeMap-Mac_OS_X-x86_64-64.dylib\r\nLD_LIBRARY_PATH=./ ./runTests 20 20 20 20 20 20 20 20 true\r\ndyld: Symbol not found: __ZNSt8ios_base4InitD1Ev\r\n  Referenced from: libNativeMap-Mac_OS_X-x86_64-64.dylib\r\n  Expected in: flat namespace\r\n in libNativeMap-Mac_OS_X-x86_64-64.dylib\r\nmake: *** [runTests] Trace/BPT trap: 5\r\nmake: *** Deleting file `runTests'\r\n{noformat}"
    ],
    [
        "ACCUMULO-1876",
        "ACCUMULO-1725",
        "Problem using accumulo artifacts from ivy Defining an accumulo dependency in an ivy.xml file for any Accyumulo version greater than 1.4.4 results in the following error \r\n\r\n{noformat}\r\n[ivy:resolve] \t\t::::::::::::::::::::::::::::::::::::::::::::::\r\n[ivy:resolve] \t\t::          UNRESOLVED DEPENDENCIES         ::\r\n[ivy:resolve] \t\t::::::::::::::::::::::::::::::::::::::::::::::\r\n[ivy:resolve] \t\t:: org.slf4j#slf4j-api;${slf4j.version}: not found\r\n[ivy:resolve] \t\t::::::::::::::::::::::::::::::::::::::::::::::\r\n[ivy:resolve] \r\n\r\n{noformat}\r\n\r\nThe issue appears to be that in the parent pom.xml, slf4j.version is only defined in profiles.  Ivy doesn't load profiles when pulling from a maven repo, so the ${slf4j.version} is never set.\r\n\r\nOne possible fix is to define a property earlier with the version, and allow the profiles to overwrite.",
        "hadoop 2 profile should mark SLF4J dependency as 1.7.5 HADOOP-9833 upped the SLF4J JAR version (mostly for the benefit of downstream code). Accumulo can pick up that JAR once it switches to a 2.1 beta hadoop version, so should update the profile settings accordingly"
    ],
    [
        "ACCUMULO-1884",
        "ACCUMULO-1845",
        "NativeMap Makefile fails under OSX for 1.6.x and higher The NativeMap Makefile was fixed in ACCUMULO-1819 to work under the latest Mac OS X. However, the Makefile changed was src/server/src/main/c++/nativeMap. In 1.6.0-SNAPSHOT and master, the Makefile resides in server/native/src/main/resources/Makefile, and that copy did not receive the fixes.",
        "Can't test nativeMap code `make test` is still broken on OSX\r\n\r\n{noformat}\r\n% make clean test \r\nrm -f libNativeMap-Mac_OS_X-x86_64-64.dylib runTests\r\ng++ -m64 -dynamiclib -undefined dynamic_lookup -O3 -I/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/include/darwin -I/usr/include/c++/4.2.1 -Ijavah  -o libNativeMap-Mac_OS_X-x86_64-64.dylib nativeMap/org_apache_accumulo_tserver_NativeMap.cc\r\nJAVA_HOME is /Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home\r\ng++ -g -Wall -undefined dynamic_lookup -I/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/include/linux -I/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/include/darwin -I/usr/include/c++/4.2.1 -InativeMap -o runTests testNativeMap/test.cc testNativeMap/util.cc libNativeMap-Mac_OS_X-x86_64-64.dylib\r\nLD_LIBRARY_PATH=./ ./runTests 20 20 20 20 20 20 20 20 true\r\ndyld: Symbol not found: __ZNSt8ios_base4InitD1Ev\r\n  Referenced from: libNativeMap-Mac_OS_X-x86_64-64.dylib\r\n  Expected in: flat namespace\r\n in libNativeMap-Mac_OS_X-x86_64-64.dylib\r\nmake: *** [runTests] Trace/BPT trap: 5\r\nmake: *** Deleting file `runTests'\r\n{noformat}"
    ],
    [
        "ACCUMULO-1884",
        "ACCUMULO-1852",
        "NativeMap Makefile fails under OSX for 1.6.x and higher The NativeMap Makefile was fixed in ACCUMULO-1819 to work under the latest Mac OS X. However, the Makefile changed was src/server/src/main/c++/nativeMap. In 1.6.0-SNAPSHOT and master, the Makefile resides in server/native/src/main/resources/Makefile, and that copy did not receive the fixes.",
        "nativeMap test doesn't run on OS X The Makefile has bad includes for the test run, but even when I fix that, it still fails looking like it's not linking stdc++ properly.  [~elserj] and I both looked at it for a bit and couldn't make it work.\r\n\r\nSince I last saw it succeed, I have both updated to Mavericks and merged in the change to have the pom manage make.\r\n\r\nI also haven't seen NativeMapIT pass since the updates, so I'm wondering if it's not just the test execution that's broken, but also the actual nativemap build."
    ],
    [
        "ACCUMULO-1912",
        "ACCUMULO-1809",
        "IllegalArgumentException in run-moru.sh: Can not set short field maxColF to String This is the command that was run:\r\n\r\n/home/vagrant/accumulo_home/bin/accumulo/bin/tool.sh /home/vagrant/accumulo_home/bin/accumulo/lib/accumulo-test.jar org.apache.accumulo.test.continuous.ContinuousMoru -libjars /home/vagrant/accumulo_home/bin/accumulo/lib/accumulo-test.jar -i instance -z affy-master:2181 -u root -p secret --table ci --min 0 --max 20000000 --maxColF 2000 --maxColQ 2000 --batchMemory 1000000 --batchLatency 60000 --batchThreads 4 --maxMappers 64\r\n\r\nLet me know if there is any more information needed to debug this issue.",
        "ContinuousMoru does not run under hadoop 2.0 "
    ],
    [
        "ACCUMULO-1929",
        "ACCUMULO-1300",
        "Current auth/auth/perm API doesn't well support multiple authentication domains The current {{Authenticator}} / {{Authorizor}} / {{PermissionHandler}} API doesn't provide a good method to support multiple authentication domains.  \r\n\r\nWhile the {{Authenticator}} object accepts abstract {{AuthenticationToken}} objects which can be used to point a request towards a particular domain (by including domain-specific knowledge in the token subclass), the {{Authorizor}} and {{PermissionHandler}} objects share no such abstract class.  A call like {{Authorizor.getCachedUserAuthorization(String user)}} can't tell if the user in question is the user for domain 1, 2, 3, and so on, without having the rest of the system play some crazy tricks to encode that string in some unnatural way.\r\n\r\nOne simple-ish solution is pass the {{AuthenticationToken}} object on to more than one call in the  {{Authenticator}} / {{Authorizor}} / {{PermissionHandler}} system.  That way, its domain knowledge can travel through to the other parts and be used to route requests accordingly.  ",
        "Allow multiple, prioritized authentication systems This continues the work started on ACCUMULO-1024, and proposed in detail on that ticket.\r\n\r\nThe basic idea is that authenticators should be configurable in a priority list, list the way it can be done on Linux, as in:\r\n{code}\r\nlocal > NIS > LDAP > AD\r\n{code}\r\nThe priority for Accumulo would be:\r\n{code}\r\nSystemUserAuthenticator > BuiltInPasswordAuthenticator > UserConfiguredAuthenticator1 > UserConfiguredAuthenticator2 > ...\r\n{code}"
    ],
    [
        "ACCUMULO-1954",
        "ACCUMULO-1900",
        "1.5.1 doesn't compile against Hadoop-1.0.4 Compilation error in DfsLogger:\r\n\r\n{noformat}\r\nserver/src/main/java/org/apache/accumulo/server/tabletserver/log/DfsLogger.java:[261,24] error: method getDefaultReplication in class FileSystem cannot be applied to given types;\r\n{noformat}\r\n\r\nNot sure if we care to fix it or not. I'm also kind of confused because, at a glance, it looked like the hadoop-1.0.4 tag in the ASF svn had the method signature that we were expecting (takes a Path and returns a short).",
        "1.5.1-SNAPHOST fails to run against Hadoop 1.0.4 Tried running 1.5.1 snapshot against hadoop 1.0.4 and it failed trying to call FileSystem.getDefaultReplication(Path) which does not exist in 1.0.4.  May have been introduced in Hadoop 1.2."
    ],
    [
        "ACCUMULO-1979",
        "ACCUMULO-1978",
        "Compact range has old logic for combined root and metadata tablets {code}\r\n    if (tableId.equals(MetadataTable.ID))\r\n      range = range.clip(new Range(RootTable.EXTENT.getMetadataEntry(), false, null, true));\r\n{code}\r\n\r\nThis is unecessary now. With this code in place, a fresh install reports failures because the garbage collector is forcing a compaction of the !METADATA table, resulting in warnings from the gc and master.",
        "Compaction warning on fresh install Master is reporting in the monitor-\r\n{code} \r\nFailed to execute Repo, tid=04a117a43b9bcadf\r\n\tjava.lang.IllegalArgumentException: Range [+r<%00; : [] 9223372036854775807 false,+inf) does not overlap [!0; : [] 9223372036854775807 false,!0<%00; : [] 9223372036854775807 false)\r\n\t\tat org.apache.accumulo.core.data.Range.clip(Range.java:439)\r\n\t\tat org.apache.accumulo.core.data.Range.clip(Range.java:409)\r\n\t\tat org.apache.accumulo.master.tableOps.CompactionDriver.isReady(CompactRange.java:105)\r\n\t\tat org.apache.accumulo.master.tableOps.CompactionDriver.isReady(CompactRange.java:65)\r\n\t\tat org.apache.accumulo.master.tableOps.TraceRepo.isReady(TraceRepo.java:44)\r\n\t\tat org.apache.accumulo.fate.Fate$TransactionRunner.run(Fate.java:64)\r\n\t\tat org.apache.accumulo.fate.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n\t\tat java.lang.Thread.run(Thread.java:701)\r\n06 19:14:08,0236\tgc:john-P15xEMx\t1\t\r\nWARN\r\norg.apache.accumulo.core.client.AccumuloException: Internal error processing waitForTableOperation\r\n\torg.apache.accumulo.core.client.AccumuloException: Internal error processing waitForTableOperation\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:330)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.compact(TableOperationsImpl.java:826)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.compact(TableOperationsImpl.java:807)\r\n\t\tat org.apache.accumulo.gc.SimpleGarbageCollector.run(SimpleGarbageCollector.java:474)\r\n\t\tat org.apache.accumulo.gc.SimpleGarbageCollector.main(SimpleGarbageCollector.java:144)\r\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\t\tat java.lang.reflect.Method.invoke(Method.java:622)\r\n\t\tat org.apache.accumulo.start.Main$1.run(Main.java:137)\r\n\t\tat java.lang.Thread.run(Thread.java:701)\r\n\tCaused by: org.apache.thrift.TApplicationException: Internal error processing waitForTableOperation\r\n\t\tat org.apache.thrift.TApplicationException.read(TApplicationException.java:108)\r\n\t\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:71)\r\n\t\tat org.apache.accumulo.core.master.thrift.MasterClientService$Client.recv_waitForTableOperation(MasterClientService.java:610)\r\n\t\tat org.apache.accumulo.core.master.thrift.MasterClientService$Client.waitForTableOperation(MasterClientService.java:595)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.waitForTableOperation(TableOperationsImpl.java:268)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:310)\r\n\t\t... 10 more{code}\r\n\r\n\r\nSeems to correspond to this from the monitor-\r\n{code}\r\norg.apache.accumulo.core.client.AccumuloException: Internal error processing waitForTableOperation\r\n\torg.apache.accumulo.core.client.AccumuloException: Internal error processing waitForTableOperation\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:330)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.compact(TableOperationsImpl.java:826)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.compact(TableOperationsImpl.java:807)\r\n\t\tat org.apache.accumulo.gc.SimpleGarbageCollector.run(SimpleGarbageCollector.java:474)\r\n\t\tat org.apache.accumulo.gc.SimpleGarbageCollector.main(SimpleGarbageCollector.java:144)\r\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\t\tat java.lang.reflect.Method.invoke(Method.java:622)\r\n\t\tat org.apache.accumulo.start.Main$1.run(Main.java:137)\r\n\t\tat java.lang.Thread.run(Thread.java:701)\r\n\tCaused by: org.apache.thrift.TApplicationException: Internal error processing waitForTableOperation\r\n\t\tat org.apache.thrift.TApplicationException.read(TApplicationException.java:108)\r\n\t\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:71)\r\n\t\tat org.apache.accumulo.core.master.thrift.MasterClientService$Client.recv_waitForTableOperation(MasterClientService.java:610)\r\n\t\tat org.apache.accumulo.core.master.thrift.MasterClientService$Client.waitForTableOperation(MasterClientService.java:595)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.waitForTableOperation(TableOperationsImpl.java:268)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:310)\r\n\t\t... 10 more{code}\r\n\r\nWhich is a bit strange, because it's forcing a compaction of the metadata table, but is complaining about the root tablet as part of the range?"
    ],
    [
        "ACCUMULO-2003",
        "ACCUMULO-1978",
        "Garbage collector fails to compact metadata table While running randomwalk test on 1.6.0-SNAPSHOT I noticed some errors on the monitor page indicating the Accumulo garbage collector could not compact the metadata table.\r\n\r\nThe following exception is in the GC log.\r\n\r\n{noformat}\r\n2013-12-11 10:49:57,447 [gc.SimpleGarbageCollector] WARN : org.apache.accumulo.core.client.AccumuloException: Internal error processing waitForTableOperation\r\norg.apache.accumulo.core.client.AccumuloException: Internal error processing waitForTableOperation\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:330)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.compact(TableOperationsImpl.java:826)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.compact(TableOperationsImpl.java:807)\r\n        at org.apache.accumulo.gc.SimpleGarbageCollector.run(SimpleGarbageCollector.java:474)\r\n        at org.apache.accumulo.gc.SimpleGarbageCollector.main(SimpleGarbageCollector.java:144)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n        at java.lang.reflect.Method.invoke(Method.java:597)\r\n        at org.apache.accumulo.start.Main$1.run(Main.java:137)\r\n        at java.lang.Thread.run(Thread.java:662)\r\nCaused by: org.apache.thrift.TApplicationException: Internal error processing waitForTableOperation\r\n        at org.apache.thrift.TApplicationException.read(TApplicationException.java:108)\r\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:71)\r\n        at org.apache.accumulo.core.master.thrift.MasterClientService$Client.recv_waitForTableOperation(MasterClientService.java:610)\r\n        at org.apache.accumulo.core.master.thrift.MasterClientService$Client.waitForTableOperation(MasterClientService.java:595)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.waitForTableOperation(TableOperationsImpl.java:268)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:310)\r\n        ... 10 more\r\n\r\n{noformat}\r\n\r\n\r\nI think the following is the cause of the problem from the master logs.  Looking at the code I suspect this is related to the creation of the root table and that an unnecessary range operation is being done. \r\n\r\n{noformat}\r\nFailed to execute Repo, tid=0f298c492c30ee6c\r\n\tjava.lang.IllegalArgumentException: Range [+r<%00; : [] 9223372036854775807 false,+inf) does not overlap [!0; : [] 9223372036854775807 false,!0<%00; : [] 9223372036854775807 false)\r\n\t\tat org.apache.accumulo.core.data.Range.clip(Range.java:439)\r\n\t\tat org.apache.accumulo.core.data.Range.clip(Range.java:409)\r\n\t\tat org.apache.accumulo.master.tableOps.CompactionDriver.isReady(CompactRange.java:105)\r\n\t\tat org.apache.accumulo.master.tableOps.CompactionDriver.isReady(CompactRange.java:65)\r\n\t\tat org.apache.accumulo.master.tableOps.TraceRepo.isReady(TraceRepo.java:44)\r\n\t\tat org.apache.accumulo.fate.Fate$TransactionRunner.run(Fate.java:64)\r\n\t\tat org.apache.accumulo.fate.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n\t\tat java.lang.Thread.run(Thread.java:662)\r\n{noformat}",
        "Compaction warning on fresh install Master is reporting in the monitor-\r\n{code} \r\nFailed to execute Repo, tid=04a117a43b9bcadf\r\n\tjava.lang.IllegalArgumentException: Range [+r<%00; : [] 9223372036854775807 false,+inf) does not overlap [!0; : [] 9223372036854775807 false,!0<%00; : [] 9223372036854775807 false)\r\n\t\tat org.apache.accumulo.core.data.Range.clip(Range.java:439)\r\n\t\tat org.apache.accumulo.core.data.Range.clip(Range.java:409)\r\n\t\tat org.apache.accumulo.master.tableOps.CompactionDriver.isReady(CompactRange.java:105)\r\n\t\tat org.apache.accumulo.master.tableOps.CompactionDriver.isReady(CompactRange.java:65)\r\n\t\tat org.apache.accumulo.master.tableOps.TraceRepo.isReady(TraceRepo.java:44)\r\n\t\tat org.apache.accumulo.fate.Fate$TransactionRunner.run(Fate.java:64)\r\n\t\tat org.apache.accumulo.fate.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n\t\tat java.lang.Thread.run(Thread.java:701)\r\n06 19:14:08,0236\tgc:john-P15xEMx\t1\t\r\nWARN\r\norg.apache.accumulo.core.client.AccumuloException: Internal error processing waitForTableOperation\r\n\torg.apache.accumulo.core.client.AccumuloException: Internal error processing waitForTableOperation\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:330)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.compact(TableOperationsImpl.java:826)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.compact(TableOperationsImpl.java:807)\r\n\t\tat org.apache.accumulo.gc.SimpleGarbageCollector.run(SimpleGarbageCollector.java:474)\r\n\t\tat org.apache.accumulo.gc.SimpleGarbageCollector.main(SimpleGarbageCollector.java:144)\r\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\t\tat java.lang.reflect.Method.invoke(Method.java:622)\r\n\t\tat org.apache.accumulo.start.Main$1.run(Main.java:137)\r\n\t\tat java.lang.Thread.run(Thread.java:701)\r\n\tCaused by: org.apache.thrift.TApplicationException: Internal error processing waitForTableOperation\r\n\t\tat org.apache.thrift.TApplicationException.read(TApplicationException.java:108)\r\n\t\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:71)\r\n\t\tat org.apache.accumulo.core.master.thrift.MasterClientService$Client.recv_waitForTableOperation(MasterClientService.java:610)\r\n\t\tat org.apache.accumulo.core.master.thrift.MasterClientService$Client.waitForTableOperation(MasterClientService.java:595)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.waitForTableOperation(TableOperationsImpl.java:268)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:310)\r\n\t\t... 10 more{code}\r\n\r\n\r\nSeems to correspond to this from the monitor-\r\n{code}\r\norg.apache.accumulo.core.client.AccumuloException: Internal error processing waitForTableOperation\r\n\torg.apache.accumulo.core.client.AccumuloException: Internal error processing waitForTableOperation\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:330)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.compact(TableOperationsImpl.java:826)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.compact(TableOperationsImpl.java:807)\r\n\t\tat org.apache.accumulo.gc.SimpleGarbageCollector.run(SimpleGarbageCollector.java:474)\r\n\t\tat org.apache.accumulo.gc.SimpleGarbageCollector.main(SimpleGarbageCollector.java:144)\r\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\t\tat java.lang.reflect.Method.invoke(Method.java:622)\r\n\t\tat org.apache.accumulo.start.Main$1.run(Main.java:137)\r\n\t\tat java.lang.Thread.run(Thread.java:701)\r\n\tCaused by: org.apache.thrift.TApplicationException: Internal error processing waitForTableOperation\r\n\t\tat org.apache.thrift.TApplicationException.read(TApplicationException.java:108)\r\n\t\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:71)\r\n\t\tat org.apache.accumulo.core.master.thrift.MasterClientService$Client.recv_waitForTableOperation(MasterClientService.java:610)\r\n\t\tat org.apache.accumulo.core.master.thrift.MasterClientService$Client.waitForTableOperation(MasterClientService.java:595)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.waitForTableOperation(TableOperationsImpl.java:268)\r\n\t\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:310)\r\n\t\t... 10 more{code}\r\n\r\nWhich is a bit strange, because it's forcing a compaction of the metadata table, but is complaining about the root tablet as part of the range?"
    ],
    [
        "ACCUMULO-2003",
        "ACCUMULO-1979",
        "Garbage collector fails to compact metadata table While running randomwalk test on 1.6.0-SNAPSHOT I noticed some errors on the monitor page indicating the Accumulo garbage collector could not compact the metadata table.\r\n\r\nThe following exception is in the GC log.\r\n\r\n{noformat}\r\n2013-12-11 10:49:57,447 [gc.SimpleGarbageCollector] WARN : org.apache.accumulo.core.client.AccumuloException: Internal error processing waitForTableOperation\r\norg.apache.accumulo.core.client.AccumuloException: Internal error processing waitForTableOperation\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:330)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.compact(TableOperationsImpl.java:826)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.compact(TableOperationsImpl.java:807)\r\n        at org.apache.accumulo.gc.SimpleGarbageCollector.run(SimpleGarbageCollector.java:474)\r\n        at org.apache.accumulo.gc.SimpleGarbageCollector.main(SimpleGarbageCollector.java:144)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n        at java.lang.reflect.Method.invoke(Method.java:597)\r\n        at org.apache.accumulo.start.Main$1.run(Main.java:137)\r\n        at java.lang.Thread.run(Thread.java:662)\r\nCaused by: org.apache.thrift.TApplicationException: Internal error processing waitForTableOperation\r\n        at org.apache.thrift.TApplicationException.read(TApplicationException.java:108)\r\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:71)\r\n        at org.apache.accumulo.core.master.thrift.MasterClientService$Client.recv_waitForTableOperation(MasterClientService.java:610)\r\n        at org.apache.accumulo.core.master.thrift.MasterClientService$Client.waitForTableOperation(MasterClientService.java:595)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.waitForTableOperation(TableOperationsImpl.java:268)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:310)\r\n        ... 10 more\r\n\r\n{noformat}\r\n\r\n\r\nI think the following is the cause of the problem from the master logs.  Looking at the code I suspect this is related to the creation of the root table and that an unnecessary range operation is being done. \r\n\r\n{noformat}\r\nFailed to execute Repo, tid=0f298c492c30ee6c\r\n\tjava.lang.IllegalArgumentException: Range [+r<%00; : [] 9223372036854775807 false,+inf) does not overlap [!0; : [] 9223372036854775807 false,!0<%00; : [] 9223372036854775807 false)\r\n\t\tat org.apache.accumulo.core.data.Range.clip(Range.java:439)\r\n\t\tat org.apache.accumulo.core.data.Range.clip(Range.java:409)\r\n\t\tat org.apache.accumulo.master.tableOps.CompactionDriver.isReady(CompactRange.java:105)\r\n\t\tat org.apache.accumulo.master.tableOps.CompactionDriver.isReady(CompactRange.java:65)\r\n\t\tat org.apache.accumulo.master.tableOps.TraceRepo.isReady(TraceRepo.java:44)\r\n\t\tat org.apache.accumulo.fate.Fate$TransactionRunner.run(Fate.java:64)\r\n\t\tat org.apache.accumulo.fate.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n\t\tat java.lang.Thread.run(Thread.java:662)\r\n{noformat}",
        "Compact range has old logic for combined root and metadata tablets {code}\r\n    if (tableId.equals(MetadataTable.ID))\r\n      range = range.clip(new Range(RootTable.EXTENT.getMetadataEntry(), false, null, true));\r\n{code}\r\n\r\nThis is unecessary now. With this code in place, a fresh install reports failures because the garbage collector is forcing a compaction of the !METADATA table, resulting in warnings from the gc and master."
    ],
    [
        "ACCUMULO-2054",
        "ACCUMULO-1965",
        "Concurrent random walk test fails Random walk testing, the namespace test sometimes fails:\r\n\r\n{noformat}\r\n18 16:21:54,810 [randomwalk.Framework] ERROR: Error during random walk\r\njava.lang.Exception: Error running node Concurrent.xml\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:285)\r\n        at org.apache.accumulo.test.randomwalk.Framework.run(Framework.java:65)\r\n        at org.apache.accumulo.test.randomwalk.Framework.main(Framework.java:125)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:606)\r\n        at org.apache.accumulo.start.Main$1.run(Main.java:137)\r\n        at java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.Exception: Error running node ct.CreateTable\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:285)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:254)\r\n        ... 8 more\r\nCaused by: org.apache.accumulo.core.client.AccumuloSecurityException: Error NAMESPACE_DOESNT_EXIST for user root on table nspc_001.ctt_002(?) - Unknown security exception\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:316)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:296)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.create(TableOperationsImpl.java:224)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.create(TableOperationsImpl.java:193)\r\n        at org.apache.accumulo.test.randomwalk.concurrent.CreateTable.visit(CreateTable.java:42)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:254)\r\n        ... 9 more\r\nCaused by: ThriftSecurityException(user:root, code:NAMESPACE_DOESNT_EXIST)\r\n        at org.apache.accumulo.core.master.thrift.MasterClientService$executeTableOperation_result$executeTableOperation_resultStandardScheme.read(MasterClientService.java:19067)\r\n        at org.apache.accumulo.core.master.thrift.MasterClientService$executeTableOperation_result$executeTableOperation_resultStandardScheme.read(MasterClientService.java:19053)\r\n        at org.apache.accumulo.core.master.thrift.MasterClientService$executeTableOperation_result.read(MasterClientService.java:18995)\r\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)\r\n        at org.apache.accumulo.core.master.thrift.MasterClientService$Client.recv_executeTableOperation(MasterClientService.java:582)\r\n        at org.apache.accumulo.core.master.thrift.MasterClientService$Client.executeTableOperation(MasterClientService.java:563)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.executeTableOperation(TableOperationsImpl.java:252)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:305)\r\n        ... 14 more\r\n{noformat}\r\n\r\n",
        "Invalid table names (& namespaces) should have dedicated error codes To improve the client API, we should minimize the number of exceptions that require String parsing to determine the exception type. Table naming errors is one of them."
    ],
    [
        "ACCUMULO-2071",
        "ACCUMULO-2057",
        "close consistency check failure {noformat}\r\nFailed to do close consistency check for tablet d;r078d8<\r\n{noformat}\r\n\r\nThis occurred twice, and this was the table name: {{bulk_hostname_14124_1387489968495}} .\r\n\r\n\r\n\r\n",
        "found two last locations for the same extent Randomwalk testing, and saw this in the master logs:\r\n\r\n{noformat}\r\n\r\n2013-12-18 19:16:58,335 [state.MetaDataTableScanner] ERROR: java.lang.RuntimeException: org.apache.accumulo.server.master.state.TabletLocationState$BadLocationState\r\nException: found two last locations for the same extent a<: ip-10-1-2-26:9997[1430679035c00c6] and ip-10-1-2-14:9997[343067903a7005b]\r\njava.lang.RuntimeException: org.apache.accumulo.server.master.state.TabletLocationState$BadLocationStateException: found two last locations for the same extent a<: \r\nip-10-1-2-26:9997[1430679035c00c6] and ip-10-1-2-14:9997[343067903a7005b]\r\n        at org.apache.accumulo.server.master.state.MetaDataTableScanner.fetch(MetaDataTableScanner.java:192)\r\n        at org.apache.accumulo.server.master.state.MetaDataTableScanner.next(MetaDataTableScanner.java:127)\r\n        at org.apache.accumulo.server.master.state.MetaDataTableScanner.next(MetaDataTableScanner.java:1)\r\n        at org.apache.accumulo.master.TabletGroupWatcher.run(TabletGroupWatcher.java:143)\r\nCaused by: org.apache.accumulo.server.master.state.TabletLocationState$BadLocationStateException: found two last locations for the same extent a<: ip-10-1-2-26:9997[1430679035c00c6] and ip-10-1-2-14:9997[343067903a7005b]\r\n        at org.apache.accumulo.server.master.state.MetaDataTableScanner.createTabletLocationState(MetaDataTableScanner.java:169)\r\n        at org.apache.accumulo.server.master.state.MetaDataTableScanner.fetch(MetaDataTableScanner.java:188)\r\n        ... 3 more\r\n2013-12-18 19:16:58,359 [master.Master] ERROR: Error processing table state for store Normal Tablets\r\njava.lang.RuntimeException: scanner closed\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchReaderIterator.hasNext(TabletServerBatchReaderIterator.java:212)\r\n        at org.apache.accumulo.server.master.state.MetaDataTableScanner.hasNext(MetaDataTableScanner.java:117)\r\n        at org.apache.accumulo.master.TabletGroupWatcher.run(TabletGroupWatcher.java:143)\r\n{noformat}\r\n"
    ],
    [
        "ACCUMULO-2082",
        "ACCUMULO-1938",
        "Major compacting files not empty Saw this during a randomwalk:\r\n\r\n{noformat}\r\nMajC Failed, message = Major compacting files not empty [hdfs://nameservice/accumulo/tables/p/t-0001pfx/C0001qir.rf, hdfs://nameservice/accumulo/tables/p/t-0001pfx/C0001qip.rf, hdfs://nameservice/accumulo/tables/p/b-0001qh4/I0001qh5.rf, hdfs://nameservice/accumulo/tables/p/b-0001qgt/I0001qgv.rf, hdfs://nameservice/accumulo/tables/p/b-0001qg5/I0001qg6.rf, hdfs://nameservice/accumulo/tables/p/b-0001qem/I0001qeo.rf, hdfs://nameservice/accumulo/tables/p/b-0001qgj/I0001qgk.rf, hdfs://nameservice/accumulo/tables/p/t-0001pfx/C0001qis.rf, hdfs://nameservice/accumulo/tables/p/b-0001qga/I0001qgb.rf, hdfs://nameservice/accumulo/tables/p/t-0001pfx/A0001pg9.rf]\r\n\tjava.lang.IllegalStateException: Major compacting files not empty [hdfs://nameservice/accumulo/tables/p/t-0001pfx/C0001qir.rf, hdfs://nameservice/accumulo/tables/p/t-0001pfx/C0001qip.rf, hdfs://nameservice/accumulo/tables/p/b-0001qh4/I0001qh5.rf, hdfs://nameservice/accumulo/tables/p/b-0001qgt/I0001qgv.rf, hdfs://nameservice/accumulo/tables/p/b-0001qg5/I0001qg6.rf, hdfs://nameservice/accumulo/tables/p/b-0001qem/I0001qeo.rf, hdfs://nameservice/accumulo/tables/p/b-0001qgj/I0001qgk.rf, hdfs://nameservice/accumulo/tables/p/t-0001pfx/C0001qis.rf, hdfs://nameservice/accumulo/tables/p/b-0001qga/I0001qgb.rf, hdfs://nameservice/accumulo/tables/p/t-0001pfx/A0001pg9.rf]\r\n\t\tat org.apache.accumulo.tserver.Tablet$DatafileManager.reserveMajorCompactingFiles(Tablet.java:933)\r\n\t\tat org.apache.accumulo.tserver.Tablet._majorCompact(Tablet.java:3187)\r\n\t\tat org.apache.accumulo.tserver.Tablet.majorCompact(Tablet.java:3371)\r\n\t\tat org.apache.accumulo.tserver.Tablet.access$4700(Tablet.java:168)\r\n\t\tat org.apache.accumulo.tserver.Tablet$CompactionRunner.run(Tablet.java:2849)\r\n\t\tat org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tat org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n\t\tat org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n\t\tat java.lang.Thread.run(Thread.java:744)\r\n{noformat}\r\n\r\nI don't actually know if it's a problem. A first read over that section of the code wasn't obvious to me one way or the other. ",
        "IllegalStateException: Major compacting files not empty Running random walk test on a single node with two walkers.\r\n\r\nGot this illegal state message: need to determine if this is a real problem or squash the error message.\r\n\r\n{noformat}\r\nMajC Failed, message = Major compacting files not empty [hdfs://localhost:9000/accumulo/tables/2g/default_tablet/C000043w.rf, hdfs://localhost:9000/accumulo/tables/2g/default_tablet/C000043p.rf, hdfs://localhost:9000/accumulo/tables/2g/t-00003c6/A00003nm.rf, hdfs://localhost:9000/accumulo/tables/2g/t-00003eo/A00003mh.rf, hdfs://localhost:9000/accumulo/tables/2g/t-00003c6/C00003ux.rf, hdfs://localhost:9000/accumulo/tables/2g/t-00003c6/C00003tx.rf, hdfs://localhost:9000/accumulo/tables/2g/default_tablet/C0000435.rf, hdfs://localhost:9000/accumulo/tables/2g/t-00003ei/C00003o4.rf, hdfs://localhost:9000/accumulo/tables/2g/default_tablet/C000043m.rf, hdfs://localhost:9000/accumulo/tables/2g/default_tablet/C000043i.rf]\r\n\tjava.lang.IllegalStateException: Major compacting files not empty [hdfs://localhost:9000/accumulo/tables/2g/default_tablet/C000043w.rf, hdfs://localhost:9000/accumulo/tables/2g/default_tablet/C000043p.rf, hdfs://localhost:9000/accumulo/tables/2g/t-00003c6/A00003nm.rf, hdfs://localhost:9000/accumulo/tables/2g/t-00003eo/A00003mh.rf, hdfs://localhost:9000/accumulo/tables/2g/t-00003c6/C00003ux.rf, hdfs://localhost:9000/accumulo/tables/2g/t-00003c6/C00003tx.rf, hdfs://localhost:9000/accumulo/tables/2g/default_tablet/C0000435.rf, hdfs://localhost:9000/accumulo/tables/2g/t-00003ei/C00003o4.rf, hdfs://localhost:9000/accumulo/tables/2g/default_tablet/C000043m.rf, hdfs://localhost:9000/accumulo/tables/2g/default_tablet/C000043i.rf]\r\n\t\tat org.apache.accumulo.tserver.Tablet$DatafileManager.reserveMajorCompactingFiles(Tablet.java:933)\r\n\t\tat org.apache.accumulo.tserver.Tablet._majorCompact(Tablet.java:3186)\r\n\t\tat org.apache.accumulo.tserver.Tablet.majorCompact(Tablet.java:3370)\r\n\t\tat org.apache.accumulo.tserver.Tablet.access$38(Tablet.java:3351)\r\n\t\tat org.apache.accumulo.tserver.Tablet$CompactionRunner.run(Tablet.java:2848)\r\n\t\tat org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n\t\tat org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n\t\tat org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n\t\tat java.lang.Thread.run(Thread.java:662)\r\n{noformat}\r\n"
    ],
    [
        "ACCUMULO-2091",
        "ACCUMULO-2070",
        "Concurrent random walk fails to rename across namespaces Saw the following walker log.   I think we should modify the test to detect when its attempting to rename across namespaces and make it expect an error.  Probably should not be getting an Accumulo exception.\r\n\r\n{noformat}\r\n24 17:45:03,730 [randomwalk.Module] DEBUG:   users: [user000, user001, user002, user003, user004]\r\n24 17:45:03,731 [randomwalk.Framework] ERROR: Error during random walk\r\njava.lang.Exception: Error running node Concurrent.xml\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:285)\r\n        at org.apache.accumulo.test.randomwalk.Framework.run(Framework.java:65)\r\n        at org.apache.accumulo.test.randomwalk.Framework.main(Framework.java:125)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:606)\r\n        at org.apache.accumulo.start.Main$1.run(Main.java:137)\r\n        at java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.Exception: Error running node ct.RenameTable\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:285)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:254)\r\n        ... 8 more\r\nCaused by: org.apache.accumulo.core.client.AccumuloException: Internal error processing waitForTableOperation\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:334)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:300)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.rename(TableOperationsImpl.java:773)\r\n        at org.apache.accumulo.test.randomwalk.concurrent.RenameTable.visit(RenameTable.java:44)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:254)\r\n        ... 9 more\r\nCaused by: org.apache.thrift.TApplicationException: Internal error processing waitForTableOperation\r\n        at org.apache.thrift.TApplicationException.read(TApplicationException.java:108)\r\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:71)\r\n        at org.apache.accumulo.core.master.thrift.MasterClientService$Client.recv_waitForTableOperation(MasterClientService.java:610)\r\n        at org.apache.accumulo.core.master.thrift.MasterClientService$Client.waitForTableOperation(MasterClientService.java:595)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.waitForTableOperation(TableOperationsImpl.java:272)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:314)\r\n        ... 13 more\r\n{noformat}\r\n\r\nSaw following in master log\r\n\r\n{noformat}\r\n2013-12-24 17:45:03,941 [thrift.ProcessFunction] ERROR: Internal error processing waitForTableOperation\r\njava.lang.IllegalArgumentException: Namespace in new table name does not match the old table name\r\n        at org.apache.accumulo.master.tableOps.RenameTable.call(RenameTable.java:67)\r\n        at org.apache.accumulo.master.tableOps.RenameTable.call(RenameTable.java:37)\r\n        at org.apache.accumulo.master.tableOps.TraceRepo.call(TraceRepo.java:54)\r\n        at org.apache.accumulo.fate.Fate$TransactionRunner.run(Fate.java:67)\r\n        at org.apache.accumulo.fate.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        at java.lang.Thread.run(Thread.java:744)\r\n{noformat}",
        "concurrent randomwalk fails {noformat}\r\n19 21:53:01,992 [randomwalk.Framework] ERROR: Error during random walk\r\njava.lang.Exception: Error running node Concurrent.xml\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:285)\r\n        at org.apache.accumulo.test.randomwalk.Framework.run(Framework.java:65)\r\n        at org.apache.accumulo.test.randomwalk.Framework.main(Framework.java:125)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:606)\r\n        at org.apache.accumulo.start.Main$1.run(Main.java:137)\r\n        at java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.Exception: Error running node ct.RenameTable\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:285)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:254)\r\n        ... 8 more\r\nCaused by: org.apache.accumulo.core.client.AccumuloException: Internal error processing waitForTableOperation\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:334)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:300)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.rename(TableOperationsImpl.java:773)\r\n        at org.apache.accumulo.test.randomwalk.concurrent.RenameTable.visit(RenameTable.java:44)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:254)\r\n        ... 9 more\r\nCaused by: org.apache.thrift.TApplicationException: Internal error processing waitForTableOperation\r\n        at org.apache.thrift.TApplicationException.read(TApplicationException.java:108)\r\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:71)\r\n        at org.apache.accumulo.core.master.thrift.MasterClientService$Client.recv_waitForTableOperation(MasterClientService.java:610)\r\n        at org.apache.accumulo.core.master.thrift.MasterClientService$Client.waitForTableOperation(MasterClientService.java:595)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.waitForTableOperation(TableOperationsImpl.java:272)\r\n        at org.apache.accumulo.core.client.admin.TableOperationsImpl.doTableOperation(TableOperationsImpl.java:314)\r\n        ... 13 more\r\n\r\n{noformat}\r\n\r\nThis is the error on the server side:\r\n\r\n{noformat}\r\nFailed to execute Repo, tid=366be645a5bfb90e\r\n\tjava.lang.IllegalArgumentException: Namespace in new table name does not match the old table name\r\n\t\tat org.apache.accumulo.master.tableOps.RenameTable.call(RenameTable.java:67)\r\n\t\tat org.apache.accumulo.master.tableOps.RenameTable.call(RenameTable.java:37)\r\n\t\tat org.apache.accumulo.master.tableOps.TraceRepo.call(TraceRepo.java:54)\r\n\t\tat org.apache.accumulo.fate.Fate$TransactionRunner.run(Fate.java:67)\r\n\t\tat org.apache.accumulo.fate.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n\t\tat java.lang.Thread.run(Thread.java:744)\r\n{noformat}\r\n\r\n"
    ],
    [
        "ACCUMULO-2106",
        "ACCUMULO-2104",
        "[RW] Multitable.Write failed writing to non-existent table On the client:\r\n\r\n{noformat}\r\n27 06:22:31,086 [randomwalk.Framework] ERROR: Error during random walk\r\njava.lang.Exception: Error running node MultiTable.xml\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:285)\r\n        at org.apache.accumulo.test.randomwalk.Framework.run(Framework.java:65)\r\n        at org.apache.accumulo.test.randomwalk.Framework.main(Framework.java:125)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:606)\r\n        at org.apache.accumulo.start.Main$1.run(Main.java:137)\r\n        at java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.Exception: Error running node mt.Write\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:285)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:254)\r\n        ... 8 more\r\nCaused by: org.apache.accumulo.core.client.MutationsRejectedException: # constraint violations : 0  security codes: {}  # server errors 3 # exceptions 3\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter.checkForFailures(TabletServerBatchWriter.java:537)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter.addMutation(TabletServerBatchWriter.java:249)\r\n        at org.apache.accumulo.core.client.impl.MultiTableBatchWriterImpl$TableBatchWriter.addMutation(MultiTableBatchWriterImpl.java:64)\r\n        at org.apache.accumulo.test.randomwalk.multitable.Write.visit(Write.java:81)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:254)\r\n        ... 9 more\r\nCaused by: org.apache.accumulo.core.client.impl.AccumuloServerException: Error on server tserver1:9997\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.sendMutationsToTabletServer(TabletServerBatchWriter.java:937)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.access$1600(TabletServerBatchWriter.java:616)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter$SendTask.send(TabletServerBatchWriter.java:801)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter$SendTask.run(TabletServerBatchWriter.java:765)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        ... 1 more\r\nCaused by: org.apache.thrift.TApplicationException: Internal error processing applyUpdates\r\n        at org.apache.thrift.TApplicationException.read(TApplicationException.java:108)\r\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:71)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Client.recv_closeUpdate(TabletClientService.java:431)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Client.closeUpdate(TabletClientService.java:417)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.sendMutationsToTabletServer(TabletServerBatchWriter.java:899)\r\n        ... 10 more\r\n{noformat}\r\n\r\nOn the server:\r\n\r\n{noformat}\r\n2013-12-27 06:22:30,475 [thrift.ProcessFunction] ERROR: Internal error processing applyUpdates\r\njava.lang.IllegalArgumentException: Table with id kq does not exist\r\n        at org.apache.accumulo.core.client.impl.Tables.getNamespace(Tables.java:218)\r\n        at org.apache.accumulo.server.security.SecurityOperation.hasNamespacePermissionForTableId(SecurityOperation.java:330)\r\n        at org.apache.accumulo.server.security.SecurityOperation.canWrite(SecurityOperation.java:410)\r\n        at org.apache.accumulo.tserver.TabletServer$ThriftClientHandler.setUpdateTablet(TabletServer.java:1477)\r\n        at org.apache.accumulo.tserver.TabletServer$ThriftClientHandler.applyUpdates(TabletServer.java:1521)\r\n        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:606)\r\n        at org.apache.accumulo.trace.instrument.thrift.TraceWrap$1.invoke(TraceWrap.java:63)\r\n        at com.sun.proxy.$Proxy17.applyUpdates(Unknown Source)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor$applyUpdates.getResult(TabletClientService.java:2347)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor$applyUpdates.getResult(TabletClientService.java:2333)\r\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\r\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\r\n        at org.apache.accumulo.server.util.TServerUtils$TimedProcessor.process(TServerUtils.java:171)\r\n        at org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:478)\r\n        at org.apache.accumulo.server.util.TServerUtils$THsHaServer$Invocation.run(TServerUtils.java:231)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        at java.lang.Thread.run(Thread.java:744)\r\n{noformat}",
        "[RW] Image failed on writing to a non-existent table On the client:\r\n\r\n{noformat}\r\n27 01:15:32,206 [randomwalk.Framework] ERROR: Error during random walk\r\njava.lang.Exception: Error running node Sequential.xml\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:285)\r\n        at org.apache.accumulo.test.randomwalk.Framework.run(Framework.java:65)\r\n        at org.apache.accumulo.test.randomwalk.Framework.main(Framework.java:125)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:606)\r\n        at org.apache.accumulo.start.Main$1.run(Main.java:137)\r\n        at java.lang.Thread.run(Thread.java:744)\r\nCaused by: org.apache.accumulo.core.client.MutationsRejectedException: # constraint violations : 0  security codes: {}  # server errors 3 # exceptions 3\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter.checkForFailures(TabletServerBatchWriter.java:537)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter.addMutation(TabletServerBatchWriter.java:249)\r\n        at org.apache.accumulo.core.client.impl.MultiTableBatchWriterImpl$TableBatchWriter.addMutation(MultiTableBatchWriterImpl.java:64)\r\n        at org.apache.accumulo.test.randomwalk.sequential.Write.visit(Write.java:45)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:203)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:254)\r\n        ... 8 more\r\nCaused by: org.apache.accumulo.core.client.impl.AccumuloServerException: Error on server tserver2:9997\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.sendMutationsToTabletServer(TabletServerBatchWriter.java:937)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.access$1600(TabletServerBatchWriter.java:616)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter$SendTask.send(TabletServerBatchWriter.java:801)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter$SendTask.run(TabletServerBatchWriter.java:765)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        ... 1 more\r\nCaused by: org.apache.thrift.TApplicationException: Internal error processing applyUpdates\r\n        at org.apache.thrift.TApplicationException.read(TApplicationException.java:108)\r\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:71)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Client.recv_closeUpdate(TabletClientService.java:431)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Client.closeUpdate(TabletClientService.java:417)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.sendMutationsToTabletServer(TabletServerBatchWriter.java:899)\r\n        ... 10 more\r\n{noformat}\r\n\r\nOn the tserver:\r\n\r\n{noformat}\r\n2013-12-27 01:15:30,334 [thrift.ProcessFunction] ERROR: Internal error processing applyUpdates\r\njava.lang.IllegalArgumentException: Table with id i9 does not exist\r\n        at org.apache.accumulo.core.client.impl.Tables.getNamespace(Tables.java:218)\r\n        at org.apache.accumulo.server.security.SecurityOperation.hasNamespacePermissionForTableId(SecurityOperation.java:330)\r\n        at org.apache.accumulo.server.security.SecurityOperation.canWrite(SecurityOperation.java:410)\r\n        at org.apache.accumulo.tserver.TabletServer$ThriftClientHandler.setUpdateTablet(TabletServer.java:1477)\r\n        at org.apache.accumulo.tserver.TabletServer$ThriftClientHandler.applyUpdates(TabletServer.java:1521)\r\n        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:606)\r\n        at org.apache.accumulo.trace.instrument.thrift.TraceWrap$1.invoke(TraceWrap.java:63)\r\n        at com.sun.proxy.$Proxy17.applyUpdates(Unknown Source)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor$applyUpdates.getResult(TabletClientService.java:2347)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor$applyUpdates.getResult(TabletClientService.java:2333)\r\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\r\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\r\n        at org.apache.accumulo.server.util.TServerUtils$TimedProcessor.process(TServerUtils.java:171)\r\n        at org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:478)\r\n        at org.apache.accumulo.server.util.TServerUtils$THsHaServer$Invocation.run(TServerUtils.java:231)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        at java.lang.Thread.run(Thread.java:744)\r\n{noformat}"
    ],
    [
        "ACCUMULO-2168",
        "ACCUMULO-1995",
        "WriteAheadLogIT failed Saw the following the tserver log when WriteAheadLogIT failed.  I think this prevented the table from loading, which caused the test timeout.  The message was repeated.   I the // in hdfs:// is what zookeeper is complaining about.\r\n\r\n{noformat}\r\n2014-01-09 19:45:54,791 [util.MetadataTableUtil] ERROR: java.lang.IllegalArgumentException: Invalid path string \"/accumulo/af3b249b-d1ff-4c94-acd5-83f35e3647b8/root_tablet/walogs/hdfs://localhost:52141/accumulo/wal/node1+41165/fb6791ec-9cdc-4d73-aef5-60ad59bb7a15\" caused by empty node name specified @72\r\njava.lang.IllegalArgumentException: Invalid path string \"/accumulo/af3b249b-d1ff-4c94-acd5-83f35e3647b8/root_tablet/walogs/hdfs://localhost:52141/accumulo/wal/node1+41165/fb6791ec-9cdc-4d73-aef5-60ad59bb7a15\" caused by empty node name specified @72\r\n        at org.apache.zookeeper.common.PathUtils.validatePath(PathUtils.java:99)\r\n        at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1231)\r\n        at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1277)\r\n        at org.apache.accumulo.fate.zookeeper.ZooUtil.recursiveDelete(ZooUtil.java:103)\r\n        at org.apache.accumulo.fate.zookeeper.ZooUtil.recursiveDelete(ZooUtil.java:117)\r\n        at org.apache.accumulo.fate.zookeeper.ZooReaderWriter.recursiveDelete(ZooReaderWriter.java:65)\r\n        at org.apache.accumulo.server.util.MetadataTableUtil.removeUnusedWALEntries(MetadataTableUtil.java:611)\r\n        at org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1393)\r\n        at org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1235)\r\n        at org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1090)\r\n        at org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1079)\r\n        at org.apache.accumulo.tserver.TabletServer$AssignmentHandler.run(TabletServer.java:2895)\r\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        at org.apache.accumulo.tserver.TabletServer$ThriftClientHandler$3.run(TabletServer.java:2262)\r\n{noformat}",
        "TServer refuses to start if WALog file has only header and no entries In the course of exercising Accumulo 1.6.0 with encryption turned on, we were able to get tablet servers into the following state many times, wherein the tablet server would not start up and instead just print this over and over to the log:\r\n\r\n{noformat}\r\n2013-12-09 14:13:04,487 [util.MetadataTableUtil] ERROR: java.lang.IllegalArgumentException: Invalid path string \"/accumulo/0147d545-64bd-4383-b842-27c62283c208/root_tablet/walogs/hdfs://10.10.1.10:9000/accumulo/wal/10.10.1.10+9997/e42efc86-6d83-4c0e-abd6-b16ec74d0a9f\" caused by empty node name specified @72\r\njava.lang.IllegalArgumentException: Invalid path string \"/accumulo/0147d545-64bd-4383-b842-27c62283c208/root_tablet/walogs/hdfs://10.10.1.10:9000/accumulo/wal/10.10.1.10+9997/e42efc86-6d83-4c0e-abd6-b16ec74d0a9f\" caused by empty node name specified @72\r\n\tat org.apache.zookeeper.common.PathUtils.validatePath(PathUtils.java:99)\r\n\tat org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1450)\r\n\tat org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1496)\r\n\tat org.apache.accumulo.fate.zookeeper.ZooUtil.recursiveDelete(ZooUtil.java:103)\r\n\tat org.apache.accumulo.fate.zookeeper.ZooUtil.recursiveDelete(ZooUtil.java:117)\r\n\tat org.apache.accumulo.fate.zookeeper.ZooReaderWriter.recursiveDelete(ZooReaderWriter.java:64)\r\n\tat org.apache.accumulo.server.util.MetadataTableUtil.removeUnusedWALEntries(MetadataTableUtil.java:611)\r\n\tat org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1394)\r\n\tat org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1236)\r\n\tat org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1091)\r\n\tat org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1079)\r\n\tat org.apache.accumulo.tserver.TabletServer$AssignmentHandler.run(TabletServer.java:2892)\r\n\tat org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n\tat org.apache.accumulo.tserver.TabletServer$ThriftClientHandler$3.run(TabletServer.java:2261)\r\n{noformat}\r\n\r\nThe log file in question had a good header but no entries in it, probably the result of killing a tablet server before it had any data to write.  The bug in the code has to do with how log file names are now held within ZooKeeper (they have a full path) versus how the paths to their metadata within  ZooKeeper are eventually constructed by {{MetadataTableUtil}}.  The fix is relatively straightforward; you just need to take apart the full HDFS path before appending it to a different ZooKeeper path.\r\n\r\nPatch coming.\r\n"
    ],
    [
        "ACCUMULO-2329",
        "ACCUMULO-2153",
        "\"egrep: /home/user/accumulo-1.5.1/conf/gc: No such file or directory\" Testing 1.5.1rc1, didn't create a gc file in conf, and I got this error on start-up.\r\n",
        "configuration file for gc role should remain optional somewhere after 1.5.0, the start up scripts changed to that the gc file is now required. in 1.5.0, the first master would be used when it was missing.\r\n\r\nWe should restore the previous behavior for hte 1.5.x line."
    ],
    [
        "ACCUMULO-2333",
        "ACCUMULO-1940",
        "\"File does not exist\" error during client ingest with agitation While running the agitator during a client ingest test, encountered a \"File does not exist\" error that stuck in the Table Problems section of the monitor page. \r\n\r\nConfirmed that the file in question had been compacted away previously.\r\n\r\nWhile it appears that no data was lost, it is strange that the error surfaced and then seemed to right itself shortly thereafter. (though not updating the Table Problems section)\r\n\r\nHere is the stacktrace from the Monitor:\r\n{code}\r\nFile does not exist: /accumulo/tables/2/t-00000dj/F0000dwj.rf at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61) at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:51) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1540) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1483) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1463) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1437) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:468) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:269) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:59566) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2053) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2047)\r\n{code}\r\n\r\n\r\n*UPDATE (from comments below):*\r\nOn a cluster with 15 slaves, two of the participating tablet servers had logs referencing the file.\r\n\r\nslave05 was one that was killed by the agitator at 20:33 and then restarted at 20:43, where it immediately compacted F0000dwj.rf. That file had been created by slave03 at 20:34 when slave05 was offline. slave03, who seems to have previously been responsible for the file, then tried to perform a MajC at 21:10, which caused the exceptions to appear in the monitor. It seems that the master was also killed at 21:02 and was revived at 21:05. It appears that the \"missing\" extent was never unloaded and re-assigned before the failure.\r\n\r\nThere were RuntimeExceptions reported by slave03 at about 20:34 as well, so there's a chance that slave03's actions at that time did not complete cleanly.\r\n\r\nI'm attaching logs for the time and pertinent servers.",
        "Data file in !METADATA differs from in memory data Found during CI run with agitation.\r\n\r\nGot the first two error messages 5 times (assuming in a retry on failure block):\r\n\r\n{noformat}\r\nFailed to do close consistency check for tablet c;79d0ab;7870a\r\n\tjava.lang.RuntimeException: Data file in !METADATA differ from in memory data c;79d0ab;7870a  {/t-0005h1j/A0005n8k.rf=797350457 19198312, /t-0005h1j/C0005skm.rf=798078368 19322025, /t-0005h1j/C0005tet.rf=89783168 2196349, /t-0005h1j/C0005u20.rf=90979448 2227972, /t-0005h1j/F0005u0v.rf=23410023 582233, /t-0005h1j/F0005u2p.rf=21958551 547159, /t-0005h1j/F0005u3g.rf=14395121 358893}  {/t-0005h1j/A0005n8k.rf=797350457 19198312, /t-0005h1j/C0005skm.rf=798078368 19322025, /t-0005h1j/C0005tet.rf=89783168 2196349, /t-0005h1j/C0005u20.rf=90979448 2227972, /t-0005h1j/F0005u2p.rf=21958551 547159, /t-0005h1j/F0005u3g.rf=14395121 358893}\r\n\t\tat org.apache.accumulo.server.tabletserver.Tablet.closeConsistencyCheck(Tablet.java:2847)\r\n\t\tat org.apache.accumulo.server.tabletserver.Tablet.completeClose(Tablet.java:2780)\r\n\t\tat org.apache.accumulo.server.tabletserver.Tablet.close(Tablet.java:2658)\r\n\t\tat org.apache.accumulo.server.tabletserver.TabletServer$UnloadTabletHandler.run(TabletServer.java:2357)\r\n\t\tat org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n\t\tat org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tat org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n\t\tat org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n\t\tat java.lang.Thread.run(Thread.java:744)\r\n{noformat}\r\n\r\nThen, we logged that we failed the consistency check\r\n\r\n{noformat}\r\nConsistency check fails, retrying java.lang.RuntimeException: Failed to do close consistency check for tablet c;79d0ab;7870a\r\n{noformat}\r\n\r\nIn the end, we gave up and closed it anyways.\r\n\r\n{noformat}\r\nTablet closed consistency check has failed for c;79d0ab;7870a giving up and closing\r\n{noformat}\r\n\r\nBefore all of this happened, we tried to bring this tablet online after a failure on a new tserver. During the minc as part of the recovery process, we failed to get the lease on the .rf_tmp file we tried to create. We failed this a couple of times, but eventually got the tmp file we needed and the recovery process completed and we could bring the tablet online. The difference between the in-memory version and the !METADATA version was this one flushed rfile that we created during this recovery process.\r\n\r\nThe problem eventually fixed itself because the tablet was migrated to a different server and we just took what was (correctly) in the !METADATA table.\r\n\r\nThere still is an unknown issue of how we missed the flush RFile in the DatafileManager's copy."
    ],
    [
        "ACCUMULO-2360",
        "ACCUMULO-2351",
        "Need a way to configure TNonblockingServer.maxReadBufferBytes to prevent OOMs from network misbehavour 1.5.0 introduced GENERAL_MAX_MESSAGE_SIZE (ACCUMULO-1141), a parameter to set the maximum frame size for the TFramedTransport. However, there is an underlying frame (I think this is a glossary conflict) read in TNonblockingServer that can still cause OOM errors if erroneously connected to (telnet, netcat, etc.), creating a stack trace as such\r\n\r\n{code}2014-02-12 10:26:40,439 [util.TServerUtils$THsHaServer] ERROR: run() exiting due to uncaught error\r\njava.lang.OutOfMemoryError: Java heap space\r\n        at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\r\n        at java.nio.ByteBuffer.allocate(ByteBuffer.java:329)\r\n        at org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.read(AbstractNonblockingServer.java:338)\r\n        at org.apache.thrift.server.AbstractNonblockingServer$AbstractSelectThread.handleRead(AbstractNonblockingServer.java:202)\r\n        at org.apache.thrift.server.TNonblockingServer$SelectAcceptThread.select(TNonblockingServer.java:198)\r\n        at org.apache.thrift.server.TNonblockingServer$SelectAcceptThread.run(TNonblockingServer.java:154){code}\r\n\r\nI believe if we set maxReadBufferBytes to the server arguments, it will filter appropriately. The only decision I'm not sure about is if we should recycle the max message property or have a separate one.",
        "Master memory leak This has been seen a few times. Master with Xms1g and Xmx4g, which should be more than enough. Most recent case, with 44 nodes, 1.74k tablets, and 22 tables including !METADATA. There are NO conspicuous messages in the master (just DefaultLoadBalancer messages for each table). Possible exception for an error of \"received invalid frame size of -..., are you using TTframeProtocol (can't remember exact message). But then the master out file has a message about OoM received, kill -9. \r\n\r\nI don't really know how to get more information out of it for when this does occur again."
    ],
    [
        "ACCUMULO-2370",
        "ACCUMULO-3658",
        "GarbageCollectorIT runs with miniDfs, only not really ",
        "GarbageCollectorIT runs with miniDfs, only not really {{useMiniDfs()}} is a getter, not a setter\r\n"
    ],
    [
        "ACCUMULO-2408",
        "ACCUMULO-1861",
        "metadata table not assigned after root table is loaded During a nightly integration test run, BigRootTableIT failed, timing out after 4 minutes:\r\n\r\n{noformat}\r\njava.lang.Exception: test timed out after 240000 milliseconds\r\n\tat sun.misc.Unsafe.park(Native Method)\r\n\tat java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1033)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1326)\r\n\tat java.util.concurrent.CountDownLatch.await(CountDownLatch.java:282)\r\n\tat org.apache.accumulo.core.client.admin.TableOperationsImpl.addSplits(TableOperationsImpl.java:437)\r\n\tat org.apache.accumulo.test.functional.BigRootTabletIT.test(BigRootTabletIT.java:50)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n{noformat}\r\n\r\nLooking at the logs, the root tablet is assigned successfully:\r\n\r\n{noformat}\r\n2014-02-26 05:17:09,414 [state.ZooTabletStateStore] DEBUG: Returning root tablet state: +r<<@(tserver1:9997[1446db2884a0002],null,null)\r\n2014-02-26 05:17:09,596 [master.EventCoordinator] INFO : tablet +r<< was loaded on tserver1:9997\r\n{noformat}\r\n\r\nNo other tablets are assigned for the next four minutes.\r\n\r\nThe logs are full of \"Failed to bin\" errors:\r\n\r\n{noformat}\r\n2014-02-26 05:19:09,613 [impl.ThriftTransportPool] TRACE: Using existing connection to tserver1:9997\r\n2014-02-26 05:19:09,615 [impl.ThriftTransportPool] TRACE: Returned connection tserver1:9997 (120000) ioCount : 562\r\n2014-02-26 05:19:09,615 [metadata.MetadataLocationObtainer] TRACE: tid=28 oid=3448  Got 2 results  from +r<< in 0.002 secs\r\n2014-02-26 05:19:09,615 [impl.TabletLocatorImpl] TRACE: tid=28 oid=3446  Binned 1 ranges for table !0 to 0 tservers in 0.003 secs\r\n2014-02-26 05:19:09,616 [impl.TabletServerBatchReaderIterator] TRACE: Failed to bin 1 ranges, tablet locations were null, retrying in 100ms\r\n{noformat}\r\n\r\nThere is an IOException, trying to do a batch read\r\n\r\n{noformat}\r\n2014-02-26 05:19:09,687 [impl.TabletServerBatchReaderIterator] DEBUG: Server : tserver1:9997 msg : java.net.SocketTimeoutException: 120000 millis timeout while\r\n waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/192.168.1.2:52818 remote=tserver1/192.168.1.1:9997]\r\n2014-02-26 05:19:09,689 [impl.TabletServerBatchReaderIterator] DEBUG: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: 120000 millis timeout while waiting\r\n for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/192.168.1.2:52818 remote=tserver1/192.168.1.1:9997]\r\njava.io.IOException: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.\r\nchannels.SocketChannel[connected local=/192.168.1.2:52818 remote=tserver1/192.168.1.1:9997]\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchReaderIterator.doLookup(TabletServerBatchReaderIterator.java:713)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchReaderIterator$QueryTask.run(TabletServerBatchReaderIterator.java:372)\r\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        at java.lang.Thread.run(Thread.java:744)\r\nCaused by: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/192.168.1.2:52818 remote=tserver1/192.168.1.1:9997]\r\n        at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:129)\r\n        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)\r\n        at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)\r\n        at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)\r\n        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)\r\n        at org.apache.accumulo.core.client.impl.ThriftTransportPool$CachedTTransport.readAll(ThriftTransportPool.java:270)\r\n        at org.apache.thrift.protocol.TCompactProtocol.readByte(TCompactProtocol.java:601)\r\n        at org.apache.thrift.protocol.TCompactProtocol.readMessageBegin(TCompactProtocol.java:470)\r\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Client.recv_startMultiScan(TabletClientService.java:311)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Client.startMultiScan(TabletClientService.java:291)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchReaderIterator.doLookup(TabletServerBatchReaderIterator.java:658)\r\n        ... 7 more\r\nCaused by: java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/192.168.1.2:52818 remote=tserver1/192.168.1.1:9997]\r\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)\r\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)\r\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)\r\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)\r\n        at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)\r\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)\r\n        at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)\r\n        ... 18 more\r\n2014-02-26 05:19:09,693 [impl.TabletServerBatchReaderIterator] TRACE: Failed to execute multiscans against 1 tablets, retrying...\r\n{noformat}\r\n\r\nThis would appear to be the batch scanner used to read the root table in the master.\r\n\r\nThe tablet server hosting the root tablet is being successfully scanned more that 24x a second, presumably from clients.\r\n\r\nThere are no errors in the tserver logs.\r\n\r\n",
        "MetadataSplitIT test failed 1.6.0-SNAPSHOT, 61a4298c60c00bc9ae1db4ef02b5dca13f2f3c5b\r\n\r\nRunning \"mvn verify\" ... MetadataSplitIT split failed.  Analysis of the logs show that the master assigned the Root Table, but did not read the Root Table and assign the !METADATA table.\r\n"
    ],
    [
        "ACCUMULO-2417",
        "ACCUMULO-2405",
        "MetadataIT fails Nightly integration test run failed:\r\n\r\n{noformat}\r\njava.lang.AssertionError\r\n\tat org.junit.Assert.fail(Assert.java:86)\r\n\tat org.junit.Assert.assertTrue(Assert.java:41)\r\n\tat org.junit.Assert.assertTrue(Assert.java:52)\r\n\tat org.apache.accumulo.test.functional.MetadataIT.mergeMeta(MetadataIT.java:103)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:622)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n{noformat}\r\n\r\nLooking at the test, I'm not sure why we're checking that the delete section of the root table has entries after a merge.",
        "Race condition in MetadataIT.mergeMeta() Saw the following error while running ITs.\r\n\r\n{noformat}\r\nmergeMeta(org.apache.accumulo.test.functional.MetadataIT)  Time elapsed: 10.39 sec  <<< FAILURE!\r\njava.lang.AssertionError\r\n\tat org.junit.Assert.fail(Assert.java:86)\r\n\tat org.junit.Assert.assertTrue(Assert.java:41)\r\n\tat org.junit.Assert.assertTrue(Assert.java:52)\r\n\tat org.apache.accumulo.test.functional.MetadataIT.mergeMeta(MetadataIT.java:103)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n{noformat}\r\n\r\nLooking at the test it merges the metadata table, sleeps for 2 secs, and then scans for delete entries. Looking in the logs I think the AGC deleted the delete entries while the test was sleeping.\r\n\r\nSaw the following in the master log\r\n\r\n{noformat}\r\n2014-02-25 12:06:36,981 [master.EventCoordinator] INFO : Merge state of !0<< set to MERGING\r\n2014-02-25 12:06:37,047 [master.EventCoordinator] INFO : Merge state of !0<< set to COMPLETE\r\n2014-02-25 12:06:37,169 [state.MergeStats] INFO : Computing next merge state for !0<< which is presently COMPLETE isDelete : false\r\n2014-02-25 12:06:37,189 [master.EventCoordinator] INFO : Merge state of !0<< set to NONE\r\n2014-02-25 12:06:37,212 [tableOps.MasterRepo] INFO : removing merge information No Merge in progress\r\n2014-02-25 12:06:37,213 [master.EventCoordinator] INFO : Merge state of !0 cleared\r\n{noformat}\r\n\r\nThen saw the following in the AGC logs.\r\n\r\n{noformat}\r\n2014-02-25 12:06:39,335 [gc.SimpleGarbageCollector] DEBUG: Deleting file:/local/disk1/jenkins/workspace/accumulo16/test/target/mini-tests/org.apache.accumulo.test.functional.SimpleMacIT/1393347971697_26818/accumulo/tables/!0/t-000003u\r\n2014-02-25 12:06:39,335 [gc.SimpleGarbageCollector] DEBUG: Deleting file:/local/disk1/jenkins/workspace/accumulo16/test/target/mini-tests/org.apache.accumulo.test.functional.SimpleMacIT/1393347971697_26818/accumulo/tables/!0/t-000003v\r\n2014-02-25 12:06:39,335 [fs.TrashPolicyDefault] INFO : Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n2014-02-25 12:06:39,335 [gc.SimpleGarbageCollector] DEBUG: Deleting file:/local/disk1/jenkins/workspace/accumulo16/test/target/mini-tests/org.apache.accumulo.test.functional.SimpleMacIT/1393347971697_26818/accumulo/tables/!0/t-000003w\r\n2014-02-25 12:06:39,336 [gc.SimpleGarbageCollector] DEBUG: Deleting file:/local/disk1/jenkins/workspace/accumulo16/test/target/mini-tests/org.apache.accumulo.test.functional.SimpleMacIT/1393347971697_26818/accumulo/tables/!0/t-000003y\r\n2014-02-25 12:06:39,336 [gc.SimpleGarbageCollector] DEBUG: Deleting file:/local/disk1/jenkins/workspace/accumulo16/test/target/mini-tests/org.apache.accumulo.test.functional.SimpleMacIT/1393347971697_26818/accumulo/tables/!0/table_info\r\n2014-02-25 12:06:39,336 [gc.SimpleGarbageCollector] DEBUG: Deleting file:/local/disk1/jenkins/workspace/accumulo16/test/target/mini-tests/org.apache.accumulo.test.functional.SimpleMacIT/1393347971697_26818/accumulo/tables/!0/t-000003x\r\n  .\r\n  .\r\n  .\r\n2014-02-25 12:06:39,343 [impl.RootTabletLocator] TRACE: tid=12 oid=91  Found root tablet at host1:40158|1446a02ccfe0002 in 0.000 secs\r\n2014-02-25 12:06:39,344 [impl.TabletServerBatchWriter] TRACE: Started sending 15 mutations to 1 tablet servers\r\n2014-02-25 12:06:39,344 [impl.ThriftTransportPool] TRACE: Using existing connection to host1:40158\r\n2014-02-25 12:06:39,366 [impl.ThriftTransportPool] TRACE: Returned connection host1:40158 (120000) ioCount : 586\r\n2014-02-25 12:06:39,367 [impl.TabletServerBatchWriter] TRACE: sent 15 mutations to host1:40158 in 0.02 secs (681.82 mutations/sec) with 0 failures\r\n{noformat}"
    ],
    [
        "ACCUMULO-2426",
        "ACCUMULO-1920",
        "monitor not updating table status Monitor displayed a new table's status as UNKNOWN even when the state in zookeeper was ONLINE.",
        "monitor not seeing zookeeper updates Started RandomWalk test, and didn't see any tables being created.  After selecting a tablet server, I started seeing entries for unknown tables."
    ],
    [
        "ACCUMULO-2481",
        "ACCUMULO-1723",
        "Monitor should still work if HDFS + Zookeeper go away While preparing to update a test cluster, I accidentally attempted to refresh some monitor pages after I had shut down the underlying HDFS and Zookeeper services. The page simply hung. I've since tried loading several different pages (overview, logs, master view) and all simple hang on the monitor's end. If I restart the underlying cluster, the pages resolve like normal.\r\n\r\nI haven't had a chance to see if this happens on later versions, nor figure out which missing component matters.\r\n\r\nExpected behavior would be for the monitor to still load, albeit with a bunch of alarm bells about everything being off.",
        "monitor service GET / hangs if Zookeeper down Once ZK is offline, you can't issue GET (or POST or HEAD, but not PUT) requests against URLs on the monitors web port. The requests just hang. Invalid HTTP requests get rejected by Jetty, so the process itself is running, it's just whatever servlet is fielding requests is blocking without timeouts waiting for ZK, or for a lock to be released by some other thread that is blocked on ZK.\r\n\r\nWhile a ZK outage is not something you'd plan for in a real cluster, it is the kind of thing you'd hope the monitor to pick up and explicit highlight as a problem -especially if the page is being fetched by a machine process rather than an individual"
    ],
    [
        "ACCUMULO-2502",
        "ACCUMULO-2419",
        "SimpleTimer#getInstanceThreadPoolSize() should be synchronized SimpleTimer#getInstanceThreadPoolSize() accesses instanceThreadPoolSize without protection.\r\n\r\ngetInstance() is synchronized.\r\ngetInstanceThreadPoolSize() should be synchronized as well.",
        "Improve SimpleTimer by replacing java.util.Timer The server utility class {{SimpleTimer}} uses a {{java.util.Timer}} under the hood for scheduling tasks. From _Java Concurrency in Practice_, p. 123:\r\n\r\nbq. {{Timer}} has some drawbacks, and {{ScheduledThreadPoolExecutor}} should be thought of as its replacement. ... there is little reason to use {{Timer}} in Java 5.0 or later.\r\n\r\nThe purpose of {{SimpleTimer}} is \"to reduce the number of threads dedicated to simple events\", but a user cannot opt to let more than one thread handle the events on systems that can take the load. Also, if any task does take a long time for some reason, execution of other tasks is affected.\r\n\r\nThe {{Timer}} in {{SimpleTimer}} should be replaced with {{ScheduledThreadPoolExecutor}}, and the class should allow for more than one thread to be used for task execution."
    ],
    [
        "ACCUMULO-2505",
        "ACCUMULO-2489",
        "IllegalTableTransitionException while attempting to drop a table Saw this while attempting a series of table deletes on a ec2 setup (i.e. minimal resources)\r\n\r\n{code}\r\njava.lang.RuntimeException: org.apache.accumulo.server.master.state.tables.TableManager$IllegalTableTransitionException\r\n        at org.apache.accumulo.server.master.state.tables.TableManager.transitionTableState(TableManager.java:150)\r\n        at org.apache.accumulo.server.master.tableOps.DeleteTable.call(DeleteTable.java:232)\r\n        at org.apache.accumulo.server.master.tableOps.DeleteTable.call(DeleteTable.java:215)\r\n        at org.apache.accumulo.server.master.tableOps.TraceRepo.call(TraceRepo.java:65)\r\n        at org.apache.accumulo.fate.Fate$TransactionRunner.run(Fate.java:64)\r\n        at org.apache.accumulo.fate.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        at java.lang.Thread.run(Thread.java:662)\r\nCaused by: org.apache.accumulo.server.master.state.tables.TableManager$IllegalTableTransitionException\r\n        at org.apache.accumulo.server.master.state.tables.TableManager$1.mutate(TableManager.java:143)\r\n        at org.apache.accumulo.fate.zookeeper.ZooReaderWriter.mutate(ZooReaderWriter.java:134)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n        at java.lang.reflect.Method.invoke(Method.java:597)\r\n        at org.apache.accumulo.server.zookeeper.ZooReaderWriter$1.invoke(ZooReaderWriter.java:68)\r\n        at com.sun.proxy.$Proxy10.mutate(Unknown Source)\r\n        at org.apache.accumulo.server.master.state.tables.TableManager.transitionTableState(TableManager.java:118)\r\n        ... 6 more\r\n{code}",
        "NPE in split tablet {code}\r\n2014-03-14 23:57:35,534 [thrift.ProcessFunction] ERROR: Internal error processing splitTablet\r\njava.lang.NullPointerException\r\n        at org.apache.accumulo.server.conf.TableConfiguration.get(TableConfiguration.java:122)\r\n        at org.apache.accumulo.server.conf.TableConfiguration.iterator(TableConfiguration.java:139)\r\n        at org.apache.accumulo.server.conf.TableConfiguration.addObserver(TableConfiguration.java:77)\r\n        at org.apache.accumulo.server.tabletserver.Tablet.<init>(Tablet.java:1375)\r\n        at org.apache.accumulo.server.tabletserver.Tablet.<init>(Tablet.java:1154)\r\n        at org.apache.accumulo.server.tabletserver.Tablet.<init>(Tablet.java:1140)\r\n        at org.apache.accumulo.server.tabletserver.TabletServer.splitTablet(TabletServer.java:2270)\r\n        at org.apache.accumulo.server.tabletserver.TabletServer.access$1100(TabletServer.java:231)\r\n        at org.apache.accumulo.server.tabletserver.TabletServer$ThriftClientHandler.splitTablet(TabletServer.java:1710)\r\n        at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n        at java.lang.reflect.Method.invoke(Method.java:597)\r\n        at org.apache.accumulo.trace.instrument.thrift.TraceWrap$1.invoke(TraceWrap.java:63)\r\n        at com.sun.proxy.$Proxy2.splitTablet(Unknown Source)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor$splitTablet.getResult(TabletClientService.java:2177)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor$splitTablet.getResult(TabletClientService.java:2161)\r\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\r\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\r\n        at org.apache.accumulo.server.util.TServerUtils$TimedProcessor.process(TServerUtils.java:155)\r\n        at org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:478)\r\n        at org.apache.accumulo.server.util.TServerUtils$THsHaServer$Invocation.run(TServerUtils.java:207)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\r\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        at java.lang.Thread.run(Thread.java:662){code}"
    ],
    [
        "ACCUMULO-2516",
        "ACCUMULO-2515",
        "TableConfigurationUpdateIT failing Not sure yet if it's just that my dev laptop is underpowered.\r\n\r\n{noformat}\r\ntest(org.apache.accumulo.test.TableConfigurationUpdateIT)  Time elapsed: 13.624 sec  <<< ERROR!\r\njava.lang.IllegalArgumentException: Accumulo volume file:/accumulo/instance_id not initialized\r\n        at org.apache.accumulo.server.ServerConstants.checkBaseDirs(ServerConstants.java:94)\r\n        at org.apache.accumulo.server.ServerConstants.getBaseDirs(ServerConstants.java:70)\r\n        at org.apache.accumulo.server.ServerConstants.getInstanceIdLocation(ServerConstants.java:141)\r\n        at org.apache.accumulo.server.client.HdfsZooInstance._getInstanceID(HdfsZooInstance.java:125)\r\n        at org.apache.accumulo.server.client.HdfsZooInstance.getInstanceID(HdfsZooInstance.java:119)\r\n        at org.apache.accumulo.core.zookeeper.ZooUtil.getRoot(ZooUtil.java:38)\r\n        at org.apache.accumulo.core.client.impl.Tables.getNamespaceId(Tables.java:303)\r\n        at org.apache.accumulo.server.conf.TableParentConfiguration.getNamespaceId(TableParentConfiguration.java:37)\r\n        at org.apache.accumulo.server.conf.TableParentConfiguration.<init>(TableParentConfiguration.java:32)\r\n        at org.apache.accumulo.test.TableConfigurationUpdateIT.test(TableConfigurationUpdateIT.java:73)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n        at java.lang.reflect.Method.invoke(Method.java:597)\r\n        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)\r\n        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)\r\n        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n        at java.lang.reflect.Method.invoke(Method.java:597)\r\n        at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)\r\n        at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)\r\n        at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)\r\n        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)\r\n        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)\r\nCaused by: java.lang.RuntimeException: Accumulo not initialized, there is no instance id at file:/accumulo/instance_id\r\n        at org.apache.accumulo.core.zookeeper.ZooUtil.getInstanceIDFromHdfs(ZooUtil.java:62)\r\n        at org.apache.accumulo.server.ServerConstants.checkBaseDirs(ServerConstants.java:87)\r\n        ... 40 more\r\n{noformat}",
        "NamespaceConfiguration doesn't invalidateCache The same changes (both the original from 1.5.1 and ACCUMULO-2489) in TableConfiguration are applicable to NamespaceConfiguration.\r\n\r\nWe should fix them rather than let 1.6.0 suffer the same bugs we already knew about. Thankfully, it's a direct copy-paste from TableConfiguration."
    ],
    [
        "ACCUMULO-2559",
        "ACCUMULO-2511",
        "Value.equals implementation is not symmetric {code}\r\n  @Override\r\n  public boolean equals(Object right_obj) {\r\n    if (right_obj instanceof byte[]) {\r\n      return compareTo((byte[]) right_obj) == 0;\r\n    }\r\n    if (right_obj instanceof Value) {\r\n      return compareTo(right_obj) == 0;\r\n    }\r\n    return false;\r\n  }\r\n{code}\r\n\r\nAlso, {{Value extends WritableComparable<Object>}} which makes the {{compareTo}} method suspect as well.",
        "Value allows equals(byte[]) Right now the equals(Object) method for Value is\r\n\r\n{noformat}\r\n @Override\r\n  public boolean equals(Object right_obj) {\r\n    if (right_obj instanceof byte[]) {\r\n      return compareTo((byte[]) right_obj) == 0;\r\n    }\r\n    if (right_obj instanceof Value) {\r\n      return compareTo(right_obj) == 0;\r\n    }\r\n    return false;\r\n  }\r\n\r\n{noformat}\r\n\r\nIt's in a section that says it was copied from BytesWritable.\r\n\r\nIf we're not using this optimization anywhere, I'd rather remove it since it is non-intuitive.\r\n\r\nIf we are using it, I'd prefer we move it into something other than the general equals"
    ],
    [
        "ACCUMULO-2875",
        "ACCUMULO-2874",
        "Scanner documentation wrong The \"[reading data|http://accumulo.apache.org/1.6/accumulo_user_manual.html#_reading_data]\" section of the user manual incorrectly identifies \"fetchFamily\" as the method name for configuring the column family to query when it should be \"fetchColumnFamily\" and shows entry.getKey().getRow() returning a String when it should be returning a Text object.\r\n\r\n(originally reported on the [dev list|http://mail-archives.apache.org/mod_mbox/accumulo-dev/201406.mbox/%3CCAPaCpY9ifHUdPMQU_u1RZEGL6JX9KFahDMO-RfV3BfzFYps4cQ%40mail.gmail.com%3E])",
        "user documentation errors Vicky Vat writes\r\n\r\n{quote}\r\nI have been looking at the \"Reading Data\" section [here|http://accumulo.apache.org/1.6/accumulo_user_manual.html#_reading_data].\r\n\r\nLooking at the code I don't see the fetchFamily method as mentioned in this line\r\n\r\n\r\n{{scan.fetchFamily(\"attributes\");}}\r\n\r\nLooks to me that the document is not updated or something had recently been changed in the API.\r\n\r\nI had a look at the history on 1.6.1-Snapshot branch, I can't make out if\r\nthe fectchFamily was renamed from it there\r\n[see ScannerOptions|https://github.com/apache/accumulo/commits/master/core/src/main/java/org/apache/accumulo/core/client/impl/ScannerOptions.java]\r\n\r\nIt seems the docs contains wrong method name of fetchFamily, wanted to\r\nconfirm before digging further or raising issue.\r\n\r\nThanks,\r\nVicky\r\n{quote}\r\n\r\nand:\r\n{quote}\r\nThe following piece also looks crazy\r\n{noformat}\r\nfor(Entry<Key,Value> entry : scan) {\r\n    String row = entry.getKey().getRow();\r\n    Value value = entry.getValue();\r\n{noformat}\r\n{quote}"
    ],
    [
        "ACCUMULO-2897",
        "ACCUMULO-2658",
        "Thrift proxy in Accumulo throws out of memory error on malformed request Thrift libraries have a well known Jira problem where malformed requests (e.g. curl) throw an Out of memory error on malformed requests. Length limit checks (possibly JIRA THRIFT-2572 issue Add string/collection length limit checks (from C++) to java.) \r\n\r\nThis is likely TBinaryProtocol. \r\n\r\n----------------------------------------------------------------------\r\nReplicating the issue: \r\n1) start Accumulo proxy \"./accumulo proxy -p ../proxy/proxy.properties \r\n2) curl -get localhost:proxyport \"Hi\"\r\n\r\nProxy will crash with OOM. ",
        "Thrift Proxy crashes with OOM on bad input The proxy server doesn't benefit from the fix for ACCUMULO-2360 because it doesn't use the TServerUtils class to set up the thrift server.\r\n\r\nThe smallest fix is really easy: just add the line:\r\n{code}\r\nargs.maxReadBufferBytes = maxFrameSize; \r\n{code}\r\nto o.a.a.proxy.Proxy.createProxyServer.\r\n\r\nI guess a more comprehensive fix would be to convert Proxy to use TServerUtils, but that's a little beyond me at this point.\r\n\r\nSteps to reproduce:\r\n1. Start your proxy server\r\n2. telnet localhost 42424\r\n3. Type \"stat\", press enter.\r\n\r\nExpected Behaviour:\r\n* The thrift server stays up and ignores the invalid input\r\n\r\nActual Behaviour:\r\n* The thrift server seems to interpret the text string as a buffer size and immediately crashes with this error written to the log file:\r\n{code}\r\n#\r\n# java.lang.OutOfMemoryError: Java heap space\r\n# -XX:OnOutOfMemoryError=\"kill -9 %p\"\r\n#   Executing /bin/sh -c \"kill -9 13396\"...\r\n{code}"
    ],
    [
        "ACCUMULO-2928",
        "ACCUMULO-1756",
        "Missing toString, hashCode and equals methods on BatchWriterConfig Tried to test equality of two BatchWriterConfig objects, found they're missing all of the methods from Object that they should be implementing.",
        "BatchWriterConfig could use a toString() method It would be convenient to have a toString method for BatchWriterConfig for debugging purposes for client code."
    ],
    [
        "ACCUMULO-3141",
        "ACCUMULO-2673",
        "Many RW failures due to balance check While running RW test against 1.5.2 RC1, 10 of 17 walkers failed with a message like the following.\r\n\r\n{noformat}\r\n16 19:35:48,820 [randomwalk.Framework] ERROR: Error during random walk\r\njava.lang.Exception: Error running node Concurrent.xml\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:285)\r\n        at org.apache.accumulo.test.randomwalk.Framework.run(Framework.java:63)\r\n        at org.apache.accumulo.test.randomwalk.Framework.main(Framework.java:122)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:606)\r\n        at org.apache.accumulo.start.Main$1.run(Main.java:107)\r\n        at java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.Exception: Error running node ct.CheckBalance\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:285)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:254)\r\n        ... 8 more\r\nCaused by: java.lang.Exception: servers are unbalanced! location 2487f8db354002f count 345 too far from average 151.86666666666667\r\n        at org.apache.accumulo.test.randomwalk.concurrent.CheckBalance.visit(CheckBalance.java:86)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:254)\r\n\r\n{noformat}",
        "Random walk balance check is still failing too frequently While running 17 randomwalk test walkers, several of them failed in under an hour due to balance check failures.  These were intermediate issues.  Perhaps the test for balance should check to see if there are offline tablets, and if so, assume that the master is rebalancing.\r\n"
    ],
    [
        "ACCUMULO-3180",
        "ACCUMULO-3179",
        "Backport \"scanners cannot be interrupted\" ACCUMULO-3030 was incorrectly fixed in only 1.6 and 1.7. This is a bug in 1.5 and should also be fixed there.",
        "Backport \"scanners cannot be interrupted\" ACCUMULO-3030 was incorrectly fixed in only 1.6 and 1.7. This is a bug in 1.5 and should also be fixed there."
    ],
    [
        "ACCUMULO-3182",
        "ACCUMULO-1759",
        "Empty or partial WAL header blocks successful recovery Haven't ever seen this one before. A replication IT failed -- looking into it, it was because the tserver that came up (after killing the original) failed to complete recovery. The below happened a few times before the test ultimately timed out.\r\n\r\n{noformat}\r\n2014-09-29 04:46:10,259 [zookeeper.DistributedWorkQueue] DEBUG: Looking for work in /accumulo/f98e79c4-9dcd-4fb0-8ec9-5804f0818839/recovery\r\n2014-09-29 04:46:10,340 [zookeeper.DistributedWorkQueue] DEBUG: got lock for af53bf1e-c293-463b-b4de-5efdb8b34962\r\n2014-09-29 04:46:10,341 [log.LogSorter] DEBUG: Sorting file:/.../test/target/mini-tests/org.apache.accumulo.test.replication.UnorderedWorkAssignerReplicationIT_dataReplicatedToCorrectTableWithoutDrain/accumulo/wal/juno+49195/af53bf1e-c293-463b-b4de-5efdb8b34962 to file:/.../test/target/mini-tests/org.apache.accumulo.test.replication.UnorderedWorkAssignerReplicationIT_dataReplicatedToCorrectTableWithoutDrain/accumulo/recovery/af53bf1e-c293-463b-b4de-5efdb8b34962 using sortId af53bf1e-c293-463b-b4de-5efdb8b34962\r\n2014-09-29 04:46:10,341 [log.LogSorter] INFO : Copying file:/var/lib/jenkins/home/jobs/Accumulo-Master-Integration-Tests/workspace/test/target/mini-tests/org.apache.accumulo.test.replication.UnorderedWorkAssignerReplicationIT_dataReplicatedToCorrectTableWithoutDrain/accumulo/wal/juno+49195/af53bf1e-c293-463b-b4de-5efdb8b34962 to file:/.../test/target/mini-tests/org.apache.accumulo.test.replication.UnorderedWorkAssignerReplicationIT_dataReplicatedToCorrectTableWithoutDrain/accumulo/recovery/af53bf1e-c293-463b-b4de-5efdb8b34962\r\n2014-09-29 04:46:10,345 [log.LogSorter] ERROR: java.io.EOFException\r\njava.io.EOFException\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:197)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:169)\r\n\tat org.apache.accumulo.tserver.log.DfsLogger.readHeaderAndReturnStream(DfsLogger.java:282)\r\n\tat org.apache.accumulo.tserver.log.LogSorter$LogProcessor.sort(LogSorter.java:113)\r\n\tat org.apache.accumulo.tserver.log.LogSorter$LogProcessor.process(LogSorter.java:93)\r\n\tat org.apache.accumulo.server.zookeeper.DistributedWorkQueue$1.run(DistributedWorkQueue.java:105)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n\tat org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n2014-09-29 04:46:10,346 [log.LogSorter] ERROR: Error during cleanup sort/copy af53bf1e-c293-463b-b4de-5efdb8b34962\r\njava.lang.NullPointerException\r\n\tat org.apache.accumulo.tserver.log.LogSorter$LogProcessor.close(LogSorter.java:183)\r\n\tat org.apache.accumulo.tserver.log.LogSorter$LogProcessor.sort(LogSorter.java:151)\r\n\tat org.apache.accumulo.tserver.log.LogSorter$LogProcessor.process(LogSorter.java:93)\r\n\tat org.apache.accumulo.server.zookeeper.DistributedWorkQueue$1.run(DistributedWorkQueue.java:105)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n\tat org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n{noformat}",
        "Empty walogs block recovery after power outage. Power was abruptly cut to the cluster. Upon restart of HDFS, there was a single Rfile that was missing a block. \r\n\r\nHere are some details about the cluster:\r\n* HDP 1.3\r\n** {{dfs.durable.sync=true}}\r\n** {{dfs.datanode.synconclose=true}}\r\n* encrytion patch from ACCUMULO-998\r\n\r\nAfter restarting Accumulo, the Master was complaining with a series of these:\r\n\r\n{code}\r\n2013-10-09 18:15:16,649 [recovery.HadoopLogCloser] INFO : Waiting for file to be closed /accumulo/wal/10.10.0.1+9997/d52ab315-5ac1-4a5c-9085-67ae29b98b88\r\n2013-10-09 18:15:16,663 [recovery.HadoopLogCloser] INFO : Waiting for file to be closed /accumulo/wal/10.10.0.2+9997/d0192739-74e2-43a0-985f-3ed668259995\r\n2013-10-09 18:15:16,742 [recovery.HadoopLogCloser] INFO : Waiting for file to be closed /accumulo/wal/10.10.0.3+9997/de54e6dc-964a-4b33-b4fb-052e81749913\r\n2013-10-09 18:15:16,833 [recovery.HadoopLogCloser] INFO : Waiting for file to be closed /accumulo/wal/10.10.0.4+9997/cda5daec-25f3-443b-818a-990d3eddd56f\r\n{code}\r\n\r\nInspection of the files above showed that they were all empty, but referenced in the {{!METADATA}} table. The solution was to move or remove the files from HDFS and delete the references from the metadata. The instance was then able to stabilize and assign the rest of the tablets.\r\n\r\nIt is unclear why these empty walogs existed in the first place. Is it possible that there should have been data in these walogs? Or should the files have been disregarded since they were empty?\r\n\r\nRegarding the Rfile that was missing the block, it was not being referenced by the {{!METADATA}} table, so removing it had no negative effect on the instance. Should there have been some reference to this file?"
    ],
    [
        "ACCUMULO-3239",
        "ACCUMULO-2341",
        "Running stop-all without accumulo running hangs indefinately Could have sworn I reported this, but couldn't find it.\r\n\r\nCalling stop-all gets stuck in the Admin command with the following:\r\n{code}\"admin\" prio=10 tid=0x0000000001973000 nid=0x548a waiting on condition [0x00007f56626d8000]\r\n   java.lang.Thread.State: TIMED_WAITING (sleeping)\r\n\tat java.lang.Thread.sleep(Native Method)\r\n\tat org.apache.accumulo.core.util.UtilWaitThread.sleep(UtilWaitThread.java:26)\r\n\tat org.apache.accumulo.core.client.impl.MasterClient.getConnectionWithRetry(MasterClient.java:48)\r\n\tat org.apache.accumulo.core.client.impl.MasterClient.executeGeneric(MasterClient.java:126)\r\n\tat org.apache.accumulo.core.client.impl.MasterClient.execute(MasterClient.java:171)\r\n\tat org.apache.accumulo.server.util.Admin.stopServer(Admin.java:303)\r\n\tat org.apache.accumulo.server.util.Admin.main(Admin.java:206)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:622)\r\n\tat org.apache.accumulo.start.Main$1.run(Main.java:141)\r\n\tat java.lang.Thread.run(Thread.java:701)\r\n{code}\r\n\r\nThere should be some sort of timeout here",
        "stop-all.sh hung when Accumulo already down {noformat}\r\n\"admin\" prio=10 tid=0x0000000040d09800 nid=0x124f waiting on condition [0x00007f85db747000]\r\n   java.lang.Thread.State: TIMED_WAITING (sleeping)\r\n\tat java.lang.Thread.sleep(Native Method)\r\n\tat org.apache.accumulo.core.util.UtilWaitThread.sleep(UtilWaitThread.java:26)\r\n\tat org.apache.accumulo.core.client.impl.MasterClient.getConnectionWithRetry(MasterClient.java:48)\r\n\tat org.apache.accumulo.core.client.impl.MasterClient.executeGeneric(MasterClient.java:126)\r\n\tat org.apache.accumulo.core.client.impl.MasterClient.execute(MasterClient.java:171)\r\n\tat org.apache.accumulo.server.util.Admin.stopServer(Admin.java:292)\r\n\tat org.apache.accumulo.server.util.Admin.main(Admin.java:195)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n\tat java.lang.reflect.Method.invoke(Method.java:597)\r\n\tat org.apache.accumulo.start.Main$1.run(Main.java:137)\r\n\tat java.lang.Thread.run(Thread.java:662)\r\n\r\n{noformat}"
    ],
    [
        "ACCUMULO-3266",
        "ACCUMULO-3264",
        "GarbageCollectorIT: gcLotsOfCandidatesIT and testInvalidDelete started failing Noticed these two tests start to fail recently. Not sure if it's something I broke or not. Need to look into it.\r\n\r\n{noformat}\r\njava.lang.AssertionError: null\r\n\tat org.junit.Assert.fail(Assert.java:86)\r\n\tat org.junit.Assert.assertTrue(Assert.java:41)\r\n\tat org.junit.Assert.assertTrue(Assert.java:52)\r\n\tat org.apache.accumulo.test.functional.GarbageCollectorIT.gcLotsOfCandidatesIT(GarbageCollectorIT.java:152)\r\n{noformat}\r\n\r\n{noformat}\r\njava.lang.Exception: test timed out after 600000 milliseconds\r\n\tat java.lang.Thread.sleep(Native Method)\r\n\tat org.apache.accumulo.core.util.UtilWaitThread.sleep(UtilWaitThread.java:26)\r\n\tat org.apache.accumulo.test.functional.GarbageCollectorIT.testInvalidDelete(GarbageCollectorIT.java:212)\r\n{noformat}\r\n\r\nFirst saw the new failures since {{5cb976b3ca0b4ecaac27b7963622c1c2f5664251}}. {{e3a743cb445723a3d5664a4bf1ebf37833152aae}} built cleanly for me.",
        "AuditMessageIT broken Looks like I might have broken the AUDIT log in ACCUMULO-3258 as AuditMessageIT started failing after those changes were pushed."
    ],
    [
        "ACCUMULO-3279",
        "ACCUMULO-2727",
        "BackupMasterIT failed with BadVersion ZK Exception Saw this one after I did some work on our ZK code. Need to investigate the failure and make sure I didn't screw anything up.\r\n\r\n{noformat}\r\nE AssertionError: KeeperErrorCode = BadVersion for /accumulo/02b584de-03df-463d-8c86-947297d609b2/masters/lock/zlock-0000000000\r\nE org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /accumulo/02b584de-03df-463d-8c86-947297d609b2/masters/lock/zlock-0000000000\r\nE at org.apache.zookeeper.KeeperException.create(KeeperException.java:115)\r\nE at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\r\nE at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)\r\nE at org.apache.accumulo.fate.zookeeper.ZooUtil.recursiveDelete(ZooUtil.java:226)\r\nE at org.apache.accumulo.fate.zookeeper.ZooUtil.recursiveDelete(ZooUtil.java:256)\r\nE at org.apache.accumulo.fate.zookeeper.ZooReaderWriter.recursiveDelete(ZooReaderWriter.java:62)\r\nE at org.apache.accumulo.test.functional.BackupMasterIT.test(BackupMasterIT.java:60)\r\n{noformat}",
        "version argument of ZooUtil.recursiveDelete is ignored The {{version}} argument of recursiveDelete is ignored. We should either use it, or remove it, instead of giving the illusion that we respect it."
    ],
    [
        "ACCUMULO-3297",
        "ACCUMULO-97",
        "FileManager semaphore acquisition may block Root and Metadata scans FileManager.java contains a semaphore to limit the max open files when calling acquireUninterruptibly( # permits ) when opening the readers. If scans against all tables causes the number of max open permits to exceed the configured system property, scans against the metadata and root extents will block until other readers are released.\r\n\r\nThis should be changed so that scans against the root and metadata extents proceed, despite exceeding the number of max open files. This likely means bypassing the semaphore acquisition for these tablets. \r\n\r\n",
        "!METADATA scans could be delayed by regular scans The tablet server only allows scans to open a max number of files.  Once the max is reached, scans wait.  Its possible that a !METADATA scan could get stuck waiting for a long running non !METADATA scan to release files.  This could slow down the whole system.  !METADATA scans wold not starve because a fair semaphore is used.  However with client timeouts, starvation may be possible."
    ],
    [
        "ACCUMULO-3297",
        "ACCUMULO-959",
        "FileManager semaphore acquisition may block Root and Metadata scans FileManager.java contains a semaphore to limit the max open files when calling acquireUninterruptibly( # permits ) when opening the readers. If scans against all tables causes the number of max open permits to exceed the configured system property, scans against the metadata and root extents will block until other readers are released.\r\n\r\nThis should be changed so that scans against the root and metadata extents proceed, despite exceeding the number of max open files. This likely means bypassing the semaphore acquisition for these tablets. \r\n\r\n",
        "compactions starving METADATA table scans A large tablet had many files, and the compaction was taking a long time.  During this time, it reserved so many readers that the scan of a metadata  tablet on the same host was starved of readers, causing scans to back up.\r\n"
    ],
    [
        "ACCUMULO-3359",
        "ACCUMULO-97",
        "FileManager.reserverReaders provides no pooling for meta table files We have tserver.scan.files.open.max, which can be used to control how many files we open at scan time. If we have several long running queries, this can cause starvation of operations reading the metadata / root tables that we should try to prevent.",
        "!METADATA scans could be delayed by regular scans The tablet server only allows scans to open a max number of files.  Once the max is reached, scans wait.  Its possible that a !METADATA scan could get stuck waiting for a long running non !METADATA scan to release files.  This could slow down the whole system.  !METADATA scans wold not starve because a fair semaphore is used.  However with client timeouts, starvation may be possible."
    ],
    [
        "ACCUMULO-3359",
        "ACCUMULO-959",
        "FileManager.reserverReaders provides no pooling for meta table files We have tserver.scan.files.open.max, which can be used to control how many files we open at scan time. If we have several long running queries, this can cause starvation of operations reading the metadata / root tables that we should try to prevent.",
        "compactions starving METADATA table scans A large tablet had many files, and the compaction was taking a long time.  During this time, it reserved so many readers that the scan of a metadata  tablet on the same host was starved of readers, causing scans to back up.\r\n"
    ],
    [
        "ACCUMULO-3359",
        "ACCUMULO-3297",
        "FileManager.reserverReaders provides no pooling for meta table files We have tserver.scan.files.open.max, which can be used to control how many files we open at scan time. If we have several long running queries, this can cause starvation of operations reading the metadata / root tables that we should try to prevent.",
        "FileManager semaphore acquisition may block Root and Metadata scans FileManager.java contains a semaphore to limit the max open files when calling acquireUninterruptibly( # permits ) when opening the readers. If scans against all tables causes the number of max open permits to exceed the configured system property, scans against the metadata and root extents will block until other readers are released.\r\n\r\nThis should be changed so that scans against the root and metadata extents proceed, despite exceeding the number of max open files. This likely means bypassing the semaphore acquisition for these tablets. \r\n\r\n"
    ],
    [
        "ACCUMULO-3382",
        "ACCUMULO-3218",
        "client only connects to first zookeeper instance When using a ClientConfiguration (or a ZookeeperInstance) to connect to accumulo, the connection will only ever use the first zookeeper in the list provided. When connections are made from a mapreduce job, this can lead to the first zookeeper server becoming overloaded.\r\n\r\nThe underlying problem seems to be in ClientConfiguration:\r\n{code:java}\r\nClientConfiguration cc = ClientConfiguration.loadDefault().withZkHosts(\"host1,host2,host3\");\r\n// Will only return \"host1\" and not \"host1,host2,host3\"\r\nSystem.out.println(cc.get(ClientConfiguration.ClientProperty.INSTANCE_ZK_HOST));\r\n{code}\r\nThis is happening because ClientConfiguration extends CompositeConfiguration, a class in commons-configuration. On of the default features is that if you set a property that is a comma-delimited list of values, it creates a list property. If you retrieve the property value using getString, then only the first value in the list will be returned.\r\n\r\nIt seems there are a couple solutions to this problem:\r\n1. Disable the automatic list parsing. This is trivial by simply calling setDelimiterParsingDisabled(true) in the ClientConfiguration constructor.\r\n2. Change the way all properties that are intended to be a comma-separated list are retrieved such that the appropriate list-based retrieval method is used.\r\n",
        "ZooKeeperInstance only uses first ZooKeeper in list of quorum Had tests running which had a quorum of 3 ZooKeeper servers. One appears to have died and the test was then unable to connect to the Accumulo shell, hanging on trying to connect to ZooKeeper.\r\n\r\nThere was no client.conf file present, so a ClientConfiguration was constructed from accumulo-site.xml.\r\n\r\n{code}\r\nthis.zooKeepers = clientConf.get(ClientProperty.INSTANCE_ZK_HOST);\r\n{code}\r\n\r\nWhen the commons configuration AbstractConfiguration class is used with the get() method, only the first element in the value is returned, as the implementation treats the other items as a list because of the default separator of a comma.\r\n\r\nIt's easily reproduced with the following:\r\n\r\n{code}\r\n    ZooKeeperInstance inst = new ZooKeeperInstance(\"accumulo\", \"localhost,127.0.0.1\");\r\n    System.out.println(inst.getZooKeepers());\r\n{code}\r\n\r\nThe above will print\r\n\r\n{noformat}\r\nlocalhost\r\n{noformat}\r\n\r\ninstead of the expected\r\n\r\n{noformat}\r\nlocalhost,127.0.0.1\r\n{noformat}"
    ],
    [
        "ACCUMULO-3485",
        "ACCUMULO-3218",
        "accumulo client tries to connect only to the first ZK in the list, making it SPOF Accumulo client tries to connect only to the first ZK server.\r\nIf that single ZK server is down, the accumulo client does not try other ZK servers.\r\nThus the first ZK server is the single point of failure.\r\nIf it is down, accumulo client cannot work\r\n\r\n{code:java}\r\n  val instance = new ZooKeeperInstance(\"zzz\", \"no-zk-here.example.com,zk1.example.com,zk2.example.com,zk3.example.com\")\r\n  val conn = instance.getConnector(\"root\", \"pass\".getBytes)\r\n  println(\"db connected, tables=\" + conn.tableOperations.list)\r\n{code}",
        "ZooKeeperInstance only uses first ZooKeeper in list of quorum Had tests running which had a quorum of 3 ZooKeeper servers. One appears to have died and the test was then unable to connect to the Accumulo shell, hanging on trying to connect to ZooKeeper.\r\n\r\nThere was no client.conf file present, so a ClientConfiguration was constructed from accumulo-site.xml.\r\n\r\n{code}\r\nthis.zooKeepers = clientConf.get(ClientProperty.INSTANCE_ZK_HOST);\r\n{code}\r\n\r\nWhen the commons configuration AbstractConfiguration class is used with the get() method, only the first element in the value is returned, as the implementation treats the other items as a list because of the default separator of a comma.\r\n\r\nIt's easily reproduced with the following:\r\n\r\n{code}\r\n    ZooKeeperInstance inst = new ZooKeeperInstance(\"accumulo\", \"localhost,127.0.0.1\");\r\n    System.out.println(inst.getZooKeepers());\r\n{code}\r\n\r\nThe above will print\r\n\r\n{noformat}\r\nlocalhost\r\n{noformat}\r\n\r\ninstead of the expected\r\n\r\n{noformat}\r\nlocalhost,127.0.0.1\r\n{noformat}"
    ],
    [
        "ACCUMULO-3485",
        "ACCUMULO-3382",
        "accumulo client tries to connect only to the first ZK in the list, making it SPOF Accumulo client tries to connect only to the first ZK server.\r\nIf that single ZK server is down, the accumulo client does not try other ZK servers.\r\nThus the first ZK server is the single point of failure.\r\nIf it is down, accumulo client cannot work\r\n\r\n{code:java}\r\n  val instance = new ZooKeeperInstance(\"zzz\", \"no-zk-here.example.com,zk1.example.com,zk2.example.com,zk3.example.com\")\r\n  val conn = instance.getConnector(\"root\", \"pass\".getBytes)\r\n  println(\"db connected, tables=\" + conn.tableOperations.list)\r\n{code}",
        "client only connects to first zookeeper instance When using a ClientConfiguration (or a ZookeeperInstance) to connect to accumulo, the connection will only ever use the first zookeeper in the list provided. When connections are made from a mapreduce job, this can lead to the first zookeeper server becoming overloaded.\r\n\r\nThe underlying problem seems to be in ClientConfiguration:\r\n{code:java}\r\nClientConfiguration cc = ClientConfiguration.loadDefault().withZkHosts(\"host1,host2,host3\");\r\n// Will only return \"host1\" and not \"host1,host2,host3\"\r\nSystem.out.println(cc.get(ClientConfiguration.ClientProperty.INSTANCE_ZK_HOST));\r\n{code}\r\nThis is happening because ClientConfiguration extends CompositeConfiguration, a class in commons-configuration. On of the default features is that if you set a property that is a comma-delimited list of values, it creates a list property. If you retrieve the property value using getString, then only the first value in the list will be returned.\r\n\r\nIt seems there are a couple solutions to this problem:\r\n1. Disable the automatic list parsing. This is trivial by simply calling setDelimiterParsingDisabled(true) in the ClientConfiguration constructor.\r\n2. Change the way all properties that are intended to be a comma-separated list are retrieved such that the appropriate list-based retrieval method is used.\r\n"
    ],
    [
        "ACCUMULO-3596",
        "ACCUMULO-3552",
        "accumulo hangs in {{InetAddress#getByName}} calls to importDirectory were hanging.  After some debugging, it was determined that the imports were stuck waiting on DNS lookups.\r\n\r\n{{InetAddress#getByName}} calls {{gethostbyname_r}}\r\n\r\nFurther, it was determined that the call only hangs if /etc/host.conf contains \"reorder on\".\r\n\r\nA bug was found in this section of the glibc resolver, and a bug report will be made to them.\r\n\r\nWorkaround: turn off address reordering.\r\n\r\nMany thanks to [~bfloss] for finding this bug.",
        "CopyFailed on BulkImport leaves metadata entries Fate shows that a BulkImport failed in CopyFailed. The metadata table shows files with the loaded prefix for tablets from this bulk import. Failing and deleting the fate transaction, then compacting the tablet, does not remove these entries."
    ],
    [
        "ACCUMULO-3863",
        "ACCUMULO-3862",
        "Improve how AsyncSpanReceiver drops short spans It just occurred to me that we are dropping 0ms spans when we poll the sendQueue for a span to deliver via thrift.  It would cause less churn in the queue if we dropped the span as soon as we received it, before inserting it into the queue.\r\n\r\nThis is already fixed in 1.6.3 which no longer drops 0ms spans.  It is only really a concern in 1.7.0 if you are using HDFS tracing, which generates a LOT of 0ms spans.",
        "Improve how AsyncSpanReceiver drops short spans It just occurred to me that we are dropping 0ms spans when we poll the sendQueue for a span to deliver via thrift.  It would cause less churn in the queue if we dropped the span as soon as we received it, before inserting it into the queue.\r\n\r\nThis is already fixed in 1.6.3 which no longer drops 0ms spans.  It is only really a concern in 1.7.0 if you are using HDFS tracing, which generates a LOT of 0ms spans."
    ],
    [
        "ACCUMULO-3910",
        "ACCUMULO-2287",
        "Automatically rotate .out/.err files instead of overwriting I was just bit by this again and am tired of dealing with old .out and .err files being overwritten when a process is restarted.\r\n\r\nAssume the master falls over because of an OOME. The log file may not have any reference to this exception depending on where the OOME was thrown from. Only the .err or .out files will have the file. Someone comes along and restarts the server process (trying to help) and suddenly the root cause of why the process fell over in the first place is gone.\r\n\r\nIt is trivial to rotate the files and keep a certain number of them present.",
        "Logs are overwritten upon restart When a server is restarted, then it overwrites the contents of the .out and .err files. This makes it more difficult to debug failures if you have something like puppet or supervisord ensuring that your processes are always running. We should figure out how to roll the files instead."
    ],
    [
        "ACCUMULO-3969",
        "ACCUMULO-3958",
        "NPE in monitor From the mailing list:\r\n{quote}\r\n  We have Accumulo 1.6.3 set up on the HortonWorks install with a basic cluster of one master and one tablet server. Initially everything seemed good but now I'm getting a NullPointerException when I go to the monitor web page. The logs also keep adding a message to that effect as well. I searched and couldn't find any posts with the same problem. I went back through all of the config files and made sure they conform to the configuration instructions. I'm hoping someone can point me to the right place to look for troubleshooting. \r\n\r\nhttp://<machine-name>:50095 responds with a page that has the following:\r\n\r\n{noformat}\r\njava.lang.NullPointerException\r\n\tat org.apache.accumulo.monitor.Monitor.fetchData(Monitor.java:252)\r\n\tat org.apache.accumulo.monitor.servlets.BasicServlet.doGet(BasicServlet.java:59)\r\n\tat org.apache.accumulo.monitor.servlets.DefaultServlet.doGet(DefaultServlet.java:101)\r\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:668)\r\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:770)\r\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\r\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:370)\r\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\r\n\tat org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)\r\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)\r\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\r\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\r\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\r\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)\r\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n{noformat}\r\n\r\n\r\nThe logs are recording the following error:\r\n{noformat}\r\n2015-08-21 16:37:21,152 [monitor.Monitor] WARN :\r\njava.lang.NullPointerException\r\n        at org.apache.accumulo.monitor.Monitor.fetchData(Monitor.java:252)\r\n        at org.apache.accumulo.monitor.Monitor$2.run(Monitor.java:508)\r\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        at java.lang.Thread.run(Thread.java:745)\r\n{noformat}\r\n{quote}",
        "Monitor.fetchData should check for null instance name Had an odd situation where the monitor would throw an NPE constantly when I tried to view any page.\r\n\r\nIt was throwing a NPE trying to accessed the cached instance name. It appears that {{HdfsZooInstance.getInstance().getInstanceName(}} may return null, but the monitor code is not written to account for that.\r\n\r\nThe monitor should check for null values before setting the instance name into {{cachedInstanceName}}. When the instance name is null, it should reschedule the timertask to try to fetch a (non-null) instance name."
    ]
]