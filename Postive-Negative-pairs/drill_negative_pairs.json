[
    [
        "DRILL-1188",
        "DRILL-4103",
        "AssertionError while running TPC-H query against hive datasource git.commit.id.abbrev=e5c2da0\r\ngit.commit.id=e5c2da0eb95df1a93d4ad909d5553757201dc903\r\n\r\n0: jdbc:drill:schema=dfs.TpcHMulti> use hive;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | Default schema changed to 'hive' |\r\n+------------+------------+\r\n1 row selected (0.325 seconds)\r\n0: jdbc:drill:schema=dfs.TpcHMulti> select\r\n. . . . . . . . . . . . . . . . . >   100.00 * sum(case\r\n. . . . . . . . . . . . . . . . . >     when p.p_type like 'PROMO%'\r\n. . . . . . . . . . . . . . . . . >       then l.l_extendedprice * (1 - l.l_discount)\r\n. . . . . . . . . . . . . . . . . >     else 0\r\n. . . . . . . . . . . . . . . . . >   end) / sum(l.l_extendedprice * (1 - l.l_discount)) as promo_revenue\r\n. . . . . . . . . . . . . . . . . > from\r\n. . . . . . . . . . . . . . . . . >   lineitem l,\r\n. . . . . . . . . . . . . . . . . >   part p\r\n. . . . . . . . . . . . . . . . . > where\r\n. . . . . . . . . . . . . . . . . >   l.l_partkey = p.p_partkey\r\n. . . . . . . . . . . . . . . . . >   and l.l_shipdate >= date '1994-08-01'\r\n. . . . . . . . . . . . . . . . . >   and l.l_shipdate < date '1994-08-01' + interval '1' month;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"7aa9bffb-38a5-41aa-9e84-1fde3a2e7735\"\r\nendpoint {\r\n  address: \"perfnode104.perf.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < AssertionError\"\r\n]\r\nError: exception while executing query (state=,code=0)",
        "Add additional metadata to Parquet files generated by Drill For future compatibility efforts, it would be good for us to automatically add metadata to Drill generated Parquet files. At a minimum, we should add information about the fact that Drill generated the files and the version of Drill that generated the files."
    ],
    [
        "DRILL-2058",
        "DRILL-2791",
        "drillbit.bat for Windows ",
        "Improve error messages in C++ client for handshake failure During handshake, authentication and bad rpc errors only return the generic handshake error "
    ],
    [
        "DRILL-3349",
        "DRILL-2347",
        "Resolve whether BoundCheckingAccessor still needs to exist BoundCheckingAccessor's getObject(int rowOffset) method suppresses index errors that would normally occur for bad values of rowOffset.\r\n\r\nIt's not clear why it does that (its other get methods don't) or whether it still needs to exist.\r\n\r\nIf it no longer needs to exist, it should be excised.\r\n\r\n",
        "Parallelize metadata reads for Parquet metadata During the planning process, reading the metadata from Parquet files is sequential, making the planning slow for large number of files.  This should be parallelized. "
    ],
    [
        "DRILL-604",
        "DRILL-1556",
        "INFORMATION_SCHEMA.SCHEMATA needs to expose the schema type INFORMATION_SCHEMA.SCHEMATA looks like this currently:\r\n\r\n0: jdbc:drill:schema=hivetg> select * from INFORMATION_SCHEMA.SCHEMATA;\r\n+--------------+-------------+--------------+\r\n| CATALOG_NAME | SCHEMA_NAME | SCHEMA_OWNER |\r\n+--------------+-------------+--------------+\r\n| DRILL        | hivestg.default | <owner>      |\r\n| DRILL        | hivestg     | <owner>      |\r\n| DRILL        | dfs.default | <owner>      |\r\n| DRILL        | dfs         | <owner>      |\r\n| DRILL        | cp.default  | <owner>      |\r\n| DRILL        | cp          | <owner>      |\r\n| DRILL        | INFORMATION_SCHEMA | <owner>      |\r\n+--------------+-------------+--------------+\r\n0: jdbc:drill:schema=hivetg> describe INFORMATION_SCHEMA.SCHEMATA;\r\n+-------------+------------+-------------+\r\n| COLUMN_NAME | DATA_TYPE  | IS_NULLABLE |\r\n+-------------+------------+-------------+\r\n| CATALOG_NAME | VARCHAR    | NO          |\r\n| SCHEMA_NAME | VARCHAR    | NO          |\r\n| SCHEMA_OWNER | VARCHAR    | NO          |\r\n+-------------+------------+-------------+\r\n3 rows selected (0.702 seconds)\r\n\r\nThe ODBC driver needs to get at the schema type from storage-plugins.json to enable the driver to determine how to operate with each schema (e.g., type \"file\" requires SHOW FILES; \"hive\" can be used verbatim)\r\n\r\nFor example, here's the default storage-plugins.json. Here \"dfs\" is tagged as of type \"file\" and \"hive\" is tagged as \"hive\".\r\n{\r\n  \"storage\":{\r\n    dfs: {\r\n      type: \"file\",\r\n      connection: \"file:///\",\r\n      formats: {\r\n        \"psv\" : {\r\n          type: \"text\",\r\n          extensions: [ \"tbl\" ],\r\n          delimiter: \"|\"\r\n        },\r\n        \"csv\" : {\r\n          type: \"text\",\r\n          extensions: [ \"csv\" ],\r\n          delimiter: \",\"\r\n        },\r\n        \"tsv\" : {\r\n          type: \"text\",\r\n          extensions: [ \"tsv\" ],\r\n          delimiter: \"\\t\"\r\n        },\r\n        \"parquet\" : {\r\n          type: \"parquet\"\r\n        },\r\n        \"json\" : {\r\n          type: \"json\"\r\n        }\r\n      }\r\n    },\r\n    cp: {\r\n      type: \"file\",\r\n      connection: \"classpath:///\"\r\n    },\r\n    hivestg : {\r\n        type:\"hive\",\r\n        config :\r\n          {\r\n            \"hive.metastore.uris\" : \"thrift://192.168.39.33:9083\",\r\n            \"hive.metastore.sasl.enabled\" : \"false\"\r\n          }\r\n    }\r\n\r\n    /*,\r\n    hive : {\r\n        type:\"hive\",\r\n        config :\r\n          {\r\n            \"hive.metastore.uris\" : \"\",\r\n            \"javax.jdo.option.ConnectionURL\" : \"jdbc:derby:;databaseName=../../sample-data/drill_hive_db;create=true\",\r\n            \"hive.metastore.warehouse.dir\" : \"/tmp/drill_hive_wh\",\r\n            \"fs.default.name\" : \"file:///\",\r\n            \"hive.metastore.sasl.enabled\" : \"false\"\r\n          }\r\n      }\r\n      */\r\n  }\r\n}\r\n\r\nA new column (name \"type\") added onto SCHEMATA would be sufficient.\r\n",
        "Querying JSON-converted-Parquet file throws parquet.io.ParquetDecodingException (Intermittent) Querying JSON data works at higher values for limit:\r\n> select * from `yelp_academic_dataset_review.json` limit 1125458;\r\n\r\nQuerying Parquet data (converted from JSON) fails at higher values for limit:\r\n> create table yelp_academic_dataset_review as select * from `yelp_academic_dataset_review.json`;\r\n[success]\r\n\r\n>select * from yelp_academic_dataset_review limit 40000;\r\n[data]\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while trying to get next result batch.\r\n\r\nLogs indicate an error in decoding the Parquet file. Drillbit.log is attached. \r\n\r\n2014-10-20 15:21:22,739 [bf4a3f58-781b-4c89-b718-e1ef6eab6da4:frag:1:0] ERROR o.a.drill.exec.ops.FragmentContext - Fragment Context received \r\nfailure.\r\nparquet.io.ParquetDecodingException: Can't read value in column [votes, funny] INT64 at value 61063 out of 61063, 61063 out of 61063 in currentPage. repetition level: 0, definition level: 2\r\n\r\nThis is at times consistent and some other times intermittent, for varied values provided to the limit clause. "
    ],
    [
        "DRILL-3283",
        "DRILL-2738",
        "Extend TestWindowFrame to check the results when PARTITION BY and/or ORDER BY are missing from the OVER clause Current unit tests for window functions only check the following cases:\r\nOVER(PARTITION BY X ORDER BY Y)\r\nOVER(PARTITION BY X)\r\n\r\nWe need to extend those tests to also check the following:\r\nOVER(ORDER BY Y)\r\nOVER()",
        "Offset with casting a column to timestamp not working \r\nIn the below query, it should skip the first row which is a header and want to cast one of the column to timestamp. But it is trying to parse the first row to cast it to timestamp. Without casting it to timestamp, simple offset query works fine.\r\n\r\n\"select cast(columns[0] as timestamp) from `guts-csv/CSV/guts_run_lab-app002.csv` offset 1;\"\r\n\r\nSo I did explain plan on the above query\r\n\r\nexplain plan without implementation for select cast(columns[0] as timestamp) from `guts-csv/CSV/guts_run_lab-app002.csv` offset 1;\r\n\r\nDrillScreenRel\r\n  DrillLimitRel(offset=[1])\r\n    DrillProjectRel(EXPR$0=[CAST(ITEM($0, 0)):TIMESTAMP(0)])\r\n      DrillScanRel(table=[[fs, drill, guts-csv/CSV/guts_run_lab-app002.csv]], groupscan=[EasyGroupScan [selectionRoot=/mapr/yarn-test/drill/guts-csv/CSV/guts_run_lab-app002.csv, numFiles=1, columns=[`columns`[0]], files=[file:/mapr/yarn-test/drill/guts-csv/CSV/guts_run_lab-app002.csv]]])\r\n\r\nIn the plan, it looks like it tries to do casting to timestamp and then the offset operation which is why its failing."
    ],
    [
        "DRILL-2578",
        "DRILL-1",
        "Add new workspace by saving a JSON file into a directory on a filesystem This is a feature request regarding Drill workspaces.\r\n\r\nI would like to see a feature where you can save a JSON file into a workspace directory on a filesystem which defines a workspace instead of always having to use the web GUI or a web request via a tool like CURL. Drill can then load these workspaces from this directory. The web GUI could also be modified to pick up/save any workspace configs in this directory so all workspaces are stored here (maybe this could be on HDFS/MapR-FS - I am not sure where workspace configs are actually stored at the moment).\r\n\r\nEach JSON file could be a separate workspace:\r\n\r\nworkspace1.json\r\nworkspace2.json \r\netc...\r\n",
        "Thrift-based wire protocol Support a Thrift-based [1] wire protocol. Contributor: Michael Hausenblas.\r\n\r\nSee [2] for the discussion.\r\n\r\n\r\n[1] http://thrift.apache.org/\r\n[2] http://mail-archives.apache.org/mod_mbox/incubator-drill-dev/201209.mbox/%3C4C785CAB-FD0E-4C5A-8D83-7AD0B7752139%40gmail.com%3E"
    ],
    [
        "DRILL-4089",
        "DRILL-3030",
        "Make JSON pretty printing configurable Currently JSON record writer emits records pretty-printed and there is no way to configure this behavior. This issue proposes to make this configurable via a prettyPrint switch in -storage- execution configuration with default value of true to ensure backward compatibility.\r\n\r\n\r\nAs a guideline, the following should be used to dictate Drill to emit records in JSON.\r\n{code:sql}\r\nalter [session|sytem] set `store.format`='json';\r\n{code}\r\n\r\nand this new switch should be used to turn off pretty printing:\r\n{code:sql}\r\nalter [session|sytem] set `store.json.writer.uglify`=true;\r\n{code}\r\n\r\nBy default, Drill will use system dependent line feed to seperate JSON blobs when pretty printing turned off.\r\n\r\n",
        "Foreman hangs trying to cancel non-root fragments Steps to repro:\r\n\r\n1. Ran long running query on a clean drill restart. \r\n2. Killed a non foreman node. \r\n3. Restarted drillbits using clush.\r\n\r\nOne of the drillbits(coincidentally a foreman node always) refused to shutdown. \r\n\r\nJstack shows that the foreman is waiting \r\n{code}\r\n  at org.apache.drill.exec.rpc.ReconnectingConnection$ConnectionListeningFuture.waitAndRun(ReconnectingConnection.java:105)\r\n        at org.apache.drill.exec.rpc.ReconnectingConnection.runCommand(ReconnectingConnection.java:81)\r\n        - locked <0x000000073878aaa8> (a org.apache.drill.exec.rpc.control.ControlConnectionManager)\r\n        at org.apache.drill.exec.rpc.control.ControlTunnel.cancelFragment(ControlTunnel.java:57)\r\n        at org.apache.drill.exec.work.foreman.QueryManager.cancelExecutingFragments(QueryManager.java:192)\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:824)\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:768)\r\n        at org.apache.drill.common.EventProcessor.sendEvent(EventProcessor.java:73)\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.moveToState(Foreman.java:770)\r\n        at org.apache.drill.exec.work.foreman.Foreman.moveToState(Foreman.java:871)\r\n        at org.apache.drill.exec.work.foreman.Foreman.access$2700(Foreman.java:107)\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateListener.moveToState(Foreman.java:1132)\r\n        at org.apache.drill.exec.work.foreman.QueryManager$1.statusUpdate(QueryManager.java:460)\r\n{code}"
    ],
    [
        "DRILL-2676",
        "DRILL-767",
        "Review Drill casting combinations re SQL standard (ISO 9705:2011 part 2 \u00a7 6.13) Drill doesn't allow casting an expression of an ARRAY type to that same ARRAY type.  \r\n\r\nThat doesn't seem to be compatible with the cast-combinations table in the SQL spec. (although I haven't checked the rest of the rules yet).\r\n\r\nIt also seems illogical, since one can't cast something to the type that it already has.\r\n\r\nWe should probably review Drill casting combinations against the casting rules in the SQL specification (ISO 9705:2011 part 2 \u00a7 6.13).",
        "Explain plan should work for create table as explain plan for create table testCreateTable as select columns[0],columns[1] from `/drill/testdata/text_storage/lineitem`;\r\n\r\nStatement results in a parse error."
    ],
    [
        "DRILL-1126",
        "DRILL-1120",
        "Add pointer support for UDAF workspace variables ",
        "mod(bigint, decimal) places decimal point at wrong location - regression #Wed Jul 09 10:28:32 PDT 2014\r\ngit.commit.id.abbrev=810a204\r\n\r\nThe following query used to work. The decimal point is placed wrong.\r\n\r\nDrill:\r\n0: jdbc:drill:schema=dfs> select mod(c_bigint, cast(c_decimal38 as decimal(38,18))) from data where c_row < 15 and cast(c_decimal38 as decimal(38,18)) <> 0;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 0E-18      |\r\n| -1000000000.000000000000000000 |\r\n| 12000000000.000000000000000000 |\r\n| 123000000000.000000000000000000 |\r\n| 12243822600000.000000000000000000 |\r\n| -36999908918111010.000000000000000000 |\r\n| 920320399049584000000000.000000000000000000 |\r\n| -922337203685477580000000000.000000000000000000 |\r\n| 2147483647000000000.000000000000000000 |\r\n+------------+\r\n9 rows selected (0.222 seconds)\r\n\r\npostgres:\r\nfoodmart=# select mod(c_bigint, cast(c_decimal38 as decimal(38,18))) from data where c_row < 15 and cast(c_decimal38 as decimal(38,18)) <> 0;\r\n                  mod\r\n----------------------------------------\r\n                   0.000000000000000000\r\n                  -1.000000000000000000\r\n                  12.000000000000000000\r\n                 123.000000000000000000\r\n                   0.000000000000000000\r\n                   0.000000000000000000\r\n               12243.822600000000000000\r\n           -36999908.918111010000000000\r\n     920320399049584.000000000000000000\r\n -922337203685477580.000000000000000000\r\n                   0.000000000000000000\r\n                   0.000000000000000000\r\n          2147483647.000000000000000000\r\n(13 rows)"
    ],
    [
        "DRILL-3651",
        "DRILL-2879",
        "Window function should not be allowed in order by clause of over clause of window function Should not parse and throw an error according to SQL standard.\r\n\"ISO/IEC 9075-2:2011(E) 7.11 <window clause>\" \r\nd) If WDX has a window ordering clause, then WDEF shall not specify <window order clause> (hope I'm reading it correctly)\r\n\r\n{code}\r\nSELECT rank() OVER (ORDER BY rank() OVER (ORDER BY c1)) from t1;\r\n{code}\r\n\r\nInstead, drill returns result:\r\n{code}\r\n0: jdbc:drill:schema=dfs> SELECT rank() OVER (ORDER BY rank() OVER (ORDER BY c1)) from t1;\r\n+---------+\r\n| EXPR$0  |\r\n+---------+\r\n| 1       |\r\n| 2       |\r\n| 3       |\r\n| 4       |\r\n| 5       |\r\n| 6       |\r\n| 7       |\r\n| 8       |\r\n| 9       |\r\n| 10      |\r\n+---------+\r\n10 rows selected (0.336 seconds)\r\n{code}\r\n\r\nPostgres throws an error in this case:\r\n{code}\r\npostgres=# SELECT rank() OVER (ORDER BY rank() OVER (ORDER BY c1)) from t1;\r\nERROR:  window functions are not allowed in window definitions\r\nLINE 1: SELECT rank() OVER (ORDER BY rank() OVER (ORDER BY c1)) from...\r\n{code}\r\n\r\nCourtesy of postgres test suite.",
        "Drill extended json's support $oid Enhancing JSON reader to parse  $oid (from mongo). "
    ],
    [
        "DRILL-4122",
        "DRILL-796",
        "Create unit test suite for checking quality of hashing for hash based operators We have encountered substantial skew in the hash based operators (hash distribution, hash aggregation, hash join) for certain data sets.  Two such issues are DRILL-2803, DRILL-4119.   \r\n\r\nIt would be very useful to have a unit test suite to test the quality of hashing.  \r\nThe number of combinations is large: num_data_types x nullability x num_hash_function_types (32bit, 64bit, AsDouble variations). Plus, the nature of the data itself.   We would have to be judicious about picking a reasonable subset of this space.   We should also look at open source test suites in this area. ",
        "Selecting * from hbase/m7 tables filtered with row_key returns empty arrays git.commit.id.abbrev=5d7e3d3\r\n\r\n0: jdbc:drill:schema=hbase> select * from voter where row_key=650;\r\n+------------+------------+------------+------------+------------+\r\n|  row_key   |   fourcf   |   onecf    |  threecf   |   twocf    |\r\n+------------+------------+------------+------------+------------+\r\n| [B@484265d9 | {}         | {}         | {}         | {}         |\r\n+------------+------------+------------+------------+------------+\r\n\r\nThe above table contains multiple regions.\r\n\r\nI have another table that belongs only to 1 region and the same query returns non-empty arrays.\r\n\r\n0: jdbc:drill:schema=hbase> select * from student where row_key=650;\r\n+------------+------------+------------+------------+------------+------------+\r\n|  row_key   |   fivecf   |   fourcf   |   onecf    |  threecf   |   twocf    |\r\n+------------+------------+------------+------------+------------+------------+\r\n| [B@59aa724f | {\"create_date\":\"MjAxNC0wMi0yMCAwMzoxMDoxOA==\"} | {\"studentnum\":\"OTQ1OTU5NTM4MTI5\"} | {\"name\":\"cHJpc2NpbGxhIGljaGFib2Q=\"} | {\"gpa\":\"MS41\"} | {\"age\":\"MjI=\"} |\r\n+------------+------------+------------+------------+------------+------------+\r\n"
    ],
    [
        "DRILL-726",
        "DRILL-3344",
        "order by fails when a column has null value postgres:\r\n\r\nfoodmart=# select c_row, c_groupby from data order by c_row;\r\n c_row | c_groupby\r\n-------+-----------\r\n     1 | a\r\n     2 | ab\r\n     3 | abc\r\n     4 |\r\n     5 |\r\n     6 | abc\r\n     7 | ab\r\n     8 | a\r\n     9 | a\r\n    10 | ab\r\n    11 | abc\r\n    12 |\r\n    13 |\r\n    14 | abc\r\n    15 | ab\r\n    16 | a\r\n    17 | a\r\n    18 | ab\r\n    19 | abc\r\n    20 |\r\n    21 |\r\n    22 | abc\r\n    23 | ab\r\n(23 rows)\r\n\r\ndrill:\r\n\r\n0: jdbc:drill:schema=dfs> select c_row, c_groupby from data order by c_row;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"8dcb47ec-9c27-4a04-8b48-61e2aad2ad19\"\r\nendpoint {\r\n  address: \"qa-node120.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < ClassCastException:[ org.apache.drill.exec.vector.NullableIntVector cannot be cast to org.apache.drill.exec.vector.NullableVarBinaryVector ]\"\r\n]\r\nError: exception while executing query (state=,code=0)",
        "When Group By clause is present, the argument in window function should not refer to any column outside Group By CTAS\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> create table tblForView(col_int, col_bigint, col_char_2, col_vchar_52, col_tmstmp, col_dt, col_booln, col_dbl, col_tm) as select cast(columns[0] as INT), cast(columns[1] as BIGINT),cast(columns[2] as CHAR(2)), cast(columns[3] as VARCHAR(52)), cast(columns[4] as TIMESTAMP), cast(columns[5] as DATE), cast(columns[6] as BOOLEAN),cast(columns[7] as DOUBLE),cast(columns[8] as TIME) from `forPrqView.csv`;\r\n+-----------+----------------------------+\r\n| Fragment  | Number of records written  |\r\n+-----------+----------------------------+\r\n| 0_0       | 30                         |\r\n+-----------+----------------------------+\r\n1 row selected (0.586 seconds)\r\n{code}\r\n\r\nFailing query\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select max(col_tm) over(), col_char_2 from tblForView group by col_char_2;\r\nError: SYSTEM ERROR: java.lang.AssertionError: Internal error: while converting MAX(`tblForView`.`col_tm`)\r\n\r\n\r\n[Error Id: 11afbdc9-d47a-4a52-aa77-40c20ffd2bc6 on centos-03.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nStack trace\r\n\r\n{code}\r\n[Error Id: 11afbdc9-d47a-4a52-aa77-40c20ffd2bc6 on centos-03.qa.lab:31010]\r\norg.apache.drill.common.exceptions.UserException: SYSTEM ERROR: java.lang.AssertionError: Internal error: while converting MAX(`tblForView`.`col_tm`)\r\n\r\n\r\n[Error Id: 11afbdc9-d47a-4a52-aa77-40c20ffd2bc6 on centos-03.qa.lab:31010]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:522) ~[drill-common-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$ForemanResult.close(Foreman.java:738) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:840) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:782) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.common.EventProcessor.sendEvent(EventProcessor.java:73) [drill-common-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.moveToState(Foreman.java:784) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.moveToState(Foreman.java:893) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:253) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\nCaused by: org.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: Internal error: while converting MAX(`tblForView`.`col_tm`)\r\n        ... 4 common frames omitted\r\nCaused by: java.lang.AssertionError: Internal error: while converting MAX(`tblForView`.`col_tm`)\r\n        at org.apache.calcite.util.Util.newInternal(Util.java:790) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.ReflectiveConvertletTable$2.convertCall(ReflectiveConvertletTable.java:152) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:60) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertOver(SqlToRelConverter.java:1762) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.access$1000(SqlToRelConverter.java:180) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:3938) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.createAggImpl(SqlToRelConverter.java:2521) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertAgg(SqlToRelConverter.java:2342) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:604) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:564) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:2741) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:522) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.prepare.PlannerImpl.convert(PlannerImpl.java:198) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToRel(DefaultSqlHandler.java:246) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:182) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:178) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:904) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:242) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        ... 3 common frames omitted\r\nCaused by: java.lang.reflect.InvocationTargetException: null\r\n        at sun.reflect.GeneratedMethodAccessor129.invoke(Unknown Source) ~[na:na]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_45]\r\n        at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_45]\r\n        at org.apache.calcite.sql2rel.ReflectiveConvertletTable$2.convertCall(ReflectiveConvertletTable.java:142) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        ... 19 common frames omitted\r\nCaused by: java.lang.AssertionError: null\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.getRootField(SqlToRelConverter.java:3811) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.adjustInputRef(SqlToRelConverter.java:3139) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:3114) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.access$1400(SqlToRelConverter.java:180) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4062) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:3490) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:274) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:3945) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.StandardConvertletTable.convertExpressionList(StandardConvertletTable.java:833) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.StandardConvertletTable.convertAggregateFunction(StandardConvertletTable.java:706) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        ... 23 common frames omitted\r\n{code}"
    ],
    [
        "DRILL-433",
        "DRILL-1925",
        "Enhance QuerySubmitter to use custom column width for printing results While testing Decimal, I ran into this issue where the my output was getting truncated because my column width was > 15. \r\n\r\nModify QuerySubmitter so it can accept a column width as one of its argument and use this for printing results.",
        "inline static nested Accessor class methods Some scalar replacement opportunities had to be disabled because the holders' references are passed in to static methods on nested Accessor classes as out parameters (their members are filled in by the static methods). If we could inline those static methods (they generally just seem to be 5-10 lines), then those holders could be replaced.\r\n\r\nI thought the best option here would be to run something based on BasicInterpreter but which would start a new interpreter for nested function calls, but which delegates to the containing interpreter to emit the code. But I'm not sure how hard this will be to set up -- it will require mapping through the transformations made to locals. And the interpreter calls are very broad, covering many instructions each."
    ],
    [
        "DRILL-4072",
        "DRILL-2851",
        "Hive partition pruning not working with avro serde's git.commit.id.abbrev=e78e286\r\n\r\nThe below plan indicates that partition pruning is not happening\r\n\r\n{code}\r\nexplain plan for select * from hive.episodes_partitioned where doctor > 4;\r\n+------+------+\r\n| text | json |\r\n+------+------+\r\n| 00-00    Screen\r\n00-01      Project(title=[$0], air_date=[$1], doctor=[$2], doctor_pt=[$3])\r\n00-02        Project(title=[$0], air_date=[$1], doctor=[$2], doctor_pt=[$3])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[>($2, 4)])\r\n00-05              Scan(groupscan=[HiveScan [table=Table(dbName:default, tableName:episodes_partitioned), inputSplits=[maprfs:///user/hive/warehouse/episodes_partitioned/doctor_pt=1/000000_0:0+367, maprfs:///user/hive/warehouse/episodes_partitioned/doctor_pt=11/000000_0:0+393, maprfs:///user/hive/warehouse/episodes_partitioned/doctor_pt=2/000000_0:0+371, maprfs:///user/hive/warehouse/episodes_partitioned/doctor_pt=4/000000_0:0+368, maprfs:///user/hive/warehouse/episodes_partitioned/doctor_pt=5/000000_0:0+357, maprfs:///user/hive/warehouse/episodes_partitioned/doctor_pt=6/000000_0:0+370, maprfs:///user/hive/warehouse/episodes_partitioned/doctor_pt=9/000000_0:0+350], columns=[`*`], numPartitions=7, partitions= [Partition(values:[1]), Partition(values:[11]), Partition(values:[2]), Partition(values:[4]), Partition(values:[5]), Partition(values:[6]), Partition(values:[9])]]])\r\n{code}\r\n\r\nI attached the data file and the hql required. Let me know if anything else is needed",
        "Memory LEAK - FLATTEN function fails when input array has 99,999 integer type elements FLATTEN function does not return results when input array has 99,999 elements of type integer. Test was run on 4 node cluster on CentOS.\r\n\r\nLooks like there is a memory leak somewhere because I see this message in the stack trace. \"Failure while closing accountor\".\r\n{code}\r\n\r\n0: jdbc:drill:> select name,flatten(num_list) from `Jsn_Arry100.json`;\r\nQuery failed: SYSTEM ERROR: initialCapacity: -2147483648 (expectd: 0+)\r\n\r\n[2b1960d3-c9e5-43cc-926e-783257cc1a0a on centos-04.qa.lab:31010]\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n{code}\r\n\r\nThere are 99,999 integer type elements in the input array.\r\n\r\n{code}\r\n0: jdbc:drill:> select repeated_count(tmp.num_list) from `Jsn_Arry100.json` tmp;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 99999      |\r\n+------------+\r\n1 row selected (0.206 seconds)\r\n{code}\r\n\r\nThe below query does NOT return all the 99,999 array elements. Instead query returns incomplete/incorrect results.\r\n\r\n{code}\r\n0: jdbc:drill:> select tmp.num_list from `Jsn_Arry100.json` tmp; \r\n+------------+\r\n|  num_list  |\r\n+------------+\r\n| [13729690148580628,33968383451250544,19729687836805948,10887002260554076,59060271975938184,87704035211118608,73861468909987168,88448989981158368,57022772257593232,39886649400114208,59634364188801728,49220606544154264,14668098707176940,1389807151115602,68917737889186128,63305048453386056,65444797852007920,45819687647405048,2572319061962803,30445371906667328,409277773257482,63616924473446912,71267711965641544,39123983528649880,33710976622262344,77269001875184320,21285915577966972,79158399148342640,59927799708140928,86792628837467632,42093378633089096,90092572909620640,27822481748467540,72874902594517600,4613424573716378,30348741516686960,15384400403234324,11312915166709258,67842306780001720,63536034928852224,34064758786460920,26742651581265676,69283348697630136,21337762946874492,33408483778102284,48525199800724472,17366398171254334,78156187420036768,55683717108215368,18931739169089156,53749386072208016,75716769953459472,70124914126143752,59670587242776496,51687272393733984,56590991314575,27798845713791992,57060186084971832,80286905552877744,11576390595076536,67263019646709888,62231475148843856,38916556483991208,56870539861336200,13820727892494552,43440054512663296,43294405699266032,22942812764355256,36368231331952648,52243742256792032,48655336740833488,87161415891865104,74521440214901872,23963586190891468,86994250559679456,64235294682694968,89237837009514944,27981168913939540,86292939130238320,9803308891945780,67756129441807768,47956437308413040,80258260743958512,33828778916469536,8419255582699858,74560162108011744,38543450292892984,21415391273461424,31658676421021728,10459153047723704,73185429768682384,2722949752940800,47317026756664832,54654463675350704,26408396249642988,43938679869946224,56284724555406552,71685791015933536,46001126444228888,68794495595243400,4132552956335268,34688634621761076,35712207345268316,60213985727177096,5309212305048668,23619206841198624,64825327846779600,57007947199911488,47136678928834984,9122467237416458,40980849927903520,20914596281235304,77253379050572112,80560013812121728,46022174419866912,22101913922421504,4767879560244378,7295985537104415,33463848115973120,21159496087635528,7209644495944438,8339341099391089,56466941472321008,49199375007197008,19176844581370920,87713551163634272,68410140933429768,22463584602000640,31007049671139340,88991676818494464,86018450036815872,68296039329719152,40855740519055456,49077682665580104,8114179710795960,19904226756293724,38777905573998560,87116624775839824,64713191586087936,23526229712701268,62999544446558568,61842195984909040,20542838487485340,64036764083509024,1241102073337856,2271939416579819,40485492208390352,32715355572929904,72573056371850208,79275623956295168,63005955205671488,80329641439025648,50636113571403528,84640203909310624,58152135741332672,76874210847790576,45638822712398088,14793955144036874,87502976063529168,58807684142910336,8542862544501658,42269245005614528,58763187554800520,751011850228275,48740246868357488,50972520565647320,72594124941772656,31070553596449896,45234757963903568,59324150925583104,84153005711977232,90541040496368064,71711996735859880,41767774512805840,6237941704680079,10085371064613816,78263530428653888,25649178272406712,78386911979125408,74871831010694784,2501120501320468,59826807125850064,11455550309471406,2789047787534469,90319173073564784,35147753006638328,90980208101318128,9910622567479808,29022215390318684,68516938237020496,13915913719658844,43980253031382328,41811554449342792,30871530529058796,37690170762904656,81110630593395616,14324825121595106,2009494929453865,37214432592899504,17099423225928572,56050443116580560,57187738511658096,30731274919325308,43715257438566712,66086595725233376,72139658658973280,81629806228098832,88214343607466688,11459521311029208,23429003870149408,59435025387927032,87438952800109392,14891802892848088,9676533574567302,27186823028758056,54804348760028984,54924273457109648,89467987292396752,69692650285458240,13851588882257624,21084991327837092,34184135079206584,81287720544047152,68354424116184832,9763366960601098,5076272451255951,60975963817311856,60203372513789336,84076654183130992,23498858210490492,19796619337627596,19703049436223560,10040949502371082,14168760473553624,29560677088141436,7238368756723210,59189735753188576,87403992458731312,12816830478611200,52541905950480408,49643679210487144,74203685134169504,46279848175852064,18972760835253096,83663642472309072,54295553706201856,64608871302467680,41794803336859056,91273876479904320,19510503273546496,21174096823508532,78025664575445136,12118597218456852,13895570027535872,40328409583119688,90090258465666512,47772926087456904,69971593804424344,30961201123684772,24798639486111008,19796544325250448,76841274718788592,83627192334576560,5934726517517281,25204549908243796,71846978502928888,43604846490509336,34110869694768200,47894781247793584,84440194478300448,61511851011957576,61521128641831048,75935420834173200,47754021630636344,21180666053858644,9934584431549716,36958889494931568,54633380674977384,15628243593734082,51392498919665072,14418520607145442,28294203469899224,9231039884803676,70200018940576312,55060314668028264,41805592567813336,69667784789645048,19874163368870584,63457847313701488,33517138148037192,38729100611607000,62264887692937728,75924975833373344,42143671487680760,30051803099963688,35970052888440740,20804318872678156,64202259133559320,89224969496660160,13599192984324056,37593036811603088,89606814956181552,28552227597987452,70488756671457448,17145464048060272,31952855373667268,16719021259572920,65571136190971688,86870559642719104,35455462197754808,70035629262079624,5814237233877566,3667688279824302,32527804673906708,39604742684966176,6653338960970537,18267942973956444,13951321415529974,37616807664818416,245164139914280,37267135279762288,80412924840730896,83320537505825424,28675761855322012,56329261929462256,52347739560820728,84637891335668448,10999384069762058,61447706697887960,40486496111949096,5897125033905091,26138393724872868,26403230650590392,14686912738831780,85000181993850224,32664538987800208,85344655404034752,752110474357944,54964104688025584,85719625463312240,13287910832305798,62660648869455872,74030353151886816,45096117411394600,41444747098005168,53758025544295208,11676918578616546,61776892476600800,34425840976378040,47979985632276600,65220402534819536,47214876583685512,4785522866167747,4996411509661819,82532490890710176,15310371979266592,13487958278328944,72572615495171824,88124137443202448,62974105825355816,22938443003344700,22218760252807556,71463215215220960,24355595948161444,55669834837735688,69044559395815960,22077102505483868,63153013299984136,54350496656156600,90828816267210656,54087476672311104,79873040446604528,52268937199458120,33414164925645640,14624230559059508,63526694004890728,66707816731801704,12985961723467294,51585323788992808,88736963570171344,18097269319441584,2938889277551309,32071836612136672,4874915565971272,27979480399327724,2328631417539911,2019400820084408,1636943567211632,16893692144490528,34307996692585932,3137453755279063,90240966405885680,5190519387913114,91971558378938208,38646106504775936,14100060496946104,35712181109832356,8691252113755187,5389022101506488,39093643829212880,57729646856263080,66298241057795840,48180451448026328,16475799417943920,3528180864814223,62462064545377680,83717871324475840,45190865064772640,46496670780493680,20556297818783632,65867465283654968,36231285292019024,78425984230913872,59310088557323592,19263520242582948,26657084161716696,26768514687177576,34845391769043824,67109862516086352,84787433767086608,32811725219738124,70946466987128088,17487431799052360,29064138224780444,38448830352246448,37919824292475320,65396741238698568,68062317142957816,25215654618159820,77682927719614336,50451906148874064,71198027460152208,81928407285746880,47977106158476096,54798795218991280,87104592426022320,80692836395850320,44742025618412464,83828703034820320,61322244404364440,65852052110978688,46201167582148704,63147684887913728,81947148443265904,33234882570247280,61824203084365664,65677773616676616,34507751060305028,85160735088238096,20250909199245496,83776390609328400,71832337939504784,9156127359953428,73739306112100528,20463550289714472,8097902663668664,57048612187389032,81572091753206912,3829847328411453,50956413862423992,26439809155293092,31034047458962320,76933834719482288,74198948210922976,44622397137614528,39881358494075816,78670828431098720,88941405691527184,72729787251564176,87477963231913024,79711111648840752,12717912830791414,29389029458694524,14376186261304638,50303495048847480,51793414994329632,31332682299419720,6806056236834048,80159745841517264,78893848103037248,13978732633703168,12524246333502792,82480072119463584,7795953954604022,33519837050152828,22501603086029968,73345532642041008,55030778779197,86737038779777952,1203114673054064,69921343463104184,238935739214981,65361843440835200,26231581589378416,16956461555850598,27015530661651252,81284469501562192,75026097714234080,59356950361550776,64852611047367088,74793804210862064,68132958833577168,4452723653819443,41666663396639160,14716222059915520,72348427991948688,59632368262572440,30537619221699932,50647738352100504,21589707182635632,87782956274734896,20618088160030772,80424637973969168,57793644568143288,91595572924028176,12319000882705684,28628328859456872,14914183162316646,55349099487758400,37312488485553504,64143239353460944,9263483809230060,20957503537485436,5003071205474325,62867518655820440,2094345702430361,56644818647364904,35442094691948984,42842292748902200,38576949302537312,86049988424730592,4332711517459917,33778101497672008,69700296469066296,53510614336070992,63483158994184184,63672532023341648,13637502915063480,8151430968818535,75170922637346336,64379628654616872,71343912677494152,70294272306149560,23194619010788016,61075095973093848,64829369100985048,43640706676835688,47391908058263080,57412591849149440,45596372988325296,52315161431461200,49453251968431184,91116412406885840,38139609592712080,59473127849679616,45434093125010000,48405275650237240,69699493929148192,39821008065583872,421570 |\r\n+------------+\r\n1 row selected (0.264 seconds)\r\n\r\n{code}\r\n\r\nStack trace from drillbit.log\r\n\r\n{code}\r\n2015-04-23 00:42:29,110 [2ac7c010-3c5c-aab9-c425-a4efcc811fb2:frag:0:0] ERROR o.a.drill.exec.ops.FragmentContext - Fragment Context received failure -- Fragment: 0:0\r\norg.apache.drill.common.exceptions.DrillUserException: SYSTEM ERROR: initialCapacity: -2147483648 (expectd: 0+)\r\n\r\n[3818da78-93f5-430a-a8e8-990d819c15bf on centos-04.qa.lab:31010]\r\n\r\n        at org.apache.drill.common.exceptions.DrillUserException$Builder.build(DrillUserException.java:115) ~[drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.exceptions.ErrorHelper.wrap(ErrorHelper.java:39) ~[drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.ops.FragmentContext.fail(FragmentContext.java:151) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:182) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]\r\nCaused by: java.lang.IllegalArgumentException: initialCapacity: -2147483648 (expectd: 0+)\r\n        at io.netty.buffer.PooledByteBufAllocatorL.validate(PooledByteBufAllocatorL.java:77) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n        at io.netty.buffer.PooledByteBufAllocatorL.directBuffer(PooledByteBufAllocatorL.java:65) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n        at org.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:227) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:234) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.vector.BigIntVector.reAlloc(BigIntVector.java:171) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.vector.BigIntVector$Mutator.setSafe(BigIntVector.java:358) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.vector.RepeatedBigIntVector$Mutator.addSafe(RepeatedBigIntVector.java:462) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.vector.RepeatedBigIntVector$Mutator.setSafe(RepeatedBigIntVector.java:456) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.test.generated.FlattenerGen207.doEval(FlattenTemplate.java:27) ~[na:na]\r\n        at org.apache.drill.exec.test.generated.FlattenerGen207.flattenRecords(FlattenTemplate.java:94) ~[na:na]\r\n        at org.apache.drill.exec.physical.impl.flatten.FlattenRecordBatch.doWork(FlattenRecordBatch.java:156) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:93) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.flatten.FlattenRecordBatch.innerNext(FlattenRecordBatch.java:122) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:89) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:135) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:74) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:76) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:64) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:164) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        ... 4 common frames omitted\r\n2015-04-23 00:42:29,112 [2ac7c010-3c5c-aab9-c425-a4efcc811fb2:frag:0:0] INFO  o.a.drill.exec.work.foreman.Foreman - State change requested.  RUNNING --> FAILED\r\norg.apache.drill.common.exceptions.DrillRemoteException: SYSTEM ERROR: initialCapacity: -2147483648 (expectd: 0+)\r\n\r\n[36479914-f336-4cb4-992f-5b3aae398c73 on centos-04.qa.lab:31010]\r\n\r\n        at org.apache.drill.exec.work.foreman.QueryManager.statusUpdate(QueryManager.java:163) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.QueryManager$RootStatusReporter.statusChange(QueryManager.java:281) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.AbstractStatusReporter.fail(AbstractStatusReporter.java:114) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.AbstractStatusReporter.fail(AbstractStatusReporter.java:110) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.internalFail(FragmentExecutor.java:235) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:183) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]\r\n\r\n...\r\n\r\n2015-04-23 00:42:29,136 [2ac7c010-3c5c-aab9-c425-a4efcc811fb2:frag:0:0] WARN  o.a.d.e.w.fragment.FragmentExecutor - Failure while closing out resources\r\njava.lang.IllegalStateException: Failure while closing accountor.  Expected private and shared pools to be set to initial values.  However, one or more were not.  Stats are\r\n        zone    init    allocated       delta\r\n        private 1000000 1000000 0\r\n        shared  9999000000      12146483648     -2147483648.\r\n        at org.apache.drill.exec.memory.AtomicRemainder.close(AtomicRemainder.java:200) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.memory.Accountor.close(Accountor.java:386) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.close(TopLevelAllocator.java:301) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.ops.OperatorContext.close(OperatorContext.java:118) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.cleanup(AbstractRecordBatch.java:170) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.cleanup(AbstractSingleRecordBatch.java:120) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.cleanup(IteratorValidatorBatchIterator.java:148) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.cleanup(AbstractSingleRecordBatch.java:121) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.cleanup(IteratorValidatorBatchIterator.java:148) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.internalStop(ScreenCreator.java:131) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.stop(ScreenCreator.java:138) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.closeOutResources(FragmentExecutor.java:209) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:188) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]\r\n\r\n{code}"
    ],
    [
        "DRILL-1506",
        "DRILL-3497",
        "Current Schema Not Shown In the sqlline Prompt The prompt isn't what I'd call user-friendly (doesn't display the schema I'm connected to).\r\n\r\n[root@n69 bin]# ./sqlline -u \"jdbc:drill:zk=n69:5181,n72:5181,n73:5181;schema=sys\" -n admin -p admin\r\nsqlline version 1.1.6\r\n\r\n0: jdbc:drill:zk=n69:5181,n72:5181,n73:5181> show tables;\r\n+--------------+------------+\r\n| TABLE_SCHEMA | TABLE_NAME |\r\n+--------------+------------+\r\n| sys          | drillbits  |\r\n| sys          | options    |\r\n+--------------+------------+\r\n2 rows selected (0.263 seconds)\r\n\r\n0: jdbc:drill:zk=n69:5181,n72:5181,n73:5181> select * from drillbits;\r\n+------------+------------+--------------+------------+\r\n|    host    | user_port  | control_port | data_port  |\r\n+------------+------------+--------------+------------+\r\n| n69        | 31010      | 31011        | 31012      |\r\n| n72        | 31010      | 31011        | 31012      |\r\n+------------+------------+--------------+------------+\r\n2 rows selected (0.077 seconds)\r\n0: jdbc:drill:zk=n69:5181,n72:5181,n73:5181> ",
        "Throw UserException#validationError for errors when modifying options "
    ],
    [
        "DRILL-1297",
        "DRILL-1786",
        "Hide Dependencies From Public API to enable using the C++ Client as a DLL Protobuf dependency is exposed to customers through public API header files. In particular, we have built C++ Client as a DLL but when linking it to our code, the public API header files do not compile as they expose additional dependencies that should be hidden from the costumer code. Especially, incubator-drill\\contrib\\native\\client\\src\\include\\drill\\protobuf\\User.pb.h",
        "Pushdown filter into InfoSchema POJO Record Generator to avoid visiting unnecessary schemas After a schema is chosen, we can query the table list in the specified schema by \"show tables\". \r\n\r\nHowever, I noticed the method \"getTableNames()\" in every storage-plugin is called through the life-cycle of this query. This seems not necessary. Especially, when you have an unresponsive data source (DRILL-1343), the query result would not be returned even if you type \"show tables\" under a healthy data source"
    ],
    [
        "DRILL-3083",
        "DRILL-3033",
        "Should display an error when an extension is define more than once in storage configuration Currently a user can define the same file extension in multiple formats in the dfs storage configuration.  This can cause drill to grab the wrong data format to display the data.  An error should be displayed when user add an extension that is already defined in another format.",
        "Add memory leak fixes found so far in DRILL-1942 to 1.0 The new memory allocator being implemented for DRILL-1942 has found a number of leaks. But the complete allocator patch isn't ready yet. However, in order to increase the stability of 1.0, we're going to include just the changes for leaks found so far using the new allocator. This ticket tracks that."
    ],
    [
        "DRILL-64",
        "DRILL-1700",
        "In reference implementation, full outer join returns too many rows Full outer join returns too many rows. The query\r\n\r\n{\r\n  \"head\" : {\r\n    \"type\" : \"apache_drill_logical_plan\",\r\n    \"version\" : 1,\r\n    \"generator\" : {\r\n      \"type\" : \"manual\",\r\n      \"info\" : \"na\"\r\n    }\r\n  },\r\n  \"storage\" : [ {\r\n    \"type\" : \"queue\",\r\n    \"name\" : \"queue\"\r\n  }, {\r\n    \"type\" : \"classpath\",\r\n    \"name\" : \"donuts-json\"\r\n  } ],\r\n  \"query\" : [ {\r\n    \"op\" : \"scan\",\r\n    \"@id\" : 1,\r\n    \"memo\" : \"initial_scan\",\r\n    \"storageengine\" : \"donuts-json\",\r\n    \"selection\" : {\r\n      \"path\" : \"/employees.json\",\r\n      \"type\" : \"JSON\"\r\n    },\r\n    \"ref\" : \"_MAP\"\r\n  }, {\r\n    \"op\" : \"project\",\r\n    \"input\" : 1,\r\n    \"@id\" : 2,\r\n    \"projections\" : [ {\r\n      \"ref\" : \"output.deptId\",\r\n      \"expr\" : \"_MAP.deptId\"\r\n    } ]\r\n  }, {\r\n    \"op\" : \"project\",\r\n    \"input\" : 2,\r\n    \"@id\" : 10,\r\n    \"projections\" : [ {\r\n      \"ref\" : \"output.deptId1\",\r\n      \"expr\" : \"deptId\"\r\n    } ]\r\n  }, {\r\n    \"op\" : \"scan\",\r\n    \"@id\" : 3,\r\n    \"memo\" : \"initial_scan\",\r\n    \"storageengine\" : \"donuts-json\",\r\n    \"selection\" : {\r\n      \"path\" : \"/departments.json\",\r\n      \"type\" : \"JSON\"\r\n    },\r\n    \"ref\" : \"_MAP\"\r\n  }, {\r\n    \"op\" : \"project\",\r\n    \"input\" : 3,\r\n    \"@id\" : 4,\r\n    \"projections\" : [ {\r\n      \"ref\" : \"output.deptId\",\r\n      \"expr\" : \"_MAP.deptId\"\r\n    } ]\r\n  },  {\r\n    \"op\": \"join\",\r\n    \"left\": 10,\r\n    \"right\": 4,\r\n    \"@id\" : 5,\r\n    \"type\": \"outer\",\r\n    \"conditions\": [\r\n      {\"relationship\": \"==\", \"left\": \"deptId1\", \"right\": \"deptId\"}\r\n    ]\r\n  }, {\r\n    \"op\" : \"store\",\r\n    \"input\" : 5,\r\n    \"@id\" : 6,\r\n    \"memo\" : \"output sink\",\r\n    \"target\" : {\r\n      \"number\" : 0\r\n    },\r\n    \"partition\" : null,\r\n    \"storageEngine\" : \"queue\"\r\n  } ]\r\n}\r\n\r\nreturns \r\n\r\n{  \"deptId\" : 31,  \"deptId1\" : 31} \r\n{  \"deptId\" : 33,  \"deptId1\" : 33} \r\n{  \"deptId\" : 33,  \"deptId1\" : 33} \r\n{  \"deptId\" : 34,  \"deptId1\" : 34} \r\n{  \"deptId\" : 34,  \"deptId1\" : 34} \r\n{  \"deptId1\" : null} \r\n{  \"deptId\" : 31} \r\n{  \"deptId\" : 33} \r\n{  \"deptId\" : 34} \r\n{  \"deptId\" : 35} \r\n\r\nbut I think it should return\r\n\r\n{  \"deptId\" : 31,  \"deptId1\" : 31} \r\n{  \"deptId\" : 33,  \"deptId1\" : 33} \r\n{  \"deptId\" : 33,  \"deptId1\" : 33} \r\n{  \"deptId\" : 34,  \"deptId1\" : 34} \r\n{  \"deptId\" : 34,  \"deptId1\" : 34} \r\n{  \"deptId1\" : null} \r\n{  \"deptId\" : 35} \r\n\r\nbecause only 35 on the left is unmatched on the right; {31, 33, 34} have at least one match.\r\n\r\nAlso, it's cosmetic, but the \"outer\" join type should be called \"full\" because \"left\" is also a kind of outer join, at least in SQL parlance.",
        "caught a memory assertion on an order by #Wed Nov 12 13:06:45 EST 2014\r\ngit.commit.id.abbrev=1e21045\r\n\r\nThe following query on a 1 million row data caused a memory assertion.\r\n\r\n0: jdbc:drill:schema=dfs> select columns[0], columns[1], columns[2] from `aggregate_1m.csv` order by cast(columns[0] as int);\r\n+------------+------------+------------+\r\n|   EXPR$0   |   EXPR$1   |   EXPR$2   |\r\n+------------+------------+------------+\r\nQuery failed: Failure while running fragment.[ 7bc42b6c-c00a-41c2-af43-331655e5a178 on qa-node119.qa.lab:31010 ]\r\n\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\r\nThe same query adding a limit (say limit 100) worked. Here is the assertion stack in drill bit log:\r\n\r\n2014-11-12 15:16:22,864 [f89183d7-f94d-4ece-818f-8e597859d529:frag:0:0] WARN  o.a.d.e.w.fragment.FragmentExecutor - Error while initializing or executing fragment\r\njava.lang.AssertionError: null\r\n        at org.apache.drill.exec.memory.AtomicRemainder.get(AtomicRemainder.java:126) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.memory.AtomicRemainder.forceGet(AtomicRemainder.java:85) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.memory.Accountor.forceAdditionalReservation(Accountor.java:142) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.memory.Accountor.transferTo(Accountor.java:111) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n        at io.netty.buffer.DrillBuf.transferAccounting(DrillBuf.java:182) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n        at org.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.takeOwnership(TopLevelAllocator.java:192) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.takeOwnership(ExternalSortBatch.java:464) ~[drill-java-exec-0.7.0-incubating-SNAPSHOT-rebuffed.jar:0.7.0-incubating-SNAPSHOT]"
    ],
    [
        "DRILL-3522",
        "DRILL-699",
        "IllegalStateException from Mongo storage plugin With a Mongo storage plugin enabled, we are sporadically getting the following exception when running queries (even not against the Mongo storage plugin):\r\n\r\n{code}\r\nSYSTEM ERROR: IllegalStateException: state should be: open\r\n\r\n\r\n\r\n  (org.apache.drill.exec.work.foreman.ForemanException) Unexpected exception during fragment initialization: org.apache.drill.common.exceptions.DrillRuntimeException: state should be: open\r\n    org.apache.drill.exec.work.foreman.Foreman.run():253\r\n    java.util.concurrent.ThreadPoolExecutor.runWorker():1145\r\n    java.util.concurrent.ThreadPoolExecutor$Worker.run():615\r\n    java.lang.Thread.run():745\r\n  Caused By (com.google.common.util.concurrent.UncheckedExecutionException) org.apache.drill.common.exceptions.DrillRuntimeException: state should be: open\r\n    com.google.common.cache.LocalCache$Segment.get():2263\r\n    com.google.common.cache.LocalCache.get():4000\r\n    com.google.common.cache.LocalCache.getOrLoad():4004\r\n    com.google.common.cache.LocalCache$LocalLoadingCache.get():4874\r\n    org.apache.drill.exec.store.mongo.schema.MongoSchemaFactory$MongoSchema.getSubSchemaNames():172\r\n    org.apache.drill.exec.store.mongo.schema.MongoSchemaFactory$MongoSchema.setHolder():159\r\n    org.apache.drill.exec.store.mongo.schema.MongoSchemaFactory.registerSchemas():127\r\n    org.apache.drill.exec.store.mongo.MongoStoragePlugin.registerSchemas():86\r\n    org.apache.drill.exec.store.StoragePluginRegistry$DrillSchemaFactory.registerSchemas():328\r\n    org.apache.drill.exec.ops.QueryContext.getRootSchema():165\r\n    org.apache.drill.exec.ops.QueryContext.getRootSchema():154\r\n    org.apache.drill.exec.ops.QueryContext.getRootSchema():142\r\n    org.apache.drill.exec.ops.QueryContext.getNewDefaultSchema():128\r\n    org.apache.drill.exec.planner.sql.DrillSqlWorker.():91\r\n    org.apache.drill.exec.work.foreman.Foreman.runSQL():901\r\n    org.apache.drill.exec.work.foreman.Foreman.run():242\r\n    java.util.concurrent.ThreadPoolExecutor.runWorker():1145\r\n    java.util.concurrent.ThreadPoolExecutor$Worker.run():615\r\n    java.lang.Thread.run():745\r\n  Caused By (org.apache.drill.common.exceptions.DrillRuntimeException) state should be: open\r\n    org.apache.drill.exec.store.mongo.schema.MongoSchemaFactory$DatabaseLoader.load():98\r\n    org.apache.drill.exec.store.mongo.schema.MongoSchemaFactory$DatabaseLoader.load():82\r\n    com.google.common.cache.LocalCache$LoadingValueReference.loadFuture():3599\r\n    com.google.common.cache.LocalCache$Segment.loadSync():2379\r\n    com.google.common.cache.LocalCache$Segment.lockedGetOrLoad():2342\r\n    com.google.common.cache.LocalCache$Segment.get():2257\r\n    com.google.common.cache.LocalCache.get():4000\r\n    com.google.common.cache.LocalCache.getOrLoad():4004\r\n    com.google.common.cache.LocalCache$LocalLoadingCache.get():4874\r\n    org.apache.drill.exec.store.mongo.schema.MongoSchemaFactory$MongoSchema.getSubSchemaNames():172\r\n    org.apache.drill.exec.store.mongo.schema.MongoSchemaFactory$MongoSchema.setHolder():159\r\n    org.apache.drill.exec.store.mongo.schema.MongoSchemaFactory.registerSchemas():127\r\n    org.apache.drill.exec.store.mongo.MongoStoragePlugin.registerSchemas():86\r\n    org.apache.drill.exec.store.StoragePluginRegistry$DrillSchemaFactory.registerSchemas():328\r\n    org.apache.drill.exec.ops.QueryContext.getRootSchema():165\r\n    org.apache.drill.exec.ops.QueryContext.getRootSchema():154\r\n    org.apache.drill.exec.ops.QueryContext.getRootSchema():142\r\n    org.apache.drill.exec.ops.QueryContext.getNewDefaultSchema():128\r\n    org.apache.drill.exec.planner.sql.DrillSqlWorker.():91\r\n    org.apache.drill.exec.work.foreman.Foreman.runSQL():901\r\n    org.apache.drill.exec.work.foreman.Foreman.run():242\r\n    java.util.concurrent.ThreadPoolExecutor.runWorker():1145\r\n    java.util.concurrent.ThreadPoolExecutor$Worker.run():615\r\n    java.lang.Thread.run():745\r\n  Caused By (java.lang.IllegalStateException) state should be: open\r\n    com.mongodb.assertions.Assertions.isTrue():70\r\n    com.mongodb.connection.BaseCluster.selectServer():79\r\n    com.mongodb.binding.ClusterBinding$ClusterBindingConnectionSource.():75\r\n    com.mongodb.binding.ClusterBinding$ClusterBindingConnectionSource.():71\r\n    com.mongodb.binding.ClusterBinding.getReadConnectionSource():63\r\n    com.mongodb.operation.OperationHelper.withConnection():166\r\n    com.mongodb.operation.ListDatabasesOperation.execute():100\r\n    com.mongodb.operation.ListDatabasesOperation.execute():52\r\n    com.mongodb.Mongo.execute():738\r\n    com.mongodb.Mongo$2.execute():725\r\n    com.mongodb.OperationIterable.iterator():47\r\n    com.mongodb.OperationIterable.forEach():66\r\n    com.mongodb.ListDatabasesIterableImpl.forEach():72\r\n    com.mongodb.MappingIterable.forEach():50\r\n    com.mongodb.MappingIterable.into():60\r\n    org.apache.drill.exec.store.mongo.schema.MongoSchemaFactory$DatabaseLoader.load():91\r\n    org.apache.drill.exec.store.mongo.schema.MongoSchemaFactory$DatabaseLoader.load():82\r\n    com.google.common.cache.LocalCache$LoadingValueReference.loadFuture():3599\r\n    com.google.common.cache.LocalCache$Segment.loadSync():2379\r\n    com.google.common.cache.LocalCache$Segment.lockedGetOrLoad():2342\r\n    com.google.common.cache.LocalCache$Segment.get():2257\r\n    com.google.common.cache.LocalCache.get():4000\r\n    com.google.common.cache.LocalCache.getOrLoad():4004\r\n    com.google.common.cache.LocalCache$LocalLoadingCache.get():4874\r\n    org.apache.drill.exec.store.mongo.schema.MongoSchemaFactory$MongoSchema.getSubSchemaNames():172\r\n    org.apache.drill.exec.store.mongo.schema.MongoSchemaFactory$MongoSchema.setHolder():159\r\n    org.apache.drill.exec.store.mongo.schema.MongoSchemaFactory.registerSchemas():127\r\n    org.apache.drill.exec.store.mongo.MongoStoragePlugin.registerSchemas():86\r\n    org.apache.drill.exec.store.StoragePluginRegistry$DrillSchemaFactory.registerSchemas():328\r\n    org.apache.drill.exec.ops.QueryContext.getRootSchema():165\r\n    org.apache.drill.exec.ops.QueryContext.getRootSchema():154\r\n    org.apache.drill.exec.ops.QueryContext.getRootSchema():142\r\n    org.apache.drill.exec.ops.QueryContext.getNewDefaultSchema():128\r\n    org.apache.drill.exec.planner.sql.DrillSqlWorker.():91\r\n    org.apache.drill.exec.work.foreman.Foreman.runSQL():901\r\n    org.apache.drill.exec.work.foreman.Foreman.run():242\r\n    java.util.concurrent.ThreadPoolExecutor.runWorker():1145\r\n    java.util.concurrent.ThreadPoolExecutor$Worker.run():615\r\n    java.lang.Thread.run():745\r\n{code}\r\n\r\nUpon investigation, the issue appears to be registering the Mongo schemas on connection (because the storage plugin is enabled).\r\n\r\nBasically, it appears that recent changes when upgrading the Mongo driver meant that MongoSchemaFactory indefinitely holds onto a MongoClient, even though they are part of a cache that expire after 24 hours.  This means that MongoSchemaFactory ends up trying to use a MongoClient that is closed, and thus the exception occurs.\r\n\r\nConsidering we already have a cache for MongoClient, it is safe for MongoSchemaFactory to call plugin.getClient() every time.",
        "Function resolutions fails to find functions in registry during the materialization due to case sensitive issues We maintain a Map<String, DrillFuncHolder> for functions that map the given function name to holder. Here the function name is added to map as it is given in FunctionTemplate without any case changes. The problem is after the Optiq-Drill conversion, all function names are converted to lowercase (in DrillOptiq.java). So if a function name is \"isTrue\" in the registry, we search for \"istrue\" and don't find any function during materialization."
    ],
    [
        "DRILL-2411",
        "DRILL-1408",
        "Scalar SUM/AVG over empty result set returns no rows instead of NULL Queries below should return NULL:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select sum(a2) from t2 where 1=0;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n+------------+\r\nNo rows selected (0.08 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select avg(a2) from t2 where 1=0;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n+------------+\r\nNo rows selected (0.074 seconds)\r\n{code}\r\n\r\nWhen grouped, result is correct:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select a2, sum(a2) from t2 where 1=0 group by a2;\r\n+------------+------------+\r\n|     a2     |   EXPR$1   |\r\n+------------+------------+\r\n+------------+------------+\r\nNo rows selected (0.11 seconds)\r\n{code}\r\n\r\nI'm not convinced and it is not very intuitive that correct result should be NULL, but this is what postgres returns and Aman thinks NULL is the correct behavior :)",
        "SELECT column from CSV with JOIN returns null if not part of JOIN condition A SELECT for a column from a CSV file with a JOIN condition always returns null for columns not included in the JOIN condition.  When querying each table separately, the values are returned as expected.\r\n\r\nNote that this works fine for any combination of JSON and Parquet, but fails when at least one of the files is CSV.\r\n\r\nSimple example with two small CSV files:\r\n\r\nbeatles.csv:\r\n---------------------------\r\n1,John,Lennon\r\n2,Paul,McCartney\r\n3,George,Harrison\r\n4,Ringo,Starr\r\n----------------------------\r\n\r\nsongs.csv:\r\n----------------------------\r\n1,Help\r\n2,Yesterday\r\n3,Blue Jay Way\r\n4,Yellow Submarine\r\n----------------------------\r\n\r\nThis queries returns values as expected:\r\n\r\nSELECT columns[0] AS id, CONCAT(columns[1], ' ', columns[2]) AS singer FROM dfs.`beatles.csv`;\r\n\r\n\r\nThis query returns 4 results, all with null values:\r\n\r\nSELECT S.columns[1] AS song, CONCAT(B.columns[1], ' ', B.columns[2]) AS singer \r\nFROM dfs.`beatles.csv` AS B\r\nINNER JOIN dfs.`songs.csv` AS S ON B.columns[0] = S.columns[0];\r\n\r\n\r\nThe only columns that return non-null values are the ones from the JOIN condition (B.columns[0] and S.columns[0] in the following query):\r\n\r\nSELECT S.columns[1] AS song, CONCAT(B.columns[1], ' ', B.columns[2]) AS singer, S.columns[0] AS beatles_id, B.columns[0] AS id \r\nFROM dfs.`beatles.csv` AS B\r\nINNER JOIN dfs.`songs.csv` AS S ON B.columns[0] = S.columns[0];\r\n"
    ],
    [
        "DRILL-4211",
        "DRILL-3371",
        "Inconsistent results from a joined sql statement to postgres tables When making an sql statement that incorporates a join to a table and then a self join to that table to get a parent value , Drill brings back inconsistent results. \r\n\r\nHere is the sql in postgres with correct output:\r\n{code:sql}\r\nselect trx.categoryguid,\r\ncat.categoryname, w1.categoryname as parentcat\r\nfrom transactions trx\r\njoin categories cat on (cat.CATEGORYGUID = trx.CATEGORYGUID)\r\njoin categories w1 on (cat.categoryparentguid = w1.categoryguid)\r\nwhere cat.categoryparentguid IS NOT NULL;\r\n{code}\r\n\r\nOutput:\r\n||categoryid||categoryname||parentcategory||\r\n|id1|restaurants|food&Dining|\r\n|id1|restaurants|food&Dining|\r\n|id2|Coffee Shops|food&Dining|\r\n|id2|Coffee Shops|food&Dining|\r\n\r\nWhen run in Drill with correct storage prefix:\r\n{code:sql}\r\nselect trx.categoryguid,\r\ncat.categoryname, w1.categoryname as parentcat\r\nfrom db.schema.transactions trx\r\njoin db.schema.categories cat on (cat.CATEGORYGUID = trx.CATEGORYGUID)\r\njoin db.schema.wpfm_categories w1 on (cat.categoryparentguid = w1.categoryguid)\r\nwhere cat.categoryparentguid IS NOT NULL\r\n{code}\r\n\r\nResults are:\r\n||categoryid||categoryname||parentcategory||\r\n|id1|restaurants|null|\r\n|id1|restaurants|null|\r\n|id2|Coffee Shops|null|\r\n|id2|Coffee Shops|null|\r\n\r\nPhysical plan is:\r\n{code:sql}\r\n00-00    Screen : rowType = RecordType(VARCHAR(50) categoryguid, VARCHAR(50) categoryname, VARCHAR(50) parentcat): rowcount = 100.0, cumulative cost = {110.0 rows, 110.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 64293\r\n00-01      Project(categoryguid=[$0], categoryname=[$1], parentcat=[$2]) : rowType = RecordType(VARCHAR(50) categoryguid, VARCHAR(50) categoryname, VARCHAR(50) parentcat): rowcount = 100.0, cumulative cost = {100.0 rows, 100.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 64292\r\n00-02        Project(categoryguid=[$9], categoryname=[$41], parentcat=[$47]) : rowType = RecordType(VARCHAR(50) categoryguid, VARCHAR(50) categoryname, VARCHAR(50) parentcat): rowcount = 100.0, cumulative cost = {100.0 rows, 100.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 64291\r\n00-03          Jdbc(sql=[SELECT *\r\nFROM \"public\".\"transactions\"\r\nINNER JOIN (SELECT *\r\nFROM \"public\".\"categories\"\r\nWHERE \"categoryparentguid\" IS NOT NULL) AS \"t\" ON \"transactions\".\"categoryguid\" = \"t\".\"categoryguid\"\r\nINNER JOIN \"public\".\"categories\" AS \"categories0\" ON \"t\".\"categoryparentguid\" = \"categories0\".\"categoryguid\"]) : rowType = RecordType(VARCHAR(255) transactionguid, VARCHAR(255) relatedtransactionguid, VARCHAR(255) transactioncode, DECIMAL(1, 0) transactionpending, VARCHAR(50) transactionrefobjecttype, VARCHAR(255) transactionrefobjectguid, VARCHAR(1024) transactionrefobjectvalue, TIMESTAMP(6) transactiondate, VARCHAR(256) transactiondescription, VARCHAR(50) categoryguid, VARCHAR(3) transactioncurrency, DECIMAL(15, 3) transactionoldbalance, DECIMAL(13, 3) transactionamount, DECIMAL(15, 3) transactionnewbalance, VARCHAR(512) transactionnotes, DECIMAL(2, 0) transactioninstrumenttype, VARCHAR(20) transactioninstrumentsubtype, VARCHAR(20) transactioninstrumentcode, VARCHAR(50) transactionorigpartyguid, VARCHAR(255) transactionorigaccountguid, VARCHAR(50) transactionrecpartyguid, VARCHAR(255) transactionrecaccountguid, VARCHAR(256) transactionstatementdesc, DECIMAL(1, 0) transactionsplit, DECIMAL(1, 0) transactionduplicated, DECIMAL(1, 0) transactionrecategorized, TIMESTAMP(6) transactioncreatedat, TIMESTAMP(6) transactionupdatedat, VARCHAR(50) transactionmatrulerefobjtype, VARCHAR(50) transactionmatrulerefobjguid, VARCHAR(50) transactionmatrulerefobjvalue, VARCHAR(50) transactionuserruleguid, DECIMAL(2, 0) transactionsplitorder, TIMESTAMP(6) transactionprocessedat, TIMESTAMP(6) transactioncategoryassignat, VARCHAR(50) transactionsystemcategoryguid, VARCHAR(50) transactionorigmandateid, VARCHAR(100) fingerprint, VARCHAR(50) categoryguid0, VARCHAR(50) categoryparentguid, DECIMAL(3, 0) categorytype, VARCHAR(50) categoryname, VARCHAR(50) categorydescription, VARCHAR(50) partyguid, VARCHAR(50) categoryguid1, VARCHAR(50) categoryparentguid0, DECIMAL(3, 0) categorytype0, VARCHAR(50) categoryname0, VARCHAR(50) categorydescription0, VARCHAR(50) partyguid0): rowcount = 100.0, cumulative cost = {100.0 rows, 100.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 64259\r\n{code}\r\n\r\nI worked around it by creating a view on postgres but not ideal. Thanks in advance.\r\nFirst Drill Jira Bug.",
        "Null Columns are getting copied from the previous one in Drill Explorer  In Drill Explorer, when querying a CSV file, in while some rows contain an extra column, the rows which do not contain those columns copy the previous column values and display them. They should have shown NULL. Web UI queries this fine but it is problem with Drill Explorer only."
    ],
    [
        "DRILL-4036",
        "DRILL-1522",
        "logs/sqlline_queries.json can not be accessed by user mapr  Drill was installed using RPM and when I try to connect to Drill from sqlline as mapr user it results in permission denied error. That file sqlline_queries.json is always empty, it has no content in it, and it is owned by root and others can not write to it.\r\n\r\nThe change was made using he below commit\r\nhttps://github.com/apache/drill/commit/42d5f818a5501dbd05808c53959db86e66202792\r\n\r\n{code}\r\nI logged in as root \r\n\r\n[root@centos-01 bin]# id\r\nuid=0(root) gid=0(root) groups=0(root)\r\n\r\nNote that the file is owned by root, and non-root users can not write to that file.\r\n[root@centos-01 bin]# ls -lrt /opt/mapr/drill/drill-1.3.0/logs/sqlline_queries.json\r\n-rw-r--r-- 1 root root 0 Nov  2 20:56 /opt/mapr/drill/drill-1.3.0/logs/sqlline_queries.json\r\n\r\nand then I connect to Drill as mapr user\r\n \r\n[root@centos-01 bin]# su - mapr\r\n-bash-4.1$ pwd\r\n/home/mapr\r\n-bash-4.1$ cd /opt/mapr/drill/drill-1.3.0/bin/\r\n-bash-4.1$ ./sqlline -u \"jdbc:drill:schema=dfs.tmp -n mapr -p mapr\"\r\n23:30:38,366 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback.groovy]\r\n23:30:38,366 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback-test.xml]\r\n23:30:38,367 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Found resource [logback.xml] at [file:/opt/mapr/drill/drill-1.3.0/conf/logback.xml]\r\n23:30:38,565 |-INFO in ch.qos.logback.classic.joran.action.ConfigurationAction - debug attribute not set\r\n23:30:38,571 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - About to instantiate appender of type [ch.qos.logback.core.ConsoleAppender]\r\n23:30:38,583 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - Naming appender as [STDOUT]\r\n23:30:38,613 |-INFO in ch.qos.logback.core.joran.action.NestedComplexPropertyIA - Assuming default type [ch.qos.logback.classic.encoder.PatternLayoutEncoder] for [encoder] property\r\n23:30:38,693 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - About to instantiate appender of type [ch.qos.logback.core.rolling.RollingFileAppender]\r\n23:30:38,696 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - Naming appender as [QUERY]\r\n23:30:38,722 |-INFO in ch.qos.logback.core.rolling.FixedWindowRollingPolicy@69663655 - No compression will be used\r\n23:30:38,736 |-INFO in ch.qos.logback.core.joran.action.NestedComplexPropertyIA - Assuming default type [ch.qos.logback.classic.encoder.PatternLayoutEncoder] for [encoder] property\r\n23:30:38,737 |-INFO in ch.qos.logback.core.rolling.RollingFileAppender[QUERY] - Active log file name: /opt/mapr/drill/drill-1.3.0/logs/sqlline_queries.json\r\n23:30:38,737 |-INFO in ch.qos.logback.core.rolling.RollingFileAppender[QUERY] - File property is set to [/opt/mapr/drill/drill-1.3.0/logs/sqlline_queries.json]\r\n23:30:38,739 |-ERROR in ch.qos.logback.core.rolling.RollingFileAppender[QUERY] - openFile(/opt/mapr/drill/drill-1.3.0/logs/sqlline_queries.json,true) call failed. java.io.FileNotFoundException: /opt/mapr/drill/drill-1.3.0/logs/sqlline_queries.json (Permission denied)\r\n\tat java.io.FileNotFoundException: /opt/mapr/drill/drill-1.3.0/logs/sqlline_queries.json (Permission denied)\r\n\tat \tat java.io.FileOutputStream.open(Native Method)\r\n\tat \tat java.io.FileOutputStream.<init>(FileOutputStream.java:221)\r\n\tat \tat ch.qos.logback.core.recovery.ResilientFileOutputStream.<init>(ResilientFileOutputStream.java:28)\r\n\tat \tat ch.qos.logback.core.FileAppender.openFile(FileAppender.java:149)\r\n\tat \tat ch.qos.logback.core.FileAppender.start(FileAppender.java:108)\r\n\tat \tat ch.qos.logback.core.rolling.RollingFileAppender.start(RollingFileAppender.java:86)\r\n\tat \tat ch.qos.logback.core.joran.action.AppenderAction.end(AppenderAction.java:96)\r\n\tat \tat ch.qos.logback.core.joran.spi.Interpreter.callEndAction(Interpreter.java:317)\r\n\tat \tat ch.qos.logback.core.joran.spi.Interpreter.endElement(Interpreter.java:196)\r\n\tat \tat ch.qos.logback.core.joran.spi.Interpreter.endElement(Interpreter.java:182)\r\n\tat \tat ch.qos.logback.core.joran.spi.EventPlayer.play(EventPlayer.java:62)\r\n\tat \tat ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:149)\r\n\tat \tat ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:135)\r\n\tat \tat ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:99)\r\n\tat \tat ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:49)\r\n\tat \tat ch.qos.logback.classic.util.ContextInitializer.configureByResource(ContextInitializer.java:75)\r\n\tat \tat ch.qos.logback.classic.util.ContextInitializer.autoConfig(ContextInitializer.java:148)\r\n\tat \tat org.slf4j.impl.StaticLoggerBinder.init(StaticLoggerBinder.java:85)\r\n\tat \tat org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:55)\r\n\tat \tat org.slf4j.LoggerFactory.bind(LoggerFactory.java:129)\r\n\tat \tat org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:108)\r\n\tat \tat org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:302)\r\n\tat \tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:276)\r\n\tat \tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:288)\r\n\tat \tat org.apache.drill.jdbc.Driver.<clinit>(Driver.java:34)\r\n\tat \tat java.lang.Class.forName0(Native Method)\r\n\tat \tat java.lang.Class.forName(Class.java:195)\r\n\tat \tat sqlline.DatabaseConnection.connect(DatabaseConnection.java:125)\r\n{code}",
        "Hbase convert_from queries are not pushed down git.commit.id.abbrev=5c220e3\r\n\r\nThe following hbase query contains the convert_from function but was not being pushed down.\r\n\r\n0: jdbc:drill:schema=hbase> explain plan for select convert_from(voter.onecf.name, 'UTF8') from voter where row_key=10;\r\n\r\n| {\r\n  \"head\" : {\r\n    \"version\" : 1,\r\n    \"generator\" : {\r\n      \"type\" : \"ExplainHandler\",\r\n      \"info\" : \"\"\r\n    },\r\n    \"type\" : \"APACHE_DRILL_PHYSICAL\",\r\n    \"options\" : [ ],\r\n    \"queue\" : 0,\r\n    \"resultMode\" : \"EXEC\"\r\n  },\r\n  \"graph\" : [ {\r\n    \"pop\" : \"hbase-scan\",\r\n    \"@id\" : 5,\r\n    \"hbaseScanSpec\" : {\r\n      \"tableName\" : \"voter\",\r\n      \"startRow\" : \"\",\r\n      \"stopRow\" : \"\",\r\n      \"serializedFilter\" : null\r\n    },"
    ],
    [
        "DRILL-4117",
        "DRILL-27",
        "Ensure proper null outcome handling during FileSelection creation and its subclasses. Hakim identified the following does not make a null check upon the result of  FileSelection.create(...). This issue is to ensure proper null outcome handling during FileSelection creation and its subclasses or to return a non-null default type.\r\n\r\n{quote}\r\nonFileSystemPartitionDescriptor.createNewGroupScan() passes the output to FileGroupScan.close() which expects it to be not null\r\n{quote}",
        "Implement Segment Reference Operator Segment (1)\r\n\r\nThe segment operator is responsible for collecting all records that share the same values of the provided expressions and outputting them together as a single segment (or group) of data.  Each record of the output segment will have the same value for all provided expressions.  The segment operator will also output a segment key in the provided ref.  \r\nOptionally, a segment operator can receive an input segment key (within).  In this case, each output segment will be limited by the provided input segment. The segment operator is stable. This means that all records within a segment will appear in the order that they appeared in the input. There is no guarantee, however, about the order that inner segments will be output.\r\n{ @id\u2020: <opref>, op: \u201csegment\u201d, \r\n\r\n  input\u2020: <input>,\r\n\r\n  within*: <name>,\r\n\r\n  ref: <name>,\r\n\r\n  exprs: [<expr>,..., <expr>]\r\n\r\n  \r\n\r\n}\r\n"
    ],
    [
        "DRILL-860",
        "DRILL-985",
        "Join between hbase/m7 tables fail git.commit.id.abbrev=01bf849\r\n\r\nThe following join query runs successfully against parquet data; however, failed against hbase or m7 tables:\r\n\r\nselect cast(student.onecf['name'] as varchar(35)) name, cast(student.twocf['age'] as integer) age, cast(student.threecf['gpa'] as decimal(4,2)) gpa, cast(s_voter.twocf['registration'] as varchar(20)) registration from student join s_voter on (student.onecf['name'] = s_voter.onecf['name']);\r\n\r\nFailure while running fragment. < UnsupportedOperationException\r\n",
        "Referencing a view via its filename does not work Since a view exists as a file in a file schema, it can be accessed via its filename in addition to via its 'simple' entry in INFORMATION_SCHEMA.\r\n\r\nBut only the simple name works, not the filename.\r\n\r\nBoth names need to be supported.\r\n\r\n{code}\r\n0: jdbc:drill:local=localhost:31010> select * from `dfs.tmp`.`georgecview`;\r\n+------------+------------+------------+------------+\r\n| timestamp  |   status   |   total    |    data    |\r\n+------------+------------+------------+------------+\r\n| 1402698853168 | OK         | 4          | [{\"id\":\"83045086184699543\",\"hostna |\r\n+------------+------------+------------+------------+\r\n1 row selected (0.391 seconds)\r\n0: jdbc:drill:local=localhost:31010> select * from `dfs.tmp`.`georgecview.drill`;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"16a3962a-18b9-4565-a1ee-5dd44e610b25\"\r\nendpoint {\r\n  address: \"192.168.39.43\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < ValidationException:[ org.eigenbase.util.EigenbaseContextException: From line 1, column 15 to line 1, column 43 ] < EigenbaseContextException:[ From line 1, column 15 to line 1, column 43 ] < SqlValidatorException:[ Table 'dfs.tmp.georgecview.drill' not found ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n{code}\r\n"
    ],
    [
        "DRILL-2530",
        "DRILL-3595",
        "getColumns() doesn't return right COLUMN_SIZE for INTERVAL types ",
        "Wrong results returned by query that uses LEAD window function Query that uses LEAD window function returns wrong results on developer's private branch (new-window-funcs).\r\n\r\nResults returned by Drill\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select lead(c1) over w from union_01 window w as (partition by c3 order by c1) limit 10;\r\n+---------+\r\n| EXPR$0  |\r\n+---------+\r\n| 878     |\r\n| -150    |\r\n| 402     |\r\n| 402     |\r\n| 402     |\r\n| 402     |\r\n| 402     |\r\n| 160     |\r\n| 160     |\r\n| 160     |\r\n+---------+\r\n10 rows selected (0.349 seconds)\r\n{code}\r\n\r\nResults returned by Postgres\r\n\r\n{code}\r\npostgres=# select lead(c1) over w from union_01 window w as (partition by c3 order by c1) limit 10;\r\n lead \r\n------\r\n     \r\n     \r\n  402\r\n  402\r\n  402\r\n  402\r\n  402\r\n     \r\n  160\r\n  160\r\n(10 rows)\r\n{code}"
    ],
    [
        "DRILL-1375",
        "DRILL-765",
        "Drill UI is not displaying appropriate error messages for query related failures git.commit.id.abbrev=711cc2f\r\n\r\nWhen a failure occurs such as trying to execute an invalid query, the UI would display the following error message:\r\n\r\n        HTTP ERROR 500\r\n        Problem accessing /query. Reason:\r\n        Request failed.\r\n\r\nWe should display the appropriate \"Reason\".",
        "Order by query will always send an empty record batch as the first record batch For example, comparing the two queries in sqlline, \r\n\r\n{code}\r\nSELECT region_id, sales_city FROM cp.`region.json`;\r\n{code}\r\n\r\nand\r\n\r\n{code}\r\nSELECT region_id, sales_city FROM cp.`region.json` ORDER BY region_id DESC;\r\n{code}\r\n\r\nAttached sqlline diff file."
    ],
    [
        "DRILL-2544",
        "DRILL-3071",
        "Quitting sqlline results in a debug message Closing: org.apache.drill.jdbc.DrillJdbc41Factory$DrillJdbc41Connection {code}\r\n[root@atsqa6c57 ~]# /opt/drill/bin/sqlline -u \"jdbc:drill:zk=localhost:5181\"\r\n/opt/drill/bin/drill-config.sh: line 94: [: =: unary operator expected\r\nsqlline version 1.1.6\r\n0: jdbc:drill:zk=localhost:5181> !q\r\nClosing: org.apache.drill.jdbc.DrillJdbc41Factory$DrillJdbc41Connection\r\n{code}\r\n",
        "RecordBatchLoader#load leaks memory if an exception is thrown while loading the batch. We should catch handle re-throw any exception that occurs inside #load."
    ],
    [
        "DRILL-169",
        "DRILL-1239",
        "Projection LOP serialization & deserialization  selections field is tagged with @JsonProperty(\"projections\") in constructor, but getter method getSelections() is tagged with @JsonProperty(\"exprs\"), which makes serialization & deserialization not quite the same.\r\n\r\nIt's typo or do you have other considerations?\r\n",
        "java.lang.AssertionError When performing select against nested JSON > 60,000 records Using a JSON file with contents like so for each record:\r\n{code}:\r\n\r\n{\"trans_id\":999999,\"date\":\"11/03/2012\",\"time\":\"09:07:05\",\"user_info\":{\"cust_id\":2,\"device\":\"AOS4.3\",\"state\":\"tx\"},\"marketing_info\":{\"camp_id\":14,\"keywords\":[\"it\",\"i\",\"wants\",\"yes\",\"things\",\"few\",\"like\"]},\"trans_info\":{\"prod_id\":[167,145,5,487,290],\"purch_flag\":\"false\"}}\r\n{code}\r\nFirst I set the following to get more verbose output:\r\n\r\n{quote}\r\n0: jdbc:drill:> alter session set `exec.errors.verbose`=true;\r\n{quote}\r\n\r\nThen performed a simple select via sqlline:\r\n\r\n{quote}\r\nselect * from dfs.`/mapr/drillram/JSON/large/mobile.json`;\r\n<50,000+ rows of output>\r\n| 56184      | 03/11/2013 | 14:19:10   | {\"cust_id\":4,\"device\":\"IOS5\",\"state\":\"va\"} | {\"camp_id\":15,\"keywords\":[\"young\"]} | {\"prod_id |\r\n| 56185      | 07/03/2013 | 14:30:38   | {\"cust_id\":1518,\"device\":\"AOS4.4\",\"state\":\"wi\"} | {\"camp_id\":11,\"keywords\":[\"so\",\"way\",\"okay |\r\n| 56186      | 07/07/2013 | 10:41:04   | {\"cust_id\":97279,\"device\":\"IOS5\",\"state\":\"ga\"} | {\"camp_id\":7,\"keywords\":[]} | {\"prod_id\":[9 |\r\nQuery failed: Failure while running fragment. null [4407eef7-06aa-4cf9-9962-a2f187ce8f17]\r\nNode details: ip-172-16-1-111:31011/31012\r\njava.lang.AssertionError\r\n\tat org.apache.drill.exec.vector.complex.WriteState.fail(WriteState.java:37)\r\n\tat org.apache.drill.exec.vector.complex.impl.AbstractBaseWriter.inform(AbstractBaseWriter.java:62)\r\n\tat org.apache.drill.exec.vector.complex.impl.RepeatedBigIntWriterImpl.inform(RepeatedBigIntWriterImpl.java:108)\r\n\tat org.apache.drill.exec.vector.complex.impl.RepeatedBigIntWriterImpl.setPosition(RepeatedBigIntWriterImpl.java:130)\r\n\tat org.apache.drill.exec.vector.complex.impl.SingleListWriter.setPosition(SingleListWriter.java:700)\r\n\tat org.apache.drill.exec.vector.complex.impl.SingleMapWriter.setPosition(SingleMapWriter.java:153)\r\n\tat org.apache.drill.exec.vector.complex.impl.SingleMapWriter.setPosition(SingleMapWriter.java:153)\r\n\tat org.apache.drill.exec.vector.complex.impl.VectorContainerWriter.setPosition(VectorContainerWriter.java:66)\r\n\tat org.apache.drill.exec.store.easy.json.JSONRecordReader2.next(JSONRecordReader2.java:80)\r\n\tat org.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:148)\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:116)\r\n\tat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:59)\r\n\tat org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:98)\r\n\tat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:49)\r\n\tat org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:116)\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:250)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while trying to get next result batch.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n\r\n{quote}\r\nIf I re-run the same query against a smaller version of the same dataset (<50,000 records), I don't have the issue.  So far I've tried modifying the  DRILL_MAX_DIRECT_MEMORY and\r\nDRILL_MAX_HEAP variables to see if I could find something that works, but neither seem to make a difference.  Note: the error appears the same if I run on standalone mode."
    ],
    [
        "DRILL-2781",
        "DRILL-2474",
        "Protobuf changes for nested loop join A couple of the protobuf files were not regenerated as part of the nested loop join change. Will regenerate the couple of files and merge as part of this issue. ",
        "Create method for mapping options to enums This came up in the context of https://reviews.apache.org/r/31691/#comment124065 (re the SCALAR_REPLACEMENT_OPTION setting checked by ClassTransformer.getImplementationClass()). Basically, we have a system level option that specifies an enum value, and we have to convert the string to an enum each time. See if it would be possible to create some kind of EnumValidator (possibly generic) that will do this check and conversion once each time the option is set.\r\n"
    ],
    [
        "DRILL-2035",
        "DRILL-2266",
        "Add ability to cancel multiple queries Currently Drill UI allows canceling one query at a time.\r\nThis could be cumbersome to manage for scenarios using with BI tools which generate multiple queries for a single action in the UI.\r\n\r\n",
        "Complex parquet reader fails on reading timestamp datatype Created a file containing timestamp column\r\n\r\nparquet-tools meta shows the column as this\r\nTIMESTAMP_col:       OPTIONAL INT64 O:TIMESTAMP R:0 D:1\r\n\r\nReading this column using the regular reader works fine. The complex reader fails with the below exception.\r\n\r\n{code}\r\njava.lang.UnsupportedOperationException: org.apache.drill.exec.store.parquet2.DrillParquetGroupConverter$DrillTimeStampConverter\r\n        at parquet.io.api.PrimitiveConverter.addLong(PrimitiveConverter.java:105) ~[parquet-column-1.5.1-drill-r6.jar:na]\r\n        at parquet.column.impl.ColumnReaderImpl$2$4.writeValue(ColumnReaderImpl.java:258) ~[parquet-column-1.5.1-drill-r6.jar:na]\r\n        at parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:354) ~[parquet-column-1.5.1-drill-r6.jar:na]\r\n        at parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:400) ~[parquet-column-1.5.1-drill-r6.jar:na]\r\n        at org.apache.drill.exec.store.parquet2.DrillParquetReader.next(DrillParquetReader.java:309) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:165) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:89) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.limit.LimitRecordBatch.innerNext(LimitRecordBatch.java:113) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:89) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:96) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:97) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:115) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:303) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\n{code}"
    ],
    [
        "DRILL-1181",
        "DRILL-1954",
        "make sqlline error messages more user friendly Current sqlline messages are not quite end user friendly. Say, running a simple query [select * from NOT_HERE;] produces the output below. We should prettify the error messages shown to end user.\r\n\r\n0: jdbc:drill:zk> select * from NOT_HERE;\r\nselect * from NOT_HERE;\r\nQuery failed: Remote failure while running query.[error_id: \"af7f4b8d-b556-49d0-a6c2-a8f27d4d8b1f\"\r\nError: exception while executing query (state=,code=0)\r\nendpoint {\r\n  address: \"192.168.168.122\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < ValidationException:[ org.eigenbase.util.EigenbaseContextException: From line 1, column 15 to line 1, column 22 ] < EigenbaseContextException:[ From line 1, column 15 to line 1, column 22 ] < SqlValidatorException:[ Table 'NOT_HERE' not found ]\"\r\n]",
        "Update CSV Writer to treat a single RepeatedVarChar column as separate columns on output rather than Erroring I am experiencing this error when attempting to create a table.  When the same query is executed using SELECT * the results display fine.\r\n\r\n0: jdbc:drill:zk=local> CREATE TABLE fcc_cell AS select * from dfs.`/Users/Documents/fcc/fcc_lic_vw.csv` WHERE columns[10] IN ('\"Mobile/Fixed Broadband\"', '\"Fixed Wireless\"') AND columns[17] = '\"A\"';\r\nQuery failed: Query stopped., Repeated types are not supported. [ cd76d8fd-b00e-47d5-962c-6566381b17fb on 192.168.1.4:31010 ]\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)"
    ],
    [
        "DRILL-4240",
        "DRILL-1078",
        "Provide 32bit version of Winutils as default This *might* make it possible to run Drill on all versions of Windows 7 etc.  Would also make updating of documentation moot. ",
        "Receivers Metrics Add input profile and num of bytes received to receivers."
    ],
    [
        "DRILL-1138",
        "DRILL-673",
        "Explicit casting to boolean fails 0: jdbc:drill:schema=dfs.TpcHMulti> select cast(C_CUSTKEY as boolean) as booleantest from customer limit 1;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"c20bd69f-bb5c-443e-a01b-8571b4c86490\"\r\nendpoint {\r\n  address: \"drillats1.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < IllegalArgumentException:[ No enum constant org.apache.drill.common.types.TypeProtos.MinorType.BOOLEAN ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n\r\n",
        "Select against M7 table fails due to missing hbase jars in classpaths I have an m7 table at this location:\r\nhbase(main):014:0> list '/test/tables'\r\nTABLE                                                                                                     \r\n/test/tables/m7voter                  \r\n\r\nFrom sqlline, I ran the following query:\r\n0: jdbc:drill:schema=hbase> select * from `/test/tables/m7voter` limit 5;\r\n\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"4e7d2b40-6511-436b-a459-ab0667cd5f74\"\r\nendpoint {\r\n  address: \"qa-node64.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < ValidationException:[ org.eigenbase.util.EigenbaseContextException: From line 1, column 15 to line 1, column 36 ] < EigenbaseContextException:[ From line 1, column 15 to line 1, column 36 ] < SqlValidatorException:[ Table \\'/test/tables/m7voter\\' not found ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n\r\n"
    ],
    [
        "DRILL-2521",
        "DRILL-1208",
        "Revert from protobuf 2.6 to 2.5 ",
        "Tpch Query 8 over text data (SF 0.01) has a data verification issue git.commit.id.abbrev=caa8b78\r\n\r\nQuery 8 :\r\n\r\nselect\r\n  o_year,\r\n  sum(case\r\n    when nation = 'EGYPT' then volume\r\n    else 0\r\n  end) / sum(volume) as mkt_share\r\nfrom\r\n  (\r\n    select\r\n      extract(year from o.o_orderdate) as o_year,\r\n      l.l_extendedprice * (1 - l.l_discount) as volume,\r\n      n2.n_name as nation\r\n    from\r\n      part p,\r\n      supplier s,\r\n      lineitem l,\r\n      orders o,\r\n      customer c,\r\n      nation n1,\r\n      nation n2,\r\n      region r\r\n    where\r\n      p.p_partkey = l.l_partkey\r\n      and s.s_suppkey = l.l_suppkey\r\n      and l.l_orderkey = o.o_orderkey\r\n      and o.o_custkey = c.c_custkey\r\n      and c.c_nationkey = n1.n_nationkey\r\n      and n1.n_regionkey = r.r_regionkey\r\n      and r.r_name = 'MIDDLE EAST'\r\n      and s.s_nationkey = n2.n_nationkey\r\n      and o.o_orderdate between date '1995-01-01' and date '1996-12-31'\r\n      and p.p_type = 'PROMO BRUSHED COPPER'\r\n  ) as all_nations\r\ngroup by\r\n  o_year\r\norder by\r\n  o_year\r\n\r\nFailure Records :\r\n    Postgres : 1995\t0.2575147489116481\r\n    Drill         : 1995\t0.2506000484442781\r\n\r\nLet me know if you need any more information"
    ],
    [
        "DRILL-2369",
        "DRILL-4246",
        "It takes too long to plan inner join query with 10 join predicates Inner join with 10 join predicates takes 11 seconds to plan.\r\nIf number of predicates goes up to 14, planning time increases to 41 seconds.\r\n\r\n(1) ON clause with 10 columns in join condition\r\n\r\n-- Total elapsed time : 11.407 seconds\r\n-- Planning time      : 11.03 seconds\r\n\r\n{code}\r\nselect\r\n        count(*)\r\nfrom\r\n        alltypes_with_nulls a\r\n        INNER JOIN\r\n        alltypes_with_nulls b ON\r\n        (\r\n        a.c_boolean = b.c_boolean\r\n        AND a.c_timestamp = b.c_timestamp\r\n        AND a.c_time = b.c_time\r\n        AND a.c_date = b.c_date\r\n        AND a.c_float = b.c_float\r\n        AND a.c_bigdecimal = b.c_bigdecimal\r\n        AND a.c_smalldecimal = b.c_smalldecimal\r\n        AND a.c_bigint = b.c_bigint\r\n        AND a.c_integer = b.c_integer\r\n        AND a.c_varchar = b.c_varchar\r\n        )\r\n;\r\n{code}\r\n\r\nExplain plan:\r\n\r\n{code}\r\n00-01      StreamAgg(group=[{}], EXPR$0=[COUNT($0)])\r\n00-02        HashAgg(group=[{0}])\r\n00-03          Project(c_date=[$3])\r\n00-04            HashJoin(condition=[AND(=($0, $10), =($1, $11), =($2, $12), =($3, $13), =($4, $14), =($5, $15), =($6, $16), =($7, $17), =($8, $18), =($9, $19))], joinType=[inner])\r\n00-06              Project(c_boolean=[$9], c_timestamp=[$7], c_time=[$0], c_date=[$1], c_float=[$4], c_bigdecimal=[$6], c_smalldecimal=[$5], c_bigint=[$2], c_integer=[$8], c_varchar=[$3])\r\n00-08                Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/aggregation/alltypes_with_nulls]], selectionRoot=/drill/testdata/aggregation/alltypes_with_nulls, numFiles=1, columns=[`c_boolean`, `c_timestamp`, `c_time`, `c_date`, `c_float`, `c_bigdecimal`, `c_smalldecimal`, `c_bigint`, `c_integer`, `c_varchar`]]])\r\n00-05              Project(c_boolean0=[$0], c_timestamp0=[$1], c_time0=[$2], c_date0=[$3], c_float0=[$4], c_bigdecimal0=[$5], c_smalldecimal0=[$6], c_bigint0=[$7], c_integer0=[$8], c_varchar0=[$9])\r\n00-07                Project(c_boolean=[$9], c_timestamp=[$7], c_time=[$0], c_date=[$1], c_float=[$4], c_bigdecimal=[$6], c_smalldecimal=[$5], c_bigint=[$2], c_integer=[$8], c_varchar=[$3])\r\n00-09                  Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/aggregation/alltypes_with_nulls]], selectionRoot=/drill/testdata/aggregation/alltypes_with_nulls, numFiles=1, columns=[`c_boolean`, `c_timestamp`, `c_time`, `c_date`, `c_float`, `c_bigdecimal`, `c_smalldecimal`, `c_bigint`, `c_integer`, `c_varchar`]]])\r\n{code}\r\n\r\n(2) 10 columns in JOIN condition, in the WHERE clause (just to check that nothing funny is going on with ON clause)\r\n\r\n-- Total elapsed time : 11.139 seconds\r\n-- Planning time      : 11.416 seconds\r\n\r\n{code}\r\nselect\r\n        count(*)\r\nfrom\r\n        alltypes_with_nulls a,\r\n        alltypes_with_nulls b\r\nwhere\r\n        a.c_boolean = b.c_boolean\r\n        AND a.c_timestamp = b.c_timestamp\r\n        AND a.c_time = b.c_time\r\n        AND a.c_date = b.c_date\r\n        AND a.c_float = b.c_float\r\n        AND a.c_bigdecimal = b.c_bigdecimal\r\n        AND a.c_smalldecimal = b.c_smalldecimal\r\n        AND a.c_bigint = b.c_bigint\r\n        AND a.c_integer = b.c_integer\r\n        AND a.c_varchar = b.c_varchar\r\n;\r\n{code}\r\n\r\nExplain plan:\r\n\r\n{code}\r\n00-01      StreamAgg(group=[{}], EXPR$0=[COUNT()])\r\n00-02        Project($f0=[0])\r\n00-03          HashJoin(condition=[AND(=($0, $10), =($1, $11), =($2, $12), =($3, $13), =($4, $14), =($5, $15), =($6, $16), =($7, $17), =($8, $18), =($9, $19))], joinType=[inner])\r\n00-05            Project(c_boolean=[$9], c_timestamp=[$7], c_time=[$0], c_date=[$1], c_float=[$4], c_bigdecimal=[$6], c_smalldecimal=[$5], c_bigint=[$2], c_integer=[$8], c_varchar=[$3])\r\n00-07              Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/aggregation/alltypes_with_nulls]], selectionRoot=/drill/testdata/aggregation/alltypes_with_nulls, numFiles=1, columns=[`c_boolean`, `c_timestamp`, `c_time`, `c_date`, `c_float`, `c_bigdecimal`, `c_smalldecimal`, `c_bigint`, `c_integer`, `c_varchar`]]])\r\n00-04            Project(c_boolean0=[$0], c_timestamp0=[$1], c_time0=[$2], c_date0=[$3], c_float0=[$4], c_bigdecimal0=[$5], c_smalldecimal0=[$6], c_bigint0=[$7], c_integer0=[$8], c_varchar0=[$9])\r\n00-06              Project(c_boolean=[$9], c_timestamp=[$7], c_time=[$0], c_date=[$1], c_float=[$4], c_bigdecimal=[$6], c_smalldecimal=[$5], c_bigint=[$2], c_integer=[$8], c_varchar=[$3])\r\n00-08                Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/aggregation/alltypes_with_nulls]], selectionRoot=/drill/testdata/aggregation/alltypes_with_nulls, numFiles=1, columns=[`c_boolean`, `c_timestamp`, `c_time`, `c_date`, `c_float`, `c_bigdecimal`, `c_smalldecimal`, `c_bigint`, `c_integer`, `c_varchar`]]])\r\n{code}\r\n\r\n(3) 14 columns in join condition\r\n\r\n-- Total elapsed time : 41.101 seconds\r\n-- Planning time        : 39.771 seconds\r\n\r\n{code}\r\nselect\r\n        count(distinct a.c_date)\r\nfrom\r\n        alltypes_with_nulls a\r\n        INNER JOIN\r\n        alltypes_with_nulls b ON\r\n        (\r\n        a.c_varchar = b.c_varchar\r\n        AND a.c_integer = b.c_integer\r\n        AND a.c_bigint = b.c_bigint\r\n        AND a.c_smalldecimal = b.c_smalldecimal\r\n        AND a.c_bigdecimal = b.c_bigdecimal\r\n        AND a.c_float = b.c_float\r\n        AND a.c_date = b.c_date\r\n        AND a.c_time = b.c_time\r\n        AND a.c_timestamp = b.c_timestamp\r\n        AND a.c_boolean = b.c_boolean\r\n        AND a.d9 = b.d9\r\n        AND a.d18 = b.d18\r\n        AND a.d28 = b.d28\r\n        AND a.d38 = b.d38\r\n        )\r\n;\r\n{code}\r\n\r\nExplain plan:\r\n{code}\r\n00-01      StreamAgg(group=[{}], EXPR$0=[COUNT($0)])\r\n00-02        HashAgg(group=[{0}])\r\n00-03          Project(c_date=[$6])\r\n00-04            HashJoin(condition=[AND(=($0, $14), =($1, $15), =($2, $16), =($3, $17), =($4, $18), =($5, $19), =($6, $20), =($7, $21), =($8, $22), =($9, $23), =($10, $24), =($11, $25), =($12, $26), =($13, $27))], joinType=[inner])\r\n00-06              Project(c_varchar=[$5], c_integer=[$11], c_bigint=[$2], c_smalldecimal=[$6], c_bigdecimal=[$4], c_float=[$1], c_date=[$10], c_time=[$3], c_timestamp=[$9], c_boolean=[$7], d9=[$13], d18=[$8], d28=[$0], d38=[$12])\r\n00-08                Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/aggregation/alltypes_with_nulls]], selectionRoot=/drill/testdata/aggregation/alltypes_with_nulls, numFiles=1, columns=[`c_varchar`, `c_integer`, `c_bigint`, `c_smalldecimal`, `c_bigdecimal`, `c_float`, `c_date`, `c_time`, `c_timestamp`, `c_boolean`, `d9`, `d18`, `d28`, `d38`]]])\r\n00-05              Project(c_varchar0=[$0], c_integer0=[$1], c_bigint0=[$2], c_smalldecimal0=[$3], c_bigdecimal0=[$4], c_float0=[$5], c_date0=[$6], c_time0=[$7], c_timestamp0=[$8], c_boolean0=[$9], d90=[$10], d180=[$11], d280=[$12], d380=[$13])\r\n00-07                Project(c_varchar=[$5], c_integer=[$11], c_bigint=[$2], c_smalldecimal=[$6], c_bigdecimal=[$4], c_float=[$1], c_date=[$10], c_time=[$3], c_timestamp=[$9], c_boolean=[$7], d9=[$13], d18=[$8], d28=[$0], d38=[$12])\r\n00-09                  Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/aggregation/alltypes_with_nulls]], selectionRoot=/drill/testdata/aggregation/alltypes_with_nulls, numFiles=1, columns=[`c_varchar`, `c_integer`, `c_bigint`, `c_smalldecimal`, `c_bigdecimal`, `c_float`, `c_date`, `c_time`, `c_timestamp`, `c_boolean`, `d9`, `d18`, `d28`, `d38`]]])\r\n{code}",
        "New allocator causing a flatten regression test to fail with IllegalStateException We are seeing the following error in the test cluster:\r\n\r\n{noformat}\r\n/framework/resources/Functional/flatten_operators/100000rows/filter3.q\r\n\r\nQuery: \r\nselect uid, flatten(events) from `data.json` where uid > 1\r\n\r\nFailed with exception\r\njava.sql.SQLException: SYSTEM ERROR: IllegalStateException: Unaccounted for outstanding allocation (851968)\r\nAllocator(op:0:0:0:Screen) 1000000/851968/1941504/10000000000 (res/actual/peak/limit)\r\n{noformat}\r\n"
    ],
    [
        "DRILL-3523",
        "DRILL-1272",
        "Drill client CLI does not reconnect to Drillbit after long period of inactivity After a period of inactivity in the Drill CLI client before issuing another statement, it appears to time out the session/connection and the client returns an error instead of reconnecting to Drill.\r\n\r\nIt should instead just implicitly \"!reconnect\" itself since I can't think of a good reason why I have to do this by hand every time.\r\n\r\nUnlike in DRILL-3514 the drillbit in this case was never restarted, although the same fix could probably solve both issues.\r\n\r\nThe error received when trying to issue a new query after some absence is:\r\n{code}Error: CONNECTION ERROR: Connection /x.x.x.x:54367 <--> /x.x.x.x:31010 (user client) closed unexpectedly.\r\n\r\n[Error Id: 68714cc2-65f5-4cac-b062-44331f8f4c31 ] (state=,code=0)\r\n{code}",
        "Support CAST() functions for HBase filter pushdown git.commit.id.abbrev=98b208e\r\n\r\nI ran the following queries in drill:\r\nexplain plan for select cast(row_key as integer) voter_id, convert_from(onecf['name'], 'UTF8') name, cast(twocf['age'] as integer) age, cast(twocf['registration'] as varchar(20)) registration, cast(threecf['contributions'] as decimal(6,2)) contributions, cast(threecf['voterzone'] as integer) voterzone,cast(fourcf['create_date'] as timestamp) create_date from voter where onecf['name'] not similar to '%(young|u|a)%';\r\n\r\nexplain plan for select cast(student.onecf['name'] as varchar(35)) name, cast(student.twocf['age'] as integer) age, cast(student.threecf['gpa'] as decimal(4,2)) gpa, cast(voter.twocf['registration'] as varchar(20)) registration from student join voter on (student.onecf['name'] = voter.onecf['name']) where cast(student.twocf['age'] as integer) = 70;\r\n\r\nexplain plan for select cast(student.onecf['name'] as varchar(35)) name, cast(student.twocf['age'] as integer) age, cast(student.threecf['gpa'] as decimal(4,2)) gpa, cast(voter.twocf['registration'] as varchar(20)) registration from student join voter on (student.onecf['name'] = voter.onecf['name']) where cast(student.twocf['age'] as integer) > 70;\r\n\r\nThe explain plans for each of the query shows the following:\r\n \"graph\" : [ {\r\n    \"pop\" : \"hbase-scan\",\r\n    \"@id\" : 5,\r\n    \"hbaseScanSpec\" : {\r\n      \"tableName\" : \"voter\",\r\n      \"startRow\" : null,\r\n      \"stopRow\" : null,\r\n      \"serializedFilter\" : null\r\n    },\r\n\r\nLooks like the serializedFilter should not be null."
    ],
    [
        "DRILL-1143",
        "DRILL-2968",
        "Enable project pushdown in HiveRecordReader Most of the code already exists in {{HiveRecordReader}}, just enable project pushdown in {{HiveScan}}.",
        "crash on parquet file On my parquet dataset I can do count but can't query content. Mapreduce jobs (parquet-mr) work fine.\r\n\r\n0: jdbc:drill:> select count(1) from dfs.root.`flows/testing`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 2997423    |\r\n+------------+\r\n0: jdbc:drill:> select * from dfs.root.`flows/testing` limit 1;\r\nQuery failed: RemoteRpcException: Failure while running fragment.[ cd44e217-4f1a-4820-a40e-9928c5af2faf on data0:31010 ]\r\n  (java.lang.NullPointerException) \r\n    org.apache.drill.exec.vector.UInt1Vector.transferTo():223\r\n    org.apache.drill.exec.vector.NullableVarCharVector.transferTo():237\r\n    org.apache.drill.exec.vector.NullableVarCharVector$TransferImpl.transfer():267\r\n    org.apache.drill.exec.vector.complex.RepeatedMapVector$RepeatedMapTransferPair.transfer():293\r\n    org.apache.drill.exec.vector.complex.MapVector$MapTransferPair.transfer():186\r\n    org.apache.drill.exec.physical.impl.limit.LimitRecordBatch.doWork():133\r\n    org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext():93\r\n    org.apache.drill.exec.physical.impl.limit.LimitRecordBatch.innerNext():113\r\n    org.apache.drill.exec.record.AbstractRecordBatch.next():142\r\n    org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next():118\r\n    org.apache.drill.exec.record.AbstractRecordBatch.next():99\r\n    org.apache.drill.exec.record.AbstractRecordBatch.next():89\r\n    org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext():51\r\n    org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext():96\r\n    org.apache.drill.exec.record.AbstractRecordBatch.next():142\r\n    org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next():118\r\n    org.apache.drill.exec.physical.impl.BaseRootExec.next():68\r\n    org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext():96\r\n    org.apache.drill.exec.physical.impl.BaseRootExec.next():58\r\n    org.apache.drill.exec.work.fragment.FragmentExecutor.run():163\r\n    .......():0\r\n"
    ],
    [
        "DRILL-1511",
        "DRILL-62",
        "C++ Client. Fix compiling issues in DRILL-1297 patch on clang Two issues:\r\n\r\n- forward enum type declaration  is not allowed in C++ 03.\r\n- clang complains:\r\n\r\n{code}\r\nerror: found '<::' after a static_cast which forms the digraph '<:\u2019 (aka '[') and a ':', did you mean '< ::'?\r\n    ::exec::shared::QueryType castedType = static_cast<::exec::shared::QueryType> (t);\r\nfound '<::' after a static_cast which forms the digraph '<:'\r\n      (aka '[') and a ':', did you mean '< ::'?\r\n{code}",
        "Storage engine for Hive SerDe It would be good to support Hive SerDe mechanism [1] in Drill.\r\n\r\n[1] https://cwiki.apache.org/confluence/display/Hive/SerDe"
    ],
    [
        "DRILL-1265",
        "DRILL-228",
        "Drill incorrectly return null value when evaluates an expression of repeated list. This is a follow-up issue after DRILL-1258.  After the compiler error is fixed, the query completes with null value in the repeated list.\r\n\r\nJSON data: \r\n\r\n{ \"a\" : [ { \"x\": [[1], [2, 20], [3, 30, 300]], \"y\": \"abc\"}] }\r\n{ \"a\" : [ { \"x\": [[1000]], \"y\": \"abc2\"}] }\r\n\r\nQ1: \r\nselect t.a[0].x from dfs.`/Users/jni/work/data/json/input.json` t;\r\n\r\nEXPR$0\r\n[]\r\n[]\r\nTotal rows returned : 2\r\n\r\nHowever, if we put the expression into a convert_to function, the query will return correct result:\r\n\r\nQ2 : \r\nselect convert_to(t.a[0].x,'JSON') from dfs.`/Users/jni/work/data/json/input.json` t;\r\n\r\nEXPR$0\r\n[ [ 1 ], [ 2, 20 ], [ 3, 30, 300 ] ]\r\n[ [ 1000 ] ]\r\nTotal rows returned : 2\r\n\r\nSeems there might be some bug in copy operation in repeated list vector. \r\n",
        "maven submodule build fails at license-maven-plugin Doing a {{mvn install}} at any level except the drill-root cause the build to fail during execution of license-maven-plugin.\r\n\r\nSample failure\r\n\r\n{noformat} \r\nAditya@MACHINE /repo/git/apache-drill/exec/java-exec\r\n$ mvn clean install -DskipTests\r\n[INFO] Scanning for projects...\r\n[INFO]\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Building exec/Java Execution Engine 1.0.0-m1-incubating-SNAPSHOT\r\n[INFO] ------------------------------------------------------------------------\r\n.....\r\n.....\r\n[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 461 licence.\r\n[INFO]\r\n[INFO] --- license-maven-plugin:2.3:check (default) @ java-exec ---\r\n[INFO] Checking licenses...\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD FAILURE\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time: 18.158s\r\n[INFO] Finished at: Tue Sep 10 23:45:56 PDT 2013\r\n[INFO] Final Memory: 37M/510M\r\n[INFO] ------------------------------------------------------------------------\r\n[ERROR] Failed to execute goal com.mycila:license-maven-plugin:2.3:check (default) on project java-exec: Resource header not found in file system, classpath or URL: no protocol: header -> [Help 1]\r\n[ERROR]\r\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n[ERROR]\r\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\r\n{noformat} "
    ],
    [
        "DRILL-389",
        "DRILL-726",
        "Nested Parquet data generated from Hive does not work Inside of Hive, I generated Parquet data from Avro data as follows.  Using the attached Avro file (avro_test.db) and the attached nested Avro schema (nobench_1.avsc), I created a Hive table:\r\n\r\n{noformat}\r\nCREATE TABLE avro_nobench_hdfs\r\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\r\nSTORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\r\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\r\nLOCATION 'hdfs:///user/hdfs/avro'\r\nTBLPROPERTIES ('avro.schema.url'='hdfs:///user/hdfs/nobench.avsc');\r\n{noformat}\r\n\r\nNote that this schema is based loosely off of the NoBench standard proposed by Craig Chasseur for JSON (http://pages.cs.wisc.edu/~chasseur/).\r\n\r\nIn order to create a Parquet Hive table you need to create a full schema.  The one attached is very large, so I used the following:\r\n\r\n{noformat}\r\nsudo -u hdfs hive -e 'describe avro_nobench_hdfs' > /tmp/temp.sql\r\n{noformat}\r\n\r\nThen, I replaced the \"from deserializer\" with commas and added the following SQL DDL around it:\r\n\r\n{noformat}\r\nCREATE TABLE avro_nobench_parquet (\r\n    // ... COLUMNS HERE\r\n)\r\nROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe'\r\nSTORED AS\r\nINPUTFORMAT \"parquet.hive.DeprecatedParquetInputFormat\"\r\nOUTPUTFORMAT \"parquet.hive.DeprecatedParquetOutputFormat\";\r\n{noformat}\r\n\r\nFinally, I generated the actual Parquet binary data using {{INSERT INTO}}:\r\n\r\n{noformat}\r\nINSERT OVERWRITE avro_nobench_parquet SELECT * FROM avro_nobench_hdfs;\r\n{noformat}\r\n\r\nThis successfully completed.  Then, the data was validated using:\r\n\r\n{noformat}\r\nSELECT COUNT(*) FROM avro_nobench_parquet;\r\nSELECT * FROM avro_nobench_parquet LIMIT 1;\r\n{noformat}\r\n\r\nIf you look in {{hdfs:///user/hive/warehouse/avro_nobench_parquet}} you'll see a single raw file (something like {{0000_0}}).  Download that to local:\r\n\r\n{noformat}\r\nsudo -u hdfs hdfs dfs -copyToLocal /user/hive/warehouse/avro_nobench_parquet/* .\r\n{noformat}\r\n\r\nThen, in DRILL I ran:\r\n\r\n{noformat}\r\nSELECT COUNT(*) FROM \"nobench.parquet\";\r\n{noformat}\r\n\r\nAnd got the following:\r\n\r\n{noformat}\r\nCaused by: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"a13783d0-d9da-4639-8809-ba4a5ac54e04\"\r\nendpoint {\r\n  address: \"ip-10-101-1-82.ec2.internal\"\r\n  user_port: 31010\r\n  bit_port: 32011\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < NullPointerException\"\r\n]\r\n        at org.apache.drill.exec.rpc.user.QueryResultHandler.batchArrived(QueryResultHandler.java:72)\r\n        at org.apache.drill.exec.rpc.user.UserClient.handle(UserClient.java:79)\r\n        at org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:48)\r\n        at org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:33)\r\n        at org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:142)\r\n        at org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:127)\r\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\r\n        at io.netty.channel.DefaultChannelHandlerContext.invokeChannelRead(DefaultChannelHandlerContext.java:334)\r\n        at io.netty.channel.DefaultChannelHandlerContext.fireChannelRead(DefaultChannelHandlerContext.java:320)\r\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\r\n        at io.netty.channel.DefaultChannelHandlerContext.invokeChannelRead(DefaultChannelHandlerContext.java:334)\r\n        at io.netty.channel.DefaultChannelHandlerContext.fireChannelRead(DefaultChannelHandlerContext.java:320)\r\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:173)\r\n        at io.netty.channel.DefaultChannelHandlerContext.invokeChannelRead(DefaultChannelHandlerContext.java:334)\r\n        at io.netty.channel.DefaultChannelHandlerContext.fireChannelRead(DefaultChannelHandlerContext.java:320)\r\n        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:785)\r\n        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:100)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:497)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:465)\r\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:359)\r\n        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:101)\r\n        at java.lang.Thread.run(Thread.java:744)\r\n{noformat}\r\n\r\nThe second time I run it I get an OOM:\r\n\r\n{noformat}\r\nException in thread \"WorkManager-3\" java.lang.OutOfMemoryError: Java heap space\r\n        at org.apache.drill.exec.store.parquet.PageReadStatus.<init>(PageReadStatus.java:41)\r\n        at org.apache.drill.exec.store.parquet.ColumnReader.<init>(ColumnReader.java:70)\r\n        at org.apache.drill.exec.store.parquet.VarLenBinaryReader$NullableVarLengthColumn.<init>(VarLenBinaryReader.java:62)\r\n        at org.apache.drill.exec.store.parquet.ParquetRecordReader.<init>(ParquetRecordReader.java:167)\r\n        at org.apache.drill.exec.store.parquet.ParquetRecordReader.<init>(ParquetRecordReader.java:99)\r\n        at org.apache.drill.exec.store.parquet.ParquetScanBatchCreator.getBatch(ParquetScanBatchCreator.java:60)\r\n        at org.apache.drill.exec.physical.impl.ImplCreator.visitSubScan(ImplCreator.java:103)\r\n        at org.apache.drill.exec.physical.impl.ImplCreator.visitSubScan(ImplCreator.java:63)\r\n        at org.apache.drill.exec.store.parquet.ParquetRowGroupScan.accept(ParquetRowGroupScan.java:107)\r\n        at org.apache.drill.exec.physical.impl.ImplCreator.getChildren(ImplCreator.java:173)\r\n        at org.apache.drill.exec.physical.impl.ImplCreator.visitProject(ImplCreator.java:90)\r\n        at org.apache.drill.exec.physical.impl.ImplCreator.visitProject(ImplCreator.java:63)\r\n        at org.apache.drill.exec.physical.config.Project.accept(Project.java:51)\r\n        at org.apache.drill.exec.physical.impl.ImplCreator.getChildren(ImplCreator.java:173)\r\n        at org.apache.drill.exec.physical.impl.ImplCreator.visitSort(ImplCreator.java:121)\r\n        at org.apache.drill.exec.physical.impl.ImplCreator.visitSort(ImplCreator.java:63)\r\n        at org.apache.drill.exec.physical.config.Sort.accept(Sort.java:58)\r\n        at org.apache.drill.exec.physical.impl.ImplCreator.getChildren(ImplCreator.java:173)\r\n        at org.apache.drill.exec.physical.impl.ImplCreator.visitStreamingAggregate(ImplCreator.java:151)\r\n        at org.apache.drill.exec.physical.impl.ImplCreator.visitStreamingAggregate(ImplCreator.java:63)\r\n        at org.apache.drill.exec.physical.config.StreamingAggregate.accept(StreamingAggregate.java:59)\r\n        at org.apache.drill.exec.physical.impl.ImplCreator.getChildren(ImplCreator.java:173)\r\n        at org.apache.drill.exec.physical.impl.ImplCreator.visitScreen(ImplCreator.java:132)\r\n        at org.apache.drill.exec.physical.impl.ImplCreator.visitScreen(ImplCreator.java:63)\r\n        at org.apache.drill.exec.physical.config.Screen.accept(Screen.java:102)\r\n        at org.apache.drill.exec.physical.impl.ImplCreator.getExec(ImplCreator.java:180)\r\n        at org.apache.drill.exec.work.foreman.RunningFragmentManager.runFragments(RunningFragmentManager.java:84)\r\n        at org.apache.drill.exec.work.foreman.Foreman.runPhysicalPlan(Foreman.java:228)\r\n        at org.apache.drill.exec.work.foreman.Foreman.parseAndRunLogicalPlan(Foreman.java:176)\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:153)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n{noformat}",
        "order by fails when a column has null value postgres:\r\n\r\nfoodmart=# select c_row, c_groupby from data order by c_row;\r\n c_row | c_groupby\r\n-------+-----------\r\n     1 | a\r\n     2 | ab\r\n     3 | abc\r\n     4 |\r\n     5 |\r\n     6 | abc\r\n     7 | ab\r\n     8 | a\r\n     9 | a\r\n    10 | ab\r\n    11 | abc\r\n    12 |\r\n    13 |\r\n    14 | abc\r\n    15 | ab\r\n    16 | a\r\n    17 | a\r\n    18 | ab\r\n    19 | abc\r\n    20 |\r\n    21 |\r\n    22 | abc\r\n    23 | ab\r\n(23 rows)\r\n\r\ndrill:\r\n\r\n0: jdbc:drill:schema=dfs> select c_row, c_groupby from data order by c_row;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"8dcb47ec-9c27-4a04-8b48-61e2aad2ad19\"\r\nendpoint {\r\n  address: \"qa-node120.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < ClassCastException:[ org.apache.drill.exec.vector.NullableIntVector cannot be cast to org.apache.drill.exec.vector.NullableVarBinaryVector ]\"\r\n]\r\nError: exception while executing query (state=,code=0)"
    ],
    [
        "DRILL-199",
        "DRILL-1406",
        "Implement Floor and Ceil drill functions to MathFunctions ",
        "Regression: In the web UI the query profile shows Parquet Group Scan incorrectly reporting 0 rows This seems like a regression.... the web UI  shows the Parquet Group Scan reporting 0 rows processed even though it clearly has processed several rows.   The downstream operators do report valid number of rows.  \r\n\r\nHere's an example query and a partial profile output: \r\n\r\n{code:sql}\r\nselect o_custkey from (select o_custkey from orders group by o_custkey) where o_custkey < 10 order by o_custkey;\r\n\r\nFragment #0 - Operator 5 (HASH_AGGREGATE)\r\nFragment\tSetup\tProcess\tWait\tMax Batches\tMax Records\r\n#0 - 0\t0.083\t0.648\t0.000\t46\t1500000\r\nFragment #0 - Operator 6 (PARQUET_ROW_GROUP_SCAN)\r\nFragment\tSetup\tProcess\tWait\tMax Batches\tMax Records\r\n#0 - 0\t0.000\t0.026\t0.000\t0\t0\r\n{code}\r\n\r\n"
    ],
    [
        "DRILL-1008",
        "DRILL-2930",
        "Windows startup script need extlib in classpath Windows bat file is missing extlib in classpath. There is a recent change in zookeeper dependency and this change is not reflected for windows startup script. Please refer to drill-config.sh to change details.",
        "Need detail of Drillbit version in output of sys.drillbits On a cluster setup, where there are several nodes and each node has a Drillbit, it will help if we can provide the detail of Drillbit version as one of the columns in the output of sys.drillbits. That will help users to verify that they are running the same version of the Drillbit on each of the nodes in the cluster.\r\n\r\nToday, we need to manually query sys.version on each of the nodes to know the version of the Drillbit running on that node."
    ],
    [
        "DRILL-1160",
        "DRILL-1138",
        "Need changes to Drill classpath to run Drill on CDH For Drill to work successfully on CDH, we need to make changes to the classpath. Otherwise while making changes to the storage plugin to point to hdfs on the UI, it simply hangs.\r\n\r\nThe required jars to be present on the Drill classpath are:\r\n/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop/hadoop-annotations-2.0.0-cdh4.7.0.jar\r\n/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop/hadoop-auth-2.0.0-cdh4.7.0.jar\r\n/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop/hadoop-common-2.0.0-cdh4.7.0.jar\r\n/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop-hdfs/hadoop-hdfs-2.0.0-cdh4.7.0.jar\r\n\r\nHowever inside these 2 folders (hadoop & hadoop-hdfs) there are multiple duplicate soft links to these jars, which i believe is causing some kind of classpath ordering issue.\r\n\r\nThese 2 commands should help us get the required jars from the folders:\r\nls -all /opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop/*.jar | grep -v '^l' | grep -v 'test' | grep -Eo '([^ ]|\\\\ )*$'\r\n\r\nls -all /opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop-hdfs/*.jar | grep -v '^l' | grep -v 'test' | grep -Eo '([^ ]|\\\\ )*$'\r\n\r\nAlso the location of these jars can change depending on how/where they are installed",
        "Explicit casting to boolean fails 0: jdbc:drill:schema=dfs.TpcHMulti> select cast(C_CUSTKEY as boolean) as booleantest from customer limit 1;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"c20bd69f-bb5c-443e-a01b-8571b4c86490\"\r\nendpoint {\r\n  address: \"drillats1.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < IllegalArgumentException:[ No enum constant org.apache.drill.common.types.TypeProtos.MinorType.BOOLEAN ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n\r\n"
    ],
    [
        "DRILL-3950",
        "DRILL-3287",
        "CAST(...) * (Interval Constant) gives Internal Exception For example,\r\n{code}\r\nselect cast(empno as Integer) * (INTERVAL '1' DAY)\r\nfrom emp\r\n{code}\r\nresults into\r\n{code}\r\njava.lang.AssertionError: Internal error: invalid literal: INTERVAL '1' DAY\r\n{code}\r\nThe reason is that INTERVAL constant is not extracted properly in the cases where this constant times a CAST() function",
        "Changing session level parameter back to the default value does not change it's status back to DEFAULT Initial state:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from sys.options where status like '%CHANGED%';\r\n+-----------------------------------+----------+---------+----------+----------+-------------+-----------+------------+\r\n|               name                |   kind   |  type   |  status  | num_val  | string_val  | bool_val  | float_val  |\r\n+-----------------------------------+----------+---------+----------+----------+-------------+-----------+------------+\r\n| planner.enable_decimal_data_type  | BOOLEAN  | SYSTEM  | CHANGED  | null     | null        | true      | null       |\r\n+-----------------------------------+----------+---------+----------+----------+-------------+-----------+------------+\r\n1 row selected (0.247 seconds)\r\n{code}\r\n\r\nI changed session parameter:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> alter session set `planner.enable_hashjoin` = false;\r\n+-------+-----------------------------------+\r\n|  ok   |              summary              |\r\n+-------+-----------------------------------+\r\n| true  | planner.enable_hashjoin updated.  |\r\n+-------+-----------------------------------+\r\n1 row selected (0.1 seconds)\r\n{code}\r\n\r\nSo far, so good: it appears on changed options list: \r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from sys.options where status like '%CHANGED%';\r\n+-----------------------------------+----------+----------+----------+----------+-------------+-----------+------------+\r\n|               name                |   kind   |   type   |  status  | num_val  | string_val  | bool_val  | float_val  |\r\n+-----------------------------------+----------+----------+----------+----------+-------------+-----------+------------+\r\n| planner.enable_decimal_data_type  | BOOLEAN  | SYSTEM   | CHANGED  | null     | null        | true      | null       |\r\n| planner.enable_hashjoin           | BOOLEAN  | SESSION  | CHANGED  | null     | null        | false     | null       |\r\n+-----------------------------------+----------+----------+----------+----------+-------------+-----------+------------+\r\n2 rows selected (0.133 seconds)\r\n{code}\r\n\r\nI changed session parameter back to it's default value:\r\n{code}\r\n0: jdbc:drill:schema=dfs> alter session set `planner.enable_hashjoin` = true;\r\n+-------+-----------------------------------+\r\n|  ok   |              summary              |\r\n+-------+-----------------------------------+\r\n| true  | planner.enable_hashjoin updated.  |\r\n+-------+-----------------------------------+\r\n1 row selected (0.096 seconds)\r\n{code}\r\n\r\n{color:red} It still appears on changed list, even though it has default value:{color}\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from sys.options where status like '%CHANGED%';\r\n+-----------------------------------+----------+----------+----------+----------+-------------+-----------+------------+\r\n|               name                |   kind   |   type   |  status  | num_val  | string_val  | bool_val  | float_val  |\r\n+-----------------------------------+----------+----------+----------+----------+-------------+-----------+------------+\r\n| planner.enable_decimal_data_type  | BOOLEAN  | SYSTEM   | CHANGED  | null     | null        | true      | null       |\r\n| planner.enable_hashjoin           | BOOLEAN  | SESSION  | CHANGED  | null     | null        | true      | null       |\r\n+-----------------------------------+----------+----------+----------+----------+-------------+-----------+------------+\r\n2 rows selected (0.124 seconds)\r\n{code}"
    ],
    [
        "DRILL-2893",
        "DRILL-1397",
        "ScanBatch throws a NullPointerException instead of returning OUT_OF_MEMORY - set _drill.exec.memory.top.max_ in _drill-override.conf_ to some low value (I used _75000000_)\r\n- disable hash aggregate (set _plannder.enable_hashagg_ to false)\r\n- disable exchanges (set _plannder.disable_exchanges_ to true)\r\n- run the following query\r\n{noformat}\r\nselect count(*) from (select * from dfs.data.`tpch1/lineitem.parquet` order by l_orderkey);\r\n{noformat}\r\nand you should get the following error message:\r\n{noformat}\r\nQuery failed: SYSTEM ERROR: null\r\n\r\nFragment 0:0\r\n\r\n[e05ff3c2-e130-449e-b721-b3442796e29b on 172.30.1.1:31010]\r\n{noformat}\r\n\r\nWe have 2 problems here:\r\n\r\n1st: \r\n- ScanBatch detects that it can't allocate it's field value vectors and right before returning _OUT_OF_MEMORY_ downstream it calls _clear() on the field vectors\r\n- one of those vectors actually threw a NullPointerException in it's _allocateNew()_ methods after it cleared it's buffer and couldn't allocate a new one\r\n- when ScanBatch tries to clean that vector, it will throw a NullPointerException which will prevent the ScanBatch from returning _OUT_OF_MEMORY_ and will cancel the query instead\r\n\r\n2nd problem:\r\n- once the query has been canceled, _ScanBatch.cleanup()_ will throw another _NullPointerException_ when cleaning the field vectors, which will prevent the cleanup of the remaining resources and will cause a memory leak",
        "Query with IN clause and correlation fails (for Text files) The following query fails. This could be related to https://issues.apache.org/jira/browse/DRILL-1396, but filing separate issue as the error is different.\r\n\r\n0: jdbc:drill:> select t.trans_info.purch_flag,\r\n. . . . . . . >           t.user_info.cust_id, t.trans_info.prod_id[0]\r\n. . . . . . . > from `Clickstream.clicks`.`/json/clicks.json` t \r\n. . . . . . . > where  t.user_info.cust_id IN (select o.cust_id from hive.orders o where o.order_total >100 );\r\n\r\nQuery failed: Failure while running fragment. Incoming batch has an empty schema. This is not allowed. [2b441a79-be49-4116-a459-513f97418738]\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n\r\nBelow is the explain plan.\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(EXPR$0=[$0], EXPR$1=[$1], EXPR$2=[$2])\r\n00-02        Project(EXPR$0=[ITEM($1, 'purch_flag')], EXPR$1=[ITEM($0, 'cust_id')], EXPR$2=[ITEM(ITEM($1, 'prod_id'), 0)])\r\n00-03          HashJoin(condition=[=($2, $3)], joinType=[inner])\r\n00-05            Project(T27\u00a6\u00a6user_info=[$1], T27\u00a6\u00a6trans_info=[$2], $f3=[ITEM($1, 'cust_id')])\r\n00-07              Project(T27\u00a6\u00a6*=[$0], T27\u00a6\u00a6user_info=[$1], T27\u00a6\u00a6trans_info=[$2])\r\n00-09                Scan(groupscan=[EasyGroupScan [selectionRoot=/mapr/my.cluster.com/demo/clicks/json/clicks.json, columns = null]])\r\n00-04            HashAgg(group=[{0}])\r\n00-06              Project(cust_id=[$0])\r\n00-08                SelectionVectorRemover\r\n00-10                  Filter(condition=[>($1, 100)])\r\n00-11                    Project(cust_id=[$1], order_total=[$0])\r\n00-12                      Scan(groupscan=[HiveScan [table=Table(tableName:orders, dbName:default, owner:root, createTime:1409956843, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:order_id, type:bigint, comment:null), FieldSchema(name:month, type:string, comment:null), FieldSchema(name:purchdate, type:timestamp, comment:null), FieldSchema(name:cust_id, type:bigint, comment:null), FieldSchema(name:state, type:string, comment:null), FieldSchema(name:prod_id, type:bigint, comment:null), FieldSchema(name:order_total, type:int, comment:null)], location:maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE, transient_lastDdlTime=1409956843}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE), inputSplits=[maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month2.agg.orders.csv:0+640155, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month5.agg.orders.csv:0+775506, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month6.agg.orders.csv:0+791685, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month8.agg.orders.csv:0+805072, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month4.agg.orders.csv:0+603886, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month9.agg.orders.csv:0+846270, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month1.agg.orders.csv:0+461090, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month7.agg.orders.csv:0+771399, maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders/month3.agg.orders.csv:0+806738], columns=[SchemaPath [`cust_id`], SchemaPath [`order_total`]]]])\r\n | {\r\n  \"head\" : {\r\n    \"version\" : 1,\r\n    \"generator\" : {\r\n      \"type\" : \"ExplainHandler\",\r\n      \"info\" : \"\"\r\n    },\r\n    \"type\" : \"APACHE_DRILL_PHYSICAL\",\r\n    \"options\" : [ ],\r\n    \"queue\" : 0,\r\n    \"resultMode\" : \"EXEC\"\r\n  },\r\n  \"graph\" : [ {\r\n    \"pop\" : \"hive-scan\",\r\n    \"@id\" : 12,\r\n    \"hive-table\" : {\r\n      \"table\" : {\r\n        \"tableName\" : \"orders\",\r\n        \"dbName\" : \"default\",\r\n        \"owner\" : \"root\",\r\n        \"createTime\" : 1409956843,\r\n        \"lastAccessTime\" : 0,\r\n        \"retention\" : 0,\r\n        \"sd\" : {\r\n          \"cols\" : [ {\r\n            \"name\" : \"order_id\",\r\n            \"type\" : \"bigint\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"month\",\r\n            \"type\" : \"string\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"purchdate\",\r\n            \"type\" : \"timestamp\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"cust_id\",\r\n            \"type\" : \"bigint\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"state\",\r\n            \"type\" : \"string\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"prod_id\",\r\n            \"type\" : \"bigint\",\r\n            \"comment\" : null\r\n          }, {\r\n            \"name\" : \"order_total\",\r\n            \"type\" : \"int\",\r\n            \"comment\" : null\r\n          } ],\r\n          \"location\" : \"maprfs:/mapr/my.cluster.com/drill-beta-demo/data/orders\",\r\n          \"inputFormat\" : \"org.apache.hadoop.mapred.TextInputFormat\",\r\n          \"outputFormat\" : \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\r\n          \"compressed\" : false,\r\n          \"numBuckets\" : -1,\r\n          \"serDeInfo\" : {\r\n            \"name\" : null,\r\n            \"serializationLib\" : \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\r\n            \"parameters\" : {\r\n              \"serialization.format\" : \",\",\r\n              \"field.delim\" : \",\"\r\n            }\r\n          },\r\n          \"sortCols\" : [ ],\r\n          \"parameters\" : { }\r\n        },\r\n        \"partitionKeys\" : [ ],\r\n        \"parameters\" : {\r\n          \"EXTERNAL\" : \"TRUE\",\r\n          \"transient_lastDdlTime\" : \"1409956843\"\r\n        },\r\n        \"viewOriginalText\" : null,\r\n        \"viewExpandedText\" : null,\r\n        \"tableType\" : \"EXTERNAL_TABLE\"\r\n      },\r\n      \"partitions\" : null,\r\n      \"hiveConfigOverride\" : {\r\n        \"hive.metastore.uris\" : \"thrift://192.168.208.143:9083\",\r\n        \"hive.metastore.sasl.enabled\" : \"false\"\r\n      }\r\n    },\r\n    \"storage-plugin\" : \"hive\",\r\n    \"columns\" : [ \"`cust_id`\", \"`order_total`\" ],\r\n    \"cost\" : 6349.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 11,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`cust_id`\",\r\n      \"expr\" : \"`cust_id`\"\r\n    }, {\r\n      \"ref\" : \"`order_total`\",\r\n      \"expr\" : \"`order_total`\"\r\n    } ],\r\n    \"child\" : 12,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 6349.0\r\n  }, {\r\n    \"pop\" : \"filter\",\r\n    \"@id\" : 10,\r\n    \"child\" : 11,\r\n    \"expr\" : \"greater_than(`order_total`, 100) \",\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 3174.5\r\n  }, {\r\n    \"pop\" : \"selection-vector-remover\",\r\n    \"@id\" : 8,\r\n    \"child\" : 10,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 3174.5\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 6,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`cust_id`\",\r\n      \"expr\" : \"`cust_id`\"\r\n    } ],\r\n    \"child\" : 8,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 3174.5\r\n  }, {\r\n    \"pop\" : \"hash-aggregate\",\r\n    \"@id\" : 4,\r\n    \"child\" : 6,\r\n    \"cardinality\" : 1.0,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 1587.25,\r\n    \"groupByExprs\" : [ {\r\n      \"ref\" : \"`cust_id`\",\r\n      \"expr\" : \"`cust_id`\"\r\n    } ],\r\n    \"aggrExprs\" : [ ]\r\n  }, {\r\n    \"pop\" : \"fs-scan\",\r\n    \"@id\" : 9,\r\n    \"files\" : [ \"maprfs:/mapr/my.cluster.com/demo/clicks/json/clicks.json\" ],\r\n    \"storage\" : {\r\n      \"type\" : \"file\",\r\n      \"enabled\" : true,\r\n      \"connection\" : \"maprfs:///\",\r\n      \"workspaces\" : {\r\n        \"root\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo\",\r\n          \"writable\" : false,\r\n          \"storageformat\" : null\r\n        },\r\n        \"clicks\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo/clicks\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        },\r\n        \"views\" : {\r\n          \"location\" : \"/mapr/my.cluster.com/demo/views\",\r\n          \"writable\" : true,\r\n          \"storageformat\" : \"parquet\"\r\n        }\r\n      },\r\n      \"formats\" : {\r\n        \"psv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tbl\" ],\r\n          \"delimiter\" : \"|\"\r\n        },\r\n        \"csv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"csv\" ],\r\n          \"delimiter\" : \",\"\r\n        },\r\n        \"tsv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tsv\" ],\r\n          \"delimiter\" : \"\\t\"\r\n        },\r\n        \"parquet\" : {\r\n          \"type\" : \"parquet\"\r\n        },\r\n        \"json\" : {\r\n          \"type\" : \"json\"\r\n        }\r\n      }\r\n    },\r\n    \"format\" : {\r\n      \"type\" : \"json\"\r\n    },\r\n    \"selectionRoot\" : \"/mapr/my.cluster.com/demo/clicks/json/clicks.json\",\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 7,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`T27\u00a6\u00a6*`\",\r\n      \"expr\" : \"`*`\"\r\n    } ],\r\n    \"child\" : 9,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 5,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`T27\u00a6\u00a6user_info`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6user_info`\"\r\n    }, {\r\n      \"ref\" : \"`T27\u00a6\u00a6trans_info`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6trans_info`\"\r\n    }, {\r\n      \"ref\" : \"`$f3`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6user_info`.`cust_id`\"\r\n    } ],\r\n    \"child\" : 7,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"hash-join\",\r\n    \"@id\" : 3,\r\n    \"left\" : 5,\r\n    \"right\" : 4,\r\n    \"conditions\" : [ {\r\n      \"relationship\" : \"==\",\r\n      \"left\" : \"`$f3`\",\r\n      \"right\" : \"`cust_id`\"\r\n    } ],\r\n    \"joinType\" : \"INNER\",\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 2,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`EXPR$0`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6trans_info`.`purch_flag`\"\r\n    }, {\r\n      \"ref\" : \"`EXPR$1`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6user_info`.`cust_id`\"\r\n    }, {\r\n      \"ref\" : \"`EXPR$2`\",\r\n      \"expr\" : \"`T27\u00a6\u00a6trans_info`.`prod_id`[0]\"\r\n    } ],\r\n    \"child\" : 3,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 1,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`EXPR$0`\",\r\n      \"expr\" : \"`EXPR$0`\"\r\n    }, {\r\n      \"ref\" : \"`EXPR$1`\",\r\n      \"expr\" : \"`EXPR$1`\"\r\n    }, {\r\n      \"ref\" : \"`EXPR$2`\",\r\n      \"expr\" : \"`EXPR$2`\"\r\n    } ],\r\n    \"child\" : 2,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  }, {\r\n    \"pop\" : \"screen\",\r\n    \"@id\" : 0,\r\n    \"child\" : 1,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5097.0\r\n  } ]\r\n} |\r\n+------------+------------+\r\n\r\n"
    ],
    [
        "DRILL-1412",
        "DRILL-1662",
        "Update Unit tests to do validation using a common interface ",
        "drillbit.sh stop should timeout We need a timeout as part of the drillbit.sh stop\r\n\r\nCan we have a configurable parameter with a default of 30 seconds and after that the timeout should kill the drillbit.sh"
    ],
    [
        "DRILL-3788",
        "DRILL-2412",
        "Directory based partition pruning not taking effect with metadata caching git.commit.id.abbrev=240a455\r\n\r\nPartition Pruning did not take place for the below query after I executed the \"refresh table metadata command\"\r\n{code}\r\n explain plan for \r\nselect\r\n  l_returnflag,\r\n  l_linestatus\r\nfrom\r\n  `lineitem/2006/1`\r\nwhere\r\n  dir0=1 or dir0=2\r\n{code}\r\n\r\nThe logs did not indicate that \"pruning did not take place\"\r\n\r\nBefore executing the refresh table metadata command, partition pruning did take effect\r\n\r\nI am not attaching the data set as it is larger than 10MB. Reach out to me if you need more information\r\n",
        "CTAS has issues when the underlying query casts a column to time datatype git.commit.id.abbrev=e92db23\r\n\r\n{code}\r\ncreate table time_parquet as select cast(columns[0] as time) time_col from `time.tbl`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 21                        |\r\n+------------+---------------------------+\r\n1 row selected (0.201 seconds)\r\n{code}\r\n\r\nNow running a count(*) on the newly created table does not have any issues\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirViews> select count(*) from time_parquet;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 21         |\r\n+------------+\r\n1 row selected (0.081 seconds)\r\n{code}\r\n\r\nHowever the below 2 queries fail\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirViews> select * from time_parquet;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.NullableTimeVector cannot be cast to org.apache.drill.exec.vector.NullableIntVector [ cdceaf64-b858-4063-8364-d119703cf6f0 on qa-node191.qa.lab:31010 ]\r\n[ cdceaf64-b858-4063-8364-d119703cf6f0 on qa-node191.qa.lab:31010 ]\r\n\r\n0: jdbc:drill:schema=dfs.drillTestDirViews> select time_col from time_parquet;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.NullableTimeVector cannot be cast to org.apache.drill.exec.vector.NullableIntVector [ 8c4254bb-7869-468d-bc6c-7151b833593f on qa-node191.qa.lab:31010 ]\r\n[ 8c4254bb-7869-468d-bc6c-7151b833593f on qa-node191.qa.lab:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n"
    ],
    [
        "DRILL-4125",
        "DRILL-749",
        "Illegal argument exception during merge join  Same setup as in DRILL-4109\r\n\r\nQuery: framework/resources/Advanced/tpcds/tpcds_sf100/original/query93.sql\r\n\r\nExcerpt from drillbit.log\r\n{code}\r\n2015-11-23 23:50:44,071 [29ac59f2-5d92-7378-bf81-e844a300efd7:frag:5:74] ERROR o.a.d.e.w.fragment.FragmentExecutor - SYSTEM ERROR: IllegalArgumentException\r\n\r\nFragment 5:74\r\n\r\n[Error Id: 1ca9758d-1864-4940-9efa-b8906d4f9b52 on atsqa4-133.qa.lab:31010]\r\norg.apache.drill.common.exceptions.UserException: SYSTEM ERROR: IllegalArgumentException\r\n\r\nFragment 5:74\r\n\r\n[Error Id: 1ca9758d-1864-4940-9efa-b8906d4f9b52 on atsqa4-133.qa.lab:31010]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:534) ~[drill-common-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.sendFinalState(FragmentExecutor.java:321) [drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.cleanup(FragmentExecutor.java:184) [drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:290) [drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_71]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]\r\nCaused by: java.lang.IllegalArgumentException: null\r\n        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:76) ~[guava-14.0.1.jar:na]\r\n        at org.apache.drill.exec.record.RecordIterator.getCurrentPosition(RecordIterator.java:242) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.test.generated.JoinWorkerGen8348.doJoin(JoinTemplate.java:63) ~[na:na]\r\n        at org.apache.drill.exec.physical.impl.join.MergeJoinBatch.innerNext(MergeJoinBatch.java:206) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:162) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:119) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:109) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:132) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:162) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:119) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:109) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.innerNext(ExternalSortBatch.java:276) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:162) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:119) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:109) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:94) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:162) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:119) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.RecordIterator.nextBatch(RecordIterator.java:89) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.RecordIterator.next(RecordIterator.java:164) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.RecordIterator.prepare(RecordIterator.java:148) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.join.JoinStatus.prepare(JoinStatus.java:85) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.join.MergeJoinBatch.innerNext(MergeJoinBatch.java:161) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:162) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:119) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:109) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:132) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:162) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:119) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.test.generated.HashAggregatorGen55.doWork(HashAggTemplate.java:314) ~[na:na]\r\n        at org.apache.drill.exec.physical.impl.aggregate.HashAggBatch.innerNext(HashAggBatch.java:133) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:162) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:119) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:109) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:132) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:162) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:104) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.SingleSenderCreator$SingleSenderRootExec.innerNext(SingleSenderCreator.java:93) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:94) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor$1.run(FragmentExecutor.java:256) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor$1.run(FragmentExecutor.java:250) ~[drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        at java.security.AccessController.doPrivileged(Native Method) ~[na:1.7.0_71]\r\n        at javax.security.auth.Subject.doAs(Subject.java:415) ~[na:1.7.0_71]\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1595) ~[hadoop-common-2.7.0-mapr-1506.jar:na]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:250) [drill-java-exec-1.4.0-SNAPSHOT.jar:1.4.0-SNAPSHOT]\r\n        ... 4 common frames omitted\r\n{code}",
        "Zero-fill the Offset Vectors buffer contents The offset vectors of a Repeated, Nullable and Variable length vectors should be zero filled on allocation of the container vector.\r\n\r\nSince such a buffer can be allocated from a recycled pool, the values may be filled with the previous content."
    ],
    [
        "DRILL-3723",
        "DRILL-2967",
        "RemoteServiceSet.getServiceSetWithFullCache() ignores arguments RemoteServiceSet.getServiceSetWithFullCache() ignores both of its arguments and is therefore functionally equivalent to getLocalServiceSet().\r\n",
        "Incompatible types error reported in a \"not in\" query with compatible data types  Two tables, parquet files (attached in the bug):\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from t1;\r\n+------------+------------+------------+\r\n|     a1     |     b1     |     c1     |\r\n+------------+------------+------------+\r\n| 1          | aaaaa      | 2015-01-01 |\r\n| 2          | bbbbb      | 2015-01-02 |\r\n| 3          | ccccc      | 2015-01-03 |\r\n| 4          | null       | 2015-01-04 |\r\n| 5          | eeeee      | 2015-01-05 |\r\n| 6          | fffff      | 2015-01-06 |\r\n| 7          | ggggg      | 2015-01-07 |\r\n| null       | hhhhh      | 2015-01-08 |\r\n| 9          | iiiii      | null       |\r\n| 10         | jjjjj      | 2015-01-10 |\r\n+------------+------------+------------+\r\n10 rows selected (0.119 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select * from t2;\r\n+------------+------------+------------+\r\n|     a2     |     b2     |     c2     |\r\n+------------+------------+------------+\r\n| 0          | zzz        | 2014-12-31 |\r\n| 1          | aaaaa      | 2015-01-01 |\r\n| 2          | bbbbb      | 2015-01-02 |\r\n| 2          | bbbbb      | 2015-01-02 |\r\n| 2          | bbbbb      | 2015-01-02 |\r\n| 3          | ccccc      | 2015-01-03 |\r\n| 4          | ddddd      | 2015-01-04 |\r\n| 5          | eeeee      | 2015-01-05 |\r\n| 6          | fffff      | 2015-01-06 |\r\n| 7          | ggggg      | 2015-01-07 |\r\n| 7          | ggggg      | 2015-01-07 |\r\n| 8          | hhhhh      | 2015-01-08 |\r\n| 9          | iiiii      | 2015-01-09 |\r\n+------------+------------+------------+\r\n13 rows selected (0.116 seconds)\r\n{code}\r\n\r\nDisable hash join and set slice_target = 1:\r\n\r\nalter session set `planner.enable_hashjoin` = false;\r\nalter session set `planner.slice_target` = 1;\r\n\r\nCorrect result:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from t1 where b1 not in (select b2 from t2);\r\n+------------+------------+------------+\r\n|     a1     |     b1     |     c1     |\r\n+------------+------------+------------+\r\n| 10         | jjjjj      | 2015-01-10 |\r\n+------------+------------+------------+\r\n1 row selected (0.625 seconds)\r\n{code}\r\n\r\n\r\nSwap tables and you get an error:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from t2 where b2 not in (select b1 from t1);\r\n+------------+------------+------------+\r\n|     a1     |     b1     |     c1     |\r\n+------------+------------+------------+\r\nQuery failed: SYSTEM ERROR: Join only supports implicit casts between 1. Numeric data\r\n 2. Varchar, Varbinary data Left type: INT, Right type: VARCHAR. Add explicit casts to avoid this error\r\n\r\nFragment 1:0\r\n\r\n[1a83aa50-39aa-452c-91dd-970bf4a8f03d on atsqa4-133.qa.lab:31010]\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n        at sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n        at sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n        at sqlline.SqlLine.print(SqlLine.java:1809)\r\n        at sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n        at sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n        at sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n        at sqlline.SqlLine.begin(SqlLine.java:763)\r\n        at sqlline.SqlLine.start(SqlLine.java:498)\r\n        at sqlline.SqlLine.main(SqlLine.java:460)\r\n{code}\r\n\r\nExplain plan for the query with an error:\r\n{code} \r\n0: jdbc:drill:schema=dfs> explain plan for select * from t2 where b2 not in (select b1 from t1); \r\n+------------+------------+ \r\n| text | json | \r\n+------------+------------+ \r\n| 00-00 Screen \r\n00-01 Project(*=[$0]) \r\n00-02 UnionExchange \r\n01-01 Project(T27\u00a6\u00a6*=[$0]) \r\n01-02 SelectionVectorRemover \r\n01-03 Filter(condition=[NOT(CASE(=($2, 0), false, IS NOT NULL($6), true, IS NULL($4), null, <($3, $2), null, false))]) \r\n01-04 MergeJoin(condition=[=($4, $5)], joinType=[left]) \r\n01-06 SelectionVectorRemover \r\n01-08 Sort(sort0=[$4], dir0=[ASC]) \r\n01-10 Project(T27\u00a6\u00a6*=[$0], b2=[$1], $f0=[$2], $f1=[$3], b20=[$4]) \r\n01-12 HashToRandomExchange(dist0=[[$4]]) \r\n02-01 UnorderedMuxExchange \r\n04-01 Project(T27\u00a6\u00a6*=[$0], b2=[$1], $f0=[$2], $f1=[$3], b20=[$4], E_X_P_R_H_A_S_H_F_I_E_L_D=[castInt(hash64AsDouble($4))]) \r\n04-02 Project(T27\u00a6\u00a6*=[$0], b2=[$1], $f0=[$2], $f1=[$3], b20=[$1]) \r\n04-03 NestedLoopJoin(condition=[true], joinType=[inner]) \r\n04-05 Project(T27\u00a6\u00a6*=[$0], b2=[$1]) \r\n04-06 Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/aggregation/t2]], selectionRoot=/drill/testdata/aggregation/t2, numFiles=1, columns=[`*`]]]) \r\n04-04 BroadcastExchange \r\n06-01 StreamAgg(group=[{}], agg#0=[$SUM0($0)], agg#1=[$SUM0($1)]) \r\n06-02 UnionExchange \r\n07-01 StreamAgg(group=[{}], agg#0=[COUNT()], agg#1=[COUNT($0)]) \r\n07-02 Project(b1=[$0], $f1=[true]) \r\n07-03 Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/aggregation/t1]], selectionRoot=/drill/testdata/aggregation/t1, numFiles=1, columns=[`b1`]]]) \r\n01-05 Project(b1=[$0], $f10=[$1]) \r\n01-07 SelectionVectorRemover \r\n01-09 Sort(sort0=[$0], dir0=[ASC]) \r\n01-11 HashAgg(group=[{0}], agg#0=[MIN($1)]) \r\n01-13 Project(b1=[$0], $f1=[$1]) \r\n01-14 HashToRandomExchange(dist0=[[$0]]) \r\n03-01 UnorderedMuxExchange \r\n05-01 Project(b1=[$0], $f1=[$1], E_X_P_R_H_A_S_H_F_I_E_L_D=[castInt(hash64AsDouble($0))]) \r\n05-02 HashAgg(group=[{0}], agg#0=[MIN($1)]) \r\n05-03 Project(b1=[$0], $f1=[true]) \r\n05-04 Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/aggregation/t1]], selectionRoot=/drill/testdata/aggregation/t1, numFiles=1, columns=[`b1`]]]) \r\n{code} \r\n\r\nCorrect result and correct plan with hash join distributed plan ( planner.slice_target = 1)\r\n\r\nalter session set `planner.enable_hashjoin` = true;\r\nalter session set `planner.slice_target` = 1;\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from t2 where b2 not in (select b1 from t1);\r\n+------------+------------+------------+\r\n|     a2     |     b2     |     c2     |\r\n+------------+------------+------------+\r\n+------------+------------+------------+\r\nNo rows selected (0.458 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> explain plan for select * from t2 where b2 not in (select b1 from t1);\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(*=[$0])\r\n00-02        Project(T25\u00a6\u00a6*=[$0])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[NOT(CASE(=($2, 0), false, IS NOT NULL($6), true, IS NULL($4), null, <($3, $2), null, false))])\r\n00-05              HashJoin(condition=[=($4, $5)], joinType=[left])\r\n00-07                Project(T25\u00a6\u00a6*=[$0], b2=[$1], $f0=[$2], $f1=[$3], b20=[$1])\r\n00-09                  NestedLoopJoin(condition=[true], joinType=[inner])\r\n00-11                    Project(T25\u00a6\u00a6*=[$0], b2=[$1])\r\n00-12                      Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/aggregation/t2]], selectionRoot=/drill/testdata/aggregation/t2, numFiles=1, columns=[`*`]]])\r\n00-10                    BroadcastExchange\r\n01-01                      StreamAgg(group=[{}], agg#0=[$SUM0($0)], agg#1=[$SUM0($1)])\r\n01-02                        UnionExchange\r\n03-01                          StreamAgg(group=[{}], agg#0=[COUNT()], agg#1=[COUNT($0)])\r\n03-02                            Project(b1=[$0], $f1=[true])\r\n03-03                              Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/aggregation/t1]], selectionRoot=/drill/testdata/aggregation/t1, numFiles=1, columns=[`b1`]]])\r\n00-06                Project(b1=[$0], $f10=[$1])\r\n00-08                  BroadcastExchange\r\n02-01                    HashAgg(group=[{0}], agg#0=[MIN($1)])\r\n02-02                      Project(b1=[$0], $f1=[$1])\r\n02-03                        HashToRandomExchange(dist0=[[$0]])\r\n04-01                          UnorderedMuxExchange\r\n05-01                            Project(b1=[$0], $f1=[$1], E_X_P_R_H_A_S_H_F_I_E_L_D=[castInt(hash64AsDouble($0))])\r\n05-02                              HashAgg(group=[{0}], agg#0=[MIN($1)])\r\n05-03                                Project(b1=[$0], $f1=[true])\r\n05-04                                  Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/aggregation/t1]], selectionRoot=/drill/testdata/aggregation/t1, numFiles=1, columns=[`b1`]]])\r\n{code}\r\n\r\nSame error with the columns of date, time and timestamp types.\r\n"
    ],
    [
        "DRILL-2353",
        "DRILL-3931",
        "Support interpreter based partition pruning There's been recent work done by [~jnadeau] to support interpreter based partition pruning to handle more general filters that are not currently processed by existing partition pruning.  Also, there's been recent work done by [~jaltekruse] to move the interpreter functionality into the exec module as part of DRILL-2060.  \r\n\r\nThese two pieces need to be integrated and tested.  This is a JIRA created to do the integration work and address any issues that arise out of it. ",
        "Upgrade fileclient dependency in mapr profile  Current dependency version is 4.1.0-mapr. There is a critical fix that went into 4.1.0.34989-mapr. Upgrade the dependency version to 4.1.0.34989-mapr. Only pom file changes."
    ],
    [
        "DRILL-1131",
        "DRILL-4223",
        "Drill should ignore files in starting with . _ Files containing . and _ as the first characters are ignored by hive and others are these are typically logs and status files written out by tools like mapreduce. Drill should not read them when querying a directory containing a list of parquet files.\r\n\r\nCurrently it fails with the error:\r\nmessage: \"Failure while setting up Foreman. < AssertionError:[ Internal error: Error while applying rule DrillPushProjIntoScan, args [rel#78:ProjectRel.NONE.ANY([]).[](child=rel#15:Subset#1.ENUMERABLE.ANY([]).[],p_partkey=$1,p_type=$2), rel#8:EnumerableTableAccessRel.ENUMERABLE.ANY([]).[](table=[dfs, drillTestDirDencTpchSF100, part])] ] < DrillRuntimeException:[ java.io.IOException: Could not read footer: java.io.IOException: Could not read footer for file com.mapr.fs.MapRFileStatus@99c9d45e ] < IOException:[ Could not read footer: java.io.IOException: Could not read footer for file com.mapr.fs.MapRFileStatus@99c9d45e ] < IOException:[ Could not read footer for file com.mapr.fs.MapRFileStatus@99c9d45e ] < IOException:[ Open failed for file: /drill/testdata/dencSF100/part/.impala_insert_staging, error: Invalid argument (22) ]\"\r\n",
        "PIVOT and UNPIVOT to rotate table valued expressions Capability to PIVOT and UNPIVOT table values expressions which are results of a SELECT query"
    ],
    [
        "DRILL-1055",
        "DRILL-3709",
        "Implement ProducerConsumer operator Currently, a minor fragment is run in a single thread, and parallelization is achieved by running multiple minor fragments. There are cases where we are limited in how much we can parallelize the leaf fragments due to the limited number of files/row groups. This can result in under-utilization of resources.\r\n\r\nThis operator will allow the scan operator to run in parallel with the downstream operators in the leaf fragments, thus increasing cluster utilization, and reducing the duration of the leaf fragments in certain cases.",
        "Memory Storage Plugin Create an in-memory storage plugin for rapid prototyping, testing, etc."
    ],
    [
        "DRILL-35",
        "DRILL-188",
        "Implement RunningAggregate Reference Operator RunningAggregate (1)\r\nThe running aggregate operator takes an input record and adds appends a set of running aggregations and outputs the resulting record.  The aggregations are re-evaluated on each record within the incoming segment in the order they are provided.  Segment focus can be defined with the \u2018within\u2019 value such that aggregations are reset at each segment boundary.\r\n\r\n{ @id\u2020: <opref>, op: \u201crunningaggregate\u201d, \r\n\r\n  input\u2020: <input>, \r\n\r\n  within*: <name>, \r\n\r\n  aggregations: [\r\n\r\n    {ref: <name>, expr: <aggexpr> },...\r\n\r\n  ]\r\n}\r\n",
        "maven profiles to use hadoop artifacts from various vendors create profiles for building using artifacts from MapR, Hortonworks, and Cloudera, keeping Apache Hadoop as default. Will use the latest released version from each vendor."
    ],
    [
        "DRILL-566",
        "DRILL-2909",
        "Storage provider for Azure Cloud Storage An Azure cloud storage plugin; the idea being that a user of the storage plugin would be able to address Azure blob storage in a similar way to they might address a LocalFileSystem. \r\n\r\nThis is a model that is similar to that developed for Azure's HDInsight service and in EMR, where WASB:// or S3:// schemes allow access to cloud storage in Hadoop rather than using the hdfs:// scheme. I realise the implementation as a plugin differs, and indeed is cleaner in Drill to my mind (before the project commences!). \r\n\r\nPerformance and data locality trade-offs are significant in all of these cloud storage services, so concepts like spanning multiple accounts (to maximise IOPs) would be built into the plugin. ",
        "Use Calcite's JoinInfo to keep track of equality and inequality components of a join Instead of Drill's  JoinCategory class, we should leverage Calcite's JoinInfo to keep track of the components of a join condition.  The JoinInfo class splits the join condition into equijoin and non-equijoin which is pretty much the same as being done by JoinCategory.  "
    ],
    [
        "DRILL-2472",
        "DRILL-231",
        "JDBC : ResultSet.wasNull should give proper error message when a user tries to call it before calling getXXX method git.commit.id.abbrev=7b4c887\r\n\r\nDrill currently throws the below error when we call wasNull before calling getXXX method\r\n\r\n{code}\r\nException in thread \"main\" 17:15:07.935 [Client-1] DEBUG o.a.drill.exec.rpc.user.UserClient - Sending response with Sender 557575560\r\njava.lang.ArrayIndexOutOfBoundsException: 1\r\n\tat org.apache.drill.jdbc.DrillAccessorList.wasNull(DrillAccessorList.java:53)\r\n\tat org.apache.drill.jdbc.DrillCursor.wasNull(DrillCursor.java:138)\r\n\tat net.hydromatic.avatica.AvaticaResultSet.wasNull(AvaticaResultSet.java:201)\r\n{code}\r\n\r\nDrill should instead give the appropriate error message",
        "Update partition sender to support dropping partitioning field reference In the case that there is only one partitioning expression and that expression is a field reference, the partition sender should support dropping the field rather than proceeding downstream"
    ],
    [
        "DRILL-2238",
        "DRILL-3426",
        "Unsupported join on OR  yields rough CannotPlanException (vs. cleaner unsupported-feature message) The unsupported combination of joining using a condition with an OR yields an internal CannotPlanException (with a dump of the plan):\r\n\r\norg.apache.drill.exec.rpc.RpcException: RelOptPlanner.CannotPlanException: Node [rel#4217:Subset#3.LOGICAL.ANY([]).[]] could not be implemented; planner state:\r\nRoot: rel#4217:Subset#3.LOGICAL.ANY([]).[]\r\nOriginal rel:\r\n...\r\n\r\nEventually this should instead use some kind of unsupported-combination error.",
        "DROP Table "
    ],
    [
        "DRILL-4258",
        "DRILL-3267",
        "Create New SYS tables: cpu, queries, fragments, threads, threadtraces, connections cpu: Drillbit, # Cores, CPU consumption (with different windows?)\r\nqueries: Foreman, QueryId, User, SQL, Start Time, rows processed, query plan, # nodes involved, number of running fragments, memory consumed\r\nfragments: Drillbit, queryid, major fragmentid, minorfragmentid, coordinate, memory usage, rows processed, start time\r\nthreads: name, priority, state, id, thread-level cpu stats\r\nthreadtraces: threads, stack trace\r\nconnections: client, server, type, establishedDate, messagesSent, bytesSent",
        "Distinguish non-query statements in SQL processing and RPC protocol Drill needs to distinguish, in some way that the client side can detect, between SQL statements whose purpose is to change state (e.g., DROP VIEW and SET SCHEMA statements) and statements whose purpose is to return a result set (e.g., SELECT statements).\r\n\r\nThis distinction is needed to support normal behavior of JDBC's execute methods:  When called to execute a DDL-like statement, an execute method will not return until the action is completed.  (See DRILL-2560.)\r\n\r\n(For query statements, an execute method has to return before the query is fully complete (the result set has been read), because of course it has to return the ResultSet through which the results are read.)\r\n\r\nCurrently, Drill returns a result set for any SQL statement:  the real result set for a query statement, and a dummy/status result set (typically with columns \"ok\" and \"summary\") for a DDL-like statement.\r\n\r\nThis means that the JDBC layer cannot distinguish between statements for which execute methods should wait for completion before returning vs. statements for which execute methods can return to client code.  (Again, see DRILL-2560.)\r\n\r\n\r\nOne solution might be to not return any dummy/status result set for DDL-like statements.  If that meant that no QueryData messages were sent for such a statement, then the first message after the query-ID message would be the termination message.  \r\n\r\nIn that case, the JDBC layer could wait for the first post-query-ID message before returning from an execute method:\r\n\r\nIf the message is a termination message, then the statement was a DDL-like statement, the JDBC should have waited for completion, it _has_ already waited, and can return (returning no result set).\r\n\r\nIf the message is a QueryData message, then the statement was a query statement, so the method doesn't need to wait any further (and has waited enough to allow parsing or other up-front errors to be reported from the execute method rather than later from ResultSet.next()), and can return (returning a result set).\r\n\r\n\r\nAnother solution might be to have multiple forms (combinations of values) of the QueryData message.\r\n\r\n\r\nAnother thing to keep in mind is JDBC's support for statements that return multiple results, where each result can be either a result set or an integer (e.g., an inserted-records count).  If Drill ever needs to support that, then QueryData or other messages will need something.)\r\n"
    ],
    [
        "DRILL-2755",
        "DRILL-2924",
        "Use and handle InterruptedException during query processing Cancellation requests don't yet handle using InterruptedException to deal with blocking operations. For example, if a thread is blocked doing I/O, or waiting for completion of write requests (via SendingAccountor), then we need to be able to interrupt it so we can continue.\r\n\r\nThis means both knowing what threads to interrupt, and for any relevant blocking sites to handle the InterruptedException correctly (see http://www.ibm.com/developerworks/library/j-jtp05236/). At present, most of these just log the exception and continue, effectively swallowing the exception, which is not correct.",
        "IOBException when querying a table which has 1 file and a subfolder with 1 file git.commit.id.abbrev=5fbd274\r\n\r\nBelow is the folder structure\r\n\r\nData Structure :\r\n{code}\r\ndata1\r\n  -- a.json\r\n  -- folder1\r\n       -- b.json\r\n{code}\r\n\r\nQuery :\r\n{code}\r\n0: jdbc:drill:schema=dfs_eea> select * from `data1`;\r\n+------------+---------------+-----------------+\r\n|    dir0    | executionTime | stationBeanList |\r\n+------------+---------------+-----------------+\r\n| folder1    | 2014-11-04 11:54:01 AM | [{\"id\":72,\"stationName\":\"W 52 St & 11 Ave\", ..........\r\njava.lang.IndexOutOfBoundsException: index: -4, length: 4 (expected: range(0, 4))\r\n\tat io.netty.buffer.DrillBuf.checkIndexD(DrillBuf.java:200)\r\n\tat io.netty.buffer.DrillBuf.chk(DrillBuf.java:222)\r\n\tat io.netty.buffer.DrillBuf.getInt(DrillBuf.java:500)\r\n\tat org.apache.drill.exec.vector.UInt4Vector$Accessor.get(UInt4Vector.java:299)\r\n\tat org.apache.drill.exec.vector.complex.RepeatedMapVector$RepeatedMapAccessor.getValueCount(RepeatedMapVector.java:470)\r\n\tat org.apache.drill.exec.vector.accessor.BoundCheckingAccessor.getObject(BoundCheckingAccessor.java:142)\r\n\tat org.apache.drill.jdbc.impl.TypeConvertingSqlAccessor.getObject(TypeConvertingSqlAccessor.java:790)\r\n\tat org.apache.drill.jdbc.AvaticaDrillSqlAccessor.getObject(AvaticaDrillSqlAccessor.java:165)\r\n\tat net.hydromatic.avatica.AvaticaResultSet.getObject(AvaticaResultSet.java:351)\r\n\tat sqlline.SqlLine$Rows$Row.<init>(SqlLine.java:2388)\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2504)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n{code}\r\n\r\nI attached the error log and the data file. Let me know if you need anything"
    ],
    [
        "DRILL-2608",
        "DRILL-3223",
        "Need a better error message when Union all query fails, json.all_text_mode=false Union all query over JSON data file fails when store.json.all_text_mode is set to false, and same query returns correct results when store.json.all_text_mode is set to true. Each JSON data file had only one type of object {\"key\":<value>}, and the values in each of the JSON data files were of same datatype. Test was executed on a 4 node cluster.\r\n\r\n{code}\r\n0: jdbc:drill:> select key from `charData.json` union all select key from `dateData.json` union all select key from `doubleData.json` union all select key from `intData.json` union all select key from `timeStmpData.json` union all select key from `vrChrData.json`;\r\nQuery failed: RemoteRpcException: Failure while running fragment., For input string: \"itzVxYBb\" [ f1f81073-161c-4f24-89e5-37379413b01b on centos-04.qa.lab:31010 ]\r\n[ f1f81073-161c-4f24-89e5-37379413b01b on centos-04.qa.lab:31010 ]\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nThen I set alter session set `store.json.all_text_mode`=true;\r\n\r\nAfter setting son.all_text_mode to true, union all query returned correct results.\r\n\r\n{code}\r\n0: jdbc:drill:> select key from `charData.json` union all select key from `dateData.json` union all select key from `doubleData.json` union all select key from `intData.json` union all select key from `timeStmpData.json` union all select key from `vrChrData.json`;\r\n...\r\n+------------+\r\n7,194 rows selected (0.462 seconds)\r\n{code}\r\n\r\nResetting it back to false gives the same Exception\r\n{code}\r\n0: jdbc:drill:> alter session set `store.json.all_text_mode`=false;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | store.json.all_text_mode updated. |\r\n+------------+------------+\r\n1 row selected (0.049 seconds)\r\n0: jdbc:drill:> select key from `charData.json` union all select key from `dateData.json` union all select key from `doubleData.json` union all select key from `intData.json` union all select key from `timeStmpData.json` union all select key from `vrChrData.json`;\r\nQuery failed: RemoteRpcException: Failure while running fragment., For input string: \"itzVxYBb\" [ 412eda0e-cc22-43ae-b763-5e40a0326551 on centos-04.qa.lab:31010 ]\r\n[ 412eda0e-cc22-43ae-b763-5e40a0326551 on centos-04.qa.lab:31010 ]\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nStack trace from drillbit.log\r\n{code}\r\n2015-03-27 18:30:56,620 [2aea5e1e-88b9-3e4e-07b5-d7e46b29756f:frag:0:0] ERROR o.a.drill.exec.work.foreman.Foreman - Error b9cb90bd-7d89-4061-8595-4c5ad983f3f3: RemoteRpcException: Failure while running fragment., For input string: \"itzVxYBb\" [ 412eda0e-cc22-43ae-b763-5e40a0326551 on centos-04.qa.lab:31010 ]\r\n[ 412eda0e-cc22-43ae-b763-5e40a0326551 on centos-04.qa.lab:31010 ]\r\n\r\norg.apache.drill.exec.rpc.RemoteRpcException: Failure while running fragment., For input string: \"itzVxYBb\" [ 412eda0e-cc22-43ae-b763-5e40a0326551 on centos-04.qa.lab:31010 ]\r\n[ 412eda0e-cc22-43ae-b763-5e40a0326551 on centos-04.qa.lab:31010 ]\r\n\r\n        at org.apache.drill.exec.work.foreman.QueryManager.statusUpdate(QueryManager.java:163) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.QueryManager$RootStatusReporter.statusChange(QueryManager.java:281) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.AbstractStatusReporter.fail(AbstractStatusReporter.java:114) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.AbstractStatusReporter.fail(AbstractStatusReporter.java:110) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.internalFail(FragmentExecutor.java:230) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:182) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]\r\n{code}",
        "Logical plan deserializer for a join node is broken Trying to submit a logical query through the web interface or ./bin/submit_plan fails with an exception \r\n\r\nbq. java.lang.IllegalArgumentException: Conflicting property-based creators: already had [constructor for org.apache.drill.common.logical.data.Join, annotations: {interface com.fasterxml.jackson.annotation.JsonCreator=@com.fasterxml.jackson.annotation.JsonCreator()}], encountered [constructor for org.apache.drill.common.logical.data.Join, annotations: {interface com.fasterxml.jackson.annotation.JsonCreator=@com.fasterxml.jackson.annotation.JsonCreator()}]\r\n\r\n(Full queried plan and error message: https://gist.github.com/pyetras/bf625b6697de62284996)\r\n\r\nThis is most likely due to the JsonCreator annotation being used in two places in https://github.com/apache/drill/blob/master/common/src/main/java/org/apache/drill/common/logical/data/Join.java#L52-52"
    ],
    [
        "DRILL-2994",
        "DRILL-24",
        "Incorrect error message when disconnecting from server (using direct connection to drillbit) If connected to the server using a direct drillbit connection, JDBC client (sqlline) prints an already disconnected error when disconnecting.\r\nThis happens because of an exception because the client is trying to close the ZK cluster coordinator which is null.",
        "Implement Join Reference Operator Join (M)\r\n\r\nJoins two inputs based on one or more join conditions. The output of this operator is the combination of the two inputs.  This is done by providing a combination record of each set of input records that matches all provided join conditions.  In the case that no conditions are provided, a cartesian join is generated.  The combination record is a single record that contains a merged map of values from both provided input records.  For example, if the left record is {donuts: [data]} and the right is {purchases: [data]}, the combination record would be {donuts: [data], purchases: [data]}. Join also requires a condition type variable which.  This describes what happens when a record doesn\u2019t match the join conditions.  Inner means only records that match the join conditions should be included. Outer means if a record is \r\nAvaiable relationship types \u2018Reltypes\u2019 include: >, >=, <=, <, !=, ==\r\n{ @id\u2020: <opref>,  op: \u201cjoin\u201d, \r\n\r\n  left: <input>, \r\n\r\n  right: <input>, \r\n\r\n  type: <inner|outer|right|left>,\r\n\r\n  conditions*: [\r\n\r\n    {relationship: <reltype>, left: <expr>, right: <expr>}, ...\r\n\r\n  ]\r\n\r\n}\r\n"
    ],
    [
        "DRILL-2396",
        "DRILL-3301",
        "Query with IS [NOT] DISTINCT FROM in join filter fails during execution if planner.slice_target=1 -- Works\r\n{code}\r\nselect\r\n        count(*)\r\nfrom    j1 inner join j2 ON\r\n        (j1.c_integer = j2.c_integer)\r\nwhere\r\n        j1.c_bigint IS NOT DISTINCT FROM j2.c_bigint;\r\n{code}\r\n\r\n-- Explain plan\r\n{code}\r\n00-01      StreamAgg(group=[{}], EXPR$0=[COUNT()])\r\n00-02        Project($f0=[0])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[CAST(CASE(IS NULL($1), IS NULL($3), IS NULL($3), IS NULL($1), =($1, $3))):BOOLEAN NOT NULL])\r\n00-05              HashJoin(condition=[=($0, $2)], joinType=[inner])\r\n00-07                Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/joins/j1]], selectionRoot=/joins/j1, numFiles=1, columns=[`c_integer`, `c_bigint`]]])\r\n00-06                Project(c_integer0=[$0], c_bigint0=[$1])\r\n00-08                  Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/joins/j2]], selectionRoot=/joins/j2, numFiles=1, columns=[`c_integer`, `c_bigint`]]])\r\n{code}\r\nFails during execution if you set : alter system set `planner.slice_target` = 1;\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select\r\n. . . . . . . . . . . . >         count(*)\r\n. . . . . . . . . . . . > from    j1 inner join j2 ON\r\n. . . . . . . . . . . . >         (j1.c_integer = j2.c_integer)\r\n. . . . . . . . . . . . > where\r\n. . . . . . . . . . . . >         j1.c_bigint IS NOT DISTINCT FROM j2.c_bigint;\r\nQuery failed: RemoteRpcException: Failure while trying to start remote fragment, Expression has syntax error! line 1:196:no viable alternative at input 'BIT' [ b916d1b0-7cfa-48ad-b5dd-2aadf955857c on atsqa4-133.qa.lab:31010 ]\r\n{code}\r\n\r\nExplain plan:\r\n{code}\r\n00-01      StreamAgg(group=[{}], EXPR$0=[$SUM0($0)])\r\n00-02        UnionExchange\r\n01-01          StreamAgg(group=[{}], EXPR$0=[COUNT()])\r\n01-02            Project($f0=[0])\r\n01-03              SelectionVectorRemover\r\n01-04                Filter(condition=[CAST(CASE(IS NULL($1), IS NULL($3), IS NULL($3), IS NULL($1), =($1, $3))):BOOLEAN NOT NULL])\r\n01-05                  HashJoin(condition=[=($0, $2)], joinType=[inner])\r\n01-07                    HashToRandomExchange(dist0=[[$0]])\r\n02-01                      Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/joins/j1]], selectionRoot=/joins/j1, numFiles=1, columns=[`c_integer`, `c_bigint`]]])\r\n01-06                    Project(c_integer0=[$0], c_bigint0=[$1])\r\n01-08                      HashToRandomExchange(dist0=[[$0]])\r\n03-01                        Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/joins/j2]], selectionRoot=/joins/j2, numFiles=1, columns=[`c_integer`, `c_bigint`]]])\r\n{code}\r\n\r\n-- If you remove join, query works correctly\r\n{code}\r\n0: jdbc:drill:schema=dfs> select count(*) from j1 where c_bigint is not distinct from c_integer;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 10000      |\r\n+------------+\r\n1 row selected (0.137 seconds)\r\n{code}\r\n\r\nExplain plan:\r\n{code}\r\n00-01      StreamAgg(group=[{}], EXPR$0=[$SUM0($0)])\r\n00-02        StreamAgg(group=[{}], EXPR$0=[COUNT()])\r\n00-03          Project($f0=[0])\r\n00-04            SelectionVectorRemover\r\n00-05              Filter(condition=[CAST(CASE(IS NULL($0), IS NULL($1), IS NULL($1), IS NULL($0), =($0, $1))):BOOLEAN NOT NULL])\r\n00-06                Project(c_bigint=[$1], c_integer=[$0])\r\n00-07                  Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/joins/j1]], selectionRoot=/joins/j1, numFiles=1, columns=[`c_bigint`, `c_integer`]]])\r\n{code}\r\n\r\ndrillbit.log\r\n{code}\r\n2015-03-06 18:11:14,455 [BitServer-5] ERROR o.a.d.exec.rpc.control.ControlServer - Error 6e9b6998-d020-456f-822c-32a5ff2e764b: Failure while trying to start remote fragment\r\norg.apache.drill.exec.rpc.UserRpcException: org.apache.drill.common.exceptions.ExpressionParsingException: Expression has syntax error! line 1:196:no viable alternative at input 'BIT'\r\n        at org.apache.drill.exec.work.batch.ControlHandlerImpl.startNewRemoteFragment(ControlHandlerImpl.java:139) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.batch.ControlHandlerImpl.handle(ControlHandlerImpl.java:99) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.rpc.control.ControlServer.handle(ControlServer.java:60) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.rpc.control.ControlServer.handle(ControlServer.java:38) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.rpc.RpcBus.handle(RpcBus.java:58) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:194) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:173) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89) [netty-codec-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:161) [netty-codec-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116) [netty-common-4.0.24.Final.jar:4.0.24.Final]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]\r\n{code}",
        "ILIKE does not support escape characters. The like operator properly supports escaping characters. Because the ILIKE is implemented as a function, it does not support escaping.\r\n\r\nThe grammar needs to be updated to accept ILIKE in the same locations as LIKE.\r\n"
    ],
    [
        "DRILL-42",
        "DRILL-2883",
        "Implement Basic argument validation as part of logical plan resolution Currently, function definitions define argument validators.  However, these validators are not currently executed as part of logical plan materialization.  It would be good if someone could pick this functionality up and propose a way to stitch it into the logical plan setup. setupAndValidate() and ErrorCollector were envisioned to help with this work. ",
        "create a new CLIENT user exception type for client side error When {{QueryResultHandler}} calls a user result listener, if an exception happens it's wrapped inside a {{SYSTEM}} error and passed again to the listener's submitFailed().\r\n\r\nWe should pass those exceptions as {{CLIENT}} errors instead to make it easier for us to separate them from server side errors."
    ],
    [
        "DRILL-3989",
        "DRILL-3367",
        "Create a sys.queries table We should create a sys.queries table that provides a clusterwide view of active queries. It could include the following columns:\r\n\r\nqueryid, user, sql, current status, number of nodes involved, number of total fragments, number of fragments completed, start time\r\n\r\nThis should be a pretty straightforward task as we should be able to leverage the capabilities around required affinity. A great model to build off of are the sys.memory and sys.threads tables.\r\n\r\n",
        "Implement SQL data type DECIMAL. "
    ],
    [
        "DRILL-3129",
        "DRILL-2795",
        "truncation (from maxwidth) not indicated in output In SQLLine, when the display of a row is truncated because of the maxwidth variable, there is no indication that the row was truncated.\r\n\r\nIn particular, the row's rendering into text seems to always include the final \"|\" (vertical bar character), which makes the row look complete:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> !set maxwidth 14\r\n0: jdbc:drill:zk=local> !tables\r\n+------------+\r\n| TABLE_CAT  |\r\n+------------+\r\n| DRILL      |\r\n| DRILL      |\r\n| DRILL      |\r\n| DRILL      |\r\n| DRILL      |\r\n| DRILL      |\r\n| DRILL      |\r\n| DRILL      |\r\n| DRILL      |\r\n| DRILL      |\r\n| DRILL      |\r\n+------------+\r\n0: jdbc:drill:zk=local> !set maxwidth 15\r\n0: jdbc:drill:zk=local> !tables\r\n+-------------+\r\n| TABLE_CAT   |\r\n+-------------+\r\n| DRILL       |\r\n| DRILL       |\r\n| DRILL       |\r\n| DRILL       |\r\n| DRILL       |\r\n| DRILL       |\r\n| DRILL       |\r\n| DRILL       |\r\n| DRILL       |\r\n| DRILL       |\r\n| DRILL       |\r\n+-------------+\r\n0: jdbc:drill:zk=local> \r\n{noformat}\r\n\r\n\r\nIf the untruncated rendering of the whole row were simply truncated to the maximum width, then, at in many cases, the line wouldn't end with a \"|\" and it would be clear that the output was truncated. \r\n\r\n(It wouldn't be clear if the truncation were at a width at which every visible line had a \"|\" or had \"|\" followed by whitespace.\r\n\r\n\r\nOne solution would be to end truncated lines with \"+\" or \"...\"  (something other than \"|\").\r\n",
        "Group-By query on TPC-DS SF100 produces results but does not terminate due to exception The following simplified group-by query with limit on TPC-DS SF100 eventually produces results (the performance seems too slow for this scale factor) but does not terminate.  The log files show an IllegalStateException.  I found that after that first minute or so, all cpu's on the cluster show 99% idle time.  \r\n\r\n{code}\r\nalter session set `planner.slice_target` = 100;\r\nselect cr_call_center_sk , cr_catalog_page_sk ,  cr_item_sk , cr_reason_sk , cr_refunded_addr_sk , cr_refunded_customer_sk  from catalog_returns_dri100 group by cr_call_center_sk, cr_catalog_page_sk, cr_item_sk, cr_reason_sk, cr_refunded_addr_sk, cr_refunded_customer_sk limit 10;\r\n{code}\r\n\r\nCardinality of the table is relatively small: \r\n{code}\r\n0: jdbc:drill:schema=dfs.tpcds> select count(*) from catalog_returns_dri100;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 14404374   |\r\n+------------+\r\n{code}\r\n\r\n{code}\r\n2015-04-14 15:41:55,163 ucs-node17.perf.lab [BitServer-3] ERROR o.a.d.exec.rpc.RpcExceptionHandler - Exception in pipeline.  Closing channel between local /10.10.120.117:31011 and\r\n remote /10.10.120.109:34899\r\nio.netty.handler.codec.DecoderException: java.lang.IllegalStateException: Get Runnable can only be run once.\r\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:99) [netty-codec-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:161) [netty-codec-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n{code}"
    ],
    [
        "DRILL-4025",
        "DRILL-1841",
        "Reduce getFileStatus() invocation for Parquet by 1 Currently we invoke getFileStatus() to list all the files under a directory even when we have the metadata cache file. The information is already present in the cache so we don't need to perform this operation.",
        "Remove warning during build if you do a {{mvn clean}} you will get the following warning at the beginning\r\n{code}\r\n[WARNING]\r\n[WARNING] Some problems were encountered while building the effective model for org.apache.drill.exec:drill-interpreter:jar:0.8.0-SNAPSHOT\r\n[WARNING] 'build.plugins.plugin.version' for org.codehaus.mojo:exec-maven-plugin is missing. @ line 57, column 15\r\n[WARNING]\r\n[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.\r\n[WARNING]\r\n[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.\r\n[WARNING]\r\n{code}\r\n"
    ],
    [
        "DRILL-722",
        "DRILL-4078",
        "Intermittent failures in TestParquetWriter The problem seems to be that ParquetReader, when reading a Decimal28 or Decimal38 type, needs to zero-out the ByteBuf that the value is being written to.",
        "Support schema changes in Hash Aggregate (Group by) Support changes in schema during hash aggregate execution. Just like hash join we can rely on current implicit casting to support equality between different types(including union types). \r\n"
    ],
    [
        "DRILL-1662",
        "DRILL-104",
        "drillbit.sh stop should timeout We need a timeout as part of the drillbit.sh stop\r\n\r\nCan we have a configurable parameter with a default of 30 seconds and after that the timeout should kill the drillbit.sh",
        "Hive storage engine "
    ],
    [
        "DRILL-3635",
        "DRILL-2882",
        "IllegalArgumentException - not a Parquet file (too small) The (MapR internal) regression suite is sporadically seeing this error:\r\n\r\n/root/private-sql-hadoop-test/framework/resources/Precommit/Functional/ctas_flatten/100000rows/filter4.q\r\nQuery: \r\nselect * from dfs.ctas_flatten.`filter4_100000rows_ctas`\r\nFailed with exception\r\njava.sql.SQLException: SYSTEM ERROR: IllegalArgumentException: maprfs:///drill/testdata/ctas_flatten/filter4_100000rows_ctas/0_0_0.parquet is not a Parquet file (too small)\r\n\r\n\r\n[Error Id: 9749d6a7-685d-4663-9b27-1a456a5dec40 on drillats3.qa.lab:31010]\r\n\tat org.apache.drill.jdbc.impl.DrillCursor.nextRowInternally(DrillCursor.java:244)\r\n\tat org.apache.drill.jdbc.impl.DrillCursor.loadInitialSchema(DrillCursor.java:287)\r\n\tat org.apache.drill.jdbc.impl.DrillResultSetImpl.execute(DrillResultSetImpl.java:1362)\r\n\tat org.apache.drill.jdbc.impl.DrillResultSetImpl.execute(DrillResultSetImpl.java:72)\r\n\tat net.hydromatic.avatica.AvaticaConnection.executeQueryInternal(AvaticaConnection.java:404)\r\n\tat net.hydromatic.avatica.AvaticaStatement.executeQueryInternal(AvaticaStatement.java:351)\r\n\tat net.hydromatic.avatica.AvaticaStatement.executeQuery(AvaticaStatement.java:78)\r\n\tat org.apache.drill.jdbc.impl.DrillStatementImpl.executeQuery(DrillStatementImpl.java:96)\r\n\tat org.apache.drill.test.framework.DrillTestJdbc.executeQuery(DrillTestJdbc.java:144)\r\n\tat org.apache.drill.test.framework.DrillTestJdbc.run(DrillTestJdbc.java:83)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\nCaused by: org.apache.drill.common.exceptions.UserRemoteException: SYSTEM ERROR: IllegalArgumentException: maprfs:///drill/testdata/ctas_flatten/filter4_100000rows_ctas/0_0_0.parquet is not a Parquet file (too small)\r\n\r\nIt doesn't happen every time, but based on looking at log files, it seems to happen more than half the time.",
        "Query with nested flatten function fails with SYSTEM ERROR: initialCapacity Querying the Mondrian Foodmart Queries JSON file using nested flatten function fails:\r\n\r\n{code:sql}\r\n> select flatten(q.`queries`[10].`rows`) from `queries.json` q;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| [\"1997\",\"5095\",\"59.0000\"] |\r\n| [\"1997\",\"8901\",\"146.0000\"] |\r\n| [\"1997\",\"3801\",\"42.0000\"] |\r\n| [\"1997\",\"3\",\"50.0000\"] |\r\n| [\"1997\",\"3169\",\"31.0000\"] |\r\n| [\"1997\",\"193\",\"21.0000\"] |\r\n| [\"1997\",\"4739\",\"110.0000\"] |\r\n| [\"1997\",\"6395\",\"42.0000\"] |\r\n| [\"1997\",\"9129\",\"78.0000\"] |\r\n| [\"1997\",\"7141\",\"157.0000\"] |\r\n+------------+\r\n10 rows selected (12.203 seconds)\r\n\r\n> select flatten(flatten(q.`queries`[10].`rows`)) from `queries.json` q;\r\nQuery failed: SYSTEM ERROR: initialCapacity: -2147483648 (expectd: 0+)\r\nFragment 0:0\r\n[d9a29de4-17a5-4dc8-8263-adc5184bccd7 on abhi8.qa.lab:31010]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nLog attached. "
    ],
    [
        "DRILL-1537",
        "DRILL-1001",
        "C++ Client: Passing the listener context to queryResultListener function When submitting queries by the function *SubmitQuery(QueryType t, string& plan, pfnQueryResultsListener l, void\\* lCtx)* of drillClientImp class, the listenerContext parameter provides a convenient way of associating returned results with submitted queries.\r\n\r\nThe way it works is that the calling application passes the context to *SubmitQuery* and when returning results, *processQueryResult* passes the context to the *queryResultListener* callback function. As such, the callback function can associate the returned result with the context.\r\n\r\nWhen we were updating QuerySubmitter example to showcase usage of *SubmitQuery* with context, we noticed *processQueryResult* function does not pass the context directly to the *queryResultListener* callback function; Instead, an instance of DrillClientQueryResult which contains the context as a data member is passed to the *queryResultListener*. This requires the *queryResultListener* function, which is implemented in consumers of the C++ Client, to know about the *DrillClientQueryResult*.\r\n\r\nHowever, *DrillClientQueryResult* is not in the public API of the C++ Client. Two solutions are imaginable at the first glance: First, passing the context instead of a *DrillClientQueryResult*, which we implemented and tested it; Second, moving *DrillClientQueryResult* to the public API; Moving *DrillClientQueryResult* to the public API does not seem to be desirable as it is internal detail for the C++ Client.\r\n\r\nI was wondering what your thoughts are on this.\r\n\r\nThanks,\r\nAlex",
        "Additional features and formatting for profile summary * Formatting should be done with HTML tables instead of tab delimited.\r\n* Include link to slow/fast fragment in time ranges.\r\n* Add tables for minor fragments.\r\n* Order tables more consistently."
    ],
    [
        "DRILL-3749",
        "DRILL-380",
        "Upgrade Hadoop dependency to latest version (2.7.1) Logging a JIRA to track and discuss upgrading Drill's Hadoop dependency version. Currently Drill depends on Hadoop 2.5.0 version. Newer version of Hadoop (2.7.1) has following features.\r\n\r\n1) Better S3 support\r\n2) Ability to check if a user has certain permissions on file/directory without performing operations on the file/dir. Useful for cases like DRILL-3467.\r\n\r\nAs Drill is going to use higher version of Hadoop fileclient, there could be potential issues when interacting with Hadoop services (such as HDFS) of lower version than the fileclient.\r\n",
        "Filter before Sort causes exception Here's a plan that contains a Filter before Sort.  The stack trace for the exception is shown further below.  Having a SelectionVectorRemover between the Filter and Sort eliminates the problem but should not be required. \r\n\r\n{\r\n  head : {\r\n    version : 1,\r\n    generator : {\r\n      type : \"optiq\",\r\n      info : \"na\"\r\n    },\r\n    type : \"APACHE_DRILL_PHYSICAL\"\r\n  },\r\n  graph : [ {\r\n    pop : \"parquet-scan\",\r\n    @id : 1,\r\n    entries : [ {\r\n      path : \"/tmp/parquet/orders/part-m-00001.parquet\"\r\n    } ],\r\n    storageengine : {\r\n      type : \"parquet\",\r\n      dfsName : \"file:///\"\r\n    },\r\n    ref : \"_MAP\",\r\n    fragmentPointer : 0\r\n  }, {\r\n    pop : \"project\",\r\n    @id : 2,\r\n    exprs : [ {\r\n      ref : \"output.$f0\",\r\n      expr : \"_MAP.O_CUSTKEY\"\r\n    }, {\r\n      ref : \"output.$f1\",\r\n      expr : \"_MAP.O_ORDERKEY\"\r\n    } ],\r\n    child : 1\r\n  }, {\r\n    pop : \"filter\",\r\n    @id : 3,\r\n    child : 2,\r\n    expr : \"$f0 > (100000) AND $f0 < (500000)\"\r\n  }, {\r\n    pop : \"sort\",\r\n    @id : 4,\r\n    child : 3,\r\n    orderings : [ {\r\n      order : \"ASC\",\r\n      expr : \"$f0\"\r\n    }, {\r\n      order : \"ASC\",\r\n      expr : \"Y\"\r\n    } ],\r\n    reverse : false\r\n  }, {\r\n    pop : \"selection-vector-remover\",\r\n    @id : 5,\r\n    child : 4\r\n  }, {\r\n    pop : \"screen\",\r\n    @id : 6,\r\n    child : 5\r\n  } ]\r\n}\r\n\r\n\r\nava.lang.IllegalArgumentException: Undefined for 0\r\n        at org.apache.hadoop.util.QuickSort.getMaxDepth(QuickSort.java:41) ~[hadoop-core-1.2.1.jar:na]\r\n        at org.apache.hadoop.util.QuickSort.sort(QuickSort.java:59) ~[hadoop-core-1.2.1.jar:na]\r\n        at org.apache.hadoop.util.QuickSort.sort(QuickSort.java:51) ~[hadoop-core-1.2.1.jar:na]\r\n        at org.apache.drill.exec.test.generated.SorterGen0.sort(SortTemplate.java:47) ~[na:na]\r\n        at org.apache.drill.exec.physical.impl.sort.SortBatch.next(SortBatch.java:152) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.next(AbstractSingleRecordBatch.java:42) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]"
    ],
    [
        "DRILL-3922",
        "DRILL-3284",
        "select * on json document with dot in key name results in failure If there is a dot in the key name of a json field, and the user issues a select * from `path/to/file.json` it will result a system error: \"Unsupported Operation Exception: Field references must be singular names\"\r\n\r\nTo reproduce:\r\n\r\n{\"hello\":\"goodbye\", \"yousayyes\":\"Isayno\"}\r\n\r\n Works fine\r\n\r\n{\"hello.yoko\":\"goodbye\", \"yousayyes\":\"Isayno\"}\r\n\r\nFails.\r\n\r\nNote, this is only if you do select * if you do select `hello.yoko` from file then it will work fine.   \r\n\r\nThe select * is needed to help with data exploration.  \r\n\r\n\r\n\r\n",
        "Document incompatibility between drill's to_date and hive's unix_timestamp \r\nThe below query from drill produces wrong results because unix_timestamp (function from hive) returns the value in seconds while to_date treats its input in milliseconds.\r\n{code}\r\nselect to_date(unix_timestamp('1998-05-06', 'yyyy-MM-dd')) from dummy limit 1;\r\n+-------------+\r\n|   EXPR$0    |\r\n+-------------+\r\n| 1970-01-11  |\r\n+-------------+\r\n{code} \r\n\r\nIn order to make this work we should use the below query\r\n{code}\r\nselect to_date(unix_timestamp('1998-05-06', 'yyyy-MM-dd')*1000) from dummy limit 1;\r\n+-------------+\r\n|   EXPR$0    |\r\n+-------------+\r\n| 1998-05-06  |\r\n+-------------+\r\n{code}\r\n\r\nIf this is not a bug on drill's side, we should atleast document this "
    ],
    [
        "DRILL-780",
        "DRILL-4205",
        "ZooKeeper connection to support configurable cluster id \r\nThe cluster-id will have to be configurable and part of the connect string. Default is drillbit1. If the connection type is local, I will ignore the cluster-id if provided.\r\n  The connect string URL will be like :\r\n\r\njdbc:drill:[cluster-id=<clusterid>;]zk=zk1:5181,zk2:5181,zk3:5181  \r\n",
        " Simple query hit IndexOutOfBoundException The following query failed due to IOB:\r\n0: jdbc:drill:schema=wf_pigprq100> select * from `store_sales/part-m-00073.parquet`;\r\n\r\nError: SYSTEM ERROR: IndexOutOfBoundsException: srcIndex: 1048587\r\n\r\nFragment 0:0\r\n\r\n[Error Id: ad8d2bc0-259f-483c-9024-93865963541e on ucs-node4.perf.lab:31010]\r\n\r\n  (org.apache.drill.common.exceptions.DrillRuntimeException) Error in parquet record reader.\r\nMessage: \r\nHadoop path: /tpcdsPigParq/SF100/store_sales/part-m-00073.parquet\r\nTotal records read: 135280\r\nMock records read: 0\r\nRecords to read: 1424\r\nRow group index: 0\r\nRecords in row group: 3775712\r\nParquet Metadata: ParquetMetaData{FileMetaData{schema: message pig_schema {\r\n  optional int64 ss_sold_date_sk;\r\n  optional int64 ss_sold_time_sk;\r\n  optional int64 ss_item_sk;\r\n  optional int64 ss_customer_sk;\r\n  optional int64 ss_cdemo_sk;\r\n  optional int64 ss_hdemo_sk;\r\n  optional int64 ss_addr_sk;\r\n  optional int64 ss_store_sk;\r\n  optional int64 ss_promo_sk;\r\n  optional int64 ss_ticket_number;\r\n  optional int64 ss_quantity;\r\n  optional double ss_wholesale_cost;\r\n  optional double ss_list_price;\r\n  optional double ss_sales_price;\r\n  optional double ss_ext_discount_amt;\r\n  optional double ss_ext_sales_price;\r\n  optional double ss_ext_wholesale_cost;\r\n  optional double ss_ext_list_price;\r\n  optional double ss_ext_tax;\r\n  optional double ss_coupon_amt;\r\n  optional double ss_net_paid;\r\n  optional double ss_net_paid_inc_tax;\r\n  optional double ss_net_profit;\r\n}\r\n"
    ],
    [
        "DRILL-2814",
        "DRILL-1303",
        "Need a better error message instead of SYSTEM ERROR. Need a better error message. Trying to do an average over date type data results in SYSTEM ERROR reported on sqlline prompt.\r\n\r\n{code}\r\n\r\n0: jdbc:drill:> select avg(key) from `jsonData/dateData.json`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\nQuery failed: SYSTEM ERROR: 1996-02-24\r\n\r\n[b385706d-c027-41dc-ba6d-8e872f26d293 on centos-02.qa.lab:31010]\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n{code}\r\nStack trace from drillbit.log\r\n{code}\r\n2015-04-17 17:32:48,864 [2acebc3e-fada-4f03-78e6-ce5551ddd55c:frag:0:0] ERROR o.a.drill.exec.ops.FragmentContext - Fragment Context received failure -- Fragment: 0:0\r\norg.apache.drill.common.exceptions.DrillUserException: SYSTEM ERROR: 1996-02-24\r\n\r\n[2d88bcdb-d1fb-43b2-bc5f-12641c2ad3b9 on centos-02.qa.lab:31010]\r\n\r\n        at org.apache.drill.common.exceptions.DrillUserException$Builder.build(DrillUserException.java:115) ~[drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.exceptions.ErrorHelper.wrap(ErrorHelper.java:39) ~[drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.ops.FragmentContext.fail(FragmentContext.java:151) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:182) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]\r\nCaused by: java.lang.NumberFormatException: 1996-02-24\r\n        at org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.nfeI(StringFunctionHelpers.java:97) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.varCharToInt(StringFunctionHelpers.java:122) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.test.generated.StreamingAggregatorGen173.addRecord(StreamingAggTemplate.java:205) ~[na:na]\r\n        at org.apache.drill.exec.test.generated.StreamingAggregatorGen173.addRecordInc(StreamingAggTemplate.java:331) ~[na:na]\r\n        at org.apache.drill.exec.test.generated.StreamingAggregatorGen173.doWork(StreamingAggTemplate.java:132) ~[na:na]\r\n        at org.apache.drill.exec.physical.impl.aggregate.StreamingAggBatch.innerNext(StreamingAggBatch.java:127) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:99) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:89) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:135) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:74) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:76) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:64) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:164) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        ... 4 common frames omitted\r\n2015-04-17 17:32:48,865 [2acebc3e-fada-4f03-78e6-ce5551ddd55c:frag:0:0] INFO  o.a.drill.exec.work.foreman.Foreman - State change requested.  RUNNING --> FAILED\r\norg.apache.drill.common.exceptions.DrillRemoteException: SYSTEM ERROR: 1996-02-24\r\n\r\n[b385706d-c027-41dc-ba6d-8e872f26d293 on centos-02.qa.lab:31010]\r\n\r\n        at org.apache.drill.exec.work.foreman.QueryManager.statusUpdate(QueryManager.java:163) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.QueryManager$RootStatusReporter.statusChange(QueryManager.java:281) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.AbstractStatusReporter.fail(AbstractStatusReporter.java:114) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.AbstractStatusReporter.fail(AbstractStatusReporter.java:110) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.internalFail(FragmentExecutor.java:235) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:183) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]\r\n{code}\r\nI see a much better message when I cast the key to date type.\r\n{code}\r\n0: jdbc:drill:> select avg(cast(key as date)) from `jsonData/dateDataLHS.json`;\r\nQuery failed: PARSE ERROR: From line 1, column 8 to line 1, column 29: Cannot apply 'AVG' to arguments of type 'AVG(<DATE>)'. Supported form(s): 'AVG(<NUMERIC>)'\r\n\r\n[17025212-02b3-43e0-8159-08b514d52632 on centos-02.qa.lab:31010]\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\nData used in the test\r\n{code}\r\n{\"key\":\"1996-02-24\"}\r\n{\"key\":\"1955-05-10\"}\r\n{\"key\":\"1987-06-12\"}\r\n{\"key\":\"1959-04-06\"}\r\n{\"key\":\"1951-04-05\"}\r\n{\"key\":\"1990-04-01\"}\r\n{\"key\":\"1971-10-13\"}\r\n{\"key\":\"1959-06-10\"}\r\n{\"key\":\"1967-04-08\"}\r\n{\"key\":\"1999-07-04\"}\r\n{\"key\":\"2008-03-25\"}\r\n{\"key\":\"2000-02-24\"}\r\n{\"key\":\"1970-11-16\"}\r\n{\"key\":\"1995-01-26\"}\r\n{\"key\":\"1999-11-20\"}\r\n{\"key\":\"1992-10-18\"}\r\n{\"key\":\"1992-08-18\"}\r\n\r\n{code}",
        "C client throws AssertionError on complex types Running:\r\n{code}\r\n./querySubmitter query=\"select * from dfs.`/path/to/mobile-small.json`;\" connectStr=\"local=localhost:31010\" type=\"sql\" api=\"sync\"\r\n{code}\r\n\r\nPrints out records partially as follows:\r\n{panel}\r\nROW: 32\t\t\t\t{\r\n  \"cust_id\" : 40,\r\n  \"device\" : \"AOS4.2\",\r\n  \"state\" : \"nj\"\r\n}\t{\r\n  \"camp_id\" : 8,\r\n  \"keywords\" : [ \"justice\", \"it's\", \"got\", \"i'll\", \"the\", \"in\", \"dad\" ]\r\n}\t{\r\n  \"prod_id\" : [ 172, 127, 1, 430, 17, 238 ],\r\n  \"purch_flag\" : \"false\"\r\n}\t\r\nROW: 33\t\t\t\t{\r\n  \"cust_id\" : 4,\r\n  \"device\" : \"IOS5\",\r\n  \"state\" : \"or\"\r\n}\t{\r\n  \"camp_id\" : 9,\r\n  \"keywords\" : [ ]\r\n}\t{\r\n  \"prod_id\" : [ 104, 65, 242, 11, 17, 23, 294, 187, 296, 208, 8 ],\r\n  \"purch_flag\" : \"false\"\r\n}\r\n{panel}\r\n\r\nAnd stops raising an AssertionError\r\n\r\n{panel}\r\nAssertion failed: ((index + sizeof(T) <= this->m_length)), function readAt, file /Users/hgunes/workspaces/mapr/incubator-drill/contrib/native/client/src/clientlib/../include/drill/recordBatch.hpp, line 128.\r\nROW: 34\t\tfish: Job 1, './querySubmitter query=\"select * from dfs.`/Users/hgunes/workspaces/mapr/incubator-drill/data/mobile-small.json`;\" connectStr=\"local=localhost:31010\" type=\"sql\" api=\"sync\"' terminated by signal SIGABRT (Abort)\r\n{panel}"
    ],
    [
        "DRILL-1815",
        "DRILL-416",
        "HiveScan is not considering data locality when applying work assignments to nodes ",
        "Make Drill work with SELECT without FROM This works with postgres:\r\n\r\n[root@qa-node120 ~]# sudo -u postgres psql foodmart\r\nfoodmart=# select 1+1.1;\r\n ?column?\r\n----------\r\n      2.1\r\n(1 row)\r\n\r\nBut does not work with Drill:\r\n\r\n0: jdbc:drill:> select 1+1.1;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"100f4d4c-1ee1-495e-9c2f-547aae75473d\"\r\nendpoint {\r\n  address: \"qa-node118.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < SqlParseException:[ Encountered \\\"<EOF>\\\" at line 1, column 12.\\nWas expecting one of:....\r\n"
    ],
    [
        "DRILL-122",
        "DRILL-3198",
        "Support out of process physical operator Implement a mechanism for executing an operator in another process.",
        "JDBC driver returns null from DatabaseMetaData.getTypeInfo(...) JDBC driver returns null from DatabaseMetaData#getTypeInfo() instead of a ResultSet, causing some client tools to throw NPEs.\r\n\r\nDriver works with SquirrelSQL (http://drill.apache.org/docs/using-jdbc/), but fails with others (DbVisualizer for example)."
    ],
    [
        "DRILL-1719",
        "DRILL-1352",
        "sqlline in embedded mode on MacOSX returns a long list of logback errors With both the released 0.6 version and the 0.7 snapshot, sqlline on my Mac returns a slew of log back errors but eventually starts. Jason commented: \r\n\r\n\"Looking in the output there is this line \"java.io.FileNotFoundException:\r\n/var/log/drill/sqlline.log\". The logging directory for drill is user\r\nconfigurable. It looks like this file is not on the machine, you can try to\r\ngo into the conf directory of your install and edit the logback.xml file to\r\nset a different directory, or just create the logging file in the location\r\nits currently looking. I assume you will need to make the logging file\r\nwritable by whatever user you run the drillbit process as.\"\r\n\r\nHere is the output I am seeing:\r\n\r\nAdministrators-MacBook-Pro-149:apache-drill-0.7.0-incubating-SNAPSHOT brumsby$ bin/sqlline -u jdbc:drill:zk=local -n admin -p admin\r\n18:35:03,880 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback.groovy]\r\n18:35:03,880 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback-test.xml]\r\n18:35:03,881 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Found resource [logback.xml] at [file:/Users/brumsby/drill/apache-drill-0.7.0-incubating-SNAPSHOT/conf/logback.xml]\r\n18:35:04,029 |-INFO in ch.qos.logback.classic.joran.action.ConfigurationAction - debug attribute not set\r\n18:35:04,315 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - About to instantiate appender of type [ch.qos.logback.core.ConsoleAppender]\r\n18:35:04,326 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - Naming appender as [STDOUT]\r\n18:35:04,344 |-INFO in ch.qos.logback.core.joran.action.NestedComplexPropertyIA - Assuming default type [ch.qos.logback.classic.encoder.PatternLayoutEncoder] for [encoder] property\r\n18:35:04,415 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - About to instantiate appender of type [ch.qos.logback.core.rolling.RollingFileAppender]\r\n18:35:04,418 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - Naming appender as [FILE]\r\n18:35:04,434 |-INFO in ch.qos.logback.core.rolling.FixedWindowRollingPolicy@2777068f - No compression will be used\r\n18:35:04,441 |-INFO in ch.qos.logback.core.joran.action.NestedComplexPropertyIA - Assuming default type [ch.qos.logback.classic.encoder.PatternLayoutEncoder] for [encoder] property\r\n18:35:04,442 |-INFO in ch.qos.logback.core.rolling.RollingFileAppender[FILE] - Active log file name: /var/log/drill/sqlline.log\r\n18:35:04,442 |-INFO in ch.qos.logback.core.rolling.RollingFileAppender[FILE] - File property is set to [/var/log/drill/sqlline.log]\r\n18:35:04,443 |-ERROR in ch.qos.logback.core.rolling.RollingFileAppender[FILE] - openFile(/var/log/drill/sqlline.log,true) call failed. java.io.FileNotFoundException: /var/log/drill/sqlline.log (Permission denied)\r\n\tat java.io.FileNotFoundException: /var/log/drill/sqlline.log (Permission denied)\r\n\tat \tat java.io.FileOutputStream.open(Native Method)\r\n\tat \tat java.io.FileOutputStream.<init>(FileOutputStream.java:221)\r\n\tat \tat ch.qos.logback.core.recovery.ResilientFileOutputStream.<init>(ResilientFileOutputStream.java:28)\r\n\tat \tat ch.qos.logback.core.FileAppender.openFile(FileAppender.java:149)\r\n\tat \tat ch.qos.logback.core.FileAppender.start(FileAppender.java:108)\r\n\tat \tat ch.qos.logback.core.rolling.RollingFileAppender.start(RollingFileAppender.java:86)\r\n\tat \tat ch.qos.logback.core.joran.action.AppenderAction.end(AppenderAction.java:96)\r\n\tat \tat ch.qos.logback.core.joran.spi.Interpreter.callEndAction(Interpreter.java:317)\r\n\tat \tat ch.qos.logback.core.joran.spi.Interpreter.endElement(Interpreter.java:196)\r\n\tat \tat ch.qos.logback.core.joran.spi.Interpreter.endElement(Interpreter.java:182)\r\n\tat \tat ch.qos.logback.core.joran.spi.EventPlayer.play(EventPlayer.java:62)\r\n\tat \tat ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:149)\r\n\tat \tat ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:135)\r\n\tat \tat ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:99)\r\n\tat \tat ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:49)\r\n\tat \tat ch.qos.logback.classic.util.ContextInitializer.configureByResource(ContextInitializer.java:75)\r\n\tat \tat ch.qos.logback.classic.util.ContextInitializer.autoConfig(ContextInitializer.java:148)\r\n\tat \tat org.slf4j.impl.StaticLoggerBinder.init(StaticLoggerBinder.java:85)\r\n\tat \tat org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:55)\r\n\tat \tat org.slf4j.LoggerFactory.bind(LoggerFactory.java:128)\r\n\tat \tat org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:107)\r\n\tat \tat org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:295)\r\n\tat \tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:269)\r\n\tat \tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:281)\r\n\tat \tat org.apache.drill.jdbc.DrillConnectionImpl.<clinit>(DrillConnectionImpl.java:51)\r\n\tat \tat org.apache.drill.jdbc.DrillJdbc41Factory.newDrillConnection(DrillJdbc41Factory.java:57)\r\n\tat \tat org.apache.drill.jdbc.DrillJdbc41Factory.newDrillConnection(DrillJdbc41Factory.java:43)\r\n\tat \tat org.apache.drill.jdbc.DrillFactory.newConnection(DrillFactory.java:51)\r\n\tat \tat net.hydromatic.avatica.UnregisteredDriver.connect(UnregisteredDriver.java:126)\r\n\tat \tat sqlline.SqlLine$DatabaseConnection.connect(SqlLine.java:4732)\r\n\tat \tat sqlline.SqlLine$DatabaseConnection.getConnection(SqlLine.java:4786)\r\n\tat \tat sqlline.SqlLine$Commands.connect(SqlLine.java:4026)\r\n\tat \tat sqlline.SqlLine$Commands.connect(SqlLine.java:3935)\r\n\tat \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n\tat \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat \tat sqlline.SqlLine$ReflectiveCommandHandler.execute(SqlLine.java:2884)\r\n\tat \tat sqlline.SqlLine.dispatch(SqlLine.java:885)\r\n\tat \tat sqlline.SqlLine.initArgs(SqlLine.java:693)\r\n\tat \tat sqlline.SqlLine.begin(SqlLine.java:745)\r\n\tat \tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat \tat sqlline.SqlLine.main(SqlLine.java:460)\r\n18:35:04,444 |-INFO in ch.qos.logback.classic.joran.action.LoggerAction - Setting additivity of logger [org.apache.drill] to false\r\n18:35:04,444 |-INFO in ch.qos.logback.classic.joran.action.LevelAction - org.apache.drill level set to INFO\r\n18:35:04,444 |-INFO in ch.qos.logback.core.joran.action.AppenderRefAction - Attaching appender named [FILE] to Logger[org.apache.drill]\r\n18:35:04,445 |-INFO in ch.qos.logback.classic.joran.action.LevelAction - ROOT level set to ERROR\r\n18:35:04,445 |-INFO in ch.qos.logback.core.joran.action.AppenderRefAction - Attaching appender named [STDOUT] to Logger[ROOT]\r\n18:35:04,445 |-INFO in ch.qos.logback.classic.joran.action.ConfigurationAction - End of configuration.\r\n18:35:04,446 |-INFO in ch.qos.logback.classic.joran.JoranConfigurator@4ca384e3 - Registering current configuration as safe fallback point\r\n\r\nNov 14, 2014 6:35:11 PM org.glassfish.jersey.server.ApplicationHandler initialize\r\nINFO: Initiating Jersey application, version Jersey: 2.8 2014-04-29 01:25:26...\r\n\r\nsqlline version 1.1.6\r\n0: jdbc:drill:zk=local> \r\n",
        "C++ Client needs to be updated to drill rpc version 2 RPC version is now 2.\r\nNullable types now use a byte per record to indicate null values. "
    ],
    [
        "DRILL-232",
        "DRILL-2668",
        "Add Logical and Physical plan viewing capabilities Lisen has made a small script which generates SVGs of the logical plan operators.  It would be nice to extend this to support physical plans and incorporate this into the web frontend.  Sub tasks associated with this that would need to be done include:\r\n\r\n- Add support for an output type in drill client (e.g. results, physical plan, logical plan, possible depending on which type of query you submit).\r\n- Add support for the new DrillClient interfaces within the REST layer that Hari is working on \r\n- Add support for these representations directly in the GUI.\r\n\r\nThe script can be found here: https://gist.github.com/immars/6512279",
        "CAST to FLOAT with numeric literal yields DOUBLE Casting a numeric literal to FLOAT results in type DOUBLE rather than type FLOAT.\r\n\r\nCurrently (before DRILL-2613 is fixed), querying for \"CAST (1.1 AS FLOAT)\" and reading the value using ResultSet.getFloat(...) yields an error about not allowed to use getFloat(...) for a FLOAT8 vector.\r\n\r\n\r\n"
    ],
    [
        "DRILL-598",
        "DRILL-1993",
        "should we support timestamptz data type? 0: jdbc:drill:schema=dfs> select cast(c_date as timestamptz) from data;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"7b141866-e832-42e1-a669-aa9c65ef4fb0\"\r\nendpoint {\r\n  address: \"qa-node119.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < ValidationException:[ org.eigenbase.util.EigenbaseContextException: From line 1, column 23 to line 1, column 33 ] < EigenbaseContextException:[ From line 1, column 23 to line 1, column 33 ] < SqlValidatorException:[ Unknown datatype name \\'timestamptz\\' ]\"\r\n]\r\nError: exception while executing query (state=,code=0)",
        "Fix allocation issues in HashTable and HashAgg to reduce memory waste Issues found:\r\n\r\n+ Key container allocation issue in HashTable\r\nCurrently we allocate 2^16 records capacity memory for \"hashValues\" and \"links\" vectors in each BatchHolder, but for \"key\" holders we use the allocateNew which by default allocates low capacity memory compared to \"hashValues\" and \"links\" vector capacities (incase of Integer key, capacity is 4096 records). This causes \"key\" holders to fill up much sooner even though \"hashValues\" and \"links\" vectors still have lot of free entries. As a result we create more BatchHolders than required causing wasted space in \"links\" and \"hashValues\" vectors in each BatchHolder. And for each new BatchHolder we create a SV4 vector in HashJoinHelper which is another overhead.\r\n\r\n+ Allocation issues in HashAggTemplate\r\nHashAggTemplate has its own BatchHolders which has vectors allocated using allocateNew (i.e small capacity). Whenever a BatchHolder in HashAggTemplate reaches its capacity, we add a new BatchHolder in HashTable. This causes the HashTable BatchHolders to be not space efficient.\r\n\r\n+ Update the HashAggTemplate.outputCurrentBatch to consider cases where all entries in a single BatchHolder are can't be copied over to output vectors in a single pass (output vectors capacity is lower than the number of records in BatchHolder)\r\n\r\n+ Lazy BatchHolder creation for both HashAgg and HashTable\r\nDon't allocate the BatchHolder until first put request is received. This way we don't waste space in fragments which don't receive any input records. This is possible when the group by key has very few distinct values (such as shipmode in TPCH) only few fragments receive the data. Our current parallelization code is not considering the distinct values when parallelizing hash exchanges."
    ],
    [
        "DRILL-3976",
        "DRILL-445",
        "Join fails when one table is from JDBC The following simple 3-table join fails when one table is from JDBC.\r\nThe exception is: \r\n\r\norg.apache.drill.common.exceptions.UserRemoteException: SYSTEM ERROR: IllegalStateException: Already had POJO for id (java.lang.Integer) [com.fasterxml.jackson.annotation.ObjectIdGenerator$IdKey@3372bbe8] Fragment 3:0 [Error Id: b22c087f-787e-4d52-8bce-6a330e197cdc on drill:31010] \r\n\r\n{code:sql}\r\nselect ps1.ps_partkey,min(ps1.ps_supplycost)\r\nfrom \r\n    dfs.data.Partsupp_0 ps1, \r\n    dfs.data.Supplier_0 s1, \r\n    sqlite.Nation n1\r\nwhere\r\ns1.s_suppkey = ps1.ps_suppkey\r\nand s1.s_nationkey = n1.n_nationkey\r\nand n1.n_regionkey=1\r\ngroup by ps1.ps_partkey\r\n{code}\r\n\r\nThe dfs plugin is a plain file system, the sqlite is jdbc to sqlite.\r\n\r\nAttached is the generated execution plan for the failed query.",
        "Enable FMPP maven plugin execution with eclipse Came across a little known feature of M2E project that allows not fully compatible (certified) plugin execution to happen during maven project import phase.\r\n\r\nhttp://wiki.eclipse.org/M2E_plugin_execution_not_covered#execute_plugin_goal\r\n\r\nHad to make some more changed but now, importing the drill projects brings the \"apache-drill\\exec\\java-exec\\target\\generated-sources\" in the source fold."
    ],
    [
        "DRILL-2125",
        "DRILL-718",
        "Add input template file in the source files generated by freemarker Currently only some generated source files include information as to which template was used to create the sources. For better readability and modifying the template it'd be good to include which template was used to generate the sources.",
        "Support selecting complex types and displaying it as a JSON string "
    ],
    [
        "DRILL-1322",
        "DRILL-1306",
        "Memory leak when first batch is filtered out completely Simple repro:\r\n\r\nCreate a directory with two files:\r\n\r\na.json\r\n{\"col\" : 1}\r\n\r\nb.json\r\n{\"col\" : 2}\r\n\r\nselect col as col1 from dfs.`/tmp/data2` where col = 2\r\n\r\nRemoving the column aliasing seems to get rid of the problem. \r\n\r\nFollowing is the allocation stack:\r\norg.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:159) org.apache.drill.exec.vector.BigIntVector.allocateNewSafe(BigIntVector.java:135) org.apache.drill.exec.vector.BigIntVector.allocateNew(BigIntVector.java:121) org.apache.drill.exec.vector.NullableBigIntVector.allocateNew(NullableBigIntVector.java:152) org.apache.drill.exec.test.generated.CopierGen1.copyRecords(CopierTemplate2.java:45) org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.doWork(RemovingRecordBatch.java:103) org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:78) org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:97) org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:95) org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:116) org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:75) org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:65) org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:45) org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:120) org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:95) org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:116) org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:59) org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:98) org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:49) org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:105) org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:250) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) java.lang.Thread.run(Thread.java:744) ",
        "Drill jdbc - java security exception: signer information does not match http://maven.corp.maprtech.com/nexus/content/repositories/snapshots/org/apache/drill/exec/drill-jdbc-all/0.5.0-incubating-SNAPSHOT/drill-jdbc-all-0.5.0-incubating-20140813.182517-7.jar\r\n\r\nI was attempting to use the Birst BI tool with the drill jdbc.  When I invoke the BirstConnect, it fails with the following error:\r\n\r\nC:\\BirstConnect\\commandline>\"C:\\Program Files\\Java\\jre7\\bin\\java\" -cp \"C:\\BirstC\r\nonnect\\dist\\*;C:\\BirstConnect\\dist\\lib\\*\" -Djnlp.file=\"C:\\BirstConnect\\2a1e5177-\r\n3dd6-4721-836a-a7c652fbde5f.jnlp\" -Xmx1024m com.birst.dataconductor.DataConducto\r\nrApp\r\nAug 15, 2014 11:23:55 AM org.jdesktop.application.Application$1 run\r\nSEVERE: Application class com.birst.dataconductor.DataConductorApp failed to lau\r\nnch\r\njava.lang.SecurityException: class \"org.apache.log4j.Layout\"'s signer informatio\r\nn does not match signer information of other classes in the same package\r\n        at java.lang.ClassLoader.checkCerts(Unknown Source)\r\n        at java.lang.ClassLoader.preDefineClass(Unknown Source)\r\n        at java.lang.ClassLoader.defineClass(Unknown Source)\r\n        at java.security.SecureClassLoader.defineClass(Unknown Source)\r\n        at java.net.URLClassLoader.defineClass(Unknown Source)\r\n        at java.net.URLClassLoader.access$100(Unknown Source)\r\n        at java.net.URLClassLoader$1.run(Unknown Source)\r\n        at java.net.URLClassLoader$1.run(Unknown Source)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at java.net.URLClassLoader.findClass(Unknown Source)\r\n        at java.lang.ClassLoader.loadClass(Unknown Source)\r\n        at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)\r\n        at java.lang.ClassLoader.loadClass(Unknown Source)\r\n        at com.birst.dataconductor.DataConductorView.<init>(DataConductorView.ja\r\nva:88)\r\n        at com.birst.dataconductor.DataConductorApp.startup(DataConductorApp.jav\r\na:32)\r\n        at org.jdesktop.application.Application$1.run(Application.java:171)\r\n        at java.awt.event.InvocationEvent.dispatch(Unknown Source)\r\n        at java.awt.EventQueue.dispatchEventImpl(Unknown Source)\r\n        at java.awt.EventQueue.access$200(Unknown Source)\r\n        at java.awt.EventQueue$3.run(Unknown Source)\r\n        at java.awt.EventQueue$3.run(Unknown Source)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at java.security.ProtectionDomain$1.doIntersectionPrivilege(Unknown Sour\r\nce)\r\n        at java.awt.EventQueue.dispatchEvent(Unknown Source)\r\n        at java.awt.EventDispatchThread.pumpOneEventForFilters(Unknown Source)\r\n        at java.awt.EventDispatchThread.pumpEventsForFilter(Unknown Source)\r\n        at java.awt.EventDispatchThread.pumpEventsForHierarchy(Unknown Source)\r\n        at java.awt.EventDispatchThread.pumpEvents(Unknown Source)\r\n        at java.awt.EventDispatchThread.pumpEvents(Unknown Source)\r\n        at java.awt.EventDispatchThread.run(Unknown Source)\r\n\r\nException in thread \"AWT-EventQueue-0\" java.lang.Error: Application class com.bi\r\nrst.dataconductor.DataConductorApp failed to launch\r\n        at org.jdesktop.application.Application$1.run(Application.java:177)\r\n        at java.awt.event.InvocationEvent.dispatch(Unknown Source)\r\n        at java.awt.EventQueue.dispatchEventImpl(Unknown Source)\r\n        at java.awt.EventQueue.access$200(Unknown Source)\r\n        at java.awt.EventQueue$3.run(Unknown Source)\r\n        at java.awt.EventQueue$3.run(Unknown Source)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at java.security.ProtectionDomain$1.doIntersectionPrivilege(Unknown Sour\r\nce)\r\n        at java.awt.EventQueue.dispatchEvent(Unknown Source)\r\n        at java.awt.EventDispatchThread.pumpOneEventForFilters(Unknown Source)\r\n        at java.awt.EventDispatchThread.pumpEventsForFilter(Unknown Source)\r\n        at java.awt.EventDispatchThread.pumpEventsForHierarchy(Unknown Source)\r\n        at java.awt.EventDispatchThread.pumpEvents(Unknown Source)\r\n        at java.awt.EventDispatchThread.pumpEvents(Unknown Source)\r\n        at java.awt.EventDispatchThread.run(Unknown Source)\r\nCaused by: java.lang.SecurityException: class \"org.apache.log4j.Layout\"'s signer\r\n information does not match signer information of other classes in the same pack\r\nage\r\n        at java.lang.ClassLoader.checkCerts(Unknown Source)\r\n        at java.lang.ClassLoader.preDefineClass(Unknown Source)\r\n        at java.lang.ClassLoader.defineClass(Unknown Source)\r\n        at java.security.SecureClassLoader.defineClass(Unknown Source)\r\n        at java.net.URLClassLoader.defineClass(Unknown Source)\r\n        at java.net.URLClassLoader.access$100(Unknown Source)\r\n        at java.net.URLClassLoader$1.run(Unknown Source)\r\n        at java.net.URLClassLoader$1.run(Unknown Source)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at java.net.URLClassLoader.findClass(Unknown Source)\r\n        at java.lang.ClassLoader.loadClass(Unknown Source)\r\n        at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)\r\n        at java.lang.ClassLoader.loadClass(Unknown Source)\r\n        at com.birst.dataconductor.DataConductorView.<init>(DataConductorView.ja\r\nva:88)\r\n        at com.birst.dataconductor.DataConductorApp.startup(DataConductorApp.jav\r\na:32)\r\n        at org.jdesktop.application.Application$1.run(Application.java:171)\r\n        ... 14 more"
    ],
    [
        "DRILL-2452",
        "DRILL-1715",
        "ResultSet.getDouble should not throw an exception when the underlying type is a FLOAT git.commit.id.abbrev=e92db23\r\n\r\nCurrently when we call ResultSet.getDouble(int) and if the underlying column's type is FLOAT, we get the below exception\r\n{code}\r\norg.apache.drill.exec.vector.accessor.AbstractSqlAccessor$InvalidAccessException: Requesting class of type double for an object of type FLOAT4:OPTIONAL is not allowed.\r\n\tat org.apache.drill.exec.vector.accessor.AbstractSqlAccessor.getDouble(AbstractSqlAccessor.java:62)\r\n\tat org.apache.drill.exec.vector.accessor.NullableFloat4Accessor.getDouble(NullableFloat4Accessor.java:87)\r\n\tat org.apache.drill.exec.vector.accessor.BoundCheckingAccessor.getDouble(BoundCheckingAccessor.java:73)\r\n\tat org.apache.drill.jdbc.AvaticaDrillSqlAccessor.getDouble(AvaticaDrillSqlAccessor.java:101)\r\n\tat net.hydromatic.avatica.AvaticaResultSet.getDouble(AvaticaResultSet.java:233)\r\n{code}\r\n\r\nAccording to the JDBC spec we should return the double representation of the value",
        "NPE in UnlimitedRawBatchBuffer while releasing the batch In UnlimitedRawBatchBuffer we are currently not checking if a batch's body is empty before attempting to release it. This can happen if its a schema batch which would cause NPE."
    ],
    [
        "DRILL-2808",
        "DRILL-2521",
        "Substr function throws weird message when wrong arguments are passed in #Wed Apr 15 17:37:25 EDT 2015\r\ngit.commit.id.abbrev=cb47df0\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select substr(0, 1, 'abc') from sys.options limit 1;\r\nQuery failed: SYSTEM ERROR: Unexpected exception during fragment initialization: Internal error: Error while applying rule ReduceExpressionsRule[Project], args [rel#53990:ProjectRel.NONE.ANY([]).[](child=rel#53989:Subset#0.ENUMERABLE.ANY([]).[],EXPR$0=SUBSTR(0, 1, 'abc'))]\r\n[1f7e0c35-fc65-4a2f-b668-12e9baf5c9b8 on atsqa4-133.qa.lab:31010]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\ndrillbit.log\r\n{code}\r\n2015-04-16 22:04:32,029 [2acfce0f-8af4-9786-4799-9180b9bac219:foreman] INFO  o.a.drill.exec.work.foreman.Foreman - State change requested.  PENDING --> FAILED\r\norg.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: Internal error: Error while applying rule ReduceExpressionsRule[Project], args [rel#54047:ProjectRel.NONE.ANY([]).[](child=rel#54046:Subset#0.ENUMERABLE.ANY([]).[],EXPR$0=SUBSTR(0, 1, 'abc'))]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:211) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_71]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]\r\nCaused by: java.lang.AssertionError: Internal error: Error while applying rule ReduceExpressionsRule[Project], args [rel#54047:ProjectRel.NONE.ANY([]).[](child=rel#54046:Subset#0.ENUMERABLE.ANY([]).[],EXPR$0=SUBSTR(0, 1, 'abc'))]\r\n        at org.eigenbase.util.Util.newInternal(Util.java:750) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:246) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:661) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at net.hydromatic.optiq.tools.Programs$RuleSetProgram.run(Programs.java:165) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at net.hydromatic.optiq.prepare.PlannerImpl.transform(PlannerImpl.java:278) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToDrel(DefaultSqlHandler.java:196) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:137) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:155) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:770) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:202) [drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        ... 3 common frames omitted\r\nCaused by: java.lang.RuntimeException: Error in evaluating function of castBIGINT\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitFunctionHolderExpression(InterpreterEvaluator.java:323) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitFunctionHolderExpression(InterpreterEvaluator.java:147) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.expression.FunctionHolderExpression.accept(FunctionHolderExpression.java:47) ~[drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitFunctionHolderExpression(InterpreterEvaluator.java:251) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitFunctionHolderExpression(InterpreterEvaluator.java:147) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.expression.FunctionHolderExpression.accept(FunctionHolderExpression.java:47) ~[drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator.evaluateConstantExpr(InterpreterEvaluator.java:65) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.logical.DrillConstExecutor.reduce(DrillConstExecutor.java:180) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.eigenbase.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:410) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.rel.rules.ReduceExpressionsRule$1.onMatch(ReduceExpressionsRule.java:196) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:223) ~[optiq-core-0.9-drill-r21.jar:na]\r\n        ... 11 common frames omitted\r\nCaused by: java.lang.NumberFormatException: abc\r\n        at org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.nfeL(StringFunctionHelpers.java:91) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.varCharToLong(StringFunctionHelpers.java:62) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.impl.gcast.CastVarCharBigInt.eval(CastVarCharBigInt.java:42) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.expr.fn.interpreter.InterpreterEvaluator$EvalVisitor.visitFunctionHolderExpression(InterpreterEvaluator.java:309) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        ... 21 common frames omitted\r\n{code}",
        "Revert from protobuf 2.6 to 2.5 "
    ],
    [
        "DRILL-1424",
        "DRILL-2479",
        "[C++ Client] Publish the binaries of Boost dynamic libraries used in the C++ Client As requested by Parth, we are going to publish the Boost binaries we built for using C++ Client.",
        "Correlated EXISTS containing an IN subquery fails to plan The following correlated EXISTS query gives a CannotPlanException : \r\n{code}\r\n select count(*) from lineitem l where exists (select ps.ps_suppkey from partsupp ps where ps.ps_suppkey = l.l_suppkey and ps.ps_partkey in (select p.p_partkey from part p));\r\n{code}\r\nThe query succeeds if the IN clause in the EXISTS subquery is removed or replaced with an IN list of constants.  It also succeeds on the latest version of Calcite.  Drill is quite out of date with Calcite and there were decorrelation related changes in Calcite sometime mid to late 2014.  This issue should get resolved once Drill is rebased on recent Calcite. "
    ],
    [
        "DRILL-573",
        "DRILL-3676",
        "SUBSTR/SUBSTRING functions of a number requires explicit cast Unlike Oracle, drill requires explicit cast a number to string when used in a substr or substring function.  \r\nHere is the test data:\r\nselect * from voter where voter_id=11;\r\n voter_id | name | age | registration | contributions | voterzone |     create_time     \r\n--------+------+-----+--------------+---------------+-----------+---------------------\r\n     11 |      |  58 | republican   |        578.08 |     16161 | 2014-08-27 06:35:33\r\n\r\nIn drill, an explicit cast is required:\r\n0: jdbc:drill:schema=dfs> select substr(cast(contributions as varchar(8)), 3, 5) from voter where voter_id=11;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 8.08       |\r\n+------------+\r\n\r\nIf explicit cast is not given, drill returns result in some form of binary data:\r\n0: jdbc:drill:schema=dfs> select substr(contributions, 3, 5) from voter where voter_id=11;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| \u382e\u3038         |\r\n+------------+\r\n\r\nIn Oracle, the cast is implicit:\r\nSQL> select substr(contributions, 3, 5) from voter where voter_id=11;\r\n\r\nSUBST\r\n-----\r\n8.08",
        "Group by ordinal number of an output column results in parse error Group by number results in parse error.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select sub_q.col1 from (select col1 from FEWRWSPQQ_101) sub_q group by 1;\r\nError: PARSE ERROR: At line 1, column 8: Expression 'q.col1' is not being grouped\r\n\r\n\r\n[Error Id: 0eedafd9-372e-4610-b7a8-d97e26458d58 on centos-02.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nWhen we use the column name instead of the number, the query compiles and returns results.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select col1 from (select col1 from FEWRWSPQQ_101) group by col1;\r\n+----------------------+\r\n|         col1         |\r\n+----------------------+\r\n| 65534                |\r\n| 10000000             |\r\n| -1                   |\r\n| 0                    |\r\n| 1                    |\r\n| 13                   |\r\n| 17                   |\r\n| 23                   |\r\n| 1000                 |\r\n| 9999999              |\r\n| 30                   |\r\n| 25                   |\r\n| 1001                 |\r\n| -65535               |\r\n| 5000                 |\r\n| 3000                 |\r\n| 200                  |\r\n| 197                  |\r\n| 4611686018427387903  |\r\n| 9223372036854775806  |\r\n| 9223372036854775807  |\r\n| 92233720385475807    |\r\n+----------------------+\r\n22 rows selected (0.218 seconds)\r\n{code}\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select sub_query.col1 from (select col1 from FEWRWSPQQ_101) sub_query group by sub_query.col1;\r\n+----------------------+\r\n|         col1         |\r\n+----------------------+\r\n| 65534                |\r\n| 10000000             |\r\n| -1                   |\r\n| 0                    |\r\n| 1                    |\r\n| 13                   |\r\n| 17                   |\r\n| 23                   |\r\n| 1000                 |\r\n| 9999999              |\r\n| 30                   |\r\n| 25                   |\r\n| 1001                 |\r\n| -65535               |\r\n| 5000                 |\r\n| 3000                 |\r\n| 200                  |\r\n| 197                  |\r\n| 4611686018427387903  |\r\n| 9223372036854775806  |\r\n| 9223372036854775807  |\r\n| 92233720385475807    |\r\n+----------------------+\r\n22 rows selected (0.177 seconds)\r\n{code}"
    ],
    [
        "DRILL-560",
        "DRILL-73",
        "unable to run sqlline quey on latest build kumar@kumar-desktop:/opt/drill$ bin/sqlline -u jdbc:drill:zk=local -n admin -p admin\r\nscan complete in 122ms\r\nscan complete in 17278ms\r\nsqlline version 1.1.6\r\n0: jdbc:drill:zk=local> SELECT \r\n. . . . . . . . . . . >       employee_id, \r\n. . . . . . . . . . . >       first_name\r\n. . . . . . . . . . . >     FROM cp.`employee.json`; \r\nException in thread \"WorkManager-1\" java.lang.IncompatibleClassChangeError: Implementing class\r\n\tat java.lang.ClassLoader.defineClass1(Native Method)\r\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:800)\r\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\r\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:449)\r\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:71)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:361)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\r\n\tat net.hydromatic.optiq.jdbc.OptiqConnectionImpl$OptiqConnectionConfigImpl.caseSensitive(OptiqConnectionImpl.java:407)\r\n\tat net.hydromatic.optiq.jdbc.OptiqConnectionImpl.<init>(OptiqConnectionImpl.java:84)\r\n\tat net.hydromatic.optiq.jdbc.OptiqJdbc41Factory$OptiqJdbc41Connection.<init>(OptiqJdbc41Factory.java:104)\r\n\tat net.hydromatic.optiq.jdbc.OptiqJdbc41Factory.newConnection(OptiqJdbc41Factory.java:50)\r\n\tat net.hydromatic.optiq.jdbc.OptiqJdbc41Factory.newConnection(OptiqJdbc41Factory.java:35)\r\n\tat net.hydromatic.optiq.jdbc.OptiqFactory.newConnection(OptiqFactory.java:53)\r\n\tat net.hydromatic.avatica.UnregisteredDriver.connect(UnregisteredDriver.java:121)\r\n\tat java.sql.DriverManager.getConnection(DriverManager.java:571)\r\n\tat java.sql.DriverManager.getConnection(DriverManager.java:233)\r\n\tat net.hydromatic.optiq.tools.Frameworks.withPrepare(Frameworks.java:124)\r\n\tat net.hydromatic.optiq.tools.Frameworks.withPlanner(Frameworks.java:93)\r\n\tat net.hydromatic.optiq.prepare.PlannerImpl.ready(PlannerImpl.java:125)\r\n\tat net.hydromatic.optiq.prepare.PlannerImpl.parse(PlannerImpl.java:154)\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:78)\r\n\tat org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:323)\r\n\tat org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:175)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)",
        "Web Site Update Create a nicer looking site. Will hopefully help with our project image and attracting new contributors."
    ],
    [
        "DRILL-3355",
        "DRILL-1284",
        "Implement ResultSetMetadata's getPrecision, getScale, getColumnDisplaySize (need RPC-level data) JDBC ResultSetMetadata methods getPrecision(...), getScale(...), and getColumnDisplaySize() are not implemented, currently because required data is not available in the RPC-level data.\r\n\r\nThe unavailable data includes:\r\n- string type lengths (N in VARCHAR(N), BINARY(N))\r\n- interval qualifier information (which units, leading digit precision, fractional seconds precision)\r\n- datetime type fractional seconds precision\r\n\r\n(Whether an interval is a YEAR/MONTH interval or is a DAY/HOUR/MINUTE/SECOND interval is available.)\r\n\r\n",
        "Better documentation describing the accepted JSON data input Since JSON data input can be very diversified, and Drill, so far, accepts only tuples defined in one mongimport format, it would be useful to have this clearly spelled out in the documentation.\r\n\r\nThis improvement to the documentation should stress:\r\n# The accepted JSON input format (single Objects describeing tuples, every one of them in a single line, with no separator between lines);\r\n# The homogeneous Arrays constraint: only elements of the same type are accepted in a JSON array in Drill;\r\n#  The need to address a single element of a JSON array in a Drill SQL query: {{SELECT foo\\[2\\] FROM bar.json}} is accepted, {{SELECT foo FROM bar.json}} is not\r\n"
    ],
    [
        "DRILL-1783",
        "DRILL-103",
        "Please create a DOAP file for your TLP Please can you set up a DOAP for your project and get it added to files.xml?\r\n\r\nPlease see http://projects.apache.org/create.html\r\n\r\nOnce you have created the DOAP and committed it to your source code repository, please submit it for inclusion in the Apache projects listing as per:\r\n\r\nhttp://projects.apache.org/create.html#submit\r\n\r\nRemember, if you ever move or rename the doap file in future, please\r\nensure that files.xml is updated to point to the new location.\r\n\r\nThanks!",
        "S3 support with range indexing "
    ],
    [
        "DRILL-3796",
        "DRILL-3761",
        "Provide an iterator over record batches In order to simplify merge join operator provide an iterator over record batch that has seek(), mark(), reset().\r\n",
        "CastIntDecimal implementation should not update the input holder.  CastIntDecimal implementation would update the input holder's value, which may cause some side effect. This is especially true, when the run-time generated code tries to re-use the holder for common expressions. \r\n\r\nIn general, Drill's build-in/UDF implementation had better not modify the input holder.\r\n\r\n"
    ],
    [
        "DRILL-3935",
        "DRILL-1535",
        "Modify VarCharWriter to allow passing byte[] In RecordReader implementations, one sometimes has variable-length string data in the form of byte arrays. It is a cumbersome idiom to pack these into a DrillBuf, only to pass it on to the write. \r\n\r\nI propose adding this method to the VarCharWriter interface: public void writeVarChar(int start, int end, byte[] bytes);\r\n\r\nWithout this I currently have to do:\r\nbyte[] bytes = value.getBytes(Charsets.UTF_8);\r\nbuffer.reallocIfNeeded(bytes.length);\r\nbuffer.setBytes(0, bytes);\r\nwriter.varChar().writeVarChar(0, bytes.length, buffer);\r\n\r\n",
        "Error in LogicalPlan deserialization for Join queries Error in Json LogicalPlan deserialization. Misplaced @JsonProperty annotation."
    ],
    [
        "DRILL-724",
        "DRILL-731",
        "select two columns with order by produce wrong results postgres:\r\n\r\nfoodmart=# select c_row, c_int from data order by c_row;\r\n c_row |    c_int\r\n-------+-------------\r\n     1 |           0\r\n     2 |           1\r\n     3 |          -1\r\n     4 |          12\r\n     5 |         123\r\n     6 |    92032039\r\n     7 |   -23395000\r\n     8 |   -99392039\r\n     9 | -2147483648\r\n    10 |  2147483647\r\n    11 |       32767\r\n    12 |      -32767\r\n    13 |       49032\r\n    14 |    -4989385\r\n    15 |    69834830\r\n    16 |         243\r\n    17 |     -426432\r\n    18 |       -3904\r\n    19 |   489392758\r\n    20 |   589032574\r\n    21 |   340000504\r\n    22 |           0\r\n    23 |           1\r\n(23 rows)\r\n\r\ndrill:\r\n\r\n0: jdbc:drill:schema=dfs> select c_row, c_int from data order by c_row;\r\n+------------+------------+\r\n|   c_int    |   c_row    |\r\n+------------+------------+\r\n| 1          | 0          |\r\n| 1          | 2          |\r\n| -1         | 3          |\r\n| 12         | 4          |\r\n| 123        | 5          |\r\n| 92032039   | 6          |\r\n| -23395000  | 7          |\r\n| -99392039  | 8          |\r\n| -2147483648 | 9          |\r\n| 10         | 2147483647 |\r\n| 32767      | 11         |\r\n| -32767     | 12         |\r\n| 13         | 49032      |\r\n| -4989385   | 14         |\r\n| 69834830   | 15         |\r\n| 243        | 16         |\r\n| -426432    | 17         |\r\n| -3904      | 18         |\r\n| 489392758  | 19         |\r\n| 589032574  | 20         |\r\n| 340000504  | 21         |\r\n| 22         | 0          |\r\n| 1          | 23         |\r\n+------------+------------+\r\n23 rows selected (10.932 seconds)\r\n\r\nnotice for drill, on some of the rows, the columns are exchanged.\r\n\r\n",
        "Speedup TestConvertFunctions test case The file has 45 test cases and takes between 1-2 minutes to complete (without doing things which would justify such time)."
    ],
    [
        "DRILL-1205",
        "DRILL-3399",
        "group by causes access to DeadBuf Might be related to drill-922\r\n\r\nThis runs just fine:\r\nselect count(*) as incidents from dfs.`/tmp/Downloads/Crimes_-_2001_to_present.csv` where columns[3] in (SELECT columns[2] FROM dfs.`/tmp/Downloads/Sex_Offenders.csv`);\r\n\r\nThis fails:\r\nselect count(*) as incidents, columns[5] as type, columns[6] as subtype from dfs.`/tmp/Downloads/Crimes_-_2001_to_present.csv` \r\nwhere columns[3] in (SELECT columns[2] FROM dfs.`/tmp/Downloads/Sex_Offenders.csv`)\r\ngroup by columns[6], columns[5]\r\norder by incidents desc limit 10;\r\n\r\nError:\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"26218e5d-5748-43b9-bc77-ab82d9a1c509\"\r\nendpoint {\r\n  address: \"myserver\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < IndexOutOfBoundsException:[ index: 28388, length: 4 (expected: range(0, 28388)) ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n0: jdbc:drill:zk=local> Exception in thread \"e5e6763f-6d42-42b8-9a75-e972c9cf90c2:frag:0:0 - Producer Thread\" java.lang.UnsupportedOperationException: Attemped to access a DeadBuf. This would happen if you attempted to interact with a buffer that has been moved or not yet initialized.\r\n\tat org.apache.drill.exec.record.DeadBuf.capacity(DeadBuf.java:70)\r\n\tat org.apache.drill.exec.vector.UInt4Vector.getValueCapacity(UInt4Vector.java:106)\r\n\tat org.apache.drill.exec.vector.VarCharVector.getValueCapacity(VarCharVector.java:120)\r\n\tat org.apache.drill.exec.vector.RepeatedVarCharVector.getValueCapacity(RepeatedVarCharVector.java:113)\r\n\tat org.apache.drill.exec.vector.RepeatedVarCharVector$Mutator.startNewGroup(RepeatedVarCharVector.java:379)\r\n\tat org.apache.drill.exec.store.text.DrillTextRecordReader.next(DrillTextRecordReader.java:123)\r\n\tat org.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:138)\r\n\tat org.apache.drill.exec.physical.impl.producer.ProducerConsumerBatch$Producer.run(ProducerConsumerBatch.java:122)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n",
        "document how to get the Drill views definition Document (if not already covered) how to get a view definition, which is sql for which the drill view is created:\r\n{code}\r\nselect VIEW_DEFINITION from INFORMATION_SCHEMA.VIEWS where TABLE_NAME ='your_view_name';\r\n{code}\r\nIn Drill, a view is just a JSON file, which will live within the workspace where you saved it. Example:\r\n{code}\r\n create or replace view dfs.workspace.myview as select * from mytable;\r\n{code}\r\n\r\n It will create a file called 'myview.view.drill' , which will look\r\nsomething like this:\r\n{code}\r\n {\r\n \"name\" : \"testview\",\r\n  \"sql\" : \"SELECT *\\nFROM `drill/new.json`\\nFETCH NEXT 10 ROWS ONLY\",\r\n  \"fields\" : [ {\r\n    \"name\" : \"*\",\r\n    \"type\" : \"ANY\",\r\n   \"isNullable\" : true\r\n } ],\r\n   \"workspaceSchemaPath\" : [ \"dfs\", \"workspace\" ]\r\n }\r\n{code}"
    ],
    [
        "DRILL-738",
        "DRILL-1384",
        "Parse failure for REPLACE function The REPLACE function which used to work before now fails with the following error:\r\n\r\nselect replace(name, 'or', 'EA') from voter where name like 'victor%';\r\n\r\nmessage: \"Failure while parsing sql. < SqlParseException:[ Encountered \"replace\" at line 1, column 8.\r\nWas expecting one of:\r\n    \"UNION\" ...\r\n    \"INTERSECT\" ...\r\n    \"EXCEPT\" ...\r\n.\r\n.\r\n.",
        "Rebase Drill on Calcite v1.0 This is a tracking item to ensure that all changes that are done to Drill's Optiq branch are pushed back (as possible) into the mainline."
    ],
    [
        "DRILL-3048",
        "DRILL-1148",
        "Disable assertions by default ",
        "Using multiple filters with Text files fails to compile the runtime generated code git.commit.id.abbrev=810a204\r\n\r\nThe below query fails :\r\n\r\nselect columns[0] widecol1 from `file1.tsv` where columns[0] > 999 and columns[1] > 999;\r\n\r\nHowever things work as expected when we use only one filter:\r\n\r\nselect columns[0] widecol1 from `file1.tsv` where columns[0] > 999\r\n\r\nI attached the relevant logs for the failed case\r\n"
    ],
    [
        "DRILL-844",
        "DRILL-3331",
        "hit java.lang.IndexOutOfBoundsException while querying some large data set 0: jdbc:drill:schema=dfs> SELECT columns[13] from `/user/root/cust-d1.tsv` where columns[13] like '%noticias%';\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\njava.lang.IndexOutOfBoundsException\r\n\tat io.netty.buffer.EmptyByteBuf.checkIndex(EmptyByteBuf.java:857)\r\n\tat io.netty.buffer.EmptyByteBuf.getBytes(EmptyByteBuf.java:321)\r\n\tat org.apache.drill.exec.vector.VarCharVector$Accessor.get(VarCharVector.java:325)\r\n\tat org.apache.drill.exec.vector.VarCharVector$Accessor.getObject(VarCharVector.java:345)\r\n\tat org.apache.drill.exec.vector.accessor.VarCharAccessor.getObject(VarCharAccessor.java:94)\r\n\tat org.apache.drill.jdbc.AvaticaDrillSqlAccessor.getObject(AvaticaDrillSqlAccessor.java:136)\r\n\tat net.hydromatic.avatica.AvaticaResultSet.getObject(AvaticaResultSet.java:336)\r\n\tat sqlline.SqlLine$Rows$Row.<init>(SqlLine.java:2388)\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2504)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n\r\nno error in Lilith\r\n\r\nIt will hang in sqlline if you just run select columns[13] from `/user/root/cust-d1.tsv` ;\r\n\r\nI have the data set and just ask for location for it.",
        "Provide the BufferManager at DrillBuf construction time DrillBuf.setBufferManager() allows for providing a buffer manager to handle reallocation and freeing by operators.\r\n\r\nAt this time, the BufferManager is not used by everything else yet. OperatorContextImpl has not yet been converted to use the BufferManager. Once it has been, then a DrillBuf either uses the BufferManager or it doesn't, and this argument can be surfaced on its constructors, and passed through (optionally null) from the allocator's buffer() call."
    ],
    [
        "DRILL-174",
        "DRILL-2847",
        "Physical plans for hash-partition-sender operator may not be evenly distributed The physical plan in {{test/resources/sender/hash_exchange.json}} is transformed into a hash-partition-sender op with only one host, while it should have two when run from TestHashToRandomExchange.java.\r\n\r\n{noformat}\r\nAttempting to read {\r\n  pop : \"hash-partition-sender\",\r\n  @id : 1,\r\n  receiver-major-fragment : 1,\r\n  child : {\r\n    pop : \"mock-sub-scan\",\r\n    @id : 2,\r\n    url : \"http://apache.org\",\r\n    entries : [ {\r\n      records : 100,\r\n      types : [ {\r\n        name : \"blue\",\r\n        type : \"INT\",\r\n        mode : \"REQUIRED\"\r\n      }, {\r\n        name : \"red\",\r\n        type : \"BIGINT\",\r\n        mode : \"REQUIRED\"\r\n      }, {\r\n        name : \"green\",\r\n        type : \"INT\",\r\n        mode : \"REQUIRED\"\r\n      } ]\r\n    } ]\r\n  },\r\n  expr : \"hash(red) \",\r\n  destinations : [ \"Cg4xOTIuMTY4LjE2OC45NhCk8gEYpfIB\" ]\r\n}\r\n{noformat}",
        "DrillBufs from the RPC layer are being leaked I've created a patch that demonstrates this. In the patch, code is added to UnsafeDirectLittleEndian to track all the instances of that class that are created (which happens when buffers are allocated inside TopLevelAllocator). release() is overridden to remove these from the tracked list when they are released. An @After action is added to TestTpchDistributed which checks on the count of outstanding buffers. If the test is run, every case fails, and each failure shows a progressively larger and larger number of outstanding buffers. There are no complaints from the allocator."
    ],
    [
        "DRILL-2578",
        "DRILL-1375",
        "Add new workspace by saving a JSON file into a directory on a filesystem This is a feature request regarding Drill workspaces.\r\n\r\nI would like to see a feature where you can save a JSON file into a workspace directory on a filesystem which defines a workspace instead of always having to use the web GUI or a web request via a tool like CURL. Drill can then load these workspaces from this directory. The web GUI could also be modified to pick up/save any workspace configs in this directory so all workspaces are stored here (maybe this could be on HDFS/MapR-FS - I am not sure where workspace configs are actually stored at the moment).\r\n\r\nEach JSON file could be a separate workspace:\r\n\r\nworkspace1.json\r\nworkspace2.json \r\netc...\r\n",
        "Drill UI is not displaying appropriate error messages for query related failures git.commit.id.abbrev=711cc2f\r\n\r\nWhen a failure occurs such as trying to execute an invalid query, the UI would display the following error message:\r\n\r\n        HTTP ERROR 500\r\n        Problem accessing /query. Reason:\r\n        Request failed.\r\n\r\nWe should display the appropriate \"Reason\"."
    ],
    [
        "DRILL-1647",
        "DRILL-3624",
        "Flatten operator cannot be used twice in a select clause ",
        "Enhance Web UI to be able to select schema (\"use\") it would be advantageous to be able to select a schema (\"use\") in the web ui, so that the information does not always have to be specified in each query.\r\n\r\nthis could be realized e.g. through a drop down where the user selects the schema from the list of available schemas. the ui should store this information until a different schema is selected."
    ],
    [
        "DRILL-2888",
        "DRILL-3318",
        "C++ Client: Setting MIN and MAX for y2038 time library for Windows platform Edge cases fail if the values are not correctly set as per the submitted patch. Basically the scope within which time values are processes/transmitted correctly expands by the correct values.",
        "SUM(CAST(col as INT)) shows different results when used in window functions I have a parquet file that contains an INT field with large enough values so that the sum of the values overflows the INT limits, and a VARCHAR column with one single value (to force all rows to be part of the same partition).\r\n\r\nComputing the sums without casting will give similar results:\r\n{noformat}\r\nSELECT SUM(col_int) OVER(PARTITION BY col_char) FROM `3278.parquet` LIMIT 1;\r\n+--------------+\r\n|    EXPR$0    |\r\n+--------------+\r\n| -3216087191  |\r\n+--------------+\r\n{noformat}\r\n\r\n{noformat}\r\nSELECT SUM(col_int) FROM `3278.parquet`;\r\n+--------------+\r\n|    EXPR$0    |\r\n+--------------+\r\n| -3216087191  |\r\n+--------------+\r\n{noformat}\r\n\r\nBut if we cast the column before doing the sum, the results are now different:\r\n{noformat}\r\nSELECT SUM(CAST(col_int AS INT)) OVER(PARTITION BY col_char) FROM `3278.parquet` LIMIT 1;\r\n+-------------+\r\n|   EXPR$0    |\r\n+-------------+\r\n| 1078880105  |\r\n+-------------+\r\n{noformat}\r\n\r\n{noformat}\r\nSELECT SUM(CAST(col_int AS INT)) FROM `3278.parquet`;\r\n+--------------+\r\n|    EXPR$0    |\r\n+--------------+\r\n| -3216087191  |\r\n+--------------+\r\n{noformat}"
    ],
    [
        "DRILL-3548",
        "DRILL-4264",
        "Add/edit various Javadoc(/etc.) (plugin-exploration--related) ",
        "Dots in identifier are not escaped correctly If you have some json data like this...\r\n{code:javascript}\r\n    {\r\n      \"0.0.1\":{\r\n        \"version\":\"0.0.1\",\r\n        \"date_created\":\"2014-03-15\"\r\n      },\r\n      \"0.1.2\":{\r\n        \"version\":\"0.1.2\",\r\n        \"date_created\":\"2014-05-21\"\r\n      }\r\n    }\r\n{code}\r\n... there is no way to select any of the rows since their identifiers contain dots and when trying to select them, Drill throws the following error:\r\n\r\nError: SYSTEM ERROR: UnsupportedOperationException: Unhandled field reference \"0.0.1\"; a field reference identifier must not have the form of a qualified name\r\n\r\nThis must be fixed since there are many json data files containing dots in some of the keys (e.g. when specifying version numbers etc)\r\n"
    ],
    [
        "DRILL-1450",
        "DRILL-1586",
        "Window function when window definition does not have order by clause would hit assertion error.  This query will hit as\r\n\r\nQ1:\r\n\r\nselect sum(position_id) over w from cp.`employee.json` window w as ( partition by position_id)\r\n\r\nStack trace:\r\n\r\njava.lang.AssertionError\r\n     at org.eigenbase.sql.validate.SqlValidatorUtil.lookup(SqlValidatorUtil.java:242) ~[optiq-core-0.9-drill-r3.jar:na]\r\n     at org.eigenbase.sql.SqlIdentifier.getMonotonicity(SqlIdentifier.java:261) ~[optiq-core-0.9-drill-r3.jar:na]\r\n     at org.eigenbase.sql.validate.SelectScope.getMonotonicity(SelectScope.java:149) ~[optiq-core-0.9-drill-r3.jar:na]\r\n     at org.eigenbase.sql.SqlWindow.isTableSorted(SqlWindow.java:340) ~[optiq-core-0.9-drill-r3.jar:na]\r\n     at org.eigenbase.sql.SqlWindow.validate(SqlWindow.java:561) ~[optiq-core-0.9-drill-r3.jar:na]\r\n\r\nThe cause of this issue is the star column in schema-less system. Drill currently will by default add \"*\" to a schema-less table.  In optiq, * would be expanded to a list of regular columns if table is schema-aware, but does not expand for schema-less table (That's the behavior starting from optiq 0.9-drill-r2).  That would cause issue in some optiq code logic, since it does not expect * ( should already be expanded).\r\n\r\nFor Q1, we could remove the logic of adding * by default to schema-less table, and it would work fine. However, if the query explicitly ask for *, then still, run into similar problem:\r\n\r\nQ2:\r\n\r\nselect *, sum(position_id) over w from cp.`employee.json` window w as ( partition by position_id)\r\n\r\nTherefore, the fix had better to be in Optiq. \r\n ",
        "NPE when the collection being queried for does not exist in Mongo DB NPE when the collection being queried for does not exist in Mongo DB."
    ],
    [
        "DRILL-2824",
        "DRILL-1911",
        "Function resolution should be deterministic Currently as part of function resolution we cost all the possible function matches and pick the one with the best cost. However we simply pick the first one with the best cost, there may be a possibility that we have multiple functions that could have the same best cost and based on which function was first in the map we will execute different functions on different clusters. This JIRA aims to resolve functions in a deterministic way so we pick the same function consistently. ",
        "Querying same field multiple times with different case would hit memory leak and return incorrect result.  git.commit.id.abbrev=309e1be\r\n\r\nIf query the same field twice, with different case, Drill will throw memory assertion error. \r\n\r\n select employee_id, Employee_id from cp.`employee.json` limit 2;\r\n+-------------+\r\n| employee_id |\r\n+-------------+\r\n| 1           |\r\n| 2           |\r\nQuery failed: Query failed: Failure while running fragment., Attempted to close accountor with 2 buffer(s) still allocatedfor QueryId: 2b5cc8eb-2817-aadb-e0fa-49272796592a, MajorFragmentId: 0, MinorFragmentId: 0.\r\n\r\n\r\n     Total 1 allocation(s) of byte size(s): 4096, at stack location:\r\n          org.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n          org.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n          org.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n          org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.doAlloc(ProjectRecordBatch.java:229)\r\n          org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.doWork(ProjectRecordBatch.java:167)\r\n          org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:93)\r\n          org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:132)\r\n          org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142)\r\n          org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118)\r\n          org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n          org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:97)\r\n          org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57)\r\n          org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:114)\r\n          org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254)\r\n          java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n          java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n          java.lang.Thread.run(Thread.java:744)\r\n\r\n\r\nAlso, notice that the query result only contains one field; the second field is missing. \r\n\r\nThe plan looks fine.\r\n\r\nDrill Physical : \r\n00-00    Screen: rowcount = 463.0, cumulative cost = {1900.3 rows, 996.3 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 103\r\n00-01      Project(employee_id=[$0], Employee_id=[$1]): rowcount = 463.0, cumulative cost = {1854.0 rows, 950.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 102\r\n00-02        SelectionVectorRemover: rowcount = 463.0, cumulative cost = {1391.0 rows, 942.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 101\r\n00-03          Limit(fetch=[2]): rowcount = 463.0, cumulative cost = {928.0 rows, 479.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 100\r\n00-04            Project(employee_id=[$0], Employee_id=[$0]): rowcount = 463.0, cumulative cost = {926.0 rows, 471.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 99\r\n00-05              Scan(groupscan=[EasyGroupScan [selectionRoot=/employee.json, numFiles=1, columns=[`employee_id`], files=[/employee.json]]]): rowcount = 463.0, cumulative cost = {463.0 rows, 463.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 98\r\n"
    ],
    [
        "DRILL-136",
        "DRILL-4035",
        "Re-use buffers via a pool We should re-use buffers rather than having to re-allocate each one.",
        "NPE seen on Functional test run using JDK 8 I am seeing an NPE in the Functional test run using JDK8 and Drill 1.3\r\n\r\nFailing test is : Functional/partition_pruning/hive/parquet/dynamic_hier_intint/data/parquetCount1.q\r\nselect count(*) from hive.dynamic_partitions.lineitem_parquet_partitioned_hive_hier_intint;\r\n\r\n{code}\r\nDrill version was, git.commit.id=e4b94a78\r\n\r\nroot@centos drill-1.3.0]# java -version\r\nopenjdk version \"1.8.0_65\"\r\nOpenJDK Runtime Environment (build 1.8.0_65-b17)\r\nOpenJDK 64-Bit Server VM (build 25.65-b01, mixed mode)\r\n[root@centos drill-1.3.0]# javac -version\r\njavac 1.8.0_65\r\n{code}\r\n\r\n{code}\r\n2015-11-05 01:37:45 INFO  DrillTestJdbc:76 - running test /root/public_framework/drill-test-framework/framework/resources/Functional/window_functions/last_val/lastValFn_9.q 981260622\r\n2015-11-05 01:37:45 INFO  DrillResultSetImpl$ResultsListener:1470 - [#137] Query failed:\r\noadd.org.apache.drill.common.exceptions.UserRemoteException: SYSTEM ERROR: NullPointerException\r\n\r\nFragment 0:0\r\n\r\n[Error Id: cefc7238-a646-4f9a-b4f2-0bd102efe393 on centos-01.qa.lab:31010]\r\n        at oadd.org.apache.drill.exec.rpc.user.QueryResultHandler.resultArrived(QueryResultHandler.java:118)\r\n        at oadd.org.apache.drill.exec.rpc.user.UserClient.handleReponse(UserClient.java:110)\r\n        at oadd.org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:47)\r\n        at oadd.org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:32)\r\n        at oadd.org.apache.drill.exec.rpc.RpcBus.handle(RpcBus.java:61)\r\n        at oadd.org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:233)\r\n        at oadd.org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:205)\r\n        at oadd.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n        at oadd.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n        at oadd.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n        at oadd.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n        at oadd.io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n        at oadd.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n        at oadd.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)\r\n        at oadd.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)\r\n        at oadd.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)\r\n        at oadd.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\r\n        at oadd.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\r\n        at oadd.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\r\n        at oadd.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\r\n        at java.lang.Thread.run(Thread.java:745)\r\n{code}"
    ],
    [
        "DRILL-857",
        "DRILL-3174",
        "localtime function returns incorrect result git.commit.id.abbrev=01bf849\r\n\r\n0: jdbc:drill:schema=dfs> select localtime from voter where voter_id=10;\r\n+------------+\r\n| localtime  |\r\n+------------+\r\n| 1970-01-01T08:02:38.332-08:00 |\r\n",
        "Calcite blocks queries whose type-missmatch can be resolved by Drill's Implicit casting {code}\r\nselect a from ... union all select b from ...\r\n{code}\r\n\r\nwhere a is int, and b is a bunch of integers in varchar types. Drill-Calcite interrupts this query by the column types. Since Drill has its own ways of handling type-mismatch, can we let Calcite ignore type checking?\r\n\r\n"
    ],
    [
        "DRILL-640",
        "DRILL-309",
        "Limit operator leaks memory when used with exchanges Limit operator currently invokes kill() when it reaches the desired limit of records. Current semantic of kill() does not stop upstream operators if they are across exchanges. So its the responsibility of the operator to drain out all the record batches and clear the memory, otherwise we will hit memory leaks. ",
        "Optiq uppercases column names Optiq uppercases column names. In a few places in Drill we've had to modify the comparison checks between the project expression and the actual field name to be case insensitive. \r\n\r\nSeems like Optiq is introducing project expressions with upper case column names. Need further investigation.  "
    ],
    [
        "DRILL-3324",
        "DRILL-2721",
        "CTAS broken with the new auto partition feature ( Not in master yet) git.commit.id.abbrev=1f02105\r\n\r\nI tried running a simple ctas query from stevens branch (https://github.com/StevenMPhillips/incubator-drill/tree/partitioning3) which contains the auto partition feature, and it failed with an IOOB exception. (\r\n\r\n{code}\r\ncreate table l as select l_orderkey, l_linenumber from cp.`tpch/lineitem.parquet`;\r\nError: SYSTEM ERROR: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: a6696e99-f1c6-4ee8-abf0-a869a829a0a9 on qa-node190.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nThis is just plain old ctas without the new auto partition feature which is broken.\r\n\r\nAttached the log file.",
        "Identify, fix _existing_ INFO._SCHEMA columns in conflict with SQL spec. by 1.0 Much of the INFORMATION_SCHEMA.COLUMNS table doesn't follow the SQL specification.\r\n\r\nMost  cases are columns that simply aren't implemented yet, because they are of lower priority, probably because they won't be useful in Drill.\r\n\r\nHowever, some columns that already exist conflict with what the SQL specification specifies for them.  (For example, non-applicable precision values are represented as -1 rather than NULL.)\r\n\r\nThose existing columns should probably be corrected before GA/version 1.0, so that users don't start depending on invalid values, which will make it harder to correct things later). \r\n\r\nTBD:  enumeration of specific problems/corrections\r\n(rough notes of current awareness:\r\n- some -1 should be NULL\r\n- some -1/NULL for numeric precision should be 0\r\n- floating-point numeric precision should be in bits, not digits\r\n- it might be that type names should be canonical names, not syntactic-shortcut names (e.g., \"VARCHAR\" -> \"CHARACTER_VARYING\")\r\n- character-type length and numeric precision might be wrongly combined into same column\r\n)\r\n\r\n"
    ],
    [
        "DRILL-400",
        "DRILL-472",
        "Parquet varbinary data is not cast by default Not sure if this is a bug or if we even want to address this.\r\nWhen you do a select from a parquet table where strings are stored as varbinary drill displays them as such. The user needs to figure out what the type is cast accordingly. We may need to think of a more user friendly solution. \r\n\r\n0: jdbc:drill:schema=dfs> select * from dfs.`region.parquet`;\r\n+-------------+------------+------------+\r\n| R_REGIONKEY |   R_NAME   | R_COMMENT  |\r\n+-------------+------------+------------+\r\n| 0           | [B@a2a95bb | [B@71cc2d33 |\r\n| 1           | [B@13aba9a3 | [B@6b0e166b |\r\n| 2           | [B@69e201b5 | [B@74de4861 |\r\n| 3           | [B@2d621438 | [B@21d7fcf |\r\n| 4           | [B@5eb1f01e | [B@740b7223 |\r\n+-------------+------------+------------+\r\n",
        "Enhance HBase engine to expose column families in schema, columns as varbytes We should expose the hbase schema we know rather than using a basic DrillDynamicTable.  "
    ],
    [
        "DRILL-2946",
        "DRILL-1205",
        "Tableau 9.0 Desktop Enablement Document Documentation for Tableau 9.0 Desktop enablement.\r\n\r\nIncludes authentication with Drill 0.9 and later.",
        "group by causes access to DeadBuf Might be related to drill-922\r\n\r\nThis runs just fine:\r\nselect count(*) as incidents from dfs.`/tmp/Downloads/Crimes_-_2001_to_present.csv` where columns[3] in (SELECT columns[2] FROM dfs.`/tmp/Downloads/Sex_Offenders.csv`);\r\n\r\nThis fails:\r\nselect count(*) as incidents, columns[5] as type, columns[6] as subtype from dfs.`/tmp/Downloads/Crimes_-_2001_to_present.csv` \r\nwhere columns[3] in (SELECT columns[2] FROM dfs.`/tmp/Downloads/Sex_Offenders.csv`)\r\ngroup by columns[6], columns[5]\r\norder by incidents desc limit 10;\r\n\r\nError:\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"26218e5d-5748-43b9-bc77-ab82d9a1c509\"\r\nendpoint {\r\n  address: \"myserver\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < IndexOutOfBoundsException:[ index: 28388, length: 4 (expected: range(0, 28388)) ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n0: jdbc:drill:zk=local> Exception in thread \"e5e6763f-6d42-42b8-9a75-e972c9cf90c2:frag:0:0 - Producer Thread\" java.lang.UnsupportedOperationException: Attemped to access a DeadBuf. This would happen if you attempted to interact with a buffer that has been moved or not yet initialized.\r\n\tat org.apache.drill.exec.record.DeadBuf.capacity(DeadBuf.java:70)\r\n\tat org.apache.drill.exec.vector.UInt4Vector.getValueCapacity(UInt4Vector.java:106)\r\n\tat org.apache.drill.exec.vector.VarCharVector.getValueCapacity(VarCharVector.java:120)\r\n\tat org.apache.drill.exec.vector.RepeatedVarCharVector.getValueCapacity(RepeatedVarCharVector.java:113)\r\n\tat org.apache.drill.exec.vector.RepeatedVarCharVector$Mutator.startNewGroup(RepeatedVarCharVector.java:379)\r\n\tat org.apache.drill.exec.store.text.DrillTextRecordReader.next(DrillTextRecordReader.java:123)\r\n\tat org.apache.drill.exec.physical.impl.ScanBatch.next(ScanBatch.java:138)\r\n\tat org.apache.drill.exec.physical.impl.producer.ProducerConsumerBatch$Producer.run(ProducerConsumerBatch.java:122)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n"
    ],
    [
        "DRILL-3562",
        "DRILL-3682",
        "Query fails when using flatten on JSON data where some documents have an empty array Drill query fails when using flatten when some records contain an empty array \r\n{noformat}\r\nSELECT COUNT(*) FROM (SELECT FLATTEN(t.a.b.c) AS c FROM dfs.`flat.json` t) flat WHERE flat.c.d.e = 'f' limit 1;\r\n{noformat}\r\n\r\nSucceeds on \r\n{ \"a\": { \"b\": { \"c\": [  { \"d\": {  \"e\": \"f\" } } ] } } }\r\n\r\nFails on\r\n{ \"a\": { \"b\": { \"c\": [] } } }\r\n\r\nError\r\n{noformat}\r\nError: SYSTEM ERROR: ClassCastException: Cannot cast org.apache.drill.exec.vector.NullableIntVector to org.apache.drill.exec.vector.complex.RepeatedValueVector\r\n{noformat}\r\n\r\nIs it possible to ignore the empty arrays, or do they need to be populated with dummy data?",
        "Inaccurate documentation for contributors Trying to follow current (8/20/2015 Drill 1.1.0) instructions to set up the patch contribution environments:\r\nhttps://drill.apache.org/docs/drill-patch-review-tool/#jira-command-line-tool\r\n\r\nWhen I try:\r\nsudo easy_install jira-python\r\n\r\nI get:\r\n\r\nReading https://pypi.python.org/simple/jira-python/\r\n\r\nDownload error on https://pypi.python.org/simple/jira-python/: unknown url type: https -- Some packages may not be found!\r\n\r\nCouldn't find index page for 'jira-python' (maybe misspelled?)\r\n\r\nScanning index of all packages (this may take a while)\r\n\r\nReading https://pypi.python.org/simple/\r\n\r\nDownload error on https://pypi.python.org/simple/: unknown url type: https -- Some packages may not be found!\r\n\r\nNo local packages or download links found for jira-python\r\n\r\nerror: Could not find suitable distribution for Requirement.parse('jira-python')\r\n\r\nI think this should be sudo easy_install jira, or pip install jira\r\n\r\nhttps://pypi.python.org/pypi/jira-cli\r\nor \r\nhttp://pythonhosted.org/jira/"
    ],
    [
        "DRILL-4268",
        "DRILL-2206",
        "Possible resource leak leading to SocketException: Too many open files I have a java app accessing Drill 1.2 via JDBC, which runs 100s of counts on various tables. No concurrency is being used. The JDBC URL uses the format: \r\njdbc:drill:drillbit=a-bits-hostname\r\n\r\nHanifi suggested I check for open file descriptors using: \r\nlsof -a -p DRILL_PID | wc -l\r\n\r\nwhich I did on the two nodes, I currently have running drill, both, before and\r\nafter restarting.\r\n\r\nNode from JDBC connection string (which had been previously restarted):\r\n   Before: 396\r\n   After: 396\r\nOther node:\r\n   Before: 14\r\n   After: 395\r\n\r\nThe error, \"Too many open files\", persists after restarting the bits.\r\n\r\nOpened as a result of this thread:\r\nhttp://mail-archives.apache.org/mod_mbox/drill-user/201601.mbox/browser",
        "Error message must be updated when querying a JSON file with arrays in first level Structure of the file:\r\n{code}\r\n[\r\n  {\r\n    \"key\": \"value\",\r\n    ...\r\n  },\r\n  {\r\n   ...\r\n  }\r\n]\r\n{code}\r\n*Valid Error Message:*\r\n{code:sql}\r\n> select * from `file.json`;\r\nQuery failed: Query stopped., Drill doesn't support objects whose first level is a scalar or array.  Objects must start as maps. [ 1dc6dfc2-77ed-4a22-8ef0-e5a31c24f6fb on abhi8.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n*Error message to be updated:*\r\n{code:sql}\r\n> select count(*) from `file.json`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\nQuery failed: RemoteRpcException: Failure while running fragment., You tried to do a batch data read operation when you were in a state of STOP.  You can only do this type of operation when you are in a state of OK or OK_NEW_SCHEMA. [ 08135c32-fbb8-445e-8c0e-05292e6613e2 on abhi8.qa.lab:31010 ]\r\n[ 08135c32-fbb8-445e-8c0e-05292e6613e2 on abhi8.qa.lab:31010 ]\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n{code}"
    ],
    [
        "DRILL-3528",
        "DRILL-2312",
        "Apache drill plugin for solr I am trying to explore apache drill project and able to set it in my local enviroment with the help of documentation. However, i was looking for some code flow document to get started with writing a custom plug-in for trial. I am trying add a type of apache solr to have a sql like interface on top of it. \r\n\r\nwhen i added the project plugin under /jars folder and tried to create new storage of type solr got an exception \"Error : Invalid JSON mapping\".\r\n\r\nCould you please help me with some debugging points.",
        "JDBC driver returning incorrect data after extended usage After executing ~20-30 queries with the JDBC driver, the data returned from a \"Show files\" query is incorrect, particularly the isFile and isDirectory columns. The first item in the schema/directory will be correct, but subsequent items will report \"false\" for isFile and isDirectory.\r\n\r\nThis was tested with just a simple program that just loops through executeQuery and prints out the values for isFile and isDirectory. The JDBC driver used was the Drill 0.7 snapshot.\r\n\r\n{code}\r\nisFile: true\r\nisDirectory: false\r\nisFile: false\r\nisDirectory: false\r\nisFile: false\r\nisDirectory: false\r\nisFile: false\r\nisDirectory: false\r\nisFile: false\r\nisDirectory: false\r\n{code}"
    ],
    [
        "DRILL-178",
        "DRILL-951",
        "Update JSON reader to support reading distributed, compressed json files  The JSON reader supports local file system reads but doesn't support dfs access or block awareness.  These are needed for making the JSON storage engine work in distributed mode.  Much of this work can leverage/be based on the work within the Parquet storage engine.",
        "CSV header row should be parsed CSV reader is currently treating header names like regular rows. There should be a way to treat the header row as the column names (optional?).\r\n\r\nI exported this dataset to a CSV: https://data.sfgov.org/Public-Safety/SFPD-Incidents-Previous-Three-Months/tmnf-yvry\r\n\r\n"
    ],
    [
        "DRILL-2488",
        "DRILL-385",
        "Wrong result on join between two subqueries with aggregation {code}\r\n0: jdbc:drill:schema=dfs> select * from t1;\r\n+------------+------------+------------+\r\n|     a1     |     b1     |     c1     |\r\n+------------+------------+------------+\r\n| 1          | aaaaa      | 2015-01-01 |\r\n| 2          | bbbbb      | 2015-01-02 |\r\n| 3          | ccccc      | 2015-01-03 |\r\n| 4          | null       | 2015-01-04 |\r\n| 5          | eeeee      | 2015-01-05 |\r\n| 6          | fffff      | 2015-01-06 |\r\n| 7          | ggggg      | 2015-01-07 |\r\n| null       | hhhhh      | 2015-01-08 |\r\n| 9          | iiiii      | null       |\r\n| 10         | jjjjj      | 2015-01-10 |\r\n+------------+------------+------------+\r\n10 rows selected (0.15 seconds)\r\n{code}\r\n\r\nThis result is incorrect, one row is missing\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from\r\n. . . . . . . . . . . . > (\r\n. . . . . . . . . . . . >                 select\r\n. . . . . . . . . . . . >                         b1,\r\n. . . . . . . . . . . . >                         count(distinct a1)\r\n. . . . . . . . . . . . >                 from\r\n. . . . . . . . . . . . >                         t1\r\n. . . . . . . . . . . . >                 group by\r\n. . . . . . . . . . . . >                         b1\r\n. . . . . . . . . . . . >                 order by\r\n. . . . . . . . . . . . >                         b1 limit 5 offset 1\r\n. . . . . . . . . . . . >         ) as sq1(x1, y1)\r\n. . . . . . . . . . . . >\r\n. . . . . . . . . . . . >         inner join\r\n. . . . . . . . . . . . >\r\n. . . . . . . . . . . . >         (\r\n. . . . . . . . . . . . >                 select\r\n. . . . . . . . . . . . >                         b1,\r\n. . . . . . . . . . . . >                         count(distinct a1)\r\n. . . . . . . . . . . . >                 from\r\n. . . . . . . . . . . . >                         t1\r\n. . . . . . . . . . . . >                 group by\r\n. . . . . . . . . . . . >                         b1\r\n. . . . . . . . . . . . >                 order by\r\n. . . . . . . . . . . . >                         b1 limit 5 offset 1\r\n. . . . . . . . . . . . >         ) as sq2(x1, y1)\r\n. . . . . . . . . . . . >         on\r\n. . . . . . . . . . . . >                 sq1.x1 = sq2.x1 and\r\n. . . . . . . . . . . . >                 sq2.y1 = sq2.y1\r\n. . . . . . . . . . . . > ;\r\n+------------+------------+------------+------------+\r\n|     x1     |     y1     |    x10     |    y10     |\r\n+------------+------------+------------+------------+\r\n| bbbbb      | 1          | bbbbb      | 1          |\r\n| ccccc      | 1          | ccccc      | 1          |\r\n| eeeee      | 1          | eeeee      | 1          |\r\n| fffff      | 1          | fffff      | 1          |\r\n+------------+------------+------------+------------+\r\n4 rows selected (0.28 seconds)\r\n{code}\r\n\r\nExplain plan for the wrong result:\r\n{code}\r\n00-01      Project(x1=[$0], y1=[$1], x10=[$2], y10=[$3])\r\n00-02        Project(x1=[$0], y1=[$1], x10=[$2], y10=[$3])\r\n00-03          MergeJoin(condition=[=($0, $2)], joinType=[inner])\r\n00-05            Limit(offset=[1], fetch=[5])\r\n00-07              StreamAgg(group=[{0}], EXPR$1=[COUNT($1)])\r\n00-09                Sort(sort0=[$0], dir0=[ASC])\r\n00-11                  StreamAgg(group=[{0, 1}])\r\n00-13                    Sort(sort0=[$0], sort1=[$1], dir0=[ASC], dir1=[ASC])\r\n00-15                      Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/aggregation/t1]], selectionRoot=/drill/testdata/aggregation/t1, numFiles=1, columns=[`b1`, `a1`]]])\r\n00-04            Project(b10=[$0], EXPR$10=[$1])\r\n00-06              SelectionVectorRemover\r\n00-08                Sort(sort0=[$0], dir0=[ASC])\r\n00-10                  Filter(condition=[=($1, $1)])\r\n00-12                    Limit(offset=[1], fetch=[5])\r\n00-14                      StreamAgg(group=[{0}], EXPR$1=[COUNT($1)])\r\n00-16                        Sort(sort0=[$0], dir0=[ASC])\r\n00-17                          StreamAgg(group=[{0, 1}])\r\n00-18                            Sort(sort0=[$0], sort1=[$1], dir0=[ASC], dir1=[ASC])\r\n00-19                              Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/aggregation/t1]], selectionRoot=/drill/testdata/aggregation/t1, numFiles=1, columns=[`b1`, `a1`]]])\r\n{code}\r\n\r\nIf you turn off merge join, query returns correct result:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from\r\n. . . . . . . . . . . . > (\r\n. . . . . . . . . . . . >     select\r\n. . . . . . . . . . . . >         b1,\r\n. . . . . . . . . . . . >         count(distinct a1)\r\n. . . . . . . . . . . . >     from\r\n. . . . . . . . . . . . >         t1\r\n. . . . . . . . . . . . >     group by\r\n. . . . . . . . . . . . >         b1\r\n. . . . . . . . . . . . >     order by\r\n. . . . . . . . . . . . >         b1 limit 5 offset 1\r\n. . . . . . . . . . . . >  ) as sq1(x1, y1)\r\n. . . . . . . . . . . . >\r\n. . . . . . . . . . . . >         inner join\r\n. . . . . . . . . . . . > (\r\n. . . . . . . . . . . . >     select\r\n. . . . . . . . . . . . >         b1,\r\n. . . . . . . . . . . . >         count(distinct a1)\r\n. . . . . . . . . . . . >     from\r\n. . . . . . . . . . . . >         t1\r\n. . . . . . . . . . . . >     group by\r\n. . . . . . . . . . . . >         b1\r\n. . . . . . . . . . . . >     order by\r\n. . . . . . . . . . . . >           b1 limit 5 offset 1\r\n. . . . . . . . . . . . > ) as sq2(x1, y1)\r\n. . . . . . . . . . . . > on\r\n. . . . . . . . . . . . >                 sq1.x1 = sq2.x1 and\r\n. . . . . . . . . . . . >                 sq2.y1 = sq2.y1\r\n. . . . . . . . . . . . > ;\r\n+------------+------------+------------+------------+\r\n|     x1     |     y1     |    x10     |    y10     |\r\n+------------+------------+------------+------------+\r\n| bbbbb      | 1          | bbbbb      | 1          |\r\n| ccccc      | 1          | ccccc      | 1          |\r\n| eeeee      | 1          | eeeee      | 1          |\r\n| fffff      | 1          | fffff      | 1          |\r\n| ggggg      | 1          | ggggg      | 1          |\r\n+------------+------------+------------+------------+\r\n5 rows selected (0.352 seconds)\r\n{code}\r\n\r\ncut/paste reproduction\r\n{code:sql}\r\nselect * from\r\n (\r\n                 select \r\n                         b1,                      \r\n                         count(distinct a1)       \r\n                 from \r\n                         t1                       \r\n                 group by                 \r\n                         b1                       \r\n                 order by                 \r\n                         b1 limit 5 offset 1      \r\n         ) as sq1(x1, y1)         \r\n  \r\n         inner join \r\n  \r\n         (\r\n                 select \r\n                         b1,                      \r\n                         count(distinct a1)       \r\n                 from \r\n                         t1                       \r\n                 group by                 \r\n                         b1                       \r\n                 order by                 \r\n                         b1 limit 5 offset 1      \r\n         ) as sq2(x1, y1)         \r\n         on \r\n                 sq1.x1 = sq2.x1 and      \r\n                 sq2.y1 = sq2.y1          \r\n ;\r\n{code}\r\n\r\n\r\nThis test has been running and passing in regression test suite until framework was switched to a new code, where JSON parsing was replaced with jackson  and for a brief period ( I believe Friday afternoon until now ) this suite was not executed.\r\nWe already have a merge join bug DRILL-2010, but this one seems to be different (small data set) and feels like a recent regression. \r\n\r\nFor QA: test Functional/Passing/aggregation/sanity/q18.sql will be running with merge join disabled until this issue is fixed. Will need to remove alter session from this file.",
        "Implement Top-N sort operator I want to give a brief summary of the design for this operator.\r\nThis operator maintains a hyperbatch and a SelectionVector4. The length of the selectionVector is (limit + 1). Indices [0..limit-1] are used to maintain a min-heap, with the value pointing to the batch and index of the value it represents. The last element of the selectionVector is used for staging the next incoming record. It is done this way because the generated methods for doing comparisons uses the values stored in the selection vector as a pointer to the records in the hyperbatch.\r\nThe first N records to come in are added to the heap, and the heap property is reestablished after each insertion. Once the heap has reached it's final size, each incoming record is compared to the top of the heap. The heap is a min-heap, meaning the root of the heap contains the lowest priority item in the heap. In other words, if we are looking for the top 10 values, the root points to the current 10th value. Each incoming record is compared to the root, and if necessary the two are swapped, and the heap property restored.\r\nPeriodically, there can be a purge, in which the values referenced by the selection vector are copied into new value vectors, and the old buffers are released, freeing up memory."
    ],
    [
        "DRILL-2380",
        "DRILL-3101",
        "TPC-DS Query 33 and simplified variants return wrong results TPC-DS query 33 returns wrong results. \r\n\r\n{code:sql}\r\nWITH ss \r\n     AS (SELECT i_manufact_id, \r\n                Sum(ss_ext_sales_price) total_sales \r\n         FROM   store_sales, \r\n                date_dim, \r\n                customer_address, \r\n                item \r\n         WHERE  i_manufact_id IN (SELECT i_manufact_id \r\n                                  FROM   item \r\n                                  WHERE  i_category IN ( 'Books' )) \r\n                AND ss_item_sk = i_item_sk \r\n                AND ss_sold_date_sk = d_date_sk \r\n                AND d_year = 1999 \r\n                AND d_moy = 3 \r\n                AND ss_addr_sk = ca_address_sk \r\n                AND ca_gmt_offset = -5 \r\n         GROUP  BY i_manufact_id), \r\n     cs \r\n     AS (SELECT i_manufact_id, \r\n                Sum(cs_ext_sales_price) total_sales \r\n         FROM   catalog_sales, \r\n                date_dim, \r\n                customer_address, \r\n                item \r\n         WHERE  i_manufact_id IN (SELECT i_manufact_id \r\n                                  FROM   item \r\n                                  WHERE  i_category IN ( 'Books' )) \r\n                AND cs_item_sk = i_item_sk \r\n                AND cs_sold_date_sk = d_date_sk \r\n                AND d_year = 1999 \r\n                AND d_moy = 3 \r\n                AND cs_bill_addr_sk = ca_address_sk \r\n                AND ca_gmt_offset = -5 \r\n         GROUP  BY i_manufact_id), \r\n     ws \r\n     AS (SELECT i_manufact_id, \r\n                Sum(ws_ext_sales_price) total_sales \r\n         FROM   web_sales, \r\n                date_dim, \r\n                customer_address, \r\n                item \r\n         WHERE  i_manufact_id IN (SELECT i_manufact_id \r\n                                  FROM   item \r\n                                  WHERE  i_category IN ( 'Books' )) \r\n                AND ws_item_sk = i_item_sk \r\n                AND ws_sold_date_sk = d_date_sk \r\n                AND d_year = 1999 \r\n                AND d_moy = 3 \r\n                AND ws_bill_addr_sk = ca_address_sk \r\n                AND ca_gmt_offset = -5 \r\n         GROUP  BY i_manufact_id) \r\nSELECT i_manufact_id, \r\n               Sum(total_sales) total_sales \r\nFROM   (SELECT i_manufact_id, total_sales \r\n        FROM   ss \r\n        UNION ALL \r\n        SELECT i_manufact_id, total_sales\r\n        FROM   cs \r\n        UNION ALL \r\n        SELECT i_manufact_id, total_sales\r\n        FROM   ws) tmp1 \r\nGROUP  BY i_manufact_id \r\nORDER  BY total_sales\r\nLIMIT 10;\r\n\r\nDrill Results:\r\n+---------------+-------------+\r\n| i_manufact_id | total_sales |\r\n+---------------+-------------+\r\n| 440           | 0.12        |\r\n| 434           | 13.16       |\r\n| 415           | 14.04       |\r\n| 449           | 15.63       |\r\n| 563           | 31.46       |\r\n| 357           | 49.50       |\r\n| 624           | 67.94       |\r\n| 192           | 74.40       |\r\n| 137           | 83.42       |\r\n| 240           | 85.26       |\r\n+---------------+-------------+\r\n10 rows selected (7.57 seconds)\r\n\r\nPostgres Results:\r\n i_manufact_id | total_sales \r\n---------------+-------------\r\n           930 |        1.18\r\n           818 |       41.86\r\n           913 |      141.90\r\n           784 |      184.90\r\n           488 |      275.08\r\n           993 |      301.60\r\n           700 |      340.52\r\n           895 |      802.30\r\n           766 |      839.76\r\n           858 |      859.18\r\n(10 rows)\r\n{code}\r\n\r\nThe following simplified variants also return wrong results:\r\n{code:sql}\r\nSELECT sum(x)\r\nFROM\r\n(SELECT ss_ext_sales_price x, ss_item_sk\r\nFROM  store_sales\r\n GROUP BY ss_item_sk, ss_ext_sales_price\r\nUNION ALL\r\nSELECT cs_ext_sales_price x, cs_item_sk\r\nFROM catalog_sales\r\nGROUP BY cs_item_sk, cs_ext_sales_price) tmp\r\nGROUP BY x\r\nLIMIT 10;\r\n\r\nDrill Results:\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 14141.40   |\r\n| 28060.00   |\r\n| 30912.70   |\r\n| 43706.88   |\r\n| 38267.64   |\r\n| 10173.00   |\r\n| 37829.25   |\r\n| 5349.50    |\r\n| 107515.80  |\r\n| 4440.84    |\r\n+------------+\r\n10 rows selected (14.435 seconds)\r\n\r\nPostgres Results:\r\n   sum    \r\n----------   \r\n 45234.00\r\n  5735.31\r\n  2275.60\r\n  6921.32\r\n  2590.46\r\n  6615.09\r\n 14080.77\r\n 24819.76\r\n 25127.20\r\n(10 rows)\r\n\r\nSELECT sum(x)\r\nFROM\r\n(SELECT sum(ss_ext_sales_price) x, ss_item_sk\r\nFROM  store_sales\r\n GROUP BY ss_item_sk\r\nUNION ALL\r\nSELECT sum(cs_ext_sales_price) x, cs_item_sk\r\nFROM catalog_sales\r\nGROUP BY cs_item_sk) tmp\r\nGROUP BY x\r\nLIMIT 10;\r\n\r\nDrill Results:\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 211411.58  |\r\n| 347027.93  |\r\n| 534760.93  |\r\n| 203028.28  |\r\n| 500939.61  |\r\n| 248226.81  |\r\n| 242664.29  |\r\n| 597659.03  |\r\n| 258909.73  |\r\n| 223624.06  |\r\n+------------+\r\n10 rows selected (5.245 seconds)\r\n\r\nPostgres Results:\r\n    sum    \r\n-----------\r\n 252711.42\r\n 173571.97\r\n 206191.60\r\n 249793.96\r\n 170825.75\r\n 127718.29\r\n 220887.50\r\n 119390.44\r\n 217495.66\r\n 284348.93\r\n(10 rows)\r\n\r\nSELECT x\r\nFROM\r\n(SELECT ss_ext_sales_price x, ss_item_sk\r\nFROM  store_sales\r\n GROUP BY ss_item_sk, ss_ext_sales_price\r\nUNION ALL\r\nSELECT cs_ext_sales_price x, cs_item_sk\r\nFROM catalog_sales\r\nGROUP BY cs_item_sk, cs_ext_sales_price) tmp\r\nGROUP BY x\r\nLIMIT 10;\r\n\r\nDrill Results:\r\n+------------+\r\n|     x      |\r\n+------------+\r\n| 271.95     |\r\n| 561.20     |\r\n| 391.30     |\r\n| 1821.12    |\r\n| 2125.98    |\r\n| 1695.50    |\r\n| 1513.17    |\r\n| 411.50     |\r\n| 4674.60    |\r\n| 193.08     |\r\n+------------+\r\n10 rows selected (9.518 seconds)\r\n\r\nPostgres Results:\r\n    x    \r\n---------\r\n 9046.80\r\n 5735.31\r\n  568.90\r\n 3460.66\r\n 1295.23\r\n 6615.09\r\n 4693.59\r\n 6204.94\r\n 6281.80\r\n(10 rows)\r\n{code}\r\n",
        "Setting \"slice_target\" to 1 changes the order of the columns in a \"select *\" query with order by git.commit.id.abbrev=d8b1975\r\n\r\nWith Default Settings :\r\n{code}\r\nselect * from region order by length(r_name);\r\n+-------------+--------+-----------+\r\n| r_regionkey | r_name | r_comment |\r\n+-------------+--------+-----------+\r\n| 2 | ASIA | ges. thinly even pinto beans ca |\r\n| 0 | AFRICA | lar deposits. blithely final packages cajole. regular waters are final requests. regular accounts are according to  |\r\n| 3 | EUROPE | ly final courts cajole furiously final excuse |\r\n| 1 | AMERICA | hs use ironic, even requests. s |\r\n| 4 | MIDDLE EAST | uickly special accounts cajole carefully blithely close requests. carefully final asymptotes haggle furiousl |\r\n{code}\r\n\r\nNow after setting the slice target to 1, the order of the columns changed\r\n{code}\r\n0: jdbc:drill:schema=dfs_eea> alter session set `planner.slice_target` = 1;\r\n+-------+--------------------------------+\r\n|  ok   |            summary             |\r\n+-------+--------------------------------+\r\n| true  | planner.slice_target updated.  |\r\n+-------+--------------------------------+\r\n1 row selected (0.11 seconds)\r\n0: jdbc:drill:schema=dfs_eea> select * from region order by length(r_name);\r\n+-----------+--------+-------------+\r\n| r_comment | r_name | r_regionkey |\r\n+-----------+--------+-------------+\r\n| ges. thinly even pinto beans ca | ASIA | 2 |\r\n| lar deposits. blithely final packages cajole. regular waters are final requests. regular accounts are according to  | AFRICA | 0 |\r\n| ly final courts cajole furiously final excuse | EUROPE | 3 |\r\n| hs use ironic, even requests. s | AMERICA | 1 |\r\n| uickly special accounts cajole carefully blithely close requests. carefully final asymptotes haggle furiousl | MIDDLE EAST | 4 |\r\n+-----------+--------+-------------+\r\n5 rows selected (0.796 seconds)\r\n{code}\r\n\r\nThis does not happen when we do not use an \"order by\" in query\r\n"
    ],
    [
        "DRILL-3018",
        "DRILL-2344",
        "Queries with scalar aggregate  and non equality (non correlated) fail to plan This is a regression.\r\nBoth queries worked and returned correct result with May 7 build\r\n(mapr-drill-1.0.0.31658-1.noarch.rpm)\r\n\r\nRunning with the latest build, I got these two failures:\r\n{code}\r\nselect * from j2 where c_integer > (select min(c_bigint) from j7 where c_boolean is null)\r\nFailed with exception\r\njava.sql.SQLException: SYSTEM ERROR: org.apache.drill.exec.work.foreman.UnsupportedRelOperatorException: This query cannot be planned possibly due to either a cartesian join or an inequality\r\n\r\nselect * from j2 where c_float < (select min(c_float) from j6 where c_boolean is null )\r\nFailed with exception\r\njava.sql.SQLException: SYSTEM ERROR: org.apache.drill.exec.work.foreman.UnsupportedRelOperatorException: This query cannot be planned possibly due to either a cartesian join or an inequality join \r\n{code}\r\n\r\nThe common pattern between these queries is that function MIN is running over no rows, in both queries there is no correlation to the outer table.\r\n{code}\r\n-- Non correlated\r\n-- Greater than\r\n-- MIN returns NULL\r\nselect * from j2 where c_integer > (select min(c_bigint) from j7 where c_boolean is null);\r\n\r\n-- Non correlated\r\n-- Less than\r\n-- MIN returns NULL\r\nselect * from j2 where c_float < (select min(c_float) from j6 where c_boolean is null );\r\n{code}\r\n",
        "JDBC : getTables() method never returns git.commit.id.abbrev=02d23cb\r\n\r\nI am trying to get the tables from drill using the below code and drill never returns\r\n{code}\r\nDatabaseMetaData meta = conn.getMetaData();\r\nResultSet rs = meta.getTables(null, null, null, new String[] {\"VIEW\"});\r\nSystem.out.println(\"Drill does not get to this point\");\r\n{code}\r\n\r\nBelow is the underlying query which drill issues from withing the getTables() method.\r\n{code}\r\nSELECT table_catalog AS TABLE_CAT, \r\n       table_schema  AS TABLE_SCHEM, \r\n       table_name, \r\n       table_type, \r\n       ''            AS REMARKS, \r\n       ''            AS TYPE_CAT, \r\n       ''            AS TYPE_SCHEM, \r\n       ''            AS TYPE_NAME, \r\n       ''            AS SELF_REFERENCING_COL_NAME, \r\n       ''            AS REF_GENERATION \r\nFROM   INFORMATION_SCHEMA.`TABLES` \r\nWHERE  1 = 1 \r\n       AND ( table_type LIKE 'VIEW' ) \r\nORDER  BY table_type, \r\n          table_catalog, \r\n          table_schema, \r\n          table_name \r\n{code}\r\n\r\nWhen I tried to manually run this query, sqlline never returned"
    ],
    [
        "DRILL-1372",
        "DRILL-2548",
        "Reorganize classb jars files to be stored in a separate directory in binary distribution ",
        "JDBC driver prints misleading SQL exception on getting record batches with no data Exceptions in the server can cause the server to return to the client a record batch that has inconsistent data, particularly the record batch may have a row count set but no data. The JDBC driver throws a SQLException which gets printed obscuring the actual error that occurred (which may also be printed).\r\n"
    ],
    [
        "DRILL-595",
        "DRILL-2891",
        "Can't project columns from a inner query that has \"select *\" Following queries don't work currently with schema-less tables such as parquet or json.\r\n\r\n{code}\r\nwith x as (select * from cp.`region.json`) select region_id, sales_city from x;\r\nselect region_id, sales_city from ( select * from cp.`region.json`);\r\n{code}\r\n\r\nBoth fail with:\r\n{code}\r\nmessage: \"Failure while parsing sql. < ValidationException:[ org.eigenbase.util.EigenbaseContextException: From line 1, column 51 to line 1, column 59 ] < EigenbaseContextException:[ From line 1, column 51 to line 1, column 59 ] < SqlValidatorException:[ Column \\'region_id\\' not found in any table ]\r\n{code}\r\n",
        "Allowing ROUND function on boolean type can cause all sorts of problems Works, and I don't think it makes much sense:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select round(c_boolean) from alltypes_with_nulls limit 1;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 1          |\r\n+------------+\r\n1 row selected (0.19 seconds)\r\n{code}\r\n\r\nFails later if used in other parts of the query.\r\n\r\nIn order by:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select round(c_boolean) from alltypes_with_nulls order by 1;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\nQuery failed: SYSTEM ERROR: java.lang.UnsupportedOperationException: Failure finding function that runtime code generation expected.  Signature: compare_to_nulls_high( TINYINT:OPTIONAL, TINYINT:OPTIONAL ) returns INT:REQUIRED\r\n\r\nFragment 0:0\r\n\r\n[7add2ed7-de6a-4c66-b511-ecad32413fcc on atsqa4-133.qa.lab:31010]\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n{code}\r\n\r\nIn group by\r\n{code}\r\n0: jdbc:drill:schema=dfs> select round(c_boolean) from alltypes group by round(c_boolean);\r\nQuery failed: SYSTEM ERROR: Failure finding function that runtime code generation expected.  Signature: compare_to_nulls_high( TINYINT:REQUIRED, TINYINT:REQUIRED ) returns INT:REQUIRED\r\n\r\nFragment 0:0\r\n\r\n[286777b2-3395-4e44-94a2-d9dafa07f9dc on atsqa4-133.qa.lab:31010]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nWe should not allow that."
    ],
    [
        "DRILL-243",
        "DRILL-1369",
        "lowercase asc/desc in generated physical plans causes failures We are not consistent about this, and when doing distributed sorts, this causes the query to fail.",
        "CodeCompiler has unlimited sized cache, ultimately ending in PermGen overflow CodeCompiler needs to constrain compilation cache."
    ],
    [
        "DRILL-121",
        "DRILL-4295",
        "Package Drill for release on distros, RPM and DEB ",
        "Obsolete protobuf generated files under protocol/ The following two files don't have a protobuf definition anymore, and are not generated when running {{mvn process-sources -P proto-compile}} under {{protocol/}}:\r\n\r\n{noformat}\r\nsrc/main/java/org/apache/drill/exec/proto/beans/RpcFailure.java\r\nsrc/main/java/org/apache/drill/exec/proto/beans/ViewPointer.java\r\n{noformat}"
    ],
    [
        "DRILL-794",
        "DRILL-2730",
        "decimal addition involving boundary numbers caused foreman setup assertion The following decimal operation caused the assertion:\r\n\r\n0: jdbc:drill:schema=dfs> select cast('999999999999999999' as decimal(18,0)) + cast('-0.0000000000000000000000000000000000001' as decimal(38,38)) from data limit 1;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"7b386343-2573-4f7b-b7fe-be9bfdf35237\"\r\nendpoint {\r\n  address: \"qa-node119.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while setting up Foreman. < AssertionError\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n\r\nHere is the stack trace:\r\n\r\n17:36:40.565 [WorkManager Event Thread] DEBUG o.apache.drill.exec.work.WorkManager - Starting pending task org.apache.drill.exec.work.foreman.Foreman@eb155db\r\n17:36:40.575 [ba6d37ab-c589-4bd2-974e-2e3854988100:foreman] ERROR o.a.drill.exec.work.foreman.Foreman - Error 7b386343-2573-4f7b-b7fe-be9bfdf35237: Failure while setting up Foreman.\r\njava.lang.AssertionError: null\r\n        at org.eigenbase.sql.type.ReturnTypes$7.inferReturnType(ReturnTypes.java:473) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.type.SqlTypeTransformCascade.inferReturnType(SqlTypeTransformCascade.java:55) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.type.SqlReturnTypeInferenceChain.inferReturnType(SqlReturnTypeInferenceChain.java:57) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.SqlOperator.inferReturnType(SqlOperator.java:451) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.SqlOperator.validateOperands(SqlOperator.java:418) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.SqlOperator.deriveType(SqlOperator.java:480) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:143) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3870) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:3857) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.SqlCall.accept(SqlCall.java:133) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1322) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1305) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:418) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:3019) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:2772) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:80) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:747) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:736) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.SqlSelect.validate(SqlSelect.java:209) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:710) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.eigenbase.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:426) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at net.hydromatic.optiq.prepare.PlannerImpl.validate(PlannerImpl.java:175) ~[optiq-core-0.7-20140513.013236-5.jar:na]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateNode(DefaultSqlHandler.java:99) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:84) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:134) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:338) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:186) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]",
        "Use different paths for ExternalSort spill files Currently, all of the spill files for a drillbit end up in the same directory (or directories if multiple are configured). There is no separation based on query or fragment. This makes it very difficult to explore the spill files via the filesystem.\r\n\r\nWe should create a directory structure that will organize the files more effectively, create a directory tree with query id, major fragment id, minor fragment id, and operator id."
    ],
    [
        "DRILL-2024",
        "DRILL-2437",
        "\".../exec/jdbc/target/classes isn't a file\" error Building with \"mvn clean install -DskipContrib -DskipJdbcAll -DskipInterpreter\" has (at least twice) yielded errors like:\r\n\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.4:single (distro-assembly) on project distribution: Failed to create assembly: Error adding file 'org.apache.drill.exec:drill-jdbc:jar:0.8.0-SNAPSHOT' to archive: /home/xxxxx/exec/jdbc/target/classes isn't a file. -> [Help 1]\r\n\r\n\r\n",
        "enhance exception injection to support session level injections Exception injection requires storage of the injection states; this is currently done in the DrillbitContext, so we can only support system level settings. We would need to have a session context that we could hang a SimulatedExceptions object off of in order to support alter session."
    ],
    [
        "DRILL-1708",
        "DRILL-74",
        "Drillbit hangs when initial connection to zookeeper fails Existing zookeeper cluster coordinator implementation blocks until initial connection succeeds. However, drillbit hangs forever in case connection does not succeed for some reason as there is no retry logic implemented. This issue proposes to implement a cleaner, well defined outcome in case zk connection fails.",
        "Links on web-site should use nofollow All links on an Apache web-site should use nofollow attributes on all outgoing links to non-Apache web-sites.  Thank-you links should also not be on a project home page.\r\n\r\n(nice site, though!)\r\n\r\nhttp://en.wikipedia.org/wiki/Nofollow\r\n\r\nhttp://www.apache.org/foundation/marks/linking#projectthanks"
    ],
    [
        "DRILL-820",
        "DRILL-517",
        "CannotPlanException with a class of queries Datasources: TPCH (10MB), three-way split parquet files\r\ngit.commit.id.abbrev=2fad21d\r\ngit.commit.id=2fad21d5a6ec43bb68fb989e48b6da180f23f73a\r\n\r\n\r\n0: jdbc:drill:schema=dfs.TpcHMulti> alter session set `planner.enable_streamagg` = false;\r\n+------------+------------+                                                              \r\n|     ok     |  summary   |                                                              \r\n+------------+------------+                                                              \r\n| true       | planner.enable_streamagg updated. |                                       \r\n+------------+------------+                                                              \r\n1 row selected (0.021 seconds)                                                           \r\n0: jdbc:drill:schema=dfs.TpcHMulti> select sum(l_extendedprice * l_discount) as revenue  \r\n. . . . . . . . . . . . . . . . . >    from lineitem                                     \r\n. . . . . . . . . . . . . . . . . >         where l_shipdate between date '1997-01-01' and date '1998-01-01'                                                                                                      \r\n. . . . . . . . . . . . . . . . . >                 and l_quantity < 24;                                 \r\nError: exception while executing query (state=,code=0)                                                   \r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"5dfa0c1e-d938-42a1-b365-f32c3fd6d24f\"                                                                        \r\nendpoint {                                                                                               \r\n  address: \"perfnode104.perf.lab\"                                                                        \r\n  user_port: 31010                                                                                       \r\n  control_port: 31011                                                                                    \r\n  data_port: 31012                                                                                       \r\n}                                                                                                        \r\nerror_type: 0                                                                                            \r\nmessage: \"Failure while parsing sql. < CannotPlanException:[ Node [rel#34102:Subset#12.PHYSICAL.SINGLETON([]).[]] could not be implemented; planner state:                                                        \r\n\r\nRoot: rel#34102:Subset#12.PHYSICAL.SINGLETON([]).[]\r\nOriginal rel:                                      \r\nAbstractConverter(subset=[rel#34102:Subset#12.PHYSICAL.SINGLETON([]).[]], convention=[PHYSICAL], DrillDistributionTraitDef=[SINGLETON([])], sort=[[]]): rowcount = 752.1875, cumulative cost = {inf}, id = 34104  \r\n  DrillScreenRel(subset=[rel#34101:Subset#12.LOGICAL.ANY([]).[]]): rowcount = 752.1875, cumulative cost = {75.21875 rows, 75.21875 cpu, 0.0 io, 0.0 network}, id = 34100                                          \r\n    DrillAggregateRel(subset=[rel#34099:Subset#11.LOGICAL.ANY([]).[]], group=[{}], revenue=[SUM($0)]): rowcount = 752.1875, cumulative cost = {752.1875 rows, 0.0 cpu, 0.0 io, 0.0 network}, id = 34098           \r\n      DrillProjectRel(subset=[rel#34097:Subset#10.LOGICAL.ANY([]).[]], $f0=[*($3, $4)]): rowcount = 7521.875, cumulative cost = {7521.875 rows, 4.0 cpu, 0.0 io, 0.0 network}, id = 34096                         \r\n        DrillFilterRel(subset=[rel#34095:Subset#9.LOGICAL.ANY([]).[]], condition=[AND(>=($2, 1997-01-01), <=($2, 1998-01-01), <($1, 24))]): rowcount = 7521.875, cumulative cost = {60175.0 rows, 722100.0 cpu, 0.0 io, 0.0 network}, id = 34094                                                                           \r\n          DrillScanRel(subset=[rel#34093:Subset#8.LOGICAL.ANY([]).[]], table=[[dfs, TpcHMulti, lineitem]]): rowcount = 60175.0, cumulative cost = {60175.0 rows, 300875.0 cpu, 0.0 io, 0.0 network}, id = 34047   \r\n\r\nSets:\r\nSet#8, type: (DrillRecordRow[*, l_quantity, l_shipdate, l_extendedprice, l_discount])\r\n        rel#34093:Subset#8.LOGICAL.ANY([]).[], best=rel#34047, importance=0.5904900000000001\r\n                rel#34047:DrillScanRel.LOGICAL.ANY([]).[](table=[dfs, TpcHMulti, lineitem]), rowcount=60175.0, cumulative cost={60175.0 rows, 300875.0 cpu, 0.0 io, 0.0 network}                                  \r\n                rel#34114:AbstractConverter.LOGICAL.ANY([]).[](child=rel#34113:Subset#8.PHYSICAL.ANY([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=60175.0, cumulative cost={inf}\r\n                rel#34118:AbstractConverter.LOGICAL.ANY([]).[](child=rel#34117:Subset#8.PHYSICAL.RANDOM_DISTRIBUTED([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=60175.0, cumulative cost={inf}                                                                                          \r\n        rel#34113:Subset#8.PHYSICAL.ANY([]).[], best=rel#34116, importance=0.531441                      \r\n                rel#34115:AbstractConverter.PHYSICAL.ANY([]).[](child=rel#34093:Subset#8.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=60175.0, cumulative cost={inf}                                                                                                        \r\n                rel#34119:AbstractConverter.PHYSICAL.ANY([]).[](child=rel#34117:Subset#8.PHYSICAL.RANDOM_DISTRIBUTED([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=60175.0, cumulative cost={inf}                                                                                        \r\n                rel#34120:AbstractConverter.PHYSICAL.RANDOM_DISTRIBUTED([]).[](child=rel#34093:Subset#8.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=RANDOM_DISTRIBUTED([]),sort=[]), rowcount=60175.0, cumulative cost={inf}                                                                          \r\n                rel#34121:AbstractConverter.PHYSICAL.RANDOM_DISTRIBUTED([]).[](child=rel#34113:Subset#8.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=RANDOM_DISTRIBUTED([]),sort=[]), rowcount=60175.0, cumulative cost={inf}                                                                         \r\n                rel#34116:ScanPrel.PHYSICAL.RANDOM_DISTRIBUTED([]).[](groupscan=ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/tpch-multi/lineitem]], selectionRoot=/drill/testdata/tpch-multi/lineitem, columns=[SchemaPath [`l_quantity`], SchemaPath [`l_shipdate`], SchemaPath [`l_extendedprice`], SchemaPath [`l_discount`]]]), rowcount=60175.0, cumulative cost={60175.0 rows, 300875.0 cpu, 0.0 io, 0.0 network}                                                                                         \r\n        rel#34117:Subset#8.PHYSICAL.RANDOM_DISTRIBUTED([]).[], best=rel#34116, importance=0.4782969000000001                                                                                                      \r\n                rel#34120:AbstractConverter.PHYSICAL.RANDOM_DISTRIBUTED([]).[](child=rel#34093:Subset#8.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=RANDOM_DISTRIBUTED([]),sort=[]), rowcount=60175.0, cumulative cost={inf}                                                                          \r\n                rel#34121:AbstractConverter.PHYSICAL.RANDOM_DISTRIBUTED([]).[](child=rel#34113:Subset#8.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=RANDOM_DISTRIBUTED([]),sort=[]), rowcount=60175.0, cumulative cost={inf}                                                                         \r\n                rel#34116:ScanPrel.PHYSICAL.RANDOM_DISTRIBUTED([]).[](groupscan=ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/drill/testdata/tpch-multi/lineitem]], selectionRoot=/drill/testdata/tpch-multi/lineitem, columns=[SchemaPath [`l_quantity`], SchemaPath [`l_shipdate`], SchemaPath [`l_extendedprice`], SchemaPath [`l_discount`]]]), rowcount=60175.0, cumulative cost={60175.0 rows, 300875.0 cpu, 0.0 io, 0.0 network}                                                                                         \r\nSet#9, type: (DrillRecordRow[*, l_quantity, l_shipdate, l_extendedprice, l_discount])                    \r\n        rel#34095:Subset#9.LOGICAL.ANY([]).[], best=rel#34094, importance=0.6561                         \r\n                rel#34094:DrillFilterRel.LOGICAL.ANY([]).[](child=rel#34093:Subset#8.LOGICAL.ANY([]).[],condition=AND(>=($2, 1997-01-01), <=($2, 1998-01-01), <($1, 24))), rowcount=7521.875, cumulative cost={120350.0 rows, 1022975.0 cpu, 0.0 io, 0.0 network}                                                          \r\n                rel#34111:AbstractConverter.LOGICAL.ANY([]).[](child=rel#34110:Subset#9.PHYSICAL.ANY([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=7521.875, cumulative cost={inf}                                                                                                        \r\n                rel#34124:AbstractConverter.LOGICAL.ANY([]).[](child=rel#34123:Subset#9.PHYSICAL.RANDOM_DISTRIBUTED([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=7521.875, cumulative cost={inf}                                                                                         \r\n        rel#34110:Subset#9.PHYSICAL.ANY([]).[], best=rel#34122, importance=0.5904900000000001            \r\n                rel#34112:AbstractConverter.PHYSICAL.ANY([]).[](child=rel#34095:Subset#9.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=7521.875, cumulative cost={inf}                                                                                                       \r\n                rel#34125:AbstractConverter.PHYSICAL.ANY([]).[](child=rel#34123:Subset#9.PHYSICAL.RANDOM_DISTRIBUTED([]).[],convention=PHYSICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=7521.875, cumulative cost={inf}                                                                                       \r\n                rel#34126:AbstractConverter.PHYSICAL.RANDOM_DISTRIBUTED([]).[](child=rel#34095:Subset#9.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=RANDOM_DISTRIBUTED([]),sort=[]), rowcount=7521.875, cumulative cost={inf}                                                                         \r\n                rel#34127:AbstractConverter.PHYSICAL.RANDOM_DISTRIBUTED([]).[](child=rel#34110:Subset#9.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=RANDOM_DISTRIBUTED([]),sort=[]), rowcount=7521.875, cumulative cost={inf}                                                                        \r\n                rel#34122:FilterPrel.PHYSICAL.RANDOM_DISTRIBUTED([]).[](child=rel#34113:Subset#8.PHYSICAL.ANY([]).[],condition=AND(>=($2, 1997-01-01), <=($2, 1998-01-01), <($1, 24))), rowcount=7521.875, cumulative cost={120350.0 rows, 1022975.0 cpu, 0.0 io, 0.0 network}                                             \r\n        rel#34123:Subset#9.PHYSICAL.RANDOM_DISTRIBUTED([]).[], best=rel#34122, importance=0.531441       \r\n                rel#34126:AbstractConverter.PHYSICAL.RANDOM_DISTRIBUTED([]).[](child=rel#34095:Subset#9.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=RANDOM_DISTRIBUTED([]),sort=[]), rowcount=7521.875, cumulative cost={inf}                                                                         \r\n                rel#34127:AbstractConverter.PHYSICAL.RANDOM_DISTRIBUTED([]).[](child=rel#34110:Subset#9.PHYSICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=RANDOM_DISTRIBUTED([]),sort=[]), rowcount=7521.875, cumulative cost={inf}                                                                        \r\n                rel#34122:FilterPrel.PHYSICAL.RANDOM_DISTRIBUTED([]).[](child=rel#34113:Subset#8.PHYSICAL.ANY([]).[],condition=AND(>=($2, 1997-01-01), <=($2, 1998-01-01), <($1, 24))), rowcount=7521.875, cumulative cost={120350.0 rows, 1022975.0 cpu, 0.0 io, 0.0 network}                                             \r\nSet#10, type: RecordType(ANY $f0)                                                                        \r\n        rel#34097:Subset#10.LOGICAL.ANY([]).[], best=rel#34096, importance=0.7290000000000001            \r\n                rel#34096:DrillProjectRel.LOGICAL.ANY([]).[](child=rel#34095:Subset#9.LOGICAL.ANY([]).[],$f0=*($3, $4)), rowcount=7521.875, cumulative cost={127871.875 rows, 1022979.0 cpu, 0.0 io, 0.0 network} \r\n                rel#34133:AbstractConverter.LOGICAL.ANY([]).[](child=rel#34132:Subset#10.PHYSICAL.RANDOM_DISTRIBUTED([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=7521.875, cumulative cost={inf}                                                                                        \r\n        rel#34132:Subset#10.PHYSICAL.RANDOM_DISTRIBUTED([]).[], best=rel#34131, importance=0.6561        \r\n                rel#34134:AbstractConverter.PHYSICAL.RANDOM_DISTRIBUTED([]).[](child=rel#34097:Subset#10.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=RANDOM_DISTRIBUTED([]),sort=[]), rowcount=7521.875, cumulative cost={inf}                                                                        \r\n                rel#34131:ProjectPrel.PHYSICAL.RANDOM_DISTRIBUTED([]).[](child=rel#34123:Subset#9.PHYSICAL.RANDOM_DISTRIBUTED([]).[],$f0=*($3, $4)), rowcount=7521.875, cumulative cost={127871.875 rows, 1022979.0 cpu, 0.0 io, 0.0 network}\r\nSet#11, type: RecordType(ANY revenue)\r\n        rel#34099:Subset#11.LOGICAL.ANY([]).[], best=rel#34098, importance=0.81\r\n                rel#34098:DrillAggregateRel.LOGICAL.ANY([]).[](child=rel#34097:Subset#10.LOGICAL.ANY([]).[],group={},revenue=SUM($0)), rowcount=752.1875, cumulative cost={127872.875 rows, 1022980.0 cpu, 0.0 io, 0.0 network}\r\n                rel#34106:AbstractConverter.LOGICAL.ANY([]).[](child=rel#34105:Subset#11.PHYSICAL.SINGLETON([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=1.7976931348623157E308, cumulative cost={inf}\r\n        rel#34105:Subset#11.PHYSICAL.SINGLETON([]).[], best=null, importance=0.7290000000000001\r\n                rel#34107:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#34099:Subset#11.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=752.1875, cumulative cost={inf}\r\nSet#12, type: RecordType(ANY revenue)\r\n        rel#34101:Subset#12.LOGICAL.ANY([]).[], best=rel#34100, importance=0.9\r\n                rel#34100:DrillScreenRel.LOGICAL.ANY([]).[](child=rel#34099:Subset#11.LOGICAL.ANY([]).[]), rowcount=752.1875, cumulative cost={127948.09375 rows, 1023055.21875 cpu, 0.0 io, 0.0 network}\r\n                rel#34103:AbstractConverter.LOGICAL.ANY([]).[](child=rel#34102:Subset#12.PHYSICAL.SINGLETON([]).[],convention=LOGICAL,DrillDistributionTraitDef=ANY([]),sort=[]), rowcount=1.7976931348623157E308, cumulative cost={inf}\r\n        rel#34102:Subset#12.PHYSICAL.SINGLETON([]).[], best=null, importance=1.0\r\n                rel#34104:AbstractConverter.PHYSICAL.SINGLETON([]).[](child=rel#34101:Subset#12.LOGICAL.ANY([]).[],convention=PHYSICAL,DrillDistributionTraitDef=SINGLETON([]),sort=[]), rowcount=752.1875, cumulative cost={inf}\r\n                rel#34108:ScreenPrel.PHYSICAL.SINGLETON([]).[](child=rel#34105:Subset#11.PHYSICAL.SINGLETON([]).[]), rowcount=1.7976931348623157E308, cumulative cost={inf}\r\n\r\n ]\"\r\n]",
        "AssertionError between aggCall type and inferred type TPCH 17 and TPCH20 fail with this error.  \r\n\r\njava.lang.AssertionError: type mismatch:\r\naggCall type:\r\nANY\r\ninferred type:\r\nANY NOT NULL\r\n\tat org.eigenbase.relopt.RelOptUtil.eq(RelOptUtil.java:1460)\r\n\tat org.eigenbase.rel.AggregateRelBase.typeMatchesInferred(AggregateRelBase.java:222)\r\n\tat org.eigenbase.rel.AggregateRelBase.access$000(AggregateRelBase.java:41)\r\n\tat org.eigenbase.rel.AggregateRelBase$2.get(AggregateRelBase.java:200)\r\n\tat org.eigenbase.rel.AggregateRelBase$2.get(AggregateRelBase.java:187)\r\n\tat org.eigenbase.util.CompositeList.get(CompositeList.java:114)\r\n\tat org.eigenbase.reltype.RelDataTypeFactoryImpl$4.getFieldName(RelDataTypeFactoryImpl.java:174)\r\n\tat org.eigenbase.reltype.RelDataTypeFactoryImpl$2.get(RelDataTypeFactoryImpl.java:143)\r\n\tat org.eigenbase.reltype.RelDataTypeFactoryImpl$2.get(RelDataTypeFactoryImpl.java:140)\r\n\tat java.util.AbstractList$Itr.next(AbstractList.java:358)\r\n\tat java.util.AbstractList.hashCode(AbstractList.java:540)\r\n\tat org.eigenbase.util.Util.hash(Util.java:225)\r\n\tat org.eigenbase.util.Pair.hashCode(Pair.java:79)\r\n\tat com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:331)\r\n\tat com.google.common.base.Equivalence.hash(Equivalence.java:104)\r\n\tat com.google.common.cache.LocalCache.hash(LocalCache.java:1899)\r\n\tat com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3988)\r\n\tat com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4783)\r\n\tat org.eigenbase.reltype.RelDataTypeFactoryImpl.canonize(RelDataTypeFactoryImpl.java:347)\r\n\tat org.eigenbase.reltype.RelDataTypeFactoryImpl.createStructType(RelDataTypeFactoryImpl.java:139)\r\n\tat org.eigenbase.reltype.RelDataTypeFactoryImpl.createStructType(RelDataTypeFactoryImpl.java:167)\r\n\tat org.eigenbase.rel.AggregateRelBase.deriveRowType(AggregateRelBase.java:172)\r\n\tat org.eigenbase.rel.AbstractRelNode.getRowType(AbstractRelNode.java:210)\r\n\tat org.eigenbase.sql2rel.RelDecorrelator.decorrelateRel(RelDecorrelator.java:918)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat org.eigenbase.util.ReflectUtil.invokeVisitorInternal(ReflectUtil.java:252)\r\n\tat org.eigenbase.util.ReflectUtil.invokeVisitor(ReflectUtil.java:209)\r\n\tat org.eigenbase.util.ReflectUtil$1.invokeVisitor(ReflectUtil.java:473)\r\n\tat org.eigenbase.sql2rel.RelDecorrelator$DecorrelateRelVisitor.visit(RelDecorrelator.java:1373)\r\n\tat org.eigenbase.rel.SingleRel.childrenAccept(SingleRel.java:69)\r\n\tat org.eigenbase.rel.RelVisitor.visit(RelVisitor.java:45)\r\n\tat org.eigenbase.sql2rel.RelDecorrelator$DecorrelateRelVisitor.visit(RelDecorrelator.java:1368)\r\n\tat org.eigenbase.rel.SingleRel.childrenAccept(SingleRel.java:69)\r\n\tat org.eigenbase.rel.RelVisitor.visit(RelVisitor.java:45)\r\n\tat org.eigenbase.sql2rel.RelDecorrelator$DecorrelateRelVisitor.visit(RelDecorrelator.java:1368)\r\n\tat org.eigenbase.rel.SingleRel.childrenAccept(SingleRel.java:69)\r\n\tat org.eigenbase.rel.RelVisitor.visit(RelVisitor.java:45)\r\n\tat org.eigenbase.sql2rel.RelDecorrelator$DecorrelateRelVisitor.visit(RelDecorrelator.java:1368)\r\n\tat org.eigenbase.rel.SingleRel.childrenAccept(SingleRel.java:69)\r\n\tat org.eigenbase.rel.RelVisitor.visit(RelVisitor.java:45)\r\n\tat org.eigenbase.sql2rel.RelDecorrelator$DecorrelateRelVisitor.visit(RelDecorrelator.java:1368)\r\n\tat org.eigenbase.rel.JoinRelBase.childrenAccept(JoinRelBase.java:182)\r\n\tat org.eigenbase.rel.RelVisitor.visit(RelVisitor.java:45)\r\n\tat org.eigenbase.sql2rel.RelDecorrelator$DecorrelateRelVisitor.visit(RelDecorrelator.java:1368)\r\n\tat org.eigenbase.rel.SingleRel.childrenAccept(SingleRel.java:69)\r\n\tat org.eigenbase.rel.RelVisitor.visit(RelVisitor.java:45)\r\n\tat org.eigenbase.sql2rel.RelDecorrelator$DecorrelateRelVisitor.visit(RelDecorrelator.java:1368)\r\n\tat org.eigenbase.rel.SingleRel.childrenAccept(SingleRel.java:69)\r\n\tat org.eigenbase.rel.RelVisitor.visit(RelVisitor.java:45)\r\n\tat org.eigenbase.sql2rel.RelDecorrelator$DecorrelateRelVisitor.visit(RelDecorrelator.java:1368)\r\n\tat org.eigenbase.sql2rel.RelDecorrelator.decorrelate(RelDecorrelator.java:131)\r\n\tat org.eigenbase.sql2rel.SqlToRelConverter.decorrelateQuery(SqlToRelConverter.java:2747)\r\n\tat org.eigenbase.sql2rel.SqlToRelConverter.decorrelate(SqlToRelConverter.java:363)\r\n\tat net.hydromatic.optiq.prepare.PlannerImpl.convert(PlannerImpl.java:189)\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getLogicalRel(DrillSqlWorker.java:127)\r\n\tat org.apache.drill.exec.planner.sql.DrillSqlWorker.getPhysicalPlan(DrillSqlWorker.java:216)\r\n\tat org.apache.drill.BaseTestQuery.testSqlPlan(BaseTestQuery.java:148)\r\n\tat org.apache.drill.BaseTestQuery.testSqlPlanFromFile(BaseTestQuery.java:107)\r\n\tat org.apache.drill.TestTpchPlanning.tpch20(TestTpchPlanning.java:128)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n"
    ],
    [
        "DRILL-775",
        "DRILL-1214",
        "verification fails for select count(distinct l_orderkey) from lineitem group by l_partkey Datasources: TPCH (10MB), three-way split parquet files\r\ngit.commit.id.abbrev=70fab8c\r\ngit.commit.id=70fab8c968a6dc05471aa1f32009cd15945d4f3d\r\nVerification fails for the following query:\r\n\r\n2014-05-19 11:01:23 INFO  QuerySubmitter:92 - Submitting query:                                           \r\nselect count(distinct l_orderkey) from lineitem group by l_partkey                                        \r\n2014-05-19 11:01:23 INFO  DrillTestBase:198 - Query submit end time: 2014/05/19 11:01:23.0023             \r\n2014-05-19 11:01:23 INFO  DrillTestBase:199 - The execution time for the query: 0 seconds.                \r\n2014-05-19 11:01:23 INFO  TestVerifier:203 - These rows are not expected:                                 \r\n2014-05-19 11:01:23 INFO  TestVerifier:206 -    2 : 1 time(s).                                            \r\n2014-05-19 11:01:23 INFO  TestVerifier:206 -    1 : 1 time(s).                                            \r\n2014-05-19 11:01:23 INFO  TestVerifier:206 -    30 : 1 time(s).                                           \r\n2014-05-19 11:01:23 INFO  TestVerifier:206 -    25 : 1 time(s).                                           \r\n2014-05-19 11:01:23 INFO  TestVerifier:213 - Total number of unexpected rows: 4                           \r\n2014-05-19 11:01:23 INFO  TestVerifier:217 - These rows are expected but are not in result set:           \r\n2014-05-19 11:01:23 INFO  TestVerifier:220 -    25 : 92 time(s).                                          \r\n2014-05-19 11:01:23 INFO  TestVerifier:220 -    26 : 116 time(s).                                         \r\n2014-05-19 11:01:23 INFO  TestVerifier:220 -    30 : 138 time(s).                                         \r\n2014-05-19 11:01:23 INFO  TestVerifier:220 -    32 : 127 time(s).                                         \r\n2014-05-19 11:01:23 INFO  TestVerifier:227 - Total number of expected but missing: 4",
        "Query with filter conditions fail while querying against Views built against Hbase tables I created a Hbase table called hbase_student as follows in hbase shell:\r\n{code}\r\ncreate 'hbase_student', 'stats'\r\nput 'hbase_student', '1', 'stats:name', 'fred ovid'\r\nput 'hbase_student', '1', 'stats:age', '76'\r\nput 'hbase_student', '1', 'stats:gpa', '1.55'\r\nput 'hbase_student', '1', 'stats:studentnum', '692315658449'\r\nput 'hbase_student', '1', 'stats:create_time', '2014-05-27 00:26:07'\r\nput 'hbase_student', '2', 'stats:name', 'bob brown'\r\nput 'hbase_student', '2', 'stats:age', '63'\r\nput 'hbase_student', '2', 'stats:gpa', '3.18'\r\nput 'hbase_student', '2', 'stats:studentnum', '650706039334'\r\nput 'hbase_student', '2', 'stats:create_time', '2014-12-04 21:43:14'\r\nput 'hbase_student', '3', 'stats:name', 'bob hernandez'\r\nput 'hbase_student', '3', 'stats:age', '28'\r\nput 'hbase_student', '3', 'stats:gpa', '1.09'\r\nput 'hbase_student', '3', 'stats:studentnum', '293612255322'\r\nput 'hbase_student', '3', 'stats:create_time', '2014-05-31 14:33:06'\r\n{code}\r\n\r\nI then logged in to sqlline using dfs.root schema and am able to successfully create a view & query it as long as filter conditions are not present\r\n\r\n{code}\r\ncreate view student_view as\r\nselect cast(tbl.row_key as int)rownum, \r\ncast(tbl.stats.name as varchar(20))name,\r\ncast(tbl.stats.age as int)age, \r\ncast(tbl.stats.gpa as float)gpa,\r\ncast(tbl.stats.studentnum as bigint)studentnum, \r\ncast(tbl.stats.create_time as varchar(20))create_time \r\nfrom hbase.hbase_student tbl\r\norder by rownum;\r\n\r\nSelect * from student_view;\r\n\r\n+------------+------------+------------+------------+------------+-------------+\r\n|   rownum   |    name    |    age     |    gpa     | studentnum | create_time |\r\n+------------+------------+------------+------------+------------+-------------+\r\n| 1          | fred ovid  | 76         | 1.55       | 692315658449 | 2014-05-27 00:26:07 |\r\n| 2          | bob brown  | 63         | 3.18       | 650706039334 | 2014-12-04 21:43:14 |\r\n| 3          | bob hernandez | 28         | 1.09       | 293612255322 | 2014-05-31 14:33:06 |\r\n+------------+------------+------------+------------+------------+-------------+\r\n\r\nSelect name,age,create_time from student_view;\r\n\r\n+------------+------------+-------------+\r\n|    name    |    age     | create_time |\r\n+------------+------------+-------------+\r\n| fred ovid  | 76         | 2014-05-27 00:26:07 |\r\n| bob brown  | 63         | 2014-12-04 21:43:14 |\r\n| bob hernandez | 28         | 2014-05-31 14:33:06 |\r\n+------------+------------+-------------+\r\n{code}\r\n\r\nHowever if I have a filter condition, the query fails as follows:\r\n\r\n{code}\r\nselect * from student_view where age > 50;\r\n\r\nerror_type: 0\r\nmessage: \"Screen received stop request sent. < SchemaChangeException:[ Failure while attempting to load generated class ] < ClassTransformationException:[ Failure generating transformation classes for value: \r\n \r\npackage org.apache.drill.exec.test.generated;\r\n\r\nimport org.apache.drill.exec.exception.SchemaChangeException;\r\nimport org.apache.drill.exec.ops.FragmentContext;\r\nimport org.apache.drill.exec.record.RecordBatch;\r\nimport org.apache.drill.exec.vector.IntVector;\r\nimport org.apache.drill.exec.vector.NullableBigIntVector;\r\nimport org.apache.drill.exec.vector.NullableFloat4Vector;\r\nimport org.apache.drill.exec.vector.NullableIntVector;\r\nimport org.apache.drill.exec.vector.NullableVarCharVector;\r\n\r\npublic class CopierGen287 {\r\n\r\n    IntVector vv0;\r\n    IntVector vv3;\r\n    NullableVarCharVector vv6;\r\n    NullableVarCharVector vv9;\r\n    NullableIntVector vv12;\r\n    NullableIntVector vv15;\r\n    NullableFloat4Vector vv18;\r\n    NullableFloat4Vector vv21;\r\n    NullableBigIntVector vv24;\r\n    NullableBigIntVector vv27;\r\n    NullableVarCharVector vv30;\r\n    NullableVarCharVector vv33;\r\n\r\n    public void doSetup(FragmentContext context, RecordBatch incoming, RecordBatch outgoing)\r\n        throws SchemaChangeException\r\n    {\r\n        {\r\n            int[] fieldIds1 = new int[ 1 ] ;\r\n            fieldIds1 [ 0 ] = 0;\r\n            Object tmp2 = (incoming).getValueAccessorById(IntVector.class, fieldIds1).getValueVector();\r\n            if (tmp2 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv0 with id: org.apache.drill.exec.record.TypedFieldId@21b323dc.\");\r\n            }\r\n            vv0 = ((IntVector) tmp2);\r\n            int[] fieldIds4 = new int[ 1 ] ;\r\n            fieldIds4 [ 0 ] = 0;\r\n            Object tmp5 = (outgoing).getValueAccessorById(IntVector.class, fieldIds4).getValueVector();\r\n            if (tmp5 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv3 with id: org.apache.drill.exec.record.TypedFieldId@21b323dc.\");\r\n            }\r\n            vv3 = ((IntVector) tmp5);\r\n            int[] fieldIds7 = new int[ 1 ] ;\r\n            fieldIds7 [ 0 ] = 1;\r\n            Object tmp8 = (incoming).getValueAccessorById(NullableVarCharVector.class, fieldIds7).getValueVector();\r\n            if (tmp8 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv6 with id: org.apache.drill.exec.record.TypedFieldId@f66d7cdd.\");\r\n            }\r\n            vv6 = ((NullableVarCharVector) tmp8);\r\n            int[] fieldIds10 = new int[ 1 ] ;\r\n            fieldIds10 [ 0 ] = 1;\r\n            Object tmp11 = (outgoing).getValueAccessorById(NullableVarCharVector.class, fieldIds10).getValueVector();\r\n            if (tmp11 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv9 with id: org.apache.drill.exec.record.TypedFieldId@f66d7cdd.\");\r\n            }\r\n            vv9 = ((NullableVarCharVector) tmp11);\r\n            int[] fieldIds13 = new int[ 1 ] ;\r\n            fieldIds13 [ 0 ] = 2;\r\n            Object tmp14 = (incoming).getValueAccessorById(NullableIntVector.class, fieldIds13).getValueVector();\r\n            if (tmp14 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv12 with id: org.apache.drill.exec.record.TypedFieldId@2376fc9d.\");\r\n            }\r\n            vv12 = ((NullableIntVector) tmp14);\r\n            int[] fieldIds16 = new int[ 1 ] ;\r\n            fieldIds16 [ 0 ] = 2;\r\n            Object tmp17 = (outgoing).getValueAccessorById(NullableIntVector.class, fieldIds16).getValueVector();\r\n            if (tmp17 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv15 with id: org.apache.drill.exec.record.TypedFieldId@2376fc9d.\");\r\n            }\r\n            vv15 = ((NullableIntVector) tmp17);\r\n            int[] fieldIds19 = new int[ 1 ] ;\r\n            fieldIds19 [ 0 ] = 3;\r\n            Object tmp20 = (incoming).getValueAccessorById(NullableFloat4Vector.class, fieldIds19).getValueVector();\r\n            if (tmp20 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv18 with id: org.apache.drill.exec.record.TypedFieldId@3d6b2cfd.\");\r\n            }\r\n            vv18 = ((NullableFloat4Vector) tmp20);\r\n            int[] fieldIds22 = new int[ 1 ] ;\r\n            fieldIds22 [ 0 ] = 3;\r\n            Object tmp23 = (outgoing).getValueAccessorById(NullableFloat4Vector.class, fieldIds22).getValueVector();\r\n            if (tmp23 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv21 with id: org.apache.drill.exec.record.TypedFieldId@3d6b2cfd.\");\r\n            }\r\n            vv21 = ((NullableFloat4Vector) tmp23);\r\n            int[] fieldIds25 = new int[ 1 ] ;\r\n            fieldIds25 [ 0 ] = 4;\r\n            Object tmp26 = (incoming).getValueAccessorById(NullableBigIntVector.class, fieldIds25).getValueVector();\r\n            if (tmp26 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv24 with id: org.apache.drill.exec.record.TypedFieldId@c6480360.\");\r\n            }\r\n            vv24 = ((NullableBigIntVector) tmp26);\r\n            int[] fieldIds28 = new int[ 1 ] ;\r\n            fieldIds28 [ 0 ] = 4;\r\n            Object tmp29 = (outgoing).getValueAccessorById(NullableBigIntVector.class, fieldIds28).getValueVector();\r\n            if (tmp29 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv27 with id: org.apache.drill.exec.record.TypedFieldId@c6480360.\");\r\n            }\r\n            vv27 = ((NullableBigIntVector) tmp29);\r\n            int[] fieldIds31 = new int[ 1 ] ;\r\n            fieldIds31 [ 0 ] = 5;\r\n            Object tmp32 = (incoming).getValueAccessorById(NullableVarCharVector.class, fieldIds31).getValueVector();\r\n            if (tmp32 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv30 with id: org.apache.drill.exec.record.TypedFieldId@fd40df59.\");\r\n            }\r\n            vv30 = ((NullableVarCharVector) tmp32);\r\n            int[] fieldIds34 = new int[ 1 ] ;\r\n            fieldIds34 [ 0 ] = 5;\r\n            Object tmp35 = (outgoing).getValueAccessorById(NullableVarCharVector.class, fieldIds34).getValueVector();\r\n            if (tmp35 == null) {\r\n                throw new SchemaChangeException(\"Failure while loading vector vv33 with id: org.apache.drill.exec.record.TypedFieldId@fd40df59.\");\r\n            }\r\n            vv33 = ((NullableVarCharVector) tmp35);\r\n        }\r\n    }\r\n\r\n    public boolean doEval(int inIndex, int outIndex)\r\n        throws SchemaChangeException\r\n    {\r\n        {\r\n            if (!vv3 .copyFromSafe(((inIndex)& 65535), (outIndex), vv0 [((inIndex)>>> 16)])) {\r\n                return false;\r\n            }\r\n            if (!vv9 .copyFromSafe(((inIndex)& 65535), (outIndex), vv6 [((inIndex)>>> 16)])) {\r\n                return false;\r\n            }\r\n            if (!vv15 .copyFromSafe(((inIndex)& 65535), (outIndex), vv12 [((inIndex)>>> 16)])) {\r\n                return false;\r\n            }\r\n            if (!vv21 .copyFromSafe(((inIndex)& 65535), (outIndex), vv18 [((inIndex)>>> 16)])) {\r\n                return false;\r\n            }\r\n            if (!vv27 .copyFromSafe(((inIndex)& 65535), (outIndex), vv24 [((inIndex)>>> 16)])) {\r\n                return false;\r\n            }\r\n            if (!vv33 .copyFromSafe(((inIndex)& 65535), (outIndex), vv30 [((inIndex)>>> 16)])) {\r\n                return false;\r\n            }\r\n        }\r\n        {\r\n            return true;\r\n        }\r\n    }\r\n\r\n}\r\n ] < CompileException:[ Line 123, Column 36: No applicable constructor/method found for actual parameters \"int, int, java.lang.Object\"; candidates are: \"public boolean org.apache.drill.exec.vector.IntVector.copyFromSafe(int, int, org.apache.drill.exec.vector.IntVector)\" ]\"\r\n]\r\n{code}\r\n\r\nThe Exception in the drillbit.log is:\r\n{code}\r\n2014-07-28 17:33:31,496 [7b0e3219-8f05-480e-a5f5-bcf1e505c044:frag:0:0] ERROR o.a.d.e.w.f.AbstractStatusReporter - Error 373d229a-9697-46c0-a776-a747d5b0cf7a: Failure while running fragment.\r\norg.codehaus.commons.compiler.CompileException: Line 123, Column 36: No applicable constructor/method found for actual parameters \"int, int, java.lang.Object\"; candidates are: \"public boolean org.apache.drill.exec.vector.IntVector.copyFromSafe(int, int, org.apache.drill.exec.vector.IntVector)\"\r\n\tat org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10056) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7466) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7336) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7239) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3860) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:182) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3261) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3978) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3288) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4354) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileBoolean2(UnitCompiler.java:2854) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.access$4800(UnitCompiler.java:182) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler$8.visitMethodInvocation(UnitCompiler.java:2815) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3978) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileBoolean(UnitCompiler.java:2842) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileBoolean2(UnitCompiler.java:2872) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.access$4900(UnitCompiler.java:182) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler$8.visitUnaryOperation(UnitCompiler.java:2808) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.Java$UnaryOperation.accept(Java.java:3651) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileBoolean(UnitCompiler.java:2842) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1743) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:182) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:941) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.Java$IfStatement.accept(Java.java:2145) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:962) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1004) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:989) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:182) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:939) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.Java$Block.accept(Java.java:2005) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:962) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1004) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2284) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:826) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:798) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:503) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:389) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:182) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:343) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1136) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:350) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:318) ~[janino-2.7.4.jar:2.7.4]\r\n\tat org.apache.drill.exec.compile.JaninoClassCompiler.getByteCode(JaninoClassCompiler.java:48) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.compile.AbstractClassCompiler.getClassByteCode(AbstractClassCompiler.java:43) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.compile.QueryClassLoader$ClassCompilerSelector.getClassByteCode(QueryClassLoader.java:127) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.compile.QueryClassLoader$ClassCompilerSelector.access$000(QueryClassLoader.java:100) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.compile.QueryClassLoader.getClassByteCode(QueryClassLoader.java:93) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.compile.ClassTransformer.getImplementationClass(ClassTransformer.java:254) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.ops.FragmentContext.getImplementationClass(FragmentContext.java:182) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.getGenerated4Copier(RemovingRecordBatch.java:264) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.getGenerated4Copier(RemovingRecordBatch.java:250) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.setupNewSchema(RemovingRecordBatch.java:80) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:66) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.svremover.RemovingRecordBatch.innerNext(RemovingRecordBatch.java:96) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:91) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:116) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:72) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:65) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:45) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:95) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:91) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:116) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:58) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:97) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:48) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:100) ~[drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:242) [drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar:1.0.0-m2-incubating-SNAPSHOT]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_55]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_55]\r\n\tat java.lang.Thread.run(Thread.java:744) [na:1.7.0_55]\r\n{code}\r\n\r\nBut The same filter condition works if I apply it directly on Hbase table instead of the view\r\n{code}\r\nselect cast(tbl.row_key as int)rownum, \r\ncast(tbl.stats.name as varchar(20))name,\r\ncast(tbl.stats.age as int)age, \r\ncast(tbl.stats.gpa as float)gpa,\r\ncast(tbl.stats.studentnum as bigint)studentnum, \r\ncast(tbl.stats.create_time as varchar(20))create_time \r\nfrom hbase.hbase_student tbl\r\nwhere tbl.stats.age > 50;\r\n\r\n+------------+------------+------------+------------+------------+-------------+\r\n|   rownum   |    name    |    age     |    gpa     | studentnum | create_time |\r\n+------------+------------+------------+------------+------------+-------------+\r\n| 1          | fred ovid  | 76         | 1.55       | 692315658449 | 2014-05-27 00:26:07 |\r\n| 2          | bob brown  | 63         | 3.18       | 650706039334 | 2014-12-04 21:43:14 |\r\n+------------+------------+------------+------------+------------+-------------+\r\n{code}\r\n\r\n"
    ],
    [
        "DRILL-3041",
        "DRILL-365",
        "Impersonation-user can create view against file that user doesn't have read access to git.commit.id.abbrev=d10769f\r\n\r\nI have a file that has the following permission:\r\n-rwx------   3 qa2 users      63078 2015-01-30 21:19 /drill/testdata/csv/voter.csv\r\n\r\nThe directory right above the file has the following permission:\r\ndrwxr-xr-x   - qa2  users          3 2015-05-12 14:22 /drill/testdata/csv\r\n\r\nLogged into sqlline as a different user and attempted to create a view:\r\n0: jdbc:drill:schema=dfs.root> CREATE VIEW `dfs.qa1`.`test_v4` AS SELECT columns[0] as column_0, columns[1] as column_1, columns[2] as column_2, columns[3] as column_3, columns[4] as column_4, columns[5] as column_5, columns[6] as column_6 FROM `dfs`.`default`.`drill/testdata/csv/voter.csv` LIMIT 100;\r\n\r\nThe view got created successfully. However if I tried to read from the view, I can't because of the lack of permission to the voter.csv table:\r\n0: jdbc:drill:schema=dfs.root> select * from `dfs.qa1`.`test_v4`;\r\nError: SYSTEM ERROR: org.apache.hadoop.security.AccessControlException: Open failed for file: /drill/testdata/csv/voter.csv, error: Permission denied (13)\r\n\r\nCurrently drill only check if the folder contains correct permission and not at the file level when creating views.  It seems odd that a user is allowed to create the view then not being able to access it afterwards.",
        "Large number is taken in as int which overflows if it is too big 0: jdbc:drill:schema=json-cp> select * from \"customer.json\";\r\nwill fail with Numeric value (xxxxxxxxxxxxx) out of range of int\r\n\r\n"
    ],
    [
        "DRILL-2180",
        "DRILL-4306",
        "Star is not expanded when being used with flatten For example,\r\nselect *, \"flatten(j.topping) tt \" +\r\n              \"from dfs_test.`%s` j \"\r\n(using the same data set in DRILL-2012)\r\n\r\n*\ttt\r\nnull\t{\"id\":\"5001\",\"type\":\"None\"}\r\nnull\t{\"id\":\"5002\",\"type\":\"Glazed\"}\r\nnull\t{\"id\":\"5005\",\"type\":\"Sugar\"}\r\nnull\t{\"id\":\"5007\",\"type\":\"Powdered Sugar\"}\r\nnull\t{\"id\":\"5006\",\"type\":\"Chocolate with Sprinkles\"}\r\nnull\t{\"id\":\"5003\",\"type\":\"Chocolate\"}\r\nnull\t{\"id\":\"5004\",\"type\":\"Maple\"}\r\n\r\nNote that the first column is messed. ",
        "Extensions for Storage Plugins are not recognized properly  Hello everyone,\r\n\r\ntoday I defined my own storage plugin and thereby I observed strange behaviour. \r\n\r\nThe plugin has the following structure: \r\n{\r\n  \"type\": \"file\",\r\n  \"enabled\": true,\r\n  \"connection\": \"file:///\",\r\n  \"workspaces\": {\r\n    \"root\": {\r\n      \"location\": \"/\",\r\n      \"writable\": false,\r\n      \"defaultInputFormat\": null\r\n    },\r\n    \"tmp\": {\r\n      \"location\": \"/tmp\",\r\n      \"writable\": true,\r\n      \"defaultInputFormat\": null\r\n    }\r\n  },\r\n  \"formats\": {\r\n    \"hs\": {\r\n      \"type\": \"json\",\r\n      \"extensions\": [\r\n        \"hs\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\nI prepared a folder with the following files inside. \r\n.../testdir/test.hs\r\n.../testdir/test1.hs\r\n.../testdir/test2.hs\r\n.../testdir/test3.hss\r\n.../testdir/test4.csv\r\n\r\nBased on this folder, I started to prepare queries:\r\n\r\nmyplugin.`C:Users\\someuser\\Desktop\\testdir`\r\n\r\nHere, I expected that the plugin only selects files with the .hs extension. However, I explored that all files were loaded instead of loading only .hs files as Drill does when querying \r\nmyplugin.`C:Users\\someuser\\Desktop\\testdir\\*.hs`\r\n\r\nHowever, I also detected more strange behaviour. \r\nWhen the first file in the folder starts with .hs, all files are loaded (independent of their extension as described above). However, when the first file starts with another extension (e.g., .json, .csv) then Apache Drill fails and says that the folder contains invalid extensions. Therefore, I currently assume that the current implementation just checks the extension of the first File in the folder and then reacts as described above. \r\n\r\nHowever, in my opinion this behaviour seems strange. \r\n\r\nUsually I would expect that drill acts like it does for performing the following query:\r\nmyplugin.`C:Users\\someuser\\Desktop\\testdir\\*.hs`\r\n\r\nBest regards,\r\nAndr\u00e9"
    ],
    [
        "DRILL-3139",
        "DRILL-423",
        "Query against yelp academic dataset causes exception - improve error message I was following along the tutorial for \"Analyzing the Yelp Academic Dataset.\"\r\nI tried the first query \"Querying Yelp Business Data\" WITHOUT the limit clause:\r\nselect * from\r\n dfs.`/users/nrentachintala/Downloads/yelp/yelp_academic_dataset_business.json`;\r\n\r\n(Adjust for your own download path).\r\n\r\nA bunch of data comes out, followed by an exception:\r\n\"Dietary Restrictions\":{}} | business   | null          |\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n0: jdbc:drill:zk=local>\r\n\r\nNote that this was tried against the 0.9.0 tarball referred to in the tutorials and download links.\r\n",
        "Need a C++ client for Drill WE need a C++ client for drill that can be used to write an ODBC driver. \r\nThe API should include \r\n1) Connect\r\n2) Handshake\r\n3) Execute Query\r\n4) Fetch Results \r\n\r\nThe API needs to provide interfaces to decode the RPC messages into RecordBatches and ValueVectors and provide accessors to get values from ValueVectors.\r\n\r\nInitial implementation of the  ValueVectors interface will probably not support mutators for ValueVectors.\r\n\r\nNot all datatypes will be supported. In particular MAP and REPEATMAP will not be supported. Nullable and repeat types will be supported. The following is the list of datatypes for which support is planned (defined in Types.proto)\r\n{code}\r\n    LATE = 0;   //  late binding type\r\n    MAP = 1;   //  an empty map column.  Useful for conceptual setup.  Children listed within here\r\n    REPEATMAP = 2;   //  a repeated map column (means that multiple children sit below this)\r\n    TINYINT = 3;   //  single byte signed integer\r\n    SMALLINT = 4;   //  two byte signed integer\r\n    INT = 5;   //  four byte signed integer\r\n    BIGINT = 6;   //  eight byte signed integer\r\n    DECIMAL9 = 7;   //  a decimal supporting precision between 1 and 9\r\n    DECIMAL18 = 8;   //  a decimal supporting precision between 10 and 18\r\n    DECIMAL28SPARSE = 9;   //  a decimal supporting precision between 19 and 28\r\n    DECIMAL38SPARSE = 10;   //  a decimal supporting precision between 29 and 38\r\n    MONEY = 11;   //  signed decimal with two digit precision\r\n    DATE = 12;   //  days since 4713bc\r\n    TIME = 13;   //  time in micros before or after 2000/1/1\r\n    TIMETZ = 14;   //  time in micros before or after 2000/1/1 with timezone\r\n    TIMESTAMPTZ = 15;   //  unix epoch time in millis\r\n    TIMESTAMP = 16;   //  TBD\r\n    INTERVAL = 17;   //  TBD\r\n    FLOAT4 = 18;   //  4 byte ieee 754 \r\n    FLOAT8 = 19;   //  8 byte ieee 754\r\n    BIT = 20;   //  single bit value (boolean)\r\n    FIXEDCHAR = 21;   //  utf8 fixed length string, padded with spaces\r\n    FIXED16CHAR = 22;\r\n    FIXEDBINARY = 23;   //  fixed length binary, padded with 0 bytes\r\n    VARCHAR = 24;   //  utf8 variable length string\r\n    VAR16CHAR = 25; // utf16 variable length string\r\n    VARBINARY = 26;   //  variable length binary\r\n    UINT1 = 29;   //  unsigned 1 byte integer\r\n    UINT2 = 30;   //  unsigned 2 byte integer\r\n    UINT4 = 31;   //  unsigned 4 byte integer\r\n    UINT8 = 32;   //  unsigned 8 byte integer\r\n    DECIMAL28DENSE = 33; // dense decimal representation, supporting precision between 19 and 28\r\n    DECIMAL38DENSE = 34; // dense decimal representation, supporting precision between 28 and 38\r\n    NULL = 37; // a value of unknown type (e.g. a missing reference).\r\n    INTERVALYEAR = 38; // Interval type specifying YEAR to MONTH\r\n    INTERVALDAY = 39; // Interval type specifying DAY to SECONDS\r\n{code}"
    ],
    [
        "DRILL-4142",
        "DRILL-1930",
        "Implement Node-level Memory maximums for a query. ",
        "Improve Error Message when running subquery with naked condition on ANY value {code}\r\n#Fri Jan 02 21:20:47 EST 2015\r\ngit.commit.id.abbrev=b491cdb\r\n{code}\r\n\r\nI was debugging TPCDS query failure and ran into this failure below:\r\n\r\n{code}\r\nselect  a.ca_state state,\r\n        count(*) cnt\r\nfrom    customer_address a\r\n        ,item i\r\nwhere\r\n        (select distinct (d.d_month_seq)\r\n         from   date_dim d\r\n         where  d.d_year = 1998\r\n                and d.d_moy = 5 )\r\n                and i.i_current_price > 1.2\r\ngroup by a.ca_state;\r\n\r\n2015-01-05 21:10:51,561 [2b550223-9064-c659-d27e-3329af3718b8:foreman] ERROR o.a.drill.exec.work.foreman.Foreman - Error 2a72168b-c23e-485b-97fd-4644aa65dbc0: Query failed: Unexpected exception during fragment initialization: Internal error: Error while applying rule DrillPushFilterPastProjectRule, args [rel#6321:FilterRel.NONE.ANY([]).[](child=rel#6320:Subset#16.NONE.ANY([]).[],condition=$1), rel#6317:ProjectRel.NONE.ANY([]).[](child=rel#6315:Subset#15.NONE.ANY([]).[],ca_state=$1,$f0=$4)]\r\norg.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: Internal error: Error while applying rule DrillPushFilterPastProjectRule, args [rel#6321:FilterRel.NONE.ANY([]).[](child=rel#6320:Subset#16.NONE.ANY([]).[],condition=$1), rel#6317:ProjectRel.NONE.ANY([]).[](child=rel#6315:Subset#15.NONE.ANY([]).[],ca_state=$1,$f0=$4)]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:194) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_71]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]\r\nCaused by: java.lang.AssertionError: Internal error: Error while applying rule DrillPushFilterPastProjectRule, args [rel#6321:FilterRel.NONE.ANY([]).[](child=rel#6320:Subset#16.NONE.ANY([]).[],condition=$1), rel#6317:ProjectRel.NONE.ANY([]).[](child=rel#6315:Subset#15.NONE.ANY([]).[],ca_state=$1,$f0=$4)]\r\n        at org.eigenbase.util.Util.newInternal(Util.java:750) ~[optiq-core-0.9-drill-r16.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:246) ~[optiq-core-0.9-drill-r16.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:661) ~[optiq-core-0.9-drill-r16.jar:na]\r\n        at net.hydromatic.optiq.tools.Programs$RuleSetProgram.run(Programs.java:165) ~[optiq-core-0.9-drill-r16.jar:na]\r\n        at net.hydromatic.optiq.prepare.PlannerImpl.transform(PlannerImpl.java:273) ~[optiq-core-0.9-drill-r16.jar:na]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToDrel(DefaultSqlHandler.java:155) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:134) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:507) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:185) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        ... 4 common frames omitted\r\nCaused by: java.lang.AssertionError: condition must be boolean\r\n        at org.eigenbase.rex.RexProgram.isValid(RexProgram.java:460) ~[optiq-core-0.9-drill-r16.jar:na]\r\n        at org.eigenbase.rex.RexProgram.<init>(RexProgram.java:102) ~[optiq-core-0.9-drill-r16.jar:na]\r\n        at org.eigenbase.rex.RexProgramBuilder.getProgram(RexProgramBuilder.java:395) ~[optiq-core-0.9-drill-r16.jar:na]\r\n        at org.eigenbase.rex.RexProgramBuilder.getProgram(RexProgramBuilder.java:392) ~[optiq-core-0.9-drill-r16.jar:na]\r\n        at org.eigenbase.rex.RexProgramBuilder.getProgram(RexProgramBuilder.java:365) ~[optiq-core-0.9-drill-r16.jar:na]\r\n        at org.eigenbase.relopt.RelOptUtil.pushFilterPastProject(RelOptUtil.java:2180) ~[optiq-core-0.9-drill-r16.jar:na]\r\n        at org.apache.drill.exec.planner.logical.DrillPushFilterPastProjectRule.onMatch(DrillPushFilterPastProjectRule.java:101) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.eigenbase.relopt.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:223) ~[optiq-core-0.9-drill-r16.jar:na]\r\n        ... 12 common frames omitted\r\n{code}\r\n\r\nIf you comment out the second filter, you get correct error that we should have received in the first place (more or less).\r\n\r\n{code}0: jdbc:drill:schema=dfs> select  a.ca_state state,\r\n. . . . . . . . . . . . >         count(*) cnt\r\n. . . . . . . . . . . . > from    customer_address a\r\n. . . . . . . . . . . . >         ,item i\r\n. . . . . . . . . . . . > where\r\n. . . . . . . . . . . . >         (select distinct (d.d_month_seq)\r\n. . . . . . . . . . . . >          from   date_dim d\r\n. . . . . . . . . . . . >          where  d.d_year = 1998\r\n. . . . . . . . . . . . >                 and d.d_moy = 5 )\r\n. . . . . . . . . . . . >                 --and i.i_current_price > 1.2\r\n. . . . . . . . . . . . > group by a.ca_state;\r\nQuery failed: Query failed: Failure validating SQL. org.eigenbase.util.EigenbaseContextException: From line 6, column 10 to line 9, column 31: WHERE clause must be a condition\r\n{code}\r\n\r\nHere is what postgres returns in such a case:\r\n{code}\r\npostgres=# select * from t1 where (select distinct a2 from t2);\r\nERROR:  argument of WHERE must be type boolean, not type integer\r\n{code}"
    ],
    [
        "DRILL-1485",
        "DRILL-221",
        "tools/verify_release.sh does not have execute permission bit set ",
        "Add license header to all files, enable Rat and enforcer  There are number of files that are missing license headers.  These need to be fixed before release.  Additionally, Rat and enforcer should be updated to automatically check this moving forward.\r\n\r\nhttp://www.apache.org/legal/src-headers.html\r\n\r\nhttp://incubator.apache.org/guides/releasemanagement.html#best-practice"
    ],
    [
        "DRILL-144",
        "DRILL-1425",
        "Explore UDT, NIO2, etc implementation Explore whether utilizing UDT as an alternative socket provider provides performance improvements over existing NIO provider.  Also try out other socket implementations provided by Jetty.  \r\n\r\nUDT Info: http://udt.sourceforge.net/",
        "Handle unknown operators in web ui If the operator type doesn't match with one of the enumerated types in CoreOperatorType, the web ui fails to load the query profile.\r\n\r\nTo solve this, we will first check if the CoreOperatorType is null, and if so, use the String \"UNKNOWN_OPERATOR\"."
    ],
    [
        "DRILL-3891",
        "DRILL-3923",
        "ROW_KEY filter IN(integer values) does not get pushed in to Scan ROW_KEY filter does not get pushed into Scan when filter involved IN (integer values). Data inserted into HBase table is byte ordered and encoded as Int32.\r\n\r\ncase 1) NOT IN (8388607,2147483647,67108863,-536870912,-2147483648);\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> explain plan for select convert_from(row_key,'INT_OB') as rk, convert_from(T.`colfam1`.`qual1`,'UTF8') val from int_Tbl T where convert_from(row_key,'INT_OB') NOT IN (8388607,2147483647,67108863,-536870912,-2147483648);\r\n+------+------+\r\n| text | json |\r\n+------+------+\r\n| 00-00    Screen\r\n00-01      Project(rk=[CONVERT_FROMINT_OB($0)], val=[CONVERT_FROMUTF8(ITEM($1, 'qual1'))])\r\n00-02        SelectionVectorRemover\r\n00-03          Filter(condition=[NOT(OR(=(CONVERT_FROM($0, 'INT_OB'), 8388607), =(CONVERT_FROM($0, 'INT_OB'), 2147483647), =(CONVERT_FROM($0, 'INT_OB'), 67108863), =(CONVERT_FROM($0, 'INT_OB'), -536870912), =(CONVERT_FROM($0, 'INT_OB'), -2147483648)))])\r\n00-04            Scan(groupscan=[HBaseGroupScan [HBaseScanSpec=HBaseScanSpec [tableName=int_Tbl, startRow=null, stopRow=null, filter=null], columns=[`*`]]])\r\n{code}\r\n\r\ncase 2) NOT IN ('8388607','2147483647','67108863','-536870912','-2147483648');\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> explain plan for select convert_from(row_key,'INT_OB') as rk, convert_from(T.`colfam1`.`qual1`,'UTF8') val from int_Tbl T where convert_from(row_key,'INT_OB') NOT IN ('8388607','2147483647','67108863','-536870912','-2147483648');\r\n+------+------+\r\n| text | json |\r\n+------+------+\r\n| 00-00    Screen\r\n00-01      Project(rk=[CONVERT_FROMINT_OB($0)], val=[CONVERT_FROMUTF8(ITEM($1, 'qual1'))])\r\n00-02        SelectionVectorRemover\r\n00-03          Filter(condition=[NOT(OR(=(CONVERT_FROM($0, 'INT_OB'), '8388607'), =(CONVERT_FROM($0, 'INT_OB'), '2147483647'), =(CONVERT_FROM($0, 'INT_OB'), '67108863'), =(CONVERT_FROM($0, 'INT_OB'), '-536870912'), =(CONVERT_FROM($0, 'INT_OB'), '-2147483648')))])\r\n00-04            Scan(groupscan=[HBaseGroupScan [HBaseScanSpec=HBaseScanSpec [tableName=int_Tbl, startRow=null, stopRow=null, filter=null], columns=[`*`]]])\r\n{code}\r\n\r\ncase 3) NOT IN (cast('8388607' as int),cast('2147483647' as int),cast('67108863' as int),cast('-536870912'as int),cast('-2147483648' as int));\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> explain plan for select convert_from(row_key,'INT_OB') as rk, convert_from(T.`colfam1`.`qual1`,'UTF8') val from int_Tbl T where convert_from(row_key,'INT_OB') NOT IN (cast('8388607' as int),cast('2147483647' as int),cast('67108863' as int),cast('-536870912'as int),cast('-2147483648' as int));\r\n+------+------+\r\n| text | json |\r\n+------+------+\r\n| 00-00    Screen\r\n00-01      Project(rk=[CONVERT_FROMINT_OB($0)], val=[CONVERT_FROMUTF8(ITEM($1, 'qual1'))])\r\n00-02        SelectionVectorRemover\r\n00-03          Filter(condition=[NOT(OR(=(CONVERT_FROM($0, 'INT_OB'), CAST('8388607'):INTEGER NOT NULL), =(CONVERT_FROM($0, 'INT_OB'), CAST('2147483647'):INTEGER NOT NULL), =(CONVERT_FROM($0, 'INT_OB'), CAST('67108863'):INTEGER NOT NULL), =(CONVERT_FROM($0, 'INT_OB'), CAST('-536870912'):INTEGER NOT NULL), =(CONVERT_FROM($0, 'INT_OB'), CAST('-2147483648'):INTEGER NOT NULL)))])\r\n00-04            Scan(groupscan=[HBaseGroupScan [HBaseScanSpec=HBaseScanSpec [tableName=int_Tbl, startRow=null, stopRow=null, filter=null], columns=[`*`]]])\r\n{code}\r\n\r\nData inserted into HBase table\r\n\r\n{code}\r\nint[] arr = {Integer.MIN_VALUE,Integer.MIN_VALUE/4,Integer.MIN_VALUE/8,Integer.MIN_VALUE/16,Integer.MIN_VALUE/32,Integer.MIN_VALUE/64,Integer.MIN_VALUE/128,Integer.MAX_VALUE,Integer.MAX_VALUE/4,Integer.MAX_VALUE/8,Integer.MAX_VALUE/16,Integer.MAX_VALUE/32,Integer.MAX_VALUE/64,Integer.MAX_VALUE/128,Integer.MAX_VALUE/256,Integer.MAX_VALUE};\r\n\r\n        for (int i = 0; i < arr.length; i++) {\r\n            byte[] bytes = new byte[5];\r\n            org.apache.hadoop.hbase.util.PositionedByteRange br =\r\n                new org.apache.hadoop.hbase.util.SimplePositionedByteRange(bytes, 0, 5);\r\n            org.apache.hadoop.hbase.util.OrderedBytes.encodeInt32(br, arr[i],\r\n                org.apache.hadoop.hbase.util.Order.ASCENDING);\r\n\r\n            Put p = new Put(bytes);\r\n            p.add(Bytes.toBytes(\"colfam1\"),Bytes.toBytes(\"qual1\"),String.format(\"value %d\", i).getBytes());\r\n            table.put(p);\r\n        }\r\n{code}",
        "getTableName(int column) function of ResultSetMetaData interface does not return table name getTableName(int column) function of ResultSetMetaData interface does not return table name, instead it returns an empty string.\r\n\r\nI am on Drill 1.2, git.commit.id=eafe0a24\r\n\r\n{code}\r\n               Statement stmt = conn.createStatement();\r\n                String query = \"select col9 from FEWRWSPQQ_101\";\r\n                ResultSet rs = stmt.executeQuery(query);\r\n                ResultSetMetaData rsmd = rs.getMetaData();\r\n                System.out.println(\"ResultSetMetadata.getTableName(1) :\"+rsmd.getTableName(1));\r\n{code}\r\n\r\nOutput :\r\n\r\n{code}\r\n17:35:11.922 [Client-1] DEBUG o.a.drill.exec.rpc.user.UserClient - Sending response with Sender 729391539\r\nResultSetMetadata.getTableName(1) :\r\n17:35:11.965 [main] DEBUG o.a.d.j.i.DrillResultSetImpl$ResultsListener - [#1] Query listener closing.\r\n{code}"
    ],
    [
        "DRILL-712",
        "DRILL-3432",
        "Right side of Left join has zero values when should be null git.commit.id=e7a486d784c072458d44b7692ea0262da368f001\r\n\r\n2014-05-13 11:16:03 INFO  QuerySubmitter:89 - Submitting query:                                          \r\nselect count(*)                                                                                          \r\n  from (select l.l_orderkey as x, c.c_custkey as y                                                       \r\n        from lineitem l                                                                                  \r\n        left outer join customer c                                                                       \r\n                        on l.l_orderkey = c.c_custkey) as foo                                            \r\n  where y < 10000                                                                                        \r\n2014-05-13 11:16:10 INFO  DrillTestBase:201 - Query submit end time: 2014/05/13 11:16:10.0010            \r\n2014-05-13 11:16:10 INFO  DrillTestBase:202 - The execution time for the query: 7 seconds.               \r\n2014-05-13 11:16:10 INFO  TestVerifier:203 - These rows are not expected:                                \r\n2014-05-13 11:16:10 INFO  TestVerifier:206 -    60175 : 1 time(s).                                       \r\n2014-05-13 11:16:10 INFO  TestVerifier:213 - Total number of unexpected rows: 1                          \r\n2014-05-13 11:16:10 INFO  TestVerifier:217 - These rows are expected but are not in result set:          \r\n2014-05-13 11:16:10 INFO  TestVerifier:220 -    1467 : 1 time(s).                                        \r\n2014-05-13 11:16:10 INFO  TestVerifier:227 - Total number of expected but missing: 1 \r\n\r\nalso,resultset from drill is inconsistent.",
        "Change or document: CAST to interval type doesn't reject non-standard strings In ISO/IEC 9075-2:2011(E) section 6.13 <cast specification>, General Rule 19 case b says that, in a <cast specification> casting to an interval type, a character string value must be a valid interval <literal> (<interval literal>) or <unquoted interval string>, or else a \"data exception \u2014 invalid interval format\" exception is raised.\r\n\r\n(<interval literal> is the \"{{INTERVAL '1-6' YEAR TO MONTH}}\" syntax; <unquoted interval string> is the \"{{1-6}}\" syntax.)\r\n\r\nHowever, in Drill, a cast specification casting to an interval type accepts strings not allowed by the SQL standard (ISO-8601 durations, e.g., \"{{P1Y6M\"}}).\r\n&nbsp;\r\n\r\nDrill should either follow the SQL standard or document the non-standard behavior in the end-user documentation.\r\n&nbsp;\r\n\r\n(Note that Drill's current behavior is not quite an extension:  Although it does extend the set of strings that can be converted, it lacks the rejection of non-standard strings (for, say, validating strings destined for input into a standard-SQL system).)\r\n\r\nTo continue providing the ability to convert ISO-8601-format duration strings into interval values without being non-compliant with the SQL standard, Drill could provide the conversions in conversion functions (perhaps CONVERT_FROM and/or CONVERT_TO).\r\n"
    ],
    [
        "DRILL-1882",
        "DRILL-2482",
        "Add cast and comparison functions needed by tpch queries Currently we are missing some cast functions from varbinary to date/time/timestamp data types.\r\n\r\nWe are also missing equality function for boolean data type.\r\n\r\nThese issues are masked due to addition of secondary level of implicit casts which will no longer be the cast once DRILL-584 is merged.",
        "JDBC : calling getObject when the actual column type is 'NVARCHAR' results in NoClassDefFoundError git.commit.id.abbrev=7b4c887\r\n\r\nI tried to call getObject(i) on a column which is of type varchar, drill failed with the below error :\r\n{code}\r\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/io/Text\r\n\tat org.apache.drill.exec.vector.VarCharVector$Accessor.getObject(VarCharVector.java:407)\r\n\tat org.apache.drill.exec.vector.NullableVarCharVector$Accessor.getObject(NullableVarCharVector.java:386)\r\n\tat org.apache.drill.exec.vector.accessor.NullableVarCharAccessor.getObject(NullableVarCharAccessor.java:98)\r\n\tat org.apache.drill.exec.vector.accessor.BoundCheckingAccessor.getObject(BoundCheckingAccessor.java:137)\r\n\tat org.apache.drill.jdbc.AvaticaDrillSqlAccessor.getObject(AvaticaDrillSqlAccessor.java:136)\r\n\tat net.hydromatic.avatica.AvaticaResultSet.getObject(AvaticaResultSet.java:351)\r\n\tat Dummy.testComplexQuery(Dummy.java:94)\r\n\tat Dummy.main(Dummy.java:30)\r\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.Text\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\r\n\t... 8 more\r\n{code}\r\n\r\n\r\nWhen the underlying type is a primitive, the getObject call succeeds"
    ],
    [
        "DRILL-597",
        "DRILL-2179",
        "shouldn't we have a interval data type in Drill? In postgres, interval itself is a valid data type. It seems in drill, it's not the case, only interval year/day... are valid.\r\n\r\n0: jdbc:drill:schema=dfs> select cast(c_interval as interval) from data;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"689c6a7b-cc66-47b2-8205-d79128060541\"\r\nendpoint {\r\n  address: \"qa-node119.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < SqlParseException:[ Encountered \\\"interval )\\\" at line 1, column 27.\\nWas expecting one of:\\n    \\\"CHARACTER\\\" ...\\n    \\\"CHAR\\\" ...\\n    \\\"VARCHAR\\\" ...\\n    \\\"DATE\\\" ...\\n    \\\"TIME\\\" ...\\n    \\\"TIMESTAMP\\\" ...\\n    \\\"DECIMAL\\\" ...\\n    \\\"DEC\\\" ...\\n    \\\"NUMERIC\\\" ...\\n    \\\"BOOLEAN\\\" ...\\n    \\\"INTEGER\\\" ...\\n    \\\"INT\\\" ...\\n    \\\"BINARY\\\" ...\\n    \\\"VARBINARY\\\" ...\\n    \\\"TINYINT\\\" ...\\n    \\\"SMALLINT\\\" ...\\n    \\\"BIGINT\\\" ...\\n    \\\"REAL\\\" ...\\n    \\\"DOUBLE\\\" ...\\n    \\\"FLOAT\\\" ...\\n    \\\"ANY\\\" ...\\n    \\\"MULTISET\\\" ...\\n    <IDENTIFIER> ...\\n    <QUOTED_IDENTIFIER> ...\\n    <BACK_QUOTED_IDENTIFIER> ...\\n    <BRACKET_QUOTED_IDENTIFIER> ...\\n    <UNICODE_QUOTED_IDENTIFIER> ...\\n    \\\"INTERVAL\\\" \\\"YEAR\\\" ...\\n    \\\"INTERVAL\\\" \\\"MONTH\\\" ...\\n    \\\"INTERVAL\\\" \\\"DAY\\\" ...\\n    \\\"INTERVAL\\\" \\\"HOUR\\\" ...\\n    \\\"INTERVAL\\\" \\\"MINUTE\\\" ...\\n    \\\"INTERVAL\\\" \\\"SECOND\\\" ...\\n     ] < ParseException:[ Encountered \\\"interval )\\\" at line 1, column 27.\\nWas expecting one of:\\n    \\\"CHARACTER\\\" ...\\n    \\\"CHAR\\\" ...\\n    \\\"VARCHAR\\\" ...\\n    \\\"DATE\\\" ...\\n    \\\"TIME\\\" ...\\n    \\\"TIMESTAMP\\\" ...\\n    \\\"DECIMAL\\\" ...\\n    \\\"DEC\\\" ...\\n    \\\"NUMERIC\\\" ...\\n    \\\"BOOLEAN\\\" ...\\n    \\\"INTEGER\\\" ...\\n    \\\"INT\\\" ...\\n    \\\"BINARY\\\" ...\\n    \\\"VARBINARY\\\" ...\\n    \\\"TINYINT\\\" ...\\n    \\\"SMALLINT\\\" ...\\n    \\\"BIGINT\\\" ...\\n    \\\"REAL\\\" ...\\n    \\\"DOUBLE\\\" ...\\n    \\\"FLOAT\\\" ...\\n    \\\"ANY\\\" ...\\n    \\\"MULTISET\\\" ...\\n    <IDENTIFIER> ...\\n    <QUOTED_IDENTIFIER> ...\\n    <BACK_QUOTED_IDENTIFIER> ...\\n    <BRACKET_QUOTED_IDENTIFIER> ...\\n    <UNICODE_QUOTED_IDENTIFIER> ...\\n    \\\"INTERVAL\\\" \\\"YEAR\\\" ...\\n    \\\"INTERVAL\\\" \\\"MONTH\\\" ...\\n    \\\"INTERVAL\\\" \\\"DAY\\\" ...\\n    \\\"INTERVAL\\\" \\\"HOUR\\\" ...\\n    \\\"INTERVAL\\\" \\\"MINUTE\\\" ...\\n    \\\"INTERVAL\\\" \\\"SECOND\\\" ...\\n     ]\"\r\n]\r\nError: exception while executing query (state=,code=0)",
        "better handle column called 'Timestamp' When querying a JSON file with a key called 'Timestamp' (select timestamp from <file>), Drill throws a non-intuitive error message (below).  It works when backticking.  Ideally the parser would look for keys called 'Timestamp' before erroring out.  If that can't be done, a better error should be printed.\r\n\r\njdbc:drill:zk=local> select Timestamp, Operation from dfs.`/...json`;\r\nQuery failed: Query failed: Failure parsing SQL. Encountered \"Timestamp ,\" at line 1, column 8.\r\nWas expecting one of:\r\n    \"UNION\" ...\r\n    \"INTERSECT\" ...\r\n    \"EXCEPT\" ...\r\n    \"ORDER\" ..."
    ],
    [
        "DRILL-2906",
        "DRILL-1362",
        "Json reader with extended json adds extra column Performing a CTAS with 'store.format' = 'json' and querying the table results in projecting an addition field '*' will null values. Below is a simple repro\r\n\r\n0: jdbc:drill:zk=local> create table t as select timestamp '1980-10-01 00:00:00' from cp.`employee.json` limit 1;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 1                         |\r\n+------------+---------------------------+\r\n1 row selected (0.314 seconds)\r\n0: jdbc:drill:zk=local> select * from t;\r\n+------------+------------+\r\n|   EXPR$0   |     *      |\r\n+------------+------------+\r\n| 1980-10-01 00:00:00.0 | null       |\r\n+------------+------------+\r\n\r\nNotice in the above result set we get an extra column '*' with null value.",
        "Count(nullable-column) is incorrectly pushed into group scan operator The following query on TPC-DS table web_returns produces wrong result because the aggregate count(wr_return_quantity) gets pushed into the parquet group scan operator even though wr_return_quantity is nullable and apparently the parquet metadata does not have stats on nullable column.  \r\n\r\n0: jdbc:drill:zk=local> select count(wr_return_quantity) from web_returns;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 71763      |\r\n+------------+\r\n\r\n0: jdbc:drill:zk=local> explain plan for select count(wr_return_quantity) from web_returns;\r\n{code:sql}\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(EXPR$0=[$0])\r\n00-02        Scan(groupscan=[org.apache.drill.exec.store.pojo.PojoRecordReader@e4acaad])\r\n{code}\r\n\r\nFor reference, here are the correct results:  \r\ntpcds=# select count(wr_return_quantity) from web_returns;\r\n count\r\n-------\r\n 68616\r\n(1 row)\r\n\r\ntpcds=# select count(*) from web_returns;\r\n count\r\n-------\r\n 71763\r\n(1 row)"
    ],
    [
        "DRILL-3348",
        "DRILL-2541",
        "NPE when two different window functions are used in projection list and order by clauses {code:sql}\r\nselect \r\n        a1, \r\n        rank() over(partition by b1 order by a1) \r\nfrom \r\n        t1 \r\norder by \r\n        row_number() over(partition by b1 order by a1);\r\n{code}\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select a1, rank() over(partition by b1 order by a1) from t1 order by row_number() over(partition by b1 order by a1);\r\nError: SYSTEM ERROR: org.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: null\r\n[Error Id: ba3e0fda-cc78-4650-a49b-51e4fd7d625d on atsqa4-133.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\ndrillbit.log\r\n{code}\r\norg.apache.drill.common.exceptions.UserException: SYSTEM ERROR: org.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: null\r\n\r\n\r\n[Error Id: ba3e0fda-cc78-4650-a49b-51e4fd7d625d on atsqa4-133.qa.lab:31010]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:523) ~[drill-common-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$ForemanResult.close(Foreman.java:738) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:840) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:782) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.common.EventProcessor.sendEvent(EventProcessor.java:73) [drill-common-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.moveToState(Foreman.java:784) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.moveToState(Foreman.java:893) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:253) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_71]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]\r\nCaused by: org.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: null\r\n        ... 4 common frames omitted\r\nCaused by: java.lang.NullPointerException: null\r\n        at org.apache.calcite.rex.RexBuilder.makeCast(RexBuilder.java:465) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.rex.RexBuilder.ensureType(RexBuilder.java:955) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertOver(SqlToRelConverter.java:1763) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.access$1000(SqlToRelConverter.java:180) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:3938) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectList(SqlToRelConverter.java:3327) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:609) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:564) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:2741) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:522) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.calcite.prepare.PlannerImpl.convert(PlannerImpl.java:198) ~[calcite-core-1.1.0-drill-r8.jar:1.1.0-drill-r8]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToRel(DefaultSqlHandler.java:448) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.validateAndConvert(DefaultSqlHandler.java:191) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:157) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:178) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:904) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:242) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        ... 3 common frames omitted\r\n{code}",
        "Plan fragment logging causes significant delay in query start In the getQueryWorkUnit() method of Foreman, we are logging information about every plan fragment at info level, and this is causing a noticeable delay (~10 seconds) in query start time.\r\n\r\nThis should be moved to trace log level."
    ],
    [
        "DRILL-3905",
        "DRILL-2429",
        "Document DROP TABLE support  Add documentation to Drill docs for DROP TABLE ",
        "Update Supported Date/Time Data Type Formats doc Test/revise/update Supported Date/Time Data Type Formats. Fold in review comments of other sections."
    ],
    [
        "DRILL-3836",
        "DRILL-3235",
        "TestBitRpc uses hard coded port {noformat}\r\n    int port = 1234;\r\n\r\n    DataResponseHandler drp = new BitComTestHandler();\r\n    DataServer server = new DataServer(c, workBus, drp);\r\n\r\n    port = server.bind(port, false);\r\n    DrillbitEndpoint ep = DrillbitEndpoint.newBuilder().setAddress(\"localhost\").setDataPort(port).build();\r\n{noformat}\r\nThis will collide if the suite is run in parallel on a machine.\r\n(like say on a jenkins server)",
        "Enhance JSON reader to leverage EmbeddedType "
    ],
    [
        "DRILL-348",
        "DRILL-2482",
        "Improve parquet scanner to read nullable data source If you use Pig to convert your data source to parquet format, currently Drill will have trouble reading the converted file. By default, Pig will convert your data into nullable type. Drill will throw indexoutofboundsexception when reading the file. ",
        "JDBC : calling getObject when the actual column type is 'NVARCHAR' results in NoClassDefFoundError git.commit.id.abbrev=7b4c887\r\n\r\nI tried to call getObject(i) on a column which is of type varchar, drill failed with the below error :\r\n{code}\r\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/io/Text\r\n\tat org.apache.drill.exec.vector.VarCharVector$Accessor.getObject(VarCharVector.java:407)\r\n\tat org.apache.drill.exec.vector.NullableVarCharVector$Accessor.getObject(NullableVarCharVector.java:386)\r\n\tat org.apache.drill.exec.vector.accessor.NullableVarCharAccessor.getObject(NullableVarCharAccessor.java:98)\r\n\tat org.apache.drill.exec.vector.accessor.BoundCheckingAccessor.getObject(BoundCheckingAccessor.java:137)\r\n\tat org.apache.drill.jdbc.AvaticaDrillSqlAccessor.getObject(AvaticaDrillSqlAccessor.java:136)\r\n\tat net.hydromatic.avatica.AvaticaResultSet.getObject(AvaticaResultSet.java:351)\r\n\tat Dummy.testComplexQuery(Dummy.java:94)\r\n\tat Dummy.main(Dummy.java:30)\r\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.Text\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\r\n\t... 8 more\r\n{code}\r\n\r\n\r\nWhen the underlying type is a primitive, the getObject call succeeds"
    ],
    [
        "DRILL-268",
        "DRILL-3153",
        "Write summary of diagnostic operator to log When the diagnostic operator is running, it may be useful to write some data to the log.  Useful information may include:\r\n\r\n- IterOutcome of upstream batch\r\n- Schema for each batch (and perhaps delta from previous batch)\r\n- Number of records per batch\r\n",
        "DatabaseMetaData.getIdentifierQuoteString() returns (standard) double-quote, not Drill's back quote Drill's implementation of java.sql.DatabaseMetaData.getIdentifierQuoteString() returns a double quote character (the standard SQL identifier quoting character and the Avatica default) instead of returning a back quote (Drill's actual identifier quoting character).\r\n\r\n"
    ],
    [
        "DRILL-2751",
        "DRILL-1799",
        "Implicit cast in filters fails if some literals in an expression are enclosed in quotes.  Drill currently expects all literals in an expression to be enclosed in quotes (in case of implicit casts between numerals & Strings with numeral values), and fails otherwise. \r\n\r\n*Drill: Between operator*\r\n{code:sql}\r\n>  select d_date_sk, d_day_name from date_dim where d_same_day_lq between 2400000 and '2500000' order by d_same_day_lq limit 3;\r\nQuery failed: SqlValidatorException: Cannot apply 'BETWEEN' to arguments of type '<ANY> BETWEEN <INTEGER> AND <CHAR(7)>'. Supported form(s): '<COMPARABLE_TYPE> BETWEEN <COMPARABLE_TYPE> AND <COMPARABLE_TYPE>'\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\n*Postgres: Between operator*\r\n{code:sql}\r\n# select d_date_sk, d_day_name from date_dim where d_same_day_lq between 2400000 and '2500000' order by d_same_day_lq limit 3;\r\n d_date_sk | d_day_name\r\n-----------+------------\r\n   2415022 | Monday\r\n   2415023 | Tuesday\r\n   2415024 | Wednesday\r\n(3 rows)\r\n{code}\r\n\r\n*Drill: IN operator*\r\n{code:sql}\r\n> select d_date_sk, d_day_name from date_dim where d_same_day_lq in ('2414930', 2414931) limit 5;\r\nQuery failed: SqlValidatorException: Values in expression list must have compatible types\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\n*Postgres: IN operator*\r\n{code:sql}\r\n#  select d_date_sk, d_day_name from date_dim where d_same_day_lq in ('2414930', 2414931) limit 5;\r\n d_date_sk | d_day_name\r\n-----------+------------\r\n   2415022 | Monday\r\n   2415023 | Tuesday\r\n(2 rows)\r\n{code}\r\n\r\n",
        "kvgen() row limit #Mon Dec 01 11:15:02 PST 2014\r\ngit.commit.id.abbrev=a60e1db\r\n\r\nRunning the following query that contains kvgen() appears running out of memory and fails. There appears to be a 50 row limit. If I add a limit to the query, limit 50 or less works, with limit 51, the query fails. \r\n\r\n0: jdbc:drill:schema=dfs.drillTestDir> select kvgen(sub.esr) from `es/esr-part` sub;\r\n\r\nThe query returns result but fails in the middle of displaying the result. No other error message except the following stack:\r\n\r\n18:06:08.158 [2b82e176-0aeb-9b05-15f2-520eea269a1c:frag:0:0] WARN  o.a.d.e.w.fragment.FragmentExecutor - Error while initializing or executing fragment\r\njava.lang.IllegalStateException: Attempted to close accountor with 151 buffer(s) still allocatedfor QueryId: 2b82e176-0aeb-9b05-15f2-520eea269a1c, MajorFragmentId: 0, MinorFragmentId: 0.\r\n\r\n\r\n        Total 1 allocation(s) of byte size(s): 4096, at stack location:\r\n                org.apache.drill.exec.memory.TopLevelAllocator$ChildAllocator.buffer(TopLevelAllocator.java:212)\r\n                org.apache.drill.exec.vector.UInt1Vector.allocateNewSafe(UInt1Vector.java:137)\r\n                org.apache.drill.exec.vector.NullableBigIntVector.allocateNewSafe(NullableBigIntVector.java:173)\r\n                org.apache.drill.exec.vector.complex.MapVector.allocateNewSafe(MapVector.java:165)\r\n                org.apache.drill.exec.vector.complex.RepeatedMapVector.allocateNewSafe(RepeatedMapVector.java:236)\r\n                org.apache.drill.exec.vector.complex.MapVector.allocateNewSafe(MapVector.java:165)\r\n                org.apache.drill.exec.vector.complex.MapVector.allocateNewSafe(MapVector.java:165)\r\n                org.apache.drill.exec.vector.complex.impl.SingleMapWriter.allocate(SingleMapWriter.java:135)\r\n                org.apache.drill.exec.vector.complex.impl.RepeatedMapWriter.allocate(RepeatedMapWriter.java:137)\r\n                org.apache.drill.exec.vector.complex.impl.SingleListWriter.allocate(SingleListWriter.java:116)\r\n                org.apache.drill.exec.vector.complex.impl.ComplexWriterImpl.allocate(ComplexWriterImpl.java:157)\r\n                org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.doAlloc(ProjectRecordBatch.java:217)\r\n                org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.doWork(ProjectRecordBatch.java:144)\r\n                org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:89)\r\n                org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n                org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n                org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n                org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:86)\r\n                org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:76)\r\n                org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:52)\r\n                org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129)\r\n                org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:106)\r\n                org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:124)\r\n                org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:67)\r\n\r\n\r\n\r\n"
    ],
    [
        "DRILL-1832",
        "DRILL-3320",
        "Select * from json file failed with java.lang.IllegalArgumentException: null git.commit.id.abbrev=201280e\r\n\r\nI have a json file that contains nested data.  When I do a \"select *\" , the query failed as follows:\r\n\r\n0: jdbc:drill:schema=dfs.root> select * from `./json/twitter_43.json`;\r\nQuery failed: Query stopped.[ cc231dec-8d28-4532-a938-c5f67592f262 on qa-node114.qa.lab:31010 ]\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nAlso, the following select fails:\r\n0: jdbc:drill:schema=dfs.root> select SUM(1) as `sum_Number_of_Records_ok` from `./json/twitter_43.json` having (COUNT(1) > 0);\r\nQuery failed: Query failed: Failure while running fragment., You tried to do a batch data read operation when you were in a state of STOP.  You can only do this type of operation when you are in a state of OK or OK_NEW_SCHEMA. [ 364d911a-3bc8-4042-b5a2-6ed074c999ce on qa-node114.qa.lab:31010 ]\r\n[ 364d911a-3bc8-4042-b5a2-6ed074c999ce on qa-node114.qa.lab:31010 ]\r\n",
        "Do away with \"rebuffing\" Drill jar The maven build process for some modules (common, protocol, java-exec), generate a \"-rebuffed\" jar during the build which are the actual jars to be used. \r\n\r\nWe should hint maven to install and deploy these jars as the primary artifact (jars without any classifiers) of that module."
    ],
    [
        "DRILL-2412",
        "DRILL-3452",
        "CTAS has issues when the underlying query casts a column to time datatype git.commit.id.abbrev=e92db23\r\n\r\n{code}\r\ncreate table time_parquet as select cast(columns[0] as time) time_col from `time.tbl`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 21                        |\r\n+------------+---------------------------+\r\n1 row selected (0.201 seconds)\r\n{code}\r\n\r\nNow running a count(*) on the newly created table does not have any issues\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirViews> select count(*) from time_parquet;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 21         |\r\n+------------+\r\n1 row selected (0.081 seconds)\r\n{code}\r\n\r\nHowever the below 2 queries fail\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirViews> select * from time_parquet;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.NullableTimeVector cannot be cast to org.apache.drill.exec.vector.NullableIntVector [ cdceaf64-b858-4063-8364-d119703cf6f0 on qa-node191.qa.lab:31010 ]\r\n[ cdceaf64-b858-4063-8364-d119703cf6f0 on qa-node191.qa.lab:31010 ]\r\n\r\n0: jdbc:drill:schema=dfs.drillTestDirViews> select time_col from time_parquet;\r\nQuery failed: RemoteRpcException: Failure while running fragment., org.apache.drill.exec.vector.NullableTimeVector cannot be cast to org.apache.drill.exec.vector.NullableIntVector [ 8c4254bb-7869-468d-bc6c-7151b833593f on qa-node191.qa.lab:31010 ]\r\n[ 8c4254bb-7869-468d-bc6c-7151b833593f on qa-node191.qa.lab:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n",
        "Storage Plugin Name starts with numeric causing exception Storage Plugin Name Created with a numeric value (Example: 02Jul) causing below exception:\r\n\r\nQuery Failed: An Error Occurred\r\norg.apache.drill.common.exceptions.UserRemoteException: PARSE ERROR: Encountered \"02\" at line 1, column 15. Was expecting one of: ... ... ... ... ... [Error Id: 1faeafcc-3e34-4639-a28b-0fac7f688b8b on maprdemo:31010]"
    ],
    [
        "DRILL-3863",
        "DRILL-510",
        "TestBuilder.baseLineColumns(...) doesn't take net strings; parses somehow--can't test some names {{TestBuilder}}'s {{baseLineColumns(String...)}} method doesn't take the given strings as net column names, and instead tries to parse them somehow, but doesn't parse them as the SQL parser would (and that method's Javadoc documentation doesn't seem to say how the strings are parsed/interpreted or indicate any third way of specifying arbitrary net column names).\r\n\r\nThat means that certain column names _cannot be checked_ for (cannot be used in the result set being checked).\r\n\r\nFor example, in Drill, the SQL delimited identifier  \"{{`Column B`}}\"  specifies a net column name of \"{{Column B}}\".  However, passing that net column name (that is, a {{String}} representing that net column name) to {{baseLineColumns}} results in a strange parsing error.  (See Test Class 1 and the error in Failure Trace 1.)\r\n\r\nChecking whether {{baseLineColumns}} takes SQL-level syntax for column names rather than net column names (by passing a string including the back-quote characters of the delimited identifier) seems to indicate that {{baseLineColumns}} doesn't take that syntax that either.  (See Test Class 2 and the three expected/returned records in Failure Trace 2.)\r\n\r\nThat seems to mean that it's impossible to use {{baseLineColumns}} to validate certain column names (including the fairly simple/common case of alias names containing spaces for output formatting purposes).\r\n\r\n\r\nTest Class 1:\r\n{noformat}\r\nimport org.junit.Test;\r\n\r\npublic class TestTEMPFileNameBugs extends BaseTestQuery {\r\n\r\n  @Test\r\n  public void test1() throws Exception {\r\n    testBuilder()\r\n    .sqlQuery( \"SELECT * FROM ( VALUES (1, 2) ) AS T(column_a, `Column B`)\" )\r\n    .unOrdered()\r\n    .baselineColumns(\"column_a\", \"Column B\")\r\n    .baselineValues(1, 2)\r\n    .go();\r\n  }\r\n}\r\n{noformat}\r\n\r\nFailure Trace 1:\r\n{noformat}\r\norg.apache.drill.common.exceptions.ExpressionParsingException: Expression has syntax error! line 1:0:no viable alternative at input 'Column'\r\n\tat org.apache.drill.common.expression.parser.ExprParser.displayRecognitionError(ExprParser.java:169)\r\n\tat org.antlr.runtime.BaseRecognizer.reportError(BaseRecognizer.java:186)\r\n\tat org.apache.drill.common.expression.parser.ExprParser.lookup(ExprParser.java:5163)\r\n\tat org.apache.drill.common.expression.parser.ExprParser.atom(ExprParser.java:4370)\r\n\tat org.apache.drill.common.expression.parser.ExprParser.unaryExpr(ExprParser.java:4252)\r\n\tat org.apache.drill.common.expression.parser.ExprParser.xorExpr(ExprParser.java:3954)\r\n\tat org.apache.drill.common.expression.parser.ExprParser.mulExpr(ExprParser.java:3821)\r\n\tat org.apache.drill.common.expression.parser.ExprParser.addExpr(ExprParser.java:3689)\r\n\tat org.apache.drill.common.expression.parser.ExprParser.relExpr(ExprParser.java:3564)\r\n\tat org.apache.drill.common.expression.parser.ExprParser.equExpr(ExprParser.java:3436)\r\n\tat org.apache.drill.common.expression.parser.ExprParser.andExpr(ExprParser.java:3310)\r\n\tat org.apache.drill.common.expression.parser.ExprParser.orExpr(ExprParser.java:3185)\r\n\tat org.apache.drill.common.expression.parser.ExprParser.condExpr(ExprParser.java:3110)\r\n\tat org.apache.drill.common.expression.parser.ExprParser.expression(ExprParser.java:3041)\r\n\tat org.apache.drill.common.expression.parser.ExprParser.parse(ExprParser.java:206)\r\n\tat org.apache.drill.TestBuilder.parsePath(TestBuilder.java:202)\r\n\tat org.apache.drill.TestBuilder.baselineColumns(TestBuilder.java:333)\r\n\tat org.apache.drill.TestTEMPFileNameBugs.test1(TestTEMPFileNameBugs.java:30)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n{noformat}\r\n\r\nTest Class 2:\r\n{noformat}\r\nimport org.junit.Test;\r\n\r\npublic class TestTEMPFileNameBugs extends BaseTestQuery {\r\n\r\n  @Test\r\n  public void test1() throws Exception {\r\n    testBuilder()\r\n    .sqlQuery( \"SELECT * FROM ( VALUES (1, 2) ) AS T(column_a, `Column B`)\" )\r\n    .unOrdered()\r\n    .baselineColumns(\"column_a\", \"`Column B`\")\r\n    .baselineValues(1, 2)\r\n    .go();\r\n  }\r\n\r\n}\r\n{noformat}\r\n\r\nFailure Trace 2:\r\n{noformat}\r\n\r\njava.lang.Exception: After matching 0 records, did not find expected record in result set: `Column B` : 2, `column_a` : 1, \r\n\r\n\r\nSome examples of expected records:`Column B` : 2, `column_a` : 1, \r\n\r\n\r\n Some examples of records returned by the test query:`Column B` : 2, `column_a` : 1, \r\n\r\n\tat org.apache.drill.DrillTestWrapper.compareResults(DrillTestWrapper.java:577)\r\n\tat org.apache.drill.DrillTestWrapper.compareUnorderedResults(DrillTestWrapper.java:303)\r\n\tat org.apache.drill.DrillTestWrapper.run(DrillTestWrapper.java:125)\r\n\tat org.apache.drill.TestBuilder.go(TestBuilder.java:129)\r\n\tat org.apache.drill.TestTEMPFileNameBugs.test1(TestTEMPFileNameBugs.java:33)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n{noformat}\r\n",
        "zookeeper connections don't get released until all queries are completed in a batch of tests In our drill automation system, a batch of over 200 tests were submitted via the JDBC connection which ended in large quantities of failures.  The following investigating analyses were performed to root cause the problem:\r\n\r\n1. I put a sleep of 15 minutes in a single test case execution, and the zookeeper connection was not closed until the test terminated after sleep time ran out.\r\n2. I then ran a couple of tests and reduced the sleep time to 5 minutes.  Two zookeeper connections were established (the second one after the second test case was running) and were not closed until both tests terminated after sleep time ran out.\r\n3. I ran a batch of four tests and further reduced the sleep time to 2 minutes.  Four zookeeper connections were established (one after another till the last test case started to run) and again, none of them was closed until all tests terminated after sleep time out.  Then it took about 10-15 seconds for all connections to close.\r\n\r\nFrom these experiments, it appears to me that zookeeper connections are eventually closed, but not before all tests executed in one batch are completed.  This explains that, with the default maximum number of zookeeper connections allowed, over 200 queries submitted in a batch are destined to fail shortly into the execution when all zookeeper connections are used up."
    ],
    [
        "DRILL-2234",
        "DRILL-2694",
        "IOOB when streaming aggregate is on the left side of hash join This issue is similar to DRILL-2107. \r\n\r\nIssue can be reproduced by enabling SwapJoinRule in DrillRuleSets and running the following query.\r\n\r\nalter session set `planner.slice_target` = 1;\r\nalter session set `planner.enable_hashagg` = false;\r\nalter session set `planner.enable_streamagg` = true;\r\n\r\nselect l_suppkey, sum(l_extendedprice)/sum(l_quantity) as avg_price \r\nfrom cp.`tpch/lineitem.parquet` where l_orderkey in\r\n(select o_orderkey from cp.`tpch/orders.parquet` where o_custkey = 2) \r\ngroup by l_suppkey having sum(l_extendedprice)/sum(l_quantity) > 1850.0;",
        "Correlated subquery can not be planned Correlated subquery can not be planned. Test was run on 4 node cluster on CentOS. Please note that Physical plan tab and Visualized plan tabs were empty for the below query.\r\n\r\n{code}\r\n0: jdbc:drill:> select * from `allTypData.csv` t1 where t1.columns[0] > (select min(columns[0]) from `allTypData2.csv` t2);\r\nQuery failed: UnsupportedRelOperatorException: This query cannot be planned possibly due to either a cartesian join or an inequality join\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nTable in outer query has 295 rows.\r\n\r\n0: jdbc:drill:> select count(*) from `allTypData.csv`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 295        |\r\n+------------+\r\n1 row selected (0.082 seconds)\r\n\r\nTable in inner query has 999 rows.\r\n\r\n0: jdbc:drill:> select count(*) from `allTypData2.csv`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 999        |\r\n+------------+\r\n1 row selected (0.083 seconds)\r\n\r\nsub query is,\r\n\r\n0: jdbc:drill:> select min(columns[0]) from `allTypData2.csv`;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| -1000041497 |\r\n+------------+\r\n1 row selected (0.097 seconds)\r\n\r\nNote that when we replace the sub-query with the value that the sub-query returns, the original query returns results. It fails only when there is correlated subquery.\r\n\r\nselect * from `allTypData.csv` t1 where t1.columns[0] > -1000041497;\r\n+------------+\r\n|  columns   |\r\n+------------+\r\n...\r\n+------------+\r\n214 rows selected (0.162 seconds)\r\n\r\n0: jdbc:drill:> explain plan for select * from `allTypData.csv` t1 where t1.columns[0] > (select min(columns[0]) from `allTypData2.csv` t2);\r\nQuery failed: UnsupportedRelOperatorException: This query cannot be planned possibly due to either a cartesian join or an inequality join\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nStack trace from drillbit.log \r\n\r\n{code}\r\n2015-04-04 21:12:40,687 [2adfac37-037b-c692-31b1-41d8004d9b13:foreman] INFO  o.a.drill.exec.work.foreman.Foreman - State change requested.  PENDING --> FAILED\r\norg.apache.drill.exec.work.foreman.UnsupportedRelOperatorException: This query cannot be planned possibly due to either a cartesian join or an inequality join\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToDrel(DefaultSqlHandler.java:217) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:138) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:773) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:204) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]\r\n2015-04-04 21:12:40,695 [2adfac37-037b-c692-31b1-41d8004d9b13:foreman] INFO  o.a.drill.exec.work.foreman.Foreman - foreman cleaning up - status: []\r\n2015-04-04 21:12:40,696 [2adfac37-037b-c692-31b1-41d8004d9b13:foreman] ERROR o.a.drill.exec.work.foreman.Foreman - Error 59dcbb50-d418-400a-9c0c-7331fcb6b344: UnsupportedRelOperatorException: This query cannot be planned possibly due to either a cartesian join or an inequality join\r\norg.apache.drill.exec.work.foreman.UnsupportedRelOperatorException: This query cannot be planned possibly due to either a cartesian join or an inequality join\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToDrel(DefaultSqlHandler.java:217) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:138) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:773) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:204) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]\r\n{code}"
    ],
    [
        "DRILL-420",
        "DRILL-414",
        "float literal is interpreted as BigInt From sqlline, issue the following:\r\n0: jdbc:drill:> select 1.1+2.6 from `customer.json` limit 1;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 3          |\r\n+------------+\r\nNotice the result is 3 instead of 3.7.\r\n\r\nsqlline log shows we are taking in the value as BigInt:\r\n11:42:05.223 [Client-1] DEBUG o.a.d.e.rpc.user.QueryResultHandler - Received QueryId part1: -4656127306686443884\r\npart2: -8283317532111349525\r\n succesfully.  Adding listener org.apache.drill.jdbc.DrillResultSet$Listener@5cd38dd\r\n11:42:05.915 [Client-1] DEBUG org.apache.drill.jdbc.DrillResultSet - Result arrived QueryResultBatch [header=query_id {\r\n  part1: -4656127306686443884\r\n  part2: -8283317532111349525\r\n}\r\nis_last_chunk: false\r\nrow_count: 1\r\ndef {\r\n  field {\r\n    def {\r\n      name {\r\n        type: NAME\r\n        name: \"EXPR$0\"\r\n      }\r\n      major_type {\r\n        minor_type: BIGINT\r\n        mode: REQUIRED\r\n      }\r\n    }\r\n    value_count: 1\r\n    buffer_length: 8\r\n  }\r\n  record_count: 1\r\n  is_selection_vector_2: false\r\n}\r\n, data=SlicedByteBuf(ridx: 0, widx: 8, cap: 8/8, unwrapped: AccountingByteBuf [Inner buffer=PooledUnsafeDirectByteBufL(ridx: 76, widx: 76, cap: 76), size=76])]\r\n11:42:05.917 [Client-1] DEBUG org.apache.drill.jdbc.DrillResultSet - Result arrived QueryResultBatch [header=query_id {\r\n  part1: -4656127306686443884\r\n  part2: -8283317532111349525\r\n}\r\nis_last_chunk: true\r\nrow_count: 0\r\ndef {\r\n}\r\n, data=null]\r\n\r\ndrillbit log confirms that:\r\n11:42:05.226 [WorkManager Event Thread] DEBUG o.apache.drill.exec.work.WorkManager - Starting pending task org.apache.drill.exec.work.foreman.Foreman@4c097753\r\n11:42:05.238 [WorkManager-3] DEBUG o.a.d.e.planner.logical.DrillOptiq - RexCall +(1.1, 2.6), {}\r\n11:42:05.238 [WorkManager-3] DEBUG o.a.d.e.planner.logical.DrillOptiq - Binary\r\n11:42:05.242 [WorkManager-3] DEBUG o.a.drill.exec.work.foreman.Foreman - Converting logical plan {\r\n  \"head\" : {\r\n    \"version\" : 1,\r\n    \"generator\" : {\r\n      \"type\" : \"org.apache.drill.exec.planner.logical.DrillImplementor\",\r\n      \"info\" : \"\"\r\n    },\r\n    \"type\" : \"APACHE_DRILL_LOGICAL\",\r\n    \"resultMode\" : \"EXEC\"\r\n  },\r\n  \"storage\" : {\r\n    \"cp\" : {\r\n      \"type\" : \"file\",\r\n      \"connection\" : \"classpath:///\",\r\n      \"workspaces\" : null,\r\n      \"formats\" : null\r\n    }\r\n  },\r\n  \"query\" : [ {\r\n    \"op\" : \"scan\",\r\n    \"@id\" : 1,\r\n    \"storageengine\" : \"cp\",\r\n    \"selection\" : {\r\n      \"format\" : {\r\n        \"type\" : \"json\"\r\n      },\r\n      \"files\" : [ \"/customer.json\" ]\r\n    },\r\n    \"ref\" : null\r\n  }, {\r\n    \"op\" : \"project\",\r\n    \"@id\" : 2,\r\n    \"input\" : 1,\r\n    \"projections\" : [ {\r\n      \"ref\" : \"output.EXPR$0\",\r\n      \"expr\" : \" (1)  + (2) \"\r\n    } ]\r\n  }, {\r\n    \"op\" : \"limit\",\r\n    \"@id\" : 3,\r\n    \"input\" : 2,\r\n    \"first\" : 0,\r\n    \"last\" : 1\r\n  }, {\r\n    \"op\" : \"store\",\r\n    \"@id\" : 4,\r\n    \"input\" : 3,\r\n    \"target\" : null,\r\n    \"storageEngine\" : \"--SCREEN--\"\r\n  } ]\r\n}.\r\n11:42:05.244 [WorkManager-3] DEBUG o.a.drill.common.config.DrillConfig - Loading configs at the following URLs [jar:file:/opt/drill/jars/drill-java-exec-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar!/drill-module.conf, jar:file:/opt/drill/jars/drill-common-1.0.0-m2-incubating-SNAPSHOT-rebuffed.jar!/drill-module.conf]\r\n11:42:05.250 [WorkManager-3] DEBUG o.a.d.c.l.data.LogicalOperatorBase - Adding Logical Operator sub types: [class org.apache.drill.common.logical.data.Transform, class org.apache.drill.common.logical.data.Limit, class org.apache.drill.common.logical.data.Union, class org.apache.drill.common.logical.data.Sequence, class org.apache.drill.common.logical.data.Scan, class org.apache.drill.common.logical.data.Order, class org.apache.drill.common.logical.data.WindowFrame, class org.apache.drill.common.logical.data.Constant, class org.apache.drill.common.logical.data.Project, class org.apache.drill.common.logical.data.Join, class org.apache.drill.common.logical.data.GroupingAggregate, class org.apache.drill.common.logical.data.Store, class org.apache.drill.common.logical.data.Filter, class org.apache.drill.common.logical.data.RunningAggregate, class org.apache.drill.common.logical.data.Flatten]\r\n11:42:05.251 [WorkManager-3] DEBUG o.a.d.c.l.StoragePluginConfigBase - Adding Storage Engine Configs including [class org.apache.drill.exec.store.ischema.InfoSchemaConfig, class org.apache.drill.exec.store.mock.MockStorageEngineConfig, class org.apache.drill.exec.store.dfs.FileSystemConfig, class org.apache.drill.exec.store.NamedStoragePluginConfig, class org.apache.drill.exec.store.dfs.FileSystemFormatConfig, class org.apache.drill.exec.store.hive.HiveStoragePluginConfig]\r\n11:42:05.252 [WorkManager-3] DEBUG o.a.d.c.l.FormatPluginConfigBase - Adding Format Plugin Configs including [class org.apache.drill.exec.store.dfs.NamedFormatPluginConfig, class org.apache.drill.exec.store.parquet.ParquetFormatConfig, class org.apache.drill.exec.store.easy.json.JSONFormatPlugin$JSONFormatConfig]\r\n11:42:05.269 [WorkManager-3] DEBUG o.a.d.e.s.schedule.BlockMapBuilder - Took 0 ms to build endpoint map\r\n11:42:05.271 [WorkManager-3] DEBUG o.a.d.e.s.schedule.BlockMapBuilder - Failure finding Drillbit running on host localhost.  Skipping affinity to that host.\r\n11:42:05.271 [WorkManager-3] DEBUG o.a.d.e.s.schedule.BlockMapBuilder - FileWork group (/customer.json,0) max bytes 0\r\n11:42:05.271 [WorkManager-3] DEBUG o.a.d.e.s.schedule.BlockMapBuilder - Took 0 ms to set endpoint bytes\r\n11:42:05.272 [WorkManager-3] DEBUG o.a.d.e.s.schedule.AffinityCreator - Took 0 ms to get operator affinity\r\n11:42:05.273 [WorkManager-3] DEBUG o.a.d.e.s.schedule.AssignmentCreator - Took 0 ms to apply assignments\r\n11:42:05.276 [WorkManager-3] DEBUG o.a.d.e.p.f.SimpleParallelizer - Root fragment:\r\n handle {\r\n  query_id {\r\n    part1: -4656127306686443884\r\n    part2: -8283317532111349525\r\n  }\r\n  major_fragment_id: 0\r\n  minor_fragment_id: 0\r\n}\r\nnetwork_cost: 0.0\r\ncpu_cost: 0.0\r\ndisk_cost: 0.0\r\nmemory_cost: 0.0\r\nfragment_json: \"{\\n  \\\"pop\\\" : \\\"screen\\\",\\n  \\\"@id\\\" : 1,\\n  \\\"child\\\" : {\\n    \\\"pop\\\" : \\\"selection-vector-remover\\\",\\n    \\\"@id\\\" : 2,\\n    \\\"child\\\" : {\\n      \\\"pop\\\" : \\\"limit\\\",\\n      \\\"@id\\\" : 3,\\n      \\\"child\\\" : {\\n        \\\"pop\\\" : \\\"project\\\",\\n        \\\"@id\\\" : 4,\\n        \\\"exprs\\\" : [ {\\n          \\\"ref\\\" : \\\"output.EXPR$0\\\",\\n          \\\"expr\\\" : \\\" (1)  + (2) \\\"\\n        } ],\\n        \\\"child\\\" : {\\n          \\\"pop\\\" : \\\"fs-sub-scan\\\",\\n          \\\"@id\\\" : 5,\\n          \\\"files\\\" : [ {\\n            \\\"start\\\" : 0,\\n            \\\"length\\\" : 1,\\n            \\\"path\\\" : \\\"/customer.json\\\"\\n          } ],\\n          \\\"storage\\\" : {\\n            \\\"type\\\" : \\\"file\\\",\\n            \\\"connection\\\" : \\\"classpath:///\\\",\\n            \\\"workspaces\\\" : null,\\n            \\\"formats\\\" : null\\n          },\\n          \\\"format\\\" : {\\n            \\\"type\\\" : \\\"json\\\"\\n          }\\n        }\\n      },\\n      \\\"first\\\" : 0,\\n      \\\"last\\\" : 1\\n    }\\n  }\\n}\"\r\nleaf_fragment: true\r\nassignment {\r\n  address: \"qa-node118.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nforeman {\r\n  address: \"qa-node118.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\n\r\n11:42:05.277 [WorkManager-3] DEBUG o.a.d.exec.rpc.control.WorkEventBus - Adding fragment status listener for queryId part1: -4656127306686443884\r\npart2: -8283317532111349525\r\n.\r\n11:42:05.277 [WorkManager-3] DEBUG o.a.drill.exec.work.foreman.Foreman - Storing fragments\r\n11:42:05.277 [WorkManager-3] DEBUG o.a.drill.exec.work.foreman.Foreman - Fragments stored.\r\n11:42:05.278 [WorkManager-3] DEBUG o.a.drill.exec.work.foreman.Foreman - Submitting fragments to run.\r\n11:42:05.278 [WorkManager-3] DEBUG o.a.d.exec.work.foreman.QueryManager - Setting up fragment runs.\r\n11:42:05.278 [WorkManager-3] DEBUG o.a.d.exec.work.foreman.QueryManager - Setting up root context.\r\n11:42:05.279 [WorkManager-3] DEBUG o.a.drill.exec.ops.FragmentContext - Getting initial memory allocation of 20000000\r\n11:42:05.279 [WorkManager-3] DEBUG o.a.d.exec.work.foreman.QueryManager - Setting up incoming buffers\r\n11:42:05.280 [WorkManager-3] DEBUG o.a.d.e.work.batch.IncomingBuffers - Came up with a list of 0 required fragments.  Fragments {}\r\n11:42:05.280 [WorkManager-3] DEBUG o.a.d.exec.work.foreman.QueryManager - Setting buffers on root context.\r\n11:42:05.281 [WorkManager-3] DEBUG o.a.d.exec.work.foreman.QueryManager - Generating Exec tree\r\n11:42:05.313 [WorkManager-3] DEBUG o.a.d.e.p.i.s.RemovingRecordBatch - Created.\r\n11:42:05.314 [WorkManager-3] DEBUG o.a.d.exec.work.foreman.QueryManager - Exec tree generated.\r\n11:42:05.314 [WorkManager-3] DEBUG o.a.d.exec.work.foreman.QueryManager - Fragment added to local node.\r\n11:42:05.315 [WorkManager-3] DEBUG o.apache.drill.exec.work.WorkManager - Adding pending task org.apache.drill.exec.work.fragment.FragmentExecutor@4ea74049\r\n11:42:05.315 [WorkManager-3] DEBUG o.a.d.exec.work.foreman.QueryManager - Fragment runs setup is complete.\r\n11:42:05.315 [WorkManager Event Thread] DEBUG o.apache.drill.exec.work.WorkManager - Starting pending task org.apache.drill.exec.work.fragment.FragmentExecutor@4ea74049\r\n11:42:05.316 [WorkManager-3] DEBUG o.a.drill.exec.work.foreman.Foreman - Fragments running.\r\n11:42:05.316 [WorkManager-4] DEBUG o.a.d.e.w.fragment.FragmentExecutor - Starting fragment runner. 0:0\r\n11:42:05.316 [WorkManager-4] DEBUG o.a.d.exec.work.foreman.QueryManager - New fragment status was provided to Foreman of memory_use: 0\r\nbatches_completed: 0\r\nrecords_completed: 0\r\nstate: RUNNING\r\ndata_processed: 0\r\nhandle {\r\n  query_id {\r\n    part1: -4656127306686443884\r\n    part2: -8283317532111349525\r\n  }\r\n  major_fragment_id: 0\r\n  minor_fragment_id: 0\r\n}\r\nrunning_time: 10523888674029567\r\n\r\n11:42:05.889 [WorkManager-4] DEBUG o.a.d.e.p.i.p.ProjectRecordBatch - Added eval.\r\n11:42:05.891 [WorkManager-4] DEBUG o.a.d.e.compile.JaninoClassCompiler - Compiling:\r\n 1:\r\n2:      package org.apache.drill.exec.test.generated;\r\n3:\r\n4:      import org.apache.drill.exec.exception.SchemaChangeException;\r\n5:      import org.apache.drill.exec.expr.holders.BigIntHolder;\r\n6:      import org.apache.drill.exec.ops.FragmentContext;\r\n7:      import org.apache.drill.exec.record.RecordBatch;\r\n8:      import org.apache.drill.exec.vector.BigIntVector;\r\n9:\r\n10:     public class ProjectorGen29 {\r\n11:\r\n12:         BigIntVector vv3;\r\n13:\r\n14:         public void doSetup(FragmentContext context, RecordBatch incoming, RecordBatch outgoing)\r\n15:             throws SchemaChangeException\r\n16:         {\r\n17:             {\r\n18:                 /** start SETUP for function add **/\r\n19:                 {\r\n20:                      {}\r\n21:                 }\r\n22:                 /** end SETUP for function add **/\r\n23:                 Object tmp4 = (outgoing).getValueAccessorById(0, BigIntVector.class).getValueVector();\r\n24:                 if (tmp4 == null) {\r\n25:                     throw new SchemaChangeException(\"Failure while loading vector vv3 with id: TypedFieldId [type=minor_type: BIGINT\\nmode: REQUIRED\\n, fieldId=0, isSuperReader=false].\");\r\n26:                 }\r\n27:                 vv3 = ((BigIntVector) tmp4);\r\n28:             }\r\n29:         }\r\n30:\r\n31:         public void doEval(int inIndex, int outIndex)\r\n32:             throws SchemaChangeException\r\n33:         {\r\n34:             {\r\n35:                 BigIntHolder out0 = new BigIntHolder();\r\n36:                 out0 .value = 1L;\r\n37:                 BigIntHolder out1 = new BigIntHolder();\r\n38:                 out1 .value = 2L;\r\n39:                 BigIntHolder out2 = new BigIntHolder();\r\n40:                 {\r\n41:                     final BigIntHolder out = new BigIntHolder();\r\n42:                     BigIntHolder in1 = out0;\r\n43:                     BigIntHolder in2 = out1;\r\n44:\r\n45:         out.value = (long) (in1.value + in2.value);\r\n46:\r\n47:                     out2 = out;\r\n48:                 }\r\n49:                 vv3 .getMutator().set((outIndex), out2 .value);\r\n50:             }\r\n51:         }\r\n52:\r\n53:     }\r\n\r\n11:42:05.900 [WorkManager-4] DEBUG o.a.drill.exec.compile.MergeAdapter - Skipping copy of 'doSetup()' since it is abstract or listed elsewhere.\r\n11:42:05.901 [WorkManager-4] DEBUG o.a.drill.exec.compile.MergeAdapter - Skipping copy of 'doEval()' since it is abstract or listed elsewhere.\r\n11:42:05.903 [WorkManager-4] DEBUG o.a.drill.exec.ops.FragmentContext - Compile time: 13 millis.\r\n11:42:05.909 [WorkManager-4] DEBUG o.a.d.e.compile.JaninoClassCompiler - Compiling:\r\n 1:\r\n2:      package org.apache.drill.exec.test.generated;\r\n3:\r\n4:      import org.apache.drill.exec.exception.SchemaChangeException;\r\n5:      import org.apache.drill.exec.ops.FragmentContext;\r\n6:      import org.apache.drill.exec.record.RecordBatch;\r\n7:      import org.apache.drill.exec.vector.BigIntVector;\r\n8:\r\n9:      public class CopierGen30 {\r\n10:\r\n11:         BigIntVector vv0;\r\n12:         BigIntVector vv2;\r\n13:\r\n14:         public void doSetup(FragmentContext context, RecordBatch incoming, RecordBatch outgoing)\r\n15:             throws SchemaChangeException\r\n16:         {\r\n17:             {\r\n18:                 Object tmp1 = (incoming).getValueAccessorById(0, BigIntVector.class).getValueVector();\r\n19:                 if (tmp1 == null) {\r\n20:                     throw new SchemaChangeException(\"Failure while loading vector vv0 with id: TypedFieldId [type=minor_type: BIGINT\\nmode: REQUIRED\\n, fieldId=0, isSuperReader=false].\");\r\n21:                 }\r\n22:                 vv0 = ((BigIntVector) tmp1);\r\n23:                 Object tmp3 = (outgoing).getValueAccessorById(0, BigIntVector.class).getValueVector();\r\n24:                 if (tmp3 == null) {\r\n25:                     throw new SchemaChangeException(\"Failure while loading vector vv2 with id: TypedFieldId [type=minor_type: BIGINT\\nmode: REQUIRED\\n, fieldId=0, isSuperReader=false].\");\r\n26:                 }\r\n27:                 vv2 = ((BigIntVector) tmp3);\r\n28:             }\r\n29:         }\r\n30:\r\n31:         public void doEval(int inIndex, int outIndex)\r\n32:             throws SchemaChangeException\r\n33:         {\r\n34:             {\r\n35:                 vv2 .copyFrom((inIndex), (outIndex), vv0);\r\n36:             }\r\n37:         }\r\n38:\r\n39:     }\r\n\r\n11:42:05.914 [WorkManager-4] DEBUG o.a.drill.exec.compile.MergeAdapter - Skipping copy of 'doSetup()' since it is abstract or listed elsewhere.\r\n11:42:05.914 [WorkManager-4] DEBUG o.a.drill.exec.compile.MergeAdapter - Skipping copy of 'doEval()' since it is abstract or listed elsewhere.\r\n11:42:05.916 [WorkManager-4] DEBUG o.a.drill.exec.ops.FragmentContext - Compile time: 7 millis.\r\n11:42:05.918 [WorkManager-4] DEBUG o.a.d.exec.work.foreman.QueryManager - New fragment status was provided to Foreman of memory_use: 0\r\nbatches_completed: 2\r\nrecords_completed: 1\r\nstate: FINISHED\r\ndata_processed: 0\r\nhandle {\r\n  query_id {\r\n    part1: -4656127306686443884\r\n    part2: -8283317532111349525\r\n  }\r\n  major_fragment_id: 0\r\n  minor_fragment_id: 0\r\n}\r\nrunning_time: 601395390\r\n\r\n11:42:05.921 [WorkManager-4] DEBUG o.a.d.e.w.fragment.FragmentExecutor - Fragment runner complete. 0:0",
        "MapR profile conflicts with enforcer rule Here's how I was attempting to build Drill:\r\n\r\n{{mvn clean install assembly:assembly -DskipTests -Dmaven.test.skip.exec -Pmapr -Drat.numUnapprovedLicenses=100}}\r\n\r\nHowever, I got this error:\r\n\r\n{{[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.2:enforce (no_commons_logging) on project drill-java-exec: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed.  -> [Help 1]}}"
    ],
    [
        "DRILL-1858",
        "DRILL-1546",
        "Parquet reader should only explicitly fill in data for a column requested but not in the file if there are no valid columns found If columns are requested from a parquet file, that do not appear in the particular file (users may have a directory full of files that share some columns but not others) then we do not need to create a vector to represent these columns in most cases. These columns can be materialized (as a vector filled with nulls) later when they are referenced in other parts of the query, such as a filter or join condition. The current behavior of the reader is to always fill vectors for these types of columns, but this just creates extra payload to ship around until the vectors are actually referenced.",
        "Support specifying extensions for JSON/Parquet data format JSON data format configuration does not let one to specify file extensions. This limitation prevents us to analyze JSON data that resides in files with no *.json extension. This issue is to add support for specifying non json extensions. Note that parquet data format suffers from the same limitation as well."
    ],
    [
        "DRILL-1729",
        "DRILL-1905",
        "Exception when selecting more fields The first query here works, while the second doesn't. The only difference is another field added to the SELECT.\r\n\r\n{code}\r\n0: jdbc:drill:zk=localhost:2181> SELECT name, categories FROM dfs.root.`Users/tshiran/Development/demo/data/yelp/business.json` WHERE true and REPEATED_CONTAINS(categories, 'Australian');\r\n\t+------------+------------+\r\n\t|    name    | categories |\r\n\t+------------+------------+\r\n\t| The Australian AZ | [\"Bars\",\"Burgers\",\"Nightlife\",\"Australian\",\"Sports Bars\",\"Restaurants\"] |\r\n\t+------------+------------+\r\n\t1 row selected (3.068 seconds)\r\n\t0: jdbc:drill:zk=localhost:2181> SELECT business_id, name, categories FROM dfs.root.`Users/tshiran/Development/demo/data/yelp/business.json` WHERE true and REPEATED_CONTAINS(categories, 'Australian');\r\n\tQuery failed: Failure while running sql.\r\n\r\n\tError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}",
        "UNION ALL of two identical json files fails with exception {code} git.commit.id.abbrev=e3ab2c1 {code}\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from `logs/2014/02/t.json`;\r\n+------------+------------+------------+\r\n|     a1     |     b1     |     c1     |\r\n+------------+------------+------------+\r\n| 0          | 0          | true       |\r\n| 0          | 0          | false      |\r\n| 0          | 0          | false      |\r\n| 1          | 1          | true       |\r\n| 1          | 1          | true       |\r\n+------------+------------+------------+\r\n5 rows selected (0.099 seconds)\r\n0: jdbc:drill:schema=dfs> select * from `logs/2014/03/t.json`;\r\n+------------+------------+------------+\r\n|     a1     |     b1     |     c1     |\r\n+------------+------------+------------+\r\n| 0          | 0          | true       |\r\n| 0          | 0          | false      |\r\n| 0          | 0          | false      |\r\n| 1          | 1          | true       |\r\n| 1          | 1          | true       |\r\n+------------+------------+------------+\r\n5 rows selected (0.065 seconds)\r\n0: jdbc:drill:schema=dfs> select * from `logs/2014/02/t.json` union all select * from `logs/2014/03/t.json`;\r\nQuery failed: Query failed: Unexpected exception during fragment initialization: DrillUnionRel#1680\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n2014-12-20 01:27:01,939 [2b6b2f9a-7e4e-1670-5210-c2a8dabea33b:foreman] ERROR o.a.drill.exec.work.foreman.Foreman - Error 4505ba6b-a2b6-4355-aac1-1edd74bc347d: Query failed: Unexpected exception during fragment initialization: DrillUnionRel#1680\r\norg.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: DrillUnionRel#1680\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:194) [drill-java-exec-0.7.0-r2-SNAPSHOT-rebuffed.jar:0.7.0-r2-SNAPSHOT]\r\n        at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:254) [drill-java-exec-0.7.0-r2-SNAPSHOT-rebuffed.jar:0.7.0-r2-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_71]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]\r\nCaused by: java.lang.AssertionError: DrillUnionRel#1680\r\n        at org.eigenbase.rel.AbstractRelNode.getRowType(AbstractRelNode.java:211) ~[optiq-core-0.9-drill-r12.jar:na]\r\n        at org.apache.drill.exec.planner.common.DrillUnionRelBase.isCompatible(DrillUnionRelBase.java:44) ~[drill-java-exec-0.7.0-r2-SNAPSHOT-rebuffed.jar:0.7.0-r2-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.common.DrillUnionRelBase.<init>(DrillUnionRelBase.java:38) ~[drill-java-exec-0.7.0-r2-SNAPSHOT-rebuffed.jar:0.7.0-r2-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.logical.DrillUnionRel.<init>(DrillUnionRel.java:42) ~[drill-java-exec-0.7.0-r2-SNAPSHOT-rebuffed.jar:0.7.0-r2-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.logical.DrillUnionRel.copy(DrillUnionRel.java:49) ~[drill-java-exec-0.7.0-r2-SNAPSHOT-rebuffed.jar:0.7.0-r2-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.logical.DrillUnionRel.copy(DrillUnionRel.java:38) ~[drill-java-exec-0.7.0-r2-SNAPSHOT-rebuffed.jar:0.7.0-r2-SNAPSHOT]\r\n        at org.eigenbase.rel.SetOpRel.copy(SetOpRel.java:73) ~[optiq-core-0.9-drill-r12.jar:na]\r\n        at org.eigenbase.rel.SetOpRel.copy(SetOpRel.java:33) ~[optiq-core-0.9-drill-r12.jar:na]\r\n        at org.eigenbase.relopt.volcano.RelSubset$CheapestPlanReplacer.visit(RelSubset.java:472) ~[optiq-core-0.9-drill-r12.jar:na]\r\n        at org.eigenbase.relopt.volcano.RelSubset.buildCheapestPlan(RelSubset.java:287) ~[optiq-core-0.9-drill-r12.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:677) ~[optiq-core-0.9-drill-r12.jar:na]\r\n        at net.hydromatic.optiq.tools.Programs$RuleSetProgram.run(Programs.java:165) ~[optiq-core-0.9-drill-r12.jar:na]\r\n        at net.hydromatic.optiq.prepare.PlannerImpl.transform(PlannerImpl.java:276) ~[optiq-core-0.9-drill-r12.jar:na]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToDrel(DefaultSqlHandler.java:155) ~[drill-java-exec-0.7.0-r2-SNAPSHOT-rebuffed.jar:0.7.0-r2-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:134) ~[drill-java-exec-0.7.0-r2-SNAPSHOT-rebuffed.jar:0.7.0-r2-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:145) ~[drill-java-exec-0.7.0-r2-SNAPSHOT-rebuffed.jar:0.7.0-r2-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:507) [drill-java-exec-0.7.0-r2-SNAPSHOT-rebuffed.jar:0.7.0-r2-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:185) [drill-java-exec-0.7.0-r2-SNAPSHOT-rebuffed.jar:0.7.0-r2-SNAPSHOT]\r\n        ... 4 common frames omitted\r\n{code}\r\n\r\nIt looks like it fails in optiq ... maybe."
    ],
    [
        "DRILL-992",
        "DRILL-206",
        "TPCH 16 returns less rows than expected for SF1 Tracking bug,\r\nAman already has the information and repro.\r\n\r\nFor TPCH 16\r\n\r\nselect\r\n  p.p_brand,\r\n  p.p_type,\r\n  p.p_size,\r\n  count(distinct ps.ps_suppkey) as supplier_cnt\r\nfrom\r\n  partsupp ps,\r\n  part p\r\nwhere\r\n  p.p_partkey = ps.ps_partkey\r\n  and p.p_brand <> 'Brand#21'\r\n  and p.p_type not like 'MEDIUM PLATED%'\r\n  and p.p_size in (38, 2, 8, 31, 44, 5, 14, 24)\r\n  and ps.ps_suppkey not in (\r\n    select\r\n      s.s_suppkey\r\n    from\r\n      supplier s\r\n    where\r\n      s.s_comment like '%Customer%Complaints%'\r\n  )\r\ngroup by\r\n  p.p_brand,\r\n  p.p_type,\r\n  p.p_size\r\norder by\r\n  supplier_cnt desc,\r\n  p.p_brand,\r\n  p.p_type,\r\n  p.p_size\r\n\r\nWe are missing around 690 rows in the result obtained from drill vs result from postgres.",
        "milestone 1 source tarball missing files For example, I only see this in the top level directory:\r\n\r\n# ls apache-drill-1.0.0-m1\r\ncommon  distribution  java-exec  ref  sqlparser"
    ],
    [
        "DRILL-2710",
        "DRILL-3527",
        "NPE in a regression run of joins/order_by/queries/q16.sql on an 8 node cluster Assigning to myself to debug. Will change summary as soon as I figure out the easier way to reproduce the problem.\r\n\r\n{code}\r\n/root/drillAutomation/framework/framework/resources/Precommit/Functional/joins/order_by/queries/q16.sql\r\nQuery: \r\nselect count(*),\r\n\t\tcount(sq1.a1),\r\n\t\tcount(sq2.a2),\r\n\t\tmin(sq1.a1),\r\n\t\tmax(sq1.a1),\r\n\t\tmin(sq2.a2),\r\n\t\tmax(sq2.a2),\r\n\t\tavg(sq1.a1),\r\n\t\tavg(sq2.a2)\r\nfrom\r\n        (select c_integer, c_date from j1 order by c_integer asc nulls first) as sq1(a1, b1)\r\n        inner join\r\n        (select c_integer, c_date from j2 order by c_integer asc) as sq2(a2, b2)\r\n        on (sq1.a1 = sq2.a2)\r\nFailed with exception\r\njava.sql.SQLException: exception while executing query: Failure while executing query.\r\n\tat net.hydromatic.avatica.Helper.createException(Helper.java:40)\r\n\tat net.hydromatic.avatica.AvaticaConnection.executeQueryInternal(AvaticaConnection.java:406)\r\n\tat net.hydromatic.avatica.AvaticaStatement.executeQueryInternal(AvaticaStatement.java:351)\r\n\tat net.hydromatic.avatica.AvaticaStatement.executeQuery(AvaticaStatement.java:78)\r\n\tat org.apache.drill.test.framework.DrillTestJdbc.executeQuery(DrillTestJdbc.java:139)\r\n\tat org.apache.drill.test.framework.DrillTestJdbc.run(DrillTestJdbc.java:80)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.sql.SQLException: Failure while executing query.\r\n\tat org.apache.drill.jdbc.DrillCursor.next(DrillCursor.java:144)\r\n\tat org.apache.drill.jdbc.DrillResultSet.execute(DrillResultSet.java:105)\r\n\tat org.apache.drill.jdbc.DrillResultSet.execute(DrillResultSet.java:44)\r\n\tat net.hydromatic.avatica.AvaticaConnection.executeQueryInternal(AvaticaConnection.java:404)\r\n\t... 9 more\r\nCaused by: org.apache.drill.exec.rpc.RpcException: NullPointerException: \r\n\r\n\tat org.apache.drill.exec.rpc.user.QueryResultHandler.resultArrived(QueryResultHandler.java:111)\r\n\tat org.apache.drill.exec.rpc.user.UserClient.handleReponse(UserClient.java:100)\r\n\tat org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:52)\r\n\tat org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:34)\r\n\tat org.apache.drill.exec.rpc.RpcBus.handle(RpcBus.java:57)\r\n\tat org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:194)\r\n\tat org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:173)\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:161)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787)\r\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)\r\n\t... 1 more\r\n\r\n{code}",
        "Apache Drill (version 1.x) unable to integrate with Mondrian (3.7.x) Dear Sir/Madam,\r\n\r\nI am writing this to you with high hopes that you will solve our problem. Can you please let us know if Mondrian (3.7.x) is supporting Apache Drill 1.x. I am unable to integrate Mondrian with Apache Drill and all the queries are failing. Please find my attached Mondrian Schema file and the datasource connection file. Appreciate your response sir."
    ],
    [
        "DRILL-3144",
        "DRILL-2411",
        "Doc.:  JDBC Driver section is SQuirrel-specific, should be moved The current \"Using JDBC\" section is not about using Drill's JDBC driver in general, but is specifically about setting up SQuirreL to use Drill and Drill's JDBC driver.\r\n\r\nGiven that, it should be moved to be among the sections for how to use various tools with Drill's ODBC and JDBC drivers.\r\n\r\n(There should be a new \"Using JDBC\" section that gives common information (information not specific to particular JDBC client tools) about Drill's JDBC driver (e.g., which Jar file, class name, form of JDBC URL).)\r\n",
        "Scalar SUM/AVG over empty result set returns no rows instead of NULL Queries below should return NULL:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select sum(a2) from t2 where 1=0;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n+------------+\r\nNo rows selected (0.08 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select avg(a2) from t2 where 1=0;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n+------------+\r\nNo rows selected (0.074 seconds)\r\n{code}\r\n\r\nWhen grouped, result is correct:\r\n{code}\r\n0: jdbc:drill:schema=dfs> select a2, sum(a2) from t2 where 1=0 group by a2;\r\n+------------+------------+\r\n|     a2     |   EXPR$1   |\r\n+------------+------------+\r\n+------------+------------+\r\nNo rows selected (0.11 seconds)\r\n{code}\r\n\r\nI'm not convinced and it is not very intuitive that correct result should be NULL, but this is what postgres returns and Aman thinks NULL is the correct behavior :)"
    ],
    [
        "DRILL-1859",
        "DRILL-579",
        "IllegalReferenceCountException in the decoder inside Netty The following query does a LIMIT inside a subquery to force a UnionExchange and then does an ORDER-BY outside that will first re-distribute the data before sorting.  It results in a DecoderException in netty.\r\n\r\n{code}\r\n0: jdbc:drill:zk=local> alter session set `planner.slice_target` = 10;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | planner.slice_target updated. |\r\n+------------+------------+\r\n0: jdbc:drill:zk=local> select t2.o_custkey from (select o_orderkey, o_custkey from cp.`tpch/orders.parquet` t1 group by o_orderkey, o_custkey limit 10) t2 order by t2.o_custkey;\r\nQuery failed: Query failed: Failure while running fragment., refCnt: 0, decrement: 1 \r\n{code}\r\n\r\nHere's partial output from the logs: (will attach full error log).  \r\n{code}\r\nio.netty.handler.codec.DecoderException: io.netty.util.IllegalReferenceCountException: refCnt: 0, decrement: 1\r\n        io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:99) [netty-codec-4.0.24.Final.jar:4.0.24.Final]\r\n        io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-transport-4.0.24.Final.jar:4.0.24.Final]\r\n{code}",
        "Select COUNT(*) returns 0 rows The following COUNT(*) should return 1 row...it is returning 0 rows. \r\n\r\n select count(*) from cp.`tpch/nation.parquet` where n_nationkey = 100;\r\n+--+\r\n|  |\r\n+--+\r\n+--+\r\nNo rows selected (0.073 seconds)"
    ],
    [
        "DRILL-2374",
        "DRILL-2042",
        "Sqlline prompt history needs improvements *On Sqlline:*\r\n{code:sql}\r\n> select id from customer \r\n. . . . . . . . . . . . . . . . . > limit 1;\r\n+---------------+\r\n| id |\r\n+---------------+\r\n| 1             |\r\n+---------------+\r\n1 row selected (0.095 seconds)\r\n\r\n[Up arrow]\r\n> limit 1;\r\n{code}\r\n\r\nUsing the arrow keys, traversing through previously run queries is currently not useful, as it displays them line by line and not as a whole. \r\n\r\n*Compare it with Postgres:*\r\n{code:sql}\r\n=# select c_customer_sk from customer\r\n-# limit 1;\r\n c_customer_sk \r\n---------------\r\n             1\r\n(1 row)\r\n\r\n[Up arrow]\r\n=# select c_customer_sk from customer\r\nlimit 1;\r\n{code}\r\n\r\n*Or with shell prompt:*\r\n{code}\r\n# for i in `seq 1 5` \r\n> do\r\n> mkdir $i\r\n> done\r\n\r\n[Up arrow]\r\n# for i in `seq 1 5` ; do mkdir $i; done\r\n{code}\r\n\r\nSqlline should also support other such enhancements such as printing entire history. ",
        "Document NamedThreadFactory (and clean up a bit). "
    ],
    [
        "DRILL-1784",
        "DRILL-2173",
        "Ignore boolean type enforcement on filter conditions during validation The title should be self describing. To give some more context on this, it would be nice if we stop boolean type enforcement on filter conditions as it is possible to create a scenario where we don't have a concrete return type but later bind it during execution. Currently we will need to `cast` condition to boolean explicitly. This does not reflect the flexibility of execution engine.",
        "Enable querying partition information without reading all data When reading a series of files in nested directories, Drill currently adds columns representing the directory structure that was traversed to reach the file currently being read. These columns are stored as varchar under tha names dir0, dir1, ...  As these are just regular columns, Drill allows arbitrary queries against this data, in terms of aggregates, filter, sort, etc. To allow optimizing reads, basic partition pruning has already been added to prune in the case of an expression like dir0 = \"2015\" or a simple in list, which is converted during planning to a series of ORs of equals expressions. If users want to query the directory information dynamically, and not include specific directory names in the query, this will prompt a full table scan and filter operation on the dir columns. This enhancement is to allow more complex queries to be run against directory metadata, and only scanning the matching directories."
    ],
    [
        "DRILL-4293",
        "DRILL-370",
        "[Drill JDBC] java.lang.NoClassDefFoundError: oadd/org/apache/commons/logging/LogFactory When I connect embedded drill from RJDBC it raises\r\n\r\njava.lang.NoClassDefFoundError:oadd/org/apache/commons/logging/LogFactory\r\n\r\nAnd it seems drill-jdbc-all-1.4.0.jar does not contain  the class\r\n\r\nHere is the MANIFEST.MF\r\n\r\nManifest-Version: 1.0\r\nImplementation-Vendor: The Apache Software Foundation\r\nImplementation-Title: exec/JDBC Driver using dependencies\r\nImplementation-Version: 1.4.0\r\nImplementation-Vendor-Id: org.apache.drill.exec\r\nBuild-Jdk: 1.7.0_45\r\nSpecification-Vendor: The Apache Software Foundation\r\nBuilt-By: \r\nSpecification-Title: exec/JDBC Driver using dependencies\r\nCreated-By: Apache Maven 3.0.5\r\nSpecification-Version: 1.4.0\r\nExtension-Name: org.apache.drill\r\nurl: http://drill.apache.org/\r\nArchiver-Version: Plexus Archiver",
        "Integrate new Drill data types (decimal, date etc) to work with Optiq Build on top of Jacques' SQL Thinning effort. Extend AvaticaDrillSqlAccessor to be able to interpret these new types. "
    ],
    [
        "DRILL-1134",
        "DRILL-995",
        "CompilationException for 2 or more Filters on an array type  The following query's generated code for the Filter consists of the doEval() block which has redefinition of variable isNull which causes a compilation failure.  Note that isNull is not even referenced in the generated code. \r\n\r\nSELECT columns[0] \r\n  FROM dfs.`/Users/asinha/data/regions.csv` \r\nWHERE cast(columns[0] as int) > 1 \r\n  ANDcast(columns[1] as varchar(20))='ASIA';\r\n\r\n}\r\n ] < CompileException:[ Line 82, Column 28: Redefinition of local variable \"isNull\"  ]\"\r\n\r\nI also get a Deadbuf access for this query, but that could be a separate issue. ",
        "Create/ delete storage configuration fail "
    ],
    [
        "DRILL-394",
        "DRILL-2480",
        "INSTALL.md instructions need to be updated bin/sqlline -u jdbc:drill:zk=local -n admin -p admin\r\n/opt/drill/bin/drill-config.sh: line 74: /etc/drill/conf/drill-env.sh: No such file or directory\r\n\r\nI did not have a directory /etc/drill/conf with the conf files in them. The instructions in install.md do not mention moving the conf files to the /etc/drill/conf directory",
        "[umbrella] Identify, fix INFORMATION_SCHEMA and JDBC metadata bugs This DRILL-2480 is a second-level umbrella/tracking bug to group metadata bugs, including other umbrella/tracking bugs, whether in INFORMATION_SCHEMA or in the JDBC code that uses INFORMATION_SCHEMA.\r\nDRILL-2480 partly covers DRILL-2436 (\"JDBC enough for relevant tools\", including metadata).\r\nDRILL-2480 covers DRILL-2522 (\"INFORMATION_SCHEMA enough for relevant tools\")."
    ],
    [
        "DRILL-2706",
        "DRILL-1996",
        "sql functions doc review feedback ",
        "C++ Client: Make Cancel API Public We need to stop C++ Client from calling the result listener for implementing the following ODBC calls sequence:\r\n\r\nSQLPrepare\r\nSQLExecute\r\nSQLCloseCursor\r\n\r\n"
    ],
    [
        "DRILL-299",
        "DRILL-3662",
        "OutgoingRecordBatch trying to get RecordCount on incoming batch with outcome NOT_YET I think this slipped in because TestHashToRandomExchange is currently ignored. This must have accidently been checked in as part of some other jira.\r\n\r\nRemoving the @Ignore, and running the test, gives this exception:\r\n\r\njava.lang.IllegalStateException: You tried to do a batch data read operation when you were in a state of NOT_YET.  You can only do this type of operation when you are in a state of OK or OK_NEW_SCHEMA.\r\n\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.validateReadState(IteratorValidatorBatchIterator.java:48) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.getRecordCount(IteratorValidatorBatchIterator.java:71) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.partitionsender.OutgoingRecordBatch.initializeBatch(OutgoingRecordBatch.java:153) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.partitionsender.OutgoingRecordBatch.<init>(OutgoingRecordBatch.java:74) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.partitionsender.PartitionSenderRootExec.<init>(PartitionSenderRootExec.java:68) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.partitionsender.PartitionSenderCreator.getRoot(PartitionSenderCreator.java:36) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.ImplCreator.visitHashPartitionSender(ImplCreator.java:146) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.ImplCreator.visitHashPartitionSender(ImplCreator.java:57) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.config.HashPartitionSender.accept(HashPartitionSender.java:65) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.ImplCreator.getExec(ImplCreator.java:207) ~[classes/:na]\r\n\torg.apache.drill.exec.work.batch.BitComHandlerImpl.startNewRemoteFragment(BitComHandlerImpl.java:119) [classes/:na]\r\n\torg.apache.drill.exec.work.batch.BitComHandlerImpl.handle(BitComHandlerImpl.java:88) [classes/:na]\r\n\torg.apache.drill.exec.rpc.bit.BitServer.handle(BitServer.java:59) [classes/:na]\r\n\torg.apache.drill.exec.rpc.bit.BitServer.handle(BitServer.java:37) [classes/:na]\r\n\torg.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:142) [classes/:na]\r\n\torg.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:127) [classes/:na]\r\n\tio.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89) [netty-codec-4.0.7.Final.jar:na]\r\n\tio.netty.channel.DefaultChannelHandlerContext.invokeChannelRead(DefaultChannelHandlerContext.java:334) [netty-transport-4.0.7.Final.jar:na]\r\n\tio.netty.channel.DefaultChannelHandlerContext.fireChannelRead(DefaultChannelHandlerContext.java:320) [netty-transport-4.0.7.Final.jar:na]\r\n\tio.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.0.7.Final.jar:na]\r\n\tio.netty.channel.DefaultChannelHandlerContext.invokeChannelRead(DefaultChannelHandlerContext.java:334) [netty-transport-4.0.7.Final.jar:na]\r\n\tio.netty.channel.DefaultChannelHandlerContext.fireChannelRead(DefaultChannelHandlerContext.java:320) [netty-transport-4.0.7.Final.jar:na]\r\n\tio.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:173) [netty-codec-4.0.7.Final.jar:na]\r\n\tio.netty.channel.DefaultChannelHandlerContext.invokeChannelRead(DefaultChannelHandlerContext.java:334) [netty-transport-4.0.7.Final.jar:na]\r\n\tio.netty.channel.DefaultChannelHandlerContext.fireChannelRead(DefaultChannelHandlerContext.java:320) [netty-transport-4.0.7.Final.jar:na]\r\n\tio.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:785) [netty-transport-4.0.7.Final.jar:na]\r\n\tio.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:100) [netty-transport-4.0.7.Final.jar:na]\r\n\tio.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:497) [netty-transport-4.0.7.Final.jar:na]\r\n\tio.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:465) [netty-transport-4.0.7.Final.jar:na]\r\n\tio.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:359) [netty-transport-4.0.7.Final.jar:na]\r\n\tio.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:101) [netty-common-4.0.7.Final.jar:na]\r\n\tjava.lang.Thread.run(Thread.java:722) [na:1.7.0_21]\r\n",
        "Improve DefaultFrameTemplate structure as part of the review for DRILL-3536 two main comments were left aside and will be addressed by this JIRA:\r\n\r\n- refactor DefaultFrameTemplate into separate templates, each one handles a different \"family\" of window functions (e.g. aggregate vs ranking)\r\n- setupEvaluatePeer() should not be called more than once per incoming batch\r\n\r\nThe review for DRILL-3952 did expose a couple of improvements that should be made as part of this JIRA (I will create separate tasks later):\r\n- there are 2 categories of the window functions: one that require all batches of the partition before they can start processing and second that can start work with partial partition as long as we have all peer rows of the current row. We should refactor the code to have derived classes dedicated to the 2 categories.\r\n- Before processing a partition, we do a first pass to compute the length of the partition. Not all window functions need to know the length of the partition. We should be able to refactor the code to avoid this pass unless it's required by one of the window functions"
    ],
    [
        "DRILL-2844",
        "DRILL-2537",
        "Flatten & order by on an inner query and outer query results in an IllegalStateException git.commit.id.abbrev=5cd36c5\r\n\r\nThe below query fails :\r\n{code}\r\nselect s1.type type, flatten(s1.rms.rptd) rptds from (select d.type type, d.uid uid, flatten(d.map.rm) rms from `data.json` d order by d.uid) s1 order by s1.rms.mapid;\r\n+------------+------------+\r\n|    type    |   rptds    |\r\n+------------+------------+\r\nQuery failed: SYSTEM ERROR: Failure while reading vector.  Expected vector class of org.apache.drill.exec.vector.NullableIntVector but was holding vector class org.apache.drill.exec.vector.NullableVarCharVector.\r\n\r\nFragment 0:0\r\n\r\n[0dc08a63-bf6e-4d68-b77d-b9d485110990 on qa-node190.qa.lab:31010]\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n{code}\r\n\r\nThe below variations work :\r\n{code}\r\n1. Get rid of outer flatten : select s1.type type from (select d.type type, d.uid uid, flatten(d.map.rm) rms from `data.json` d order by d.uid) s1 order by s1.rms.mapid;\r\n2. Get rid of the outer order by : select s1.type type, flatten(s1.rms.rptd) rptds from (select d.type type, d.uid uid, flatten(d.map.rm) rms from `data.json` d order by d.uid) s1;\r\n3. Get rid of the inner order by : select s1.type type, flatten(s1.rms.rptd) rptds from (select d.type type, d.uid uid, flatten(d.map.rm) rms from `data.json` d) s1 order by s1.rms.mapid;\r\n{code}\r\n\r\nI attached the log file and data file. Let me know if you have any questions",
        "Scalar replacement should only be run on moderately sized source code. Seeing large amounts of time spent in scalar replacement when running against large source code files (such as those generated by TestLargeFileCompilation)."
    ],
    [
        "DRILL-4300",
        "DRILL-3092",
        "Query fails with \"DrillRuntimeException: Failed to pre-allocate memory for SV\" The following query fails when other queries were running concurrently:\r\n\r\n{code}\r\nalter session set `planner.enable_hashjoin` = false;\r\nselect ws1.* from widestrings ws1 INNER JOIN (select str_var_null_empty from widestrings where str_var_null_empty is not null and length(str_var_null_empty) <> 0 )ws2 on ws1.str_empty=ws2.str_var_null_empty where ws1.str_empty is not null and length(ws1.str_empty) <> 0;\r\nalter session set `planner.enable_hashjoin` = true;\r\n{code}\r\n\r\nError:\r\n{code}\r\n[#2157] Query failed: \r\noadd.org.apache.drill.common.exceptions.UserRemoteException: SYSTEM ERROR: DrillRuntimeException: Failed to pre-allocate memory for SV. Existing recordCount*4 = 0, incoming batch recordCount*4 = 3348\r\n{code}\r\n\r\nLog attached. ",
        "Memory leak when an allocation fails near the creation of a RecordBatchData object A number of locations in the code need try/finally blocks around the code that interacts with the buffers stored in a RecordBatchData object. Runtime exceptions (like running out of memory) in these code blocks can cause buffers to leak."
    ],
    [
        "DRILL-3512",
        "DRILL-3861",
        "json queries fails on unexpected EOL querying directories or files that are open by some writer is a create way to keep up with data (csv/tsv) being logged out by servers, processes etc.\r\n\r\nWe did a small test where we appended a copy of our message stream (1k msg./sec.) to a json file and queried it with Drill. This worked just fine but every once in a while we would get Unexpected End of Line errors as the writer was appending to the file as Drill was reading it.\r\n\r\nIt would be great if an incomplete entry at the end of the file would simply be ignored. ",
        "Apparent uncontrolled format string error in table name error reporting It seems that a data string is being used as a printf format string.\r\n\r\nIn the following, note the percent character in name of the table file (which does not exist, apparently trying to cause an expected no-such-table error) and that the actual error mentions format conversion characters:\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> select * from `test%percent.json`;\r\nSep 29, 2015 2:59:37 PM org.apache.calcite.sql.validate.SqlValidatorException <init>\r\nSEVERE: org.apache.calcite.sql.validate.SqlValidatorException: Table 'test%percent.json' not found\r\nSep 29, 2015 2:59:37 PM org.apache.calcite.runtime.CalciteException <init>\r\nSEVERE: org.apache.calcite.runtime.CalciteContextException: From line 1, column 15 to line 1, column 33: Table 'test%percent.json' not found\r\nError: SYSTEM ERROR: UnknownFormatConversionException: Conversion = 'p'\r\n\r\n\r\n[Error Id: 8025e561-6ba1-4045-bbaa-a96cafc7f719 on dev-linux2:31010] (state=,code=0)\r\n0: jdbc:drill:zk=local> \r\n{noformat}\r\n\r\n(Selecting SQL Parser component because I _think_ table/file existing is checked in validation called in or near the parsing step.)"
    ],
    [
        "DRILL-3182",
        "DRILL-2164",
        "Window function with DISTINCT qualifier returns seemingly incorrect result Both count(distinct <colname>) and count(all <colname>) return the same result. It does not look correct to me and I'm not sure what the correct behavior is going to be.\r\n\r\n(1) Latest postgres does not support distinct with Window functions:\r\n\r\npostgres=# select a2, count(distinct b2) over(partition by a2) from t2;\r\nERROR:  DISTINCT is not implemented for window functions\r\nLINE 1: select a2, count(distinct b2) over(partition by a2) from t2;\r\n                   ^\r\npostgres=# select a2, avg(distinct a2) over(partition by a2) from t2;\r\nERROR:  DISTINCT is not implemented for window functions\r\nLINE 1: select a2, avg(distinct a2) over(partition by a2) from t2;\r\n\r\n(2) Calcite does not support this either:  https://github.com/apache/incubator-calcite/blob/master/doc/reference.md\r\n\r\nDo we support it ? If not,  I think we should throw an error ...\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from t2;\r\n+-----+--------+-------------+\r\n| a2  |   b2   |     c2      |\r\n+-----+--------+-------------+\r\n| 0   | zzz    | 2014-12-31  |\r\n| 1   | aaaaa  | 2015-01-01  |\r\n| 2   | bbbbb  | 2015-01-02  |\r\n| 2   | bbbbb  | 2015-01-02  |\r\n| 2   | bbbbb  | 2015-01-02  |\r\n| 3   | ccccc  | 2015-01-03  |\r\n| 4   | ddddd  | 2015-01-04  |\r\n| 5   | eeeee  | 2015-01-05  |\r\n| 6   | fffff  | 2015-01-06  |\r\n| 7   | ggggg  | 2015-01-07  |\r\n| 7   | ggggg  | 2015-01-07  |\r\n| 8   | hhhhh  | 2015-01-08  |\r\n| 9   | iiiii  | 2015-01-09  |\r\n+-----+--------+-------------+\r\n13 rows selected (0.134 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select a2, count(distinct b2) over(partition by a2) from t2;\r\n+-----+---------+\r\n| a2  | EXPR$1  |\r\n+-----+---------+\r\n| 0   | 1       |\r\n| 1   | 1       |\r\n| 2   | 3       |\r\n| 2   | 3       |\r\n| 2   | 3       |\r\n| 3   | 1       |\r\n| 4   | 1       |\r\n| 5   | 1       |\r\n| 6   | 1       |\r\n| 7   | 2       |\r\n| 7   | 2       |\r\n| 8   | 1       |\r\n| 9   | 1       |\r\n+-----+---------+\r\n13 rows selected (0.224 seconds)\r\n\r\n0: jdbc:drill:schema=dfs> select a2, count(b2) over(partition by a2) from t2;\r\n+-----+---------+\r\n| a2  | EXPR$1  |\r\n+-----+---------+\r\n| 0   | 1       |\r\n| 1   | 1       |\r\n| 2   | 3       |\r\n| 2   | 3       |\r\n| 2   | 3       |\r\n| 3   | 1       |\r\n| 4   | 1       |\r\n| 5   | 1       |\r\n| 6   | 1       |\r\n| 7   | 2       |\r\n| 7   | 2       |\r\n| 8   | 1       |\r\n| 9   | 1       |\r\n+-----+---------+\r\n13 rows selected (0.219 seconds)\r\n{code}",
        "Composite vectors should rely on VectorContainer for maintaining child vectors As of DRILL-1885, the logic that handles maintaining vectors is mostly scattered and repeated across AbstractContainerVector and VectorContainer. We should come up with an abstraction unifying vector container logic for better code re-use."
    ],
    [
        "DRILL-457",
        "DRILL-3727",
        "SQL parse error in joining 2 Hive tables Below is a join query I tried between 2 hive tables (TPC-H schema). Queries on single tables just work fine.\r\nSELECT c.c_custkey, o.o_totalprice FROM hive.customer as c join hive.orders as o on c.c_custkey = o.o_custkey;\r\n\r\nI get the error as below.\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"cfdd6849-5971-455d-a397-cc921963846b\"\r\nendpoint {\r\n  address: \"ubuntu\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < IllegalArgumentException:[ duplicate key: hive ]\"\r\n]Error: exception while executing query (state=,code=0)",
        "Drill should return NULL instead of failure if cast column is empty If Drill is casting an empty string to date, it will fail with error:\r\nError: SYSTEM ERROR: IllegalFieldValueException: Value 0 for monthOfYear must be in the range [1,12]\r\nHowever Hive can just return a NULL instead.\r\nI think it makes sense for Drill to have the same behavior as Hive in this case.\r\n\r\nRepro:\r\nHive:\r\n{code}\r\ncreate table h1db.testempty(col0 string)\r\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\r\nSTORED AS TEXTFILE\r\n;\r\n\r\nhive> select * from h1db.testempty ;\r\nOK\r\n\r\n2015-01-01\r\nTime taken: 0.28 seconds, Fetched: 2 row(s)\r\n\r\nhive> select cast(col0 as date) from  h1db.testempty;\r\nOK\r\nNULL\r\n2015-01-01\r\nTime taken: 0.078 seconds, Fetched: 2 row(s)\r\n{code}\r\n\r\nDrill:\r\n{code}\r\nuse hive;\r\n> select * from h1db.testempty ;\r\n+-------------+\r\n|    col0     |\r\n+-------------+\r\n|             |\r\n| 2015-01-01  |\r\n+-------------+\r\n2 rows selected (0.232 seconds)\r\n\r\n> select cast(col0 as date) from  h1db.testempty;\r\nError: SYSTEM ERROR: IllegalFieldValueException: Value 0 for monthOfYear must be in the range [1,12]\r\n{code}\r\n\r\nWorkaround:\r\n{code}\r\n> select case when col0='' then null else cast(col0 as date) end from  h1db.testempty;\r\n+-------------+\r\n|   EXPR$0    |\r\n+-------------+\r\n| null        |\r\n| 2015-01-01  |\r\n+-------------+\r\n2 rows selected (0.287 seconds)\r\n{code}"
    ],
    [
        "DRILL-3982",
        "DRILL-671",
        "RESET ALL command - IOException: Error getting user info for current user, anonymous ALTER SYSTEM RESET ALL command reports IOException when impersonation is enabled.\r\n\r\ngit.commit.id=fab061e6\r\n\r\n{code}\r\nDetails of user that connected to Drill\r\n\r\n[root@centos-01 bin]# id\r\nuid=0(root) gid=0(root) groups=0(root)\r\n\r\n[root@centos-01 bin]# ./sqlline -u \"jdbc:drill:schema=dfs.tmp\"\r\napache drill 1.3.0-SNAPSHOT\r\n\"say hello to my little drill\"\r\n0: jdbc:drill:schema=dfs.tmp> ALTER SYSTEM RESET ALL;\r\nError: SYSTEM ERROR: IOException: Error getting user info for current user, anonymous\r\n\r\n\r\n[Error Id: de36ec03-b4c1-4b3a-a726-ae49f068fae4 on centos-02.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nStack trace from drillbit.log\r\n\r\n{code}\r\n2015-10-26 23:48:40,975 [29d1443b-cff4-f7d2-9083-40a5c38f8db2:foreman] ERROR o.a.drill.exec.work.foreman.Foreman - SYSTEM ERROR: IOException: Error getting user info for current user, anonymous\r\n\r\n\r\n[Error Id: de36ec03-b4c1-4b3a-a726-ae49f068fae4 on centos-02.qa.lab:31010]\r\norg.apache.drill.common.exceptions.UserException: SYSTEM ERROR: IOException: Error getting user info for current user, anonymous\r\n\r\n\r\n[Error Id: de36ec03-b4c1-4b3a-a726-ae49f068fae4 on centos-02.qa.lab:31010]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:534) ~[drill-common-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$ForemanResult.close(Foreman.java:742) [drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:841) [drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:786) [drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.common.EventProcessor.sendEvent(EventProcessor.java:73) [drill-common-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.moveToState(Foreman.java:788) [drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.moveToState(Foreman.java:894) [drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:255) [drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_85]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_85]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_85]\r\nCaused by: org.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: Failed to create schema tree: Error getting user info for current user, anonymous\r\n        ... 4 common frames omitted\r\nCaused by: org.apache.drill.common.exceptions.DrillRuntimeException: Failed to create schema tree: Error getting user info for current user, anonymous\r\n        at org.apache.drill.exec.ops.QueryContext.getRootSchema(QueryContext.java:171) ~[drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.ops.QueryContext.getRootSchema(QueryContext.java:153) ~[drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.ops.QueryContext.getRootSchema(QueryContext.java:141) ~[drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.ops.QueryContext.getNewDefaultSchema(QueryContext.java:127) ~[drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.<init>(DrillSqlWorker.java:91) ~[drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:903) [drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:244) [drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        ... 3 common frames omitted\r\nCaused by: java.io.IOException: Error getting user info for current user, anonymous\r\n        at com.mapr.fs.MapRFileSystem.lookupClient(MapRFileSystem.java:612) ~[maprfs-4.1.0-mapr.jar:4.1.0-mapr]\r\n        at com.mapr.fs.MapRFileSystem.lookupClient(MapRFileSystem.java:649) ~[maprfs-4.1.0-mapr.jar:4.1.0-mapr]\r\n        at com.mapr.fs.MapRFileSystem.listMapRStatus(MapRFileSystem.java:1376) ~[maprfs-4.1.0-mapr.jar:4.1.0-mapr]\r\n        at com.mapr.fs.MapRFileSystem.listStatus(MapRFileSystem.java:1436) ~[maprfs-4.1.0-mapr.jar:4.1.0-mapr]\r\n        at com.mapr.fs.MapRFileSystem.listStatus(MapRFileSystem.java:78) ~[maprfs-4.1.0-mapr.jar:4.1.0-mapr]\r\n        at org.apache.drill.exec.store.dfs.DrillFileSystem.listStatus(DrillFileSystem.java:520) ~[drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory.accessible(WorkspaceSchemaFactory.java:135) ~[drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.dfs.FileSystemSchemaFactory$FileSystemSchema.<init>(FileSystemSchemaFactory.java:78) ~[drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.dfs.FileSystemSchemaFactory.registerSchemas(FileSystemSchemaFactory.java:65) ~[drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.dfs.FileSystemPlugin.registerSchemas(FileSystemPlugin.java:132) ~[drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.store.StoragePluginRegistry$DrillSchemaFactory.registerSchemas(StoragePluginRegistry.java:335) ~[drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        at org.apache.drill.exec.ops.QueryContext.getRootSchema(QueryContext.java:164) ~[drill-java-exec-1.3.0-SNAPSHOT.jar:1.3.0-SNAPSHOT]\r\n        ... 9 common frames omitted\r\n{code}\r\n\r\nImpersonation was enabled on all nodes of cluster\r\n{code}\r\n[root@centos-01 ~]# clush -a tail -n 5 /opt/mapr/drill/drill-1.3.0/conf/drill-override.conf\r\n:   impersonation: {\r\n:        enabled: true,\r\n:        max_chained_user_hops: 3\r\n:       }\r\n: }\r\n:   impersonation: {\r\n:        enabled: true,\r\n:        max_chained_user_hops: 3\r\n:       }\r\n: }\r\n:   impersonation: {\r\n:        enabled: true,\r\n:        max_chained_user_hops: 3\r\n:       }\r\n: }\r\n:   impersonation: {\r\n:        enabled: true,\r\n:        max_chained_user_hops: 3\r\n:       }\r\n: }\r\n{code}\r\n\r\n{code}\r\nroot@centos-01 conf]# clush -a grep \"MAPR_IMPERSONATION_ENABLED\" /opt/mapr/drill/drill-1.3.0/conf/drill-env.sh\r\n: export MAPR_IMPERSONATION_ENABLED=true\r\n: export MAPR_IMPERSONATION_ENABLED=true\r\n: export MAPR_IMPERSONATION_ENABLED=true\r\n: export MAPR_IMPERSONATION_ENABLED=true\r\n{code}",
        "Select against hbase table with filter against row_key fails Properties of hbase table:\r\nhbase(main):005:0> describe 'voter'\r\nDESCRIPTION                                                          ENABLED                              \r\n 'voter', {NAME => 'fourcf', DATA_BLOCK_ENCODING => 'NONE', BLOOMFIL true                                 \r\n TER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', MIN_VERSI                                      \r\n ONS => '0', TTL => '2147483647', KEEP_DELETED_CELLS => 'false', BLO                                      \r\n CKSIZE => '65536', IN_MEMORY => 'false', ENCODE_ON_DISK => 'true',                                       \r\n BLOCKCACHE => 'true'}, {NAME => 'onecf', DATA_BLOCK_ENCODING => 'NO                                      \r\n NE', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '                                      \r\n 3', MIN_VERSIONS => '0', TTL => '2147483647', KEEP_DELETED_CELLS =>                                      \r\n  'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', ENCODE_ON_DIS                                      \r\n K => 'true', BLOCKCACHE => 'true'}, {NAME => 'threecf', DATA_BLOCK_                                      \r\n ENCODING => 'NONE', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0'                                      \r\n , VERSIONS => '3', MIN_VERSIONS => '0', TTL => '2147483647', KEEP_D                                      \r\n ELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false'                                      \r\n , ENCODE_ON_DISK => 'true', BLOCKCACHE => 'true'}, {NAME => 'twocf'                                      \r\n , DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'NONE', REPLICATION                                      \r\n _SCOPE => '0', VERSIONS => '3', MIN_VERSIONS => '0', TTL => '214748                                      \r\n 3647', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMO                                      \r\n RY => 'false', ENCODE_ON_DISK => 'true', BLOCKCACHE => 'true'}                  \r\n\r\nWith the latest build, the following query fails:\r\nselect * from voter where row_key=100;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"0dbabd27-b027-4afe-b509-efa2974a0ff2\"\r\nendpoint {\r\n  address: \"qa-node64.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < IllegalStateException:[ Failure while closing accountor.  Expected private and shared pools to be set to initial values.  However, one or more were not.  Stats are\\n\\tzone\\tinit\\tallocated\\tdelta \\n\\tprivate\\t1000000\\t999778\\t222 \\n\\tshared\\t9999000000\\t9999000000\\t0. ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n\r\nThis could be due to the \"voter\" table expands multiple regions.  I have a table that exists in only 1 region and the same select went fine:\r\n\r\nselect * from student where row_key=100;\r\n+------------+-------------+------------+------------+------------+------------+\r\n|  row_key   | create_date | studentnum |    name    |    gpa     |    age     |\r\n+------------+-------------+------------+------------+------------+------------+\r\n| [B@31174ed3 | [B@713817d2 | [B@19a41610 | [B@63a48196 | [B@4537d1f5 | [B@53b94f53 |\r\n+------------+-------------+------------+------------+------------+------------+\r\n"
    ],
    [
        "DRILL-3480",
        "DRILL-3400",
        "Some tpcds queries fail with with timeout errors Commit Id 9a85b2c\r\n\r\nSome failed queries contained the following errors:\r\n{code}\r\nFailed while running cleanup query. Not returning connection to pool.\r\njava.lang.InterruptedException: sleep interrupted\r\n\tat java.lang.Thread.sleep(Native Method)\r\n\tat org.apache.drill.test.framework.DrillTestJdbc.run(DrillTestJdbc.java:100)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\nChannel closed /10.10.104.85:59334 <--> /10.10.104.85:31010.\r\n{code}\r\nOthers failed with error:\r\n{code}\r\norg.apache.drill.common.exceptions.UserRemoteException: CONNECTION ERROR: Exceeded timeout (40000) while waiting send intermediate work fragments to remote nodes. Sent 8 and only heard response back from 4 nodes.\r\n\r\n[Error Id: b85205b5-3134-4f90-aca8-7d67af04f3ed]\r\n\tat org.apache.drill.exec.rpc.user.QueryResultHandler.resultArrived(QueryResultHandler.java:118)\r\n\tat org.apache.drill.exec.rpc.user.UserClient.handleReponse(UserClient.java:111)\r\n\tat org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:47)\r\n\tat org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:32)\r\n\tat org.apache.drill.exec.rpc.RpcBus.handle(RpcBus.java:61)\r\n\tat org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:233)\r\n\tat org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:205)\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n\tat io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)\r\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n{code}\r\n\r\n\r\n\r\n\r\n",
        "After shifting CTAS's data, query on CTAS table failed  A simple query like this:\r\ncreate table `ttt` partition by (r_regionkey) as select * from cp.`tpch/region.parquet`;\r\n\r\nWithout touching generated data from CTAS, this query select * from `ttt`; works. \r\n\r\nThen, I tried to reorganize the parquet generated by CTAS as:\r\n|-----Q1/ ... .pq\r\n       Q2/ ... .pq\r\n       Q3/ ... .pq\r\n\r\nHowever, after this manual moving, any query fails. Surprisingly, even after I manually move the data to the original place, the queries which worked before all failed with:\r\n\r\nError: PARSE ERROR: From line 1, column 15 to line 1, column 19: Table 'ttt' not found"
    ],
    [
        "DRILL-3754",
        "DRILL-1252",
        "Remove redundancy in run-time generated code for common column references.  When a operator (Filter, project) has expression which refer one same field multiple times, Drill will initialize a value vector and do value holder assignment   for each field reference in the run-time generated code.  The redundancy might impact the expression evaluation, after the compiled code is executed over large number of incoming rows.\r\n\r\nThis has been seen in recent performance issue reported on the drill user list,  where the query contains multiple multiple in list filter conditions. \r\n\r\nIn this JIRA, we'll remove the redundancy for the common field reference, so that only one initialization and assignment happen in the run-time generated code.\r\n",
        "Implement Complex parquet and json writers The current parquet writer does not handle complex types. And there currently is no json writer.\r\n\r\nThe goal is to enhance the record writer interface to handle complex types, and implement this for parquet and json. The text writer will also handle complex types, by printing out the JSON object when the field is complex."
    ],
    [
        "DRILL-2820",
        "DRILL-1718",
        "Error message must be updated when show files is executed on a non-existent workspace The following message is thrown when SHOW FILES is executed on an existing directory, which is not configured as a workspace:\r\n\r\n{code:sql}\r\n> show files from dfs.tmp.drill.tpch;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| false      | Current schema 'dfs.tmp.drill.tpch' is not a file system schema. Can't execute show files on this schema. |\r\n+------------+------------+\r\n1 row selected (0.375 seconds)\r\n\r\n(Query executes when the following syntax is used:)\r\n0: jdbc:drill:zk=local> show files from dfs.tmp.`drill/tpch`;\r\n+------------+-------------+------------+------------+------------+------------+-------------+------------+------------------+\r\n|    name    | isDirectory |   isFile   |   length   |   owner    |   group    | permissions | accessTime | modificationTime |\r\n+------------+-------------+------------+------------+------------+------------+-------------+------------+------------------+\r\n| customer.psv | false       | true       | 239490     | agirish    | wheel      | rwxr-xr-x   | 1969-12-31 16:00:00.0 | 2015-04-17 15:37:31.0 |\r\n| lineitem.psv | false       | true       | 7204075    | agirish    | wheel      | rwxr-xr-x   | 1969-12-31 16:00:00.0 | 2015-04-17 15:37:30.0 |\r\n+------------+-------------+------------+------------+------------+------------+-------------+------------+------------------+\r\n{code}\r\n\r\nThis should be updated to:\r\n{code:sql}\r\n> show files from dfs.tmp.drill.tpch;\r\nQuery failed: Schema 'dfs.tmp.drill.tpch' does not exist.\r\n{code}",
        "FLATTEN query returns \"buffer is neither a method, a field, nor a member class...\" error A query with a FLATTEN function and a WHERE clause constraint on the flattened column fails.\r\n\r\n0: jdbc:drill:zk=local> select name, flatten(categories) as categories from dfs.yelp.`yelp_academic_dataset_business.json` where categories='Pizza';\r\nQuery failed: Failure while running fragment., Line 68, Column 95: \"buffer\" is neither a method, a field, nor a member class of \"org.apache.drill.exec.expr.holders.RepeatedVarCharHolder\" [ 67016271-7266-4438-9529-d8330b1e39a4 on 10.250.0.28:31010 ]\r\n\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n0: jdbc:drill:zk=local> explain plan for select name, flatten(categories) as categories from dfs.yelp.`yelp_academic_dataset_business.json` where categories='Pizza';\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(name=[$1], categories=[$2])\r\n00-02        Flatten(flattenField=[$2])\r\n00-03          Project(EXPR$0=[$0], EXPR$1=[$1], EXPR$2=[$0])\r\n00-04            SelectionVectorRemover\r\n00-05              Filter(condition=[=($0, 'Pizza')])\r\n00-06                Scan(groupscan=[EasyGroupScan [selectionRoot=/Users/brumsby/drill/apache-drill-0.7.0-incubating-SNAPSHOT/yelpdata/yelp_academic_dataset_business.json, numFiles=1, columns=[`categories`, `name`], files=[file:/Users/brumsby/drill/apache-drill-0.7.0-incubating-SNAPSHOT/yelpdata/yelp_academic_dataset_business.json]]])\r\n | {\r\n  \"head\" : {\r\n    \"version\" : 1,\r\n    \"generator\" : {\r\n      \"type\" : \"ExplainHandler\",\r\n      \"info\" : \"\"\r\n    },\r\n    \"type\" : \"APACHE_DRILL_PHYSICAL\",\r\n    \"options\" : [ ],\r\n    \"queue\" : 0,\r\n    \"resultMode\" : \"EXEC\"\r\n  },\r\n  \"graph\" : [ {\r\n    \"pop\" : \"fs-scan\",\r\n    \"@id\" : 6,\r\n    \"files\" : [ \"file:/Users/brumsby/drill/apache-drill-0.7.0-incubating-SNAPSHOT/yelpdata/yelp_academic_dataset_business.json\" ],\r\n    \"storage\" : {\r\n      \"type\" : \"file\",\r\n      \"enabled\" : true,\r\n      \"connection\" : \"file:///\",\r\n      \"workspaces\" : {\r\n        \"root\" : {\r\n          \"location\" : \"/\",\r\n          \"writable\" : false,\r\n          \"defaultInputFormat\" : null\r\n        },\r\n        \"yelp\" : {\r\n          \"location\" : \"/Users/brumsby/drill/apache-drill-0.7.0-incubating-SNAPSHOT/yelpdata\",\r\n          \"writable\" : true,\r\n          \"defaultInputFormat\" : null\r\n        }\r\n      },\r\n      \"formats\" : {\r\n        \"psv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tbl\" ],\r\n          \"delimiter\" : \"|\"\r\n        },\r\n        \"csv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"csv\" ],\r\n          \"delimiter\" : \",\"\r\n        },\r\n        \"tsv\" : {\r\n          \"type\" : \"text\",\r\n          \"extensions\" : [ \"tsv\" ],\r\n          \"delimiter\" : \"\\t\"\r\n        },\r\n        \"parquet\" : {\r\n          \"type\" : \"parquet\"\r\n        },\r\n        \"json\" : {\r\n          \"type\" : \"json\"\r\n        }\r\n      }\r\n    },\r\n    \"format\" : {\r\n      \"type\" : \"json\"\r\n    },\r\n    \"columns\" : [ \"`categories`\", \"`name`\" ],\r\n    \"selectionRoot\" : \"/Users/brumsby/drill/apache-drill-0.7.0-incubating-SNAPSHOT/yelpdata/yelp_academic_dataset_business.json\",\r\n    \"cost\" : 36382.0\r\n  }, {\r\n    \"pop\" : \"filter\",\r\n    \"@id\" : 5,\r\n    \"child\" : 6,\r\n    \"expr\" : \"equal(`categories`, 'Pizza') \",\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5457.3\r\n  }, {\r\n    \"pop\" : \"selection-vector-remover\",\r\n    \"@id\" : 4,\r\n    \"child\" : 5,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5457.3\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 3,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`EXPR$0`\",\r\n      \"expr\" : \"`categories`\"\r\n    }, {\r\n      \"ref\" : \"`EXPR$1`\",\r\n      \"expr\" : \"`name`\"\r\n    }, {\r\n      \"ref\" : \"`EXPR$2`\",\r\n      \"expr\" : \"`categories`\"\r\n    } ],\r\n    \"child\" : 4,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5457.3\r\n  }, {\r\n    \"pop\" : \"flatten\",\r\n    \"@id\" : 2,\r\n    \"child\" : 3,\r\n    \"column\" : \"`EXPR$2`\",\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5457.3\r\n  }, {\r\n    \"pop\" : \"project\",\r\n    \"@id\" : 1,\r\n    \"exprs\" : [ {\r\n      \"ref\" : \"`name`\",\r\n      \"expr\" : \"`EXPR$1`\"\r\n    }, {\r\n      \"ref\" : \"`categories`\",\r\n      \"expr\" : \"`EXPR$2`\"\r\n    } ],\r\n    \"child\" : 2,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5457.3\r\n  }, {\r\n    \"pop\" : \"screen\",\r\n    \"@id\" : 0,\r\n    \"child\" : 1,\r\n    \"initialAllocation\" : 1000000,\r\n    \"maxAllocation\" : 10000000000,\r\n    \"cost\" : 5457.3\r\n  } ]\r\n} |\r\n+------------+------------+\r\n1 row selected (0.115 seconds)\r\n0: jdbc:drill:zk=local> \r\n"
    ],
    [
        "DRILL-1059",
        "DRILL-3592",
        "tracking: Full outer join not currently supported with hash join disabled git.commit.id.abbrev=79c1502\r\ngit.commit.id=79c1502c1e96596d4db302c2dd1c9f78d0f4d43d\r\n\r\n0: jdbc:drill:schema=dfs> alter session set `planner.enable_hashjoin` = false;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | planner.enable_hashjoin updated. |\r\n+------------+------------+\r\n1 row selected (0.075 seconds)\r\n0: jdbc:drill:schema=dfs> select store.store_id, store.store_name, store.store_city, store.store_state, store.store_postal_code, store.store_country, region.region_id, region.sales_city from store full outer join region on store.region_id = region.region_id order by region.region_id;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"008e5aec-3e70-42e3-9b45-74bc5f733ae0\"\r\nendpoint {\r\n  address: \"perfnode104.perf.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < IllegalArgumentException:[ Full outer join not currently supported ]\"\r\n]\r\nError: exception while executing query (state=,code=0)",
        "Constant reduction breaks compilation if function has complex output It appears that constant reduction is trying to reduce items whose output is complex.  This can't be done since constant reduction can't handle a complex output.  A workaround is using isRandom=true in the annotation.  However, this will snag udf creators."
    ],
    [
        "DRILL-2719",
        "DRILL-2291",
        "Behavior of ValueVector#getBuffers(clear) is inconsistent across VV types The major cause of inconsistency here is that reference count does not change upon a call to BaseDataVV#getBuffers. However, it does change when the method is invoked on a composite vector type. See for instance VariableLengthVV[1] or a NullableVV[2].\r\n\r\nThe proposal is to make this behavior consistent first and later we can consider separating out the logic that clears a vector from the logic that returns underlying buffers.\r\n\r\n[1]: https://github.com/apache/drill/blob/master/exec/java-exec/src/main/codegen/templates/VariableLengthVectors.java#L151\r\n[2]: https://github.com/apache/drill/blob/master/exec/java-exec/src/main/codegen/templates/NullableValueVectors.java#L82\r\n",
        "CTAS broken when we have repeated lists git.commit.id.abbrev=6676f2d\r\n\r\nData Set :\r\n{code}\r\n{\r\n  \"id\" : 1,\r\n  \"lst_lst\" : [[1,2],[3,4]]\r\n}\r\n{code}\r\n\r\nThe below CTAS statement fails (The underlying query succeeds)\r\n{code}\r\ncreate table nested_list as select d.lst_lst from `temp.json` d;\r\nQuery failed: RemoteRpcException: Failure while running fragment.[ e5f3cdf2-d507-412e-a11a-24c9d2362975 on qa-node190.qa.lab:31010 ]\r\n[ e5f3cdf2-d507-412e-a11a-24c9d2362975 on qa-node190.qa.lab:31010 ]\r\n{code}\r\n\r\n\r\nBelow is the log file :\r\n{code}\r\n2015-02-24 18:45:56,332 [2b13391c-2000-e539-4adc-a661458b24c1:frag:0:0] ERROR o.a.d.e.w.f.AbstractStatusReporter - Error 3d91c456-3a24-4a8d-84b3-57a287aa2c54: Failure while running fragment.\r\njava.lang.NullPointerException: null\r\n        at org.apache.drill.exec.store.parquet.ParquetRecordWriter.cleanup(ParquetRecordWriter.java:318) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.WriterRecordBatch.cleanup(WriterRecordBatch.java:187) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.cleanup(IteratorValidatorBatchIterator.java:148) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.cleanup(AbstractSingleRecordBatch.java:121) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.cleanup(IteratorValidatorBatchIterator.java:148) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.internalStop(ScreenCreator.java:178) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:101) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:116) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:303) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_71]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]\r\n2015-02-24 18:45:56,334 [2b13391c-2000-e539-4adc-a661458b24c1:frag:0:0] INFO  o.a.drill.exec.work.foreman.Foreman - State change requested.  RUNNING --> FAILED\r\norg.apache.drill.exec.rpc.RemoteRpcException: Failure while running fragment.[ 3d91c456-3a24-4a8d-84b3-57a287aa2c54 on qa-node190.qa.lab:31010 ]\r\n[ 3d91c456-3a24-4a8d-84b3-57a287aa2c54 on qa-node190.qa.lab:31010 ]\r\n\r\n        at org.apache.drill.exec.work.foreman.QueryManager.statusUpdate(QueryManager.java:95) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.QueryManager$RootStatusReporter.statusChange(QueryManager.java:154) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.AbstractStatusReporter.fail(AbstractStatusReporter.java:114) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.AbstractStatusReporter.fail(AbstractStatusReporter.java:110) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.internalFail(FragmentExecutor.java:168) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:131) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:303) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_71]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]\r\n        at java.lang.Thread.run(Thread.java:745)\r\n{code}"
    ],
    [
        "DRILL-3788",
        "DRILL-4061",
        "Directory based partition pruning not taking effect with metadata caching git.commit.id.abbrev=240a455\r\n\r\nPartition Pruning did not take place for the below query after I executed the \"refresh table metadata command\"\r\n{code}\r\n explain plan for \r\nselect\r\n  l_returnflag,\r\n  l_linestatus\r\nfrom\r\n  `lineitem/2006/1`\r\nwhere\r\n  dir0=1 or dir0=2\r\n{code}\r\n\r\nThe logs did not indicate that \"pruning did not take place\"\r\n\r\nBefore executing the refresh table metadata command, partition pruning did take effect\r\n\r\nI am not attaching the data set as it is larger than 10MB. Reach out to me if you need more information\r\n",
        "Incorrect results returned by window function query. Window function query that uses lag function returns incorrect results.\r\nsys.version => 3a73f098\r\nDrill 1.3\r\nTest parquet file is attached here.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> CREATE TABLE testrepro AS SELECT CAST(columns[0] AS INT) col0, CAST(columns[1] AS INT) col1 FROM `testRepro.csv`;\r\n+-----------+----------------------------+\r\n| Fragment  | Number of records written  |\r\n+-----------+----------------------------+\r\n| 0_0       | 11                         |\r\n+-----------+----------------------------+\r\n1 row selected (0.542 seconds)\r\n0: jdbc:drill:schema=dfs.tmp> select col1, 1 / (col1 - lag(col1) OVER (ORDER BY col0)) from testrepro;\r\n+-------+---------+\r\n| col1  | EXPR$1  |\r\n+-------+---------+\r\n| 11    | null    |\r\n| 9     | 0       |\r\n| 0     | 0       |\r\n| 10    | 0       |\r\n| 19    | 0       |\r\n| 13    | 0       |\r\n| 17    | 0       |\r\n| -1    | 0       |\r\n| 1     | 0       |\r\n| 20    | 0       |\r\n| 100   | 0       |\r\n+-------+---------+\r\n11 rows selected (0.451 seconds)\r\n{code}"
    ],
    [
        "DRILL-1918",
        "DRILL-1819",
        "Drill does very obscure things when a JRE is used instead of a JDK. In http://answers.mapr.com/questions/161911/apache-drill-07-errors.html#comment-161933 a user describes the consequences of running Drill with a JRE instead of a JDK.  \r\n\r\nSurely this could be detected and this confusion could be avoided.\r\n\r\n",
        "Relax maven-enforcer-plugin to allow latest builds of java 1.8.0 Currently maven-enforcer-plugin is configured to restrict the java versions to 1.7 and 1.8. Unfortunately this does not allow to make use of the latest builds of java 1.8.0. For example, we won't be able to build drill on java version \"1.8.0_25\". While having the plugin gives us a lot of advantage in terms of ensuring we have the right platform setup before, we build the code on the machine; not including the latest builds of java for the already enabled java version 1.8, makes it too restrictive. This task is to relax it a bit to include all builds for java 1.8.0"
    ],
    [
        "DRILL-3273",
        "DRILL-2809",
        "Hive function 'concat_ws' not working from drill git.commit.id.abbrev=5f26b8b\r\n\r\nThe below query from drill fails. It works from hive.\r\n{code}\r\n0: jdbc:drill:schema=dfs_eea> select concat_ws(varchar_col, interval_col, '|') from hive.crosssources.fewtypes_null_hive;\r\nError: SYSTEM ERROR: java.lang.RuntimeException: GenericUDF.evaluate method failed\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 2d318049-26c9-4903-a7d5-78fa61aee70c on qa-node190.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nI attached the error log from drill. Let me know if you need anything.",
        "Increase the default value of partitioner_sender_threads_factor The current default value of planner.partitioner_sender_threads_factor is 1.  This is too low for most requirements.  In one scenario the elapsed time of a query on a 20 node cluster went from 158 secs to 60 secs when increasing the value to 2.   However, we have to determine a reasonable default for this and related partition_sender settings. "
    ],
    [
        "DRILL-2856",
        "DRILL-590",
        "StreamingAggBatch goes into infinite loop due to state management issues ",
        "Queries which return 0 rows result in no column information 0: jdbc:drill:schema=dfs.drillTestDirP1> select * from student where gpa>20 limit 10;\r\n+--+\r\n|  |\r\n+--+\r\n+--+\r\n\r\nMight be a dup because Jacques spoke to me about this. Could not find the corresponding bug though."
    ],
    [
        "DRILL-3759",
        "DRILL-2176",
        "Make partition pruning multi-phased to reduce the working set kept in memory Currently, partition pruning gets all file names in the table and applies the pruning.  Suppose the files are spread out over several directories and there is a filter  on dirN,  this is not efficient - both in terms of elapsed time and memory usage.  This has been seen in a few use cases recently. \r\n\r\nWherever possible, we should ideally perform the pruning in N steps (where N is the number of directory levels referenced in the filter conditions):   \r\n  1. Get the directory and  filenames at level i\r\n  2. Materialize into the in-memory table \r\n  3. Apply interpreter-based evaluation of filter condition\r\n  4. Determine qualifying directories, increment i and repeat from step 1\r\n \r\nThis multi phase approach may not be possible for certain types of filters - e,g for disjunctions. This analysis needs to be done. \r\n",
        "IndexOutOfBoundsException for count(*) on a subquery which does order-by The IOBE occurs in creating the collation trait in Calcite.  \r\n{code}\r\n0: jdbc:drill:zk=local> select count(*) from (select n_nationkey, n_regionkey from cp.`tpch/nation.parquet` order by 1, 2);\r\nQuery failed: IndexOutOfBoundsException: index (1) must be less than size (1)\r\n{code}\r\n\r\nFull stack trace: \r\n{code}\r\naused by: java.lang.IndexOutOfBoundsException: index (1) must be less than size (1)\r\n        at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305) ~[guava-14.0.1.jar:na]\r\n        at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284) ~[guava-14.0.1.jar:na]\r\n        at com.google.common.collect.SingletonImmutableList.get(SingletonImmutableList.java:45) ~[guava-14.0.1.jar:na]\r\n        at org.eigenbase.rex.RexBuilder.makeInputRef(RexBuilder.java:764) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.SortRel.<init>(SortRel.java:94) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.SortRel.<init>(SortRel.java:59) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:78) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.rel.RelCollationTraitDef.convert(RelCollationTraitDef.java:1) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1011) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1102) ~[optiq-core-0.9-drill-r18.jar:na]\r\n        at org.eigenbase.relopt.volcano.AbstractConverter$ExpandConversionRule.onMatch(AbstractConverter.java:108) ~[optiq-core-0.9-drill-r18.jar:na]\r\n{code}\r\n\r\nThis might be related to CALCITE-569 (and possibly DRILL-1978) but the stack traces are different, so I am treating this as a separate issue.  "
    ],
    [
        "DRILL-1539",
        "DRILL-357",
        "RepeatedMapVector maintains incorrect last non-empty element index. RepeatedMapVector maintains the last non-empty element index. The invariant for the variable is that it must start from -1 and it must be less than the group count. Current implementation violates this invariant. Drill will raise IndexOutOfBounds exception for certain cases led by this violation.",
        "Hive Storage Engine phase 2 - hive record reader Using Hive serde and hadoop input formats, support querying hive tables.\r\n\r\nAt first, only support flat data types. Supporting complex data types in hive tables will come later."
    ],
    [
        "DRILL-1334",
        "DRILL-37",
        "Sequence and numbering of operators in query profile does not match physical plan The query profile for the fragment 2 below shows operators 0 to 4 (Parquet Group Scan is 0 and top Project is 4).  This does not match the physical plan which shows Parquet Group Scan as operator 5 and the top Project as operator id 1.   We should make the profile consistent with the plan. \r\n\r\n{code:sql}\r\n#2 - Op 0\tPARQUET_ROW_GROUP_SCAN\t\r\n#2 - Op 1\tPROJECT\r\n#2 - Op 2\tSELECTION_VECTOR_REMOVER\r\n#2 - Op 3\tFILTER\r\n#2 - Op 4\tPROJECT\r\n{code}\t\r\n\r\n{code:sql}\r\n00-00    Screen\r\n00-01      StreamAgg(group=[{}], EXPR$0=[COUNT()])\r\n00-02        UnionExchange\r\n01-01          Project($f0=[0])\r\n01-02            HashAgg(group=[{0, 1}])\r\n01-03              HashToRandomExchange(dist0=[[$0]], dist1=[[$1]])\r\n02-01                Project(sssk=[$0], ssik=[$1])\r\n02-02                  SelectionVectorRemover\r\n02-03                    Filter(condition=[AND(>=($2, 2451911), <=($2, 2452275))])\r\n02-04                      Project(ss_store_sk=[$1], ss_item_sk=[$2], ss_sold_date_sk=[$0])\r\n02-05                        Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=maprfs:/tmp/store_sales]], selectionRoot=/tmp/store_sales, columns=[SchemaPath [`ss_store_sk`], SchemaPath [`ss_item_sk`], SchemaPath [`ss_sold_date_sk`]]]])\r\n{code}",
        "Reference Operator Unit Tests Build out a suite of unit tests for the reference operators."
    ],
    [
        "DRILL-3478",
        "DRILL-442",
        "Bson Record Reader for Mongo storage plugin Improve the mongo query performance.\r\nWe are considering the suggestions provided by [~dragoncurve] and [~hgunes] in drill mailing chain.",
        "Implement text format plugin Implement a format plugin to read text files, e.g. csv, tsv. This will be able to read compressed and uncompressed data.\r\n\r\nThe record reader will return a single column of type RepeatedVarChar. Compression and delimiter type will be determined from the file extensions."
    ],
    [
        "DRILL-3122",
        "DRILL-1794",
        "Changing a session option to default value results in status as changed Alter session option hash join to true(which is the default) and the following query shows that the option has changed which could be misleading to users relying on the status field to see if an option has changed or not. Especially in the case of a boolean value. \r\n{code}\r\n0: jdbc:drill:zk=10.10.100.171:5181> select * from sys.options where name like '%hash%';\r\n+--------------------------------------------+----------+---------+----------+-------------+-------------+-----------+------------+\r\n|                    name                    |   kind   |  type   |  status  |   num_val   | string_val  | bool_val  | float_val  |\r\n+--------------------------------------------+----------+---------+----------+-------------+-------------+-----------+------------+\r\n| exec.max_hash_table_size                   | LONG     | SYSTEM  | DEFAULT  | 1073741824  | null        | null      | null       |\r\n| exec.min_hash_table_size                   | LONG     | SYSTEM  | DEFAULT  | 65536       | null        | null      | null       |\r\n| planner.enable_hash_single_key             | BOOLEAN  | SYSTEM  | DEFAULT  | null        | null        | true      | null       |\r\n| planner.enable_hashagg                     | BOOLEAN  | SYSTEM  | DEFAULT  | null        | null        | true      | null       |\r\n| planner.enable_hashjoin                    | BOOLEAN  | SYSTEM  | DEFAULT  | null        | null        | true      | null       |\r\n| planner.enable_hashjoin_swap               | BOOLEAN  | SYSTEM  | DEFAULT  | null        | null        | true      | null       |\r\n| planner.join.hash_join_swap_margin_factor  | DOUBLE   | SYSTEM  | DEFAULT  | null        | null        | null      | 10.0       |\r\n| planner.memory.hash_agg_table_factor       | DOUBLE   | SYSTEM  | DEFAULT  | null        | null        | null      | 1.1        |\r\n| planner.memory.hash_join_table_factor      | DOUBLE   | SYSTEM  | DEFAULT  | null        | null        | null      | 1.1        |\r\n+--------------------------------------------+----------+---------+----------+-------------+-------------+-----------+------------+\r\n9 rows selected (0.191 seconds)\r\n0: jdbc:drill:zk=10.10.100.171:5181> alter session set `planner.enable_hashjoin`=true;\r\n+-------+-----------------------------------+\r\n|  ok   |              summary              |\r\n+-------+-----------------------------------+\r\n| true  | planner.enable_hashjoin updated.  |\r\n+-------+-----------------------------------+\r\n1 row selected (0.083 seconds)\r\n0: jdbc:drill:zk=10.10.100.171:5181> select * from sys.options where name like '%hash%';\r\n+--------------------------------------------+----------+----------+----------+-------------+-------------+-----------+------------+\r\n|                    name                    |   kind   |   type   |  status  |   num_val   | string_val  | bool_val  | float_val  |\r\n+--------------------------------------------+----------+----------+----------+-------------+-------------+-----------+------------+\r\n| exec.max_hash_table_size                   | LONG     | SYSTEM   | DEFAULT  | 1073741824  | null        | null      | null       |\r\n| exec.min_hash_table_size                   | LONG     | SYSTEM   | DEFAULT  | 65536       | null        | null      | null       |\r\n| planner.enable_hash_single_key             | BOOLEAN  | SYSTEM   | DEFAULT  | null        | null        | true      | null       |\r\n| planner.enable_hashagg                     | BOOLEAN  | SYSTEM   | DEFAULT  | null        | null        | true      | null       |\r\n| planner.enable_hashjoin                    | BOOLEAN  | SYSTEM   | DEFAULT  | null        | null        | true      | null       |\r\n| planner.enable_hashjoin                    | BOOLEAN  | SESSION  | CHANGED  | null        | null        | true      | null       |\r\n| planner.enable_hashjoin_swap               | BOOLEAN  | SYSTEM   | DEFAULT  | null        | null        | true      | null       |\r\n| planner.join.hash_join_swap_margin_factor  | DOUBLE   | SYSTEM   | DEFAULT  | null        | null        | null      | 10.0       |\r\n| planner.memory.hash_agg_table_factor       | DOUBLE   | SYSTEM   | DEFAULT  | null        | null        | null      | 1.1        |\r\n| planner.memory.hash_join_table_factor      | DOUBLE   | SYSTEM   | DEFAULT  | null        | null        | null      | 1.1        |\r\n+--------------------------------------------+----------+----------+----------+-------------+-------------+-----------+------------+\r\n{code}",
        "Can not make files with extension \"log\" to be recognized as json format? If we want to use \".log\" as the file extension, and also want it to be recognized as json format, I tried to use below storage engine , but failed to read the .log file..\r\n\r\n\r\n{code}\r\n  \"formats\": {\r\n    \"log\": {\r\n      \"type\": \"json\"\r\n    },\r\n    \"csv\": {\r\n      \"type\": \"text\",\r\n      \"extensions\": [\r\n        \"csv\"\r\n      ],\r\n      \"delimiter\": \",\"\r\n    }\r\n  }\r\n{code} \r\n\r\n\r\n{code}\r\n0: jdbc:drill:zk=n1a:5181,n2a:5181,n3a:5181> select * from  logtest.`test.json`;\r\n+------------+------------+------------+------------+\r\n|   field1   |   field2   |   field3   |   field4   |\r\n+------------+------------+------------+------------+\r\n| data1      | 100.0      | more data1 | 123.001    |\r\n+------------+------------+------------+------------+\r\n1 row selected (0.159 seconds)\r\n0: jdbc:drill:zk=n1a:5181,n2a:5181,n3a:5181> select * from  logtest.`test.log`;\r\nQuery failed: Failure while validating sql : org.eigenbase.util.EigenbaseContextException: From line 1, column 16 to line 1, column 22: Table 'logtest.test.log' not found\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nDo we support above requirement?\r\nIf so, what is the storage plugin text?"
    ],
    [
        "DRILL-3758",
        "DRILL-4106",
        "InvalidRecordException while selecting from a table with multiple parquet file {code}\r\n0: jdbc:drill:drillbit=localhost> select * from test2;\r\nError: SYSTEM ERROR: InvalidRecordException: in not found in message root {\r\n  optional int64 id;\r\n  optional int64 gbyi;\r\n  optional binary gbyt (UTF8);\r\n  optional double fl;\r\n  optional binary nul (UTF8);\r\n  optional boolean bool;\r\n  optional binary str (UTF8);\r\n  repeated int64 sia;\r\n  repeated double sfa;\r\n  repeated group soa {\r\n    optional int64 in;\r\n    optional double fl;\r\n    optional binary nul (UTF8);\r\n    optional boolean bool;\r\n    optional binary str (UTF8);\r\n  }\r\n  repeated group ooa {\r\n    optional int64 in;\r\n    optional group fl {\r\n      optional double f1;\r\n      optional double f2;\r\n    }\r\n    optional group a {\r\n      optional group aa {\r\n        optional binary aaa (UTF8);\r\n      }\r\n    }\r\n    optional group b {\r\n      optional group bb {\r\n        optional binary bbb (UTF8);\r\n      }\r\n      optional group c {\r\n        optional binary cc (UTF8);\r\n      }\r\n    }\r\n  }\r\n  optional group oooi {\r\n    optional group oa {\r\n      optional group oab {\r\n        optional int64 oabc;\r\n      }\r\n    }\r\n  }\r\n  optional group ooof {\r\n    optional group oa {\r\n      optional group oab {\r\n        optional double oabc;\r\n      }\r\n    }\r\n  }\r\n  optional group ooos {\r\n    optional group oa {\r\n      optional group oab {\r\n        optional binary oabc (UTF8);\r\n      }\r\n    }\r\n  }\r\n  optional group oooa {\r\n    optional group oa {\r\n      optional group oab {\r\n        repeated group oabc {\r\n          optional int64 rowId;\r\n          optional int64 rowValue1;\r\n          optional int64 rowValue2;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n[Error Id: 494d2faf-ff59-4488-bfaf-26337f80c60e on atsqa4-133.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nAttached:\r\n* drillbit.log\r\n* test2.tar  (tar xzvf test2.tar to extract directory)",
        "Redundant Project on top of Scan in query plan Why doe we see two Projects after the Scan in the query plan ? \r\nTable is auto partitioned by column c1\r\n4 node cluster on CentOS, Drill 1.3, git.commit.id=a639c51c\r\n\r\n#CTAS statement is,\r\n\r\n{code}\r\nCREATE TABLE inNstedDirAutoPrtn PARTITION BY(c1) AS SELECT cast(columns[0] AS INT) c1, cast(columns[1] AS BIGINT) c2, cast(columns[2] AS CHAR(2)) c3, cast(columns[3] AS VARCHAR(54)) c4, cast(columns[4] AS TIMESTAMP) c5, cast(columns[5] AS DATE) c6, cast(columns[6] as BOOLEAN) c7, cast(columns[7] as DOUBLE) c8, cast(columns[8] as TIME) c9 FROM `nested_dirs/data/csv/allData.csv`;\r\n\r\nWhy do we see two Projects on top of Scan in query plan ? One of them looks redundant.\r\n\r\n0: jdbc:drill:schema=dfs.tmp> explain plan for select * from inNstedDirAutoPrtn where c1 IN (1,2,3,4,-1,0,100,-1710);\r\n+------+------+\r\n| text | json |\r\n+------+------+\r\n| 00-00    Screen\r\n00-01      Project(*=[$0])\r\n00-02        Project(*=[$0])\r\n00-03          Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=/tmp/inNstedDirAutoPrtn/0_0_48.parquet], ReadEntryWithPath [path=/tmp/inNstedDirAutoPrtn/0_0_31.parquet], ReadEntryWithPath [path=/tmp/inNstedDirAutoPrtn/0_0_50.parquet], ReadEntryWithPath [path=/tmp/inNstedDirAutoPrtn/0_0_47.parquet], ReadEntryWithPath [path=/tmp/inNstedDirAutoPrtn/0_0_49.parquet], ReadEntryWithPath [path=/tmp/inNstedDirAutoPrtn/0_0_46.parquet]], selectionRoot=maprfs:/tmp/inNstedDirAutoPrtn, numFiles=6, usedMetadataFile=false, columns=[`*`]]])\r\n\r\n{code}"
    ],
    [
        "DRILL-3833",
        "DRILL-410",
        "Concurrent Queries Failed Unexpectedly Concurrent queries with a relatively large LIMIT size *failed*, where the failure occurred randomly.\r\n\r\n\r\n\r\nTo reproduce:\r\n- Test data: a JSON file test.json (at least 10,000,000 records, two fields id<int>, title<string>);\r\n\r\n\r\n- Submit 5 queries with 5 separate threads using jdbc, where the query is:\r\n{panel}\r\nSELECT id, title FROM dfs.`test.json` LIMIT 10000000;\r\n{panel}\r\n\r\n\r\n- The error message in drillbit.log:\r\n{noformat}\r\n2015-09-24 19:15:15,393 [Client-1] INFO  o.a.d.j.i.DrillResultSetImpl$ResultsListener - [#4] Query failed: \r\norg.apache.drill.common.exceptions.UserRemoteException: SYSTEM ERROR: ChannelClosedException: Channel closed /192.168.4.201:31010 <--> /192.168.4.201:58795.\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 60a7baa8-a2ed-47e6-b7ca-68afd82c852a on TEST-101:31010]\r\n\tat org.apache.drill.exec.rpc.user.QueryResultHandler.resultArrived(QueryResultHandler.java:118) [drill-java-exec-1.1.0.jar:1.1.0]\r\n\tat org.apache.drill.exec.rpc.user.UserClient.handleReponse(UserClient.java:111) [drill-java-exec-1.1.0.jar:1.1.0]\r\n\tat org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:47) [drill-java-exec-1.1.0.jar:1.1.0]\r\n\tat org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:32) [drill-java-exec-1.1.0.jar:1.1.0]\r\n\tat org.apache.drill.exec.rpc.RpcBus.handle(RpcBus.java:61) [drill-java-exec-1.1.0.jar:1.1.0]\r\n\tat org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:233) [drill-java-exec-1.1.0.jar:1.1.0]\r\n\tat org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:205) [drill-java-exec-1.1.0.jar:1.1.0]\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89) [netty-codec-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254) [netty-handler-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242) [netty-codec-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [netty-transport-4.0.27.Final.jar:4.0.27.Final]\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) [netty-common-4.0.27.Final.jar:4.0.27.Final]\r\n\tat java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]\r\n{noformat}\r\n\r\n- Related to DRILL-3763. But the differences are, the queries were submitted through jdbc directly, and there was no external interruption in this case.",
        "Create/modify workspace without restarting drill bit(s) Considering users might need to create workspaces often we need a way to achieve that without restarting the drill bit which is the case now.\r\n"
    ],
    [
        "DRILL-795",
        "DRILL-2877",
        "Selecting all columns (*) from hbase/m7 tables returns incorrect column order git.commit.id.abbrev=5d7e3d3\r\n\r\n0: jdbc:drill:schema=hbase> select * from m7voter limit 5;\r\n+------------+------------+------------+------------+------------+\r\n|  row_key   |   fourcf   |   onecf    |  threecf   |   twocf    |\r\n+------------+------------+------------+------------+------------+\r\n| [B@35d87abc | {\"create_date\":\"MjAxNC0wOC0xNiAxMzoxMToxMg==\"} | {\"name\":\"bHVrZSBnYXJjaWE=\"} | {\"voterzone\":\"MTI0OTQ=\",\"contributions\":\"MTAuMDc=\"} | {\"age\":\"NDE=\",\"registration\":\"ZGVtb2NyYXQ=\"} |\r\n| [B@1761e842 | {\"create_date\":\"MjAxNC0wNC0wNCAxMDo1OToxMg==\"} | {\"name\":\"dWx5c3NlcyBoZXJuYW5kZXo=\"} | {\"voterzone\":\"NzUwNg==\",\"contributions\":\"NTMuMTk=\"} | {\"age\":\"MjA=\",\"registration\":\"aW5kZXBlbmRlbnQ=\"} |\r\n\r\nThe order should be:\r\nrow_key   onecf    twocf     threecf     fourcf",
        "\"mvn validate\" builds .tar file Running \"mvn validate\" doesn't execute just the normal validation checks (checkstyle-validation, rat-checks, etc.). \r\n\r\nIn the ./distribution module, it builds the .tar file and installs it, changing the state of the repository.\r\n"
    ],
    [
        "DRILL-3224",
        "DRILL-1706",
        "DrillTestWrapper test-failure message doesn't show actual values For at least one of the test-failure (validation) error messages from DrillTestWrapper, the message does not include the actual values.\r\n\r\nNote how the message below does tell you what was expected (redundant with what's in the test source code, but still convenient), but doesn't tell you what the test actually got (not available elsewhere (before/without debugging)).\r\n\r\n\r\n{noformat}\r\njava.lang.Exception: Did not find expected record in result set: `CHARACTER_MAXIMUM_LENGTH` : -1, `COLUMN_NAME` : inttype, `NUMERIC_SCALE` : -1, `DATA_TYPE` : INTEGER, `NUMERIC_PRECISION` : -1, \r\n\r\n\tat org.apache.drill.DrillTestWrapper.compareResults(DrillTestWrapper.java:541)\r\n\tat org.apache.drill.DrillTestWrapper.compareUnorderedResults(DrillTestWrapper.java:295)\r\n\tat org.apache.drill.DrillTestWrapper.run(DrillTestWrapper.java:119)\r\n\tat org.apache.drill.TestBuilder.go(TestBuilder.java:125)\r\n\tat org.apache.drill.exec.hive.TestInfoSchemaOnHiveStorage.varCharMaxLengthAndDecimalPrecisionInInfoSchema(TestInfoSchemaOnHiveStorage.java:94)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n{noformat}\r\n\r\n",
        "date_sub function does not accept string as input in Drill \"date_sub\" function does not accept string as input in Drill, however it does in Hive.\r\nThis different behavior of the function will make customer re-write their query to use \"cast as date\".\r\n\r\nMinimum reproduce :\r\n{code}\r\n0: jdbc:drill:zk=local> select date_sub('2014-11-12 16:45:22',15) from dfs.tmp.`drilltest/test.csv` ;\r\nQuery failed: Failure while running fragment., Invalid format: \"2014-11-12 16:45:22\" is malformed at \"14-11-12 16:45:22\" [ 9a6f18da-eb1e-4d91-879a-8d9d528efd59 on 10.250.0.115:31010 ]\r\n  (java.lang.IllegalArgumentException) Invalid format: \"2014-11-12 16:45:22\" is malformed at \"14-11-12 16:45:22\"\r\n    org.joda.time.format.DateTimeFormatter.parseDateTime():873\r\n    org.apache.drill.exec.test.generated.ProjectorGen23.doSetup():63\r\n    org.apache.drill.exec.test.generated.ProjectorGen23.setup():97\r\n    org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.setupNewSchema():427\r\n    org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.buildSchema():270\r\n    org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.buildSchema():80\r\n    org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.buildSchema():95\r\n    org.apache.drill.exec.work.fragment.FragmentExecutor.run():111\r\n    org.apache.drill.exec.work.WorkManager$RunnableWrapper.run():249\r\n    .......():0\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nComparing to Hive which is good.\r\n{code}\r\n0: jdbc:hive2://n1a:10000/default> select date_sub('2014-11-12 16:45:22',15) from passwords limit 1 ; \r\n+-------------+\r\n|     _c0     |\r\n+-------------+\r\n| 2014-10-28  |\r\n+-------------+\r\n1 row selected (6.568 seconds)\r\n{code}\r\n\r\nWorkaround in Drill:\r\n{code}\r\n0: jdbc:drill:zk=local> select date_sub(cast('2014-11-12 16:45:22' as date),15) from dfs.tmp.`drilltest/test.csv` ;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 2014-10-28 |\r\n+------------+\r\n1 row selected (0.082 seconds)\r\n{code}\r\n"
    ],
    [
        "DRILL-285",
        "DRILL-1080",
        "Some operators fail to process all records The test below shows that the Filter operator cannot process more than  more than 15625 records from the mock scanner.  The issue may affect multiple operators and scanners.\r\n\r\nh4. Steps to reproduce:\r\n# Edit {{filter/test1.json}} and increase the record count to some number above 15625 (e.g. 20k).\r\n# Update the assertion in TestSimpleFilter.testFilter().\r\n# Run TestSimpleFilter.testFilter().\r\n\r\nh4. Expected Results:\r\nThe test would return 1/2 the record count.\r\n\r\nh4. Results:\r\nFilterRecordBatch.doWork() gets a value of 15625 from incoming.getRecordCount().  Only one batch is processed, and never more than 15625 records.  The same holds true when the mock reader generates multiple batches.\r\n\r\n",
        "Handle out of space condition in Hash Join In some situtations, this is not properly handled, it causes index out of bounds errors downstream."
    ],
    [
        "DRILL-1358",
        "DRILL-729",
        "Apache release build failing with \"java.lang.OutOfMemoryError: PermGen space\" In preparation for 0.5 release, I tried running the build with Apache release profile {{mvn install -DskipTests -Papache-release}}, however the build failed with {{java.lang.OutOfMemoryError: PermGen space}} error while building {{jdbc-all}} project.",
        "Implement hash functions for DateTime and Decimal data types  "
    ],
    [
        "DRILL-2445",
        "DRILL-1791",
        "JDBC : Connection.rollback method currently throws UnsuportedOperationException Drill's JDBC throws \"UnsupportedOperationException\" when the user tries to call Connection.rollback() method.\r\n\r\nSince this method is used by analytics tools we should make sure it works.\r\n\r\nLet me know if you need anything else",
        "CTAS fails due to issue with stale file handles CTAS statement failed with RuntimeException and logs indicate it is due to invalid / stale file handle. \r\n\r\nPrevious CTAS statement was incomplete due to a different issue and had to be redone. The existing files that were created from the previous CTAS statement were deleted via the hadoop fs command. \r\n\r\nOn restarting CTAS, after about a few minutes of delay, this issue was seen. \r\n\r\nA snippet of the drillbit.log file has been attached (truncated due to it's large size)\r\n"
    ],
    [
        "DRILL-603",
        "DRILL-2404",
        "Query with single Count(Distinct) and Group-By hangs Following query hangs : \r\nselect count(distinct l_orderkey) from cp.`tpch/lineitem.parquet` group by l_partksy;\r\n\r\nNo errors are shown the log.  The last few entries in the log are as follows: \r\n18:31:11.850 [BitServer-1] DEBUG o.a.d.exec.work.foreman.QueryManager - New fragment status was provided to Foreman of memory_use: 1920000\r\nbatches_completed: 1\r\nrecords_completed: 11919\r\nstate: FINISHED\r\ndata_processed: 0\r\nhandle {\r\n  query_id {\r\n    part1: 6663191588966647760\r\n    part2: -7786394921952230089\r\n  }\r\n  major_fragment_id: 2\r\n  minor_fragment_id: 3\r\n}\r\nrunning_time: 219806000\r\n\r\n18:31:11.851 [WorkManager-2] DEBUG o.a.d.e.w.f.NonRootStatusReporter - Sending status change message message to remote node: memory_use: 0\r\nbatches_completed: 0\r\nrecords_completed: 0\r\nstate: RUNNING\r\ndata_processed: 0\r\nhandle {\r\n  query_id {\r\n    part1: 6663191588966647760\r\n    part2: -7786394921952230089\r\n  }\r\n  major_fragment_id: 1\r\n  minor_fragment_id: 3\r\n}\r\nrunning_time: 1398821471850886000\r\n\r\n18:31:11.852 [BitServer-1] DEBUG o.a.d.exec.work.foreman.QueryManager - New fragment status was provided to Foreman of memory_use: 0\r\nbatches_completed: 0\r\nrecords_completed: 0\r\nstate: RUNNING\r\ndata_processed: 0\r\nhandle {\r\n  query_id {\r\n    part1: 6663191588966647760\r\n    part2: -7786394921952230089\r\n  }\r\n  major_fragment_id: 1\r\n  minor_fragment_id: 3\r\n}\r\nrunning_time: 1398821471850886000",
        "After we cancel a query, DRILL sometimes hangs for the next query git.commit.id.abbrev=e92db23\r\n\r\nI tried running the below sequence of commands from sqlline :\r\n\r\n{code}\r\nselect count(*) from widestrings;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 10001      |\r\n+------------+\r\n{code}\r\n\r\n{code}\r\nselect * from widestrings;\r\n{code}\r\n\r\n{code}\r\nCancel the running query while it is printing the output\r\n{code}\r\n\r\n{code}\r\nselect count(*) from widestrings;\r\nThis causes SQLLINE to hang indefinitely.\r\n{code}\r\n\r\nWe should see this problem when we kill a query in sqlline while it is displaying the output. If needed I can provide the data that I used."
    ],
    [
        "DRILL-2935",
        "DRILL-1220",
        "Casting varchar to varbinary fails git.commit.id.abbrev=5fbd274\r\n\r\nThe below query fails :\r\n{code}\r\nselect concat(cast(cast('apache ' as varchar(7)) as varbinary(20)), 'drill') from `dummy.json`;\r\nQuery failed: PARSE ERROR: From line 1, column 15 to line 1, column 66: Cast function cannot convert value of type VARCHAR(7) to type VARBINARY(20)\r\n\r\n\r\n[4b5916d1-6b96-42a0-9afa-80706f2bd263 on qa-node191.qa.lab:31010]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nI attached the error log. ",
        "Process the 'star' columns appropriately in Project operator based on new plans For queries that involve  '*'  (star) in the SELECT list,  the Drill plan has to perform special handling of such columns since it must distinguish columns coming from both sides of a join.  Recent planner enhancements add prefixes at the appropriate intermediate Project nodes and remove it at the top level Project.  \r\n\r\nThe Project execution operator must be enhanced in order to correctly process such 'annotated' Project expressions that are handed by the plan.  This issue tracks this enhancement. "
    ],
    [
        "DRILL-1260",
        "DRILL-2693",
        "Embedded Type Vector Value vectors currently have a single type for all of the values they contain. This enforces that the schema within a single record batch be constant, thus we cannot support changing the type of a column without starting a new batch. In cases where the schema changes frequently this does not provide enough flexibility for storing and processing records. For this case we will eventually be extending out Value Vector model with a new Embedded Type vector, which will have type information associated with individual values within a batch rather than applied to all values.",
        "doc programmatically submit queries to Drill  Document how to start up an embedded Drillbit inside your\r\nan application. Use a complete JDBC driver (rather than\r\n the remote-only driver).\r\nThe standalone jdbc-all driver is designed to be a remote driver.  As such, you must have zookeeper and a separate Drill daemon up and running to leverage it.  If you want to embed a Drillbit inside your application, you'll need to source the same classpath that the drillbit.sh start-up script sources.\r\n\r\nIf you just try to use the jdbc jar that gets shipped, you get this exception:\r\n\r\n java.sql.SQLException: Running Drill in embedded mode using the JDBC jar\r\nalone is not supported."
    ],
    [
        "DRILL-1715",
        "DRILL-3648",
        "NPE in UnlimitedRawBatchBuffer while releasing the batch In UnlimitedRawBatchBuffer we are currently not checking if a batch's body is empty before attempting to release it. This can happen if its a schema batch which would cause NPE.",
        "NTILE function returns incorrect results NTILE function returns incorrect results for larger dataset. I am working on reproducing the problem with a smaller dataset.\r\n\r\nThe inner query that uses NTILE should have divided the rows into two sets (tiles)  where each tile consists of (937088 + 1 ) rows , 937088 rows\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select ntile_key2, count(ntile_key2) from (select ntile(2) over(partition by key2 order by key1) ntile_key2 from `twoKeyJsn.json` where key2 = 'm') group by ntile_key2;\r\n+-------------+----------+\r\n| ntile_key2  |  EXPR$1  |\r\n+-------------+----------+\r\n| 1           | 1        |\r\n| 2           | 1874176  |\r\n+-------------+----------+\r\n2 rows selected (49.406 seconds)\r\n{code}\r\n\r\nExplain plan for  inner query that returns wrong results.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> explain plan for select ntile(2) over(partition by key2 order by key1) from `twoKeyJsn.json` where key2 = 'm';\r\n+------+------+\r\n| text | json |\r\n+------+------+\r\n| 00-00    Screen\r\n00-01      UnionExchange\r\n01-01        Project(EXPR$0=[$0])\r\n01-02          Project($0=[$2])\r\n01-03            Window(window#0=[window(partition {0} order by [1] range between UNBOUNDED PRECEDING and CURRENT ROW aggs [NTILE($2)])])\r\n01-04              SelectionVectorRemover\r\n01-05                Sort(sort0=[$0], sort1=[$1], dir0=[ASC], dir1=[ASC])\r\n01-06                  Project(key2=[$0], key1=[$1])\r\n01-07                    HashToRandomExchange(dist0=[[$0]])\r\n02-01                      UnorderedMuxExchange\r\n03-01                        Project(key2=[$0], key1=[$1], E_X_P_R_H_A_S_H_F_I_E_L_D=[castInt(hash64AsDouble($0))])\r\n03-02                          SelectionVectorRemover\r\n03-03                            Filter(condition=[=($0, 'm')])\r\n03-04                              Scan(groupscan=[EasyGroupScan [selectionRoot=maprfs:/tmp/twoKeyJsn.json, numFiles=1, columns=[`key2`, `key1`], files=[maprfs:///tmp/twoKeyJsn.json]]])\r\n{code}\r\n\r\nTotal number of rows in partition that has key2 = 'm'\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select count(key1) from `twoKeyJsn.json` where key2 = 'm';\r\n+----------+\r\n|  EXPR$0  |\r\n+----------+\r\n| 1874177  |\r\n+----------+\r\n1 row selected (37.581 seconds)\r\n{code}"
    ],
    [
        "DRILL-1188",
        "DRILL-1945",
        "AssertionError while running TPC-H query against hive datasource git.commit.id.abbrev=e5c2da0\r\ngit.commit.id=e5c2da0eb95df1a93d4ad909d5553757201dc903\r\n\r\n0: jdbc:drill:schema=dfs.TpcHMulti> use hive;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | Default schema changed to 'hive' |\r\n+------------+------------+\r\n1 row selected (0.325 seconds)\r\n0: jdbc:drill:schema=dfs.TpcHMulti> select\r\n. . . . . . . . . . . . . . . . . >   100.00 * sum(case\r\n. . . . . . . . . . . . . . . . . >     when p.p_type like 'PROMO%'\r\n. . . . . . . . . . . . . . . . . >       then l.l_extendedprice * (1 - l.l_discount)\r\n. . . . . . . . . . . . . . . . . >     else 0\r\n. . . . . . . . . . . . . . . . . >   end) / sum(l.l_extendedprice * (1 - l.l_discount)) as promo_revenue\r\n. . . . . . . . . . . . . . . . . > from\r\n. . . . . . . . . . . . . . . . . >   lineitem l,\r\n. . . . . . . . . . . . . . . . . >   part p\r\n. . . . . . . . . . . . . . . . . > where\r\n. . . . . . . . . . . . . . . . . >   l.l_partkey = p.p_partkey\r\n. . . . . . . . . . . . . . . . . >   and l.l_shipdate >= date '1994-08-01'\r\n. . . . . . . . . . . . . . . . . >   and l.l_shipdate < date '1994-08-01' + interval '1' month;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"7aa9bffb-38a5-41aa-9e84-1fde3a2e7735\"\r\nendpoint {\r\n  address: \"perfnode104.perf.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < AssertionError\"\r\n]\r\nError: exception while executing query (state=,code=0)",
        "max() of all 0.0 value gives wrong results #Fri Jan 02 21:20:47 EST 2015\r\ngit.commit.id.abbrev=b491cdb\r\n\r\ntest data can be accessed from  https://s3.amazonaws.com/apache-drill/files/complex.json.gz\r\n\r\nUsing the above dataset, the t.sfa[0] field is all 0.0.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirMondrian> select t.sfa[0] from `complex.json` t limit 10;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 0.0        |\r\n| 0.0        |\r\n| 0.0        |\r\n| 0.0        |\r\n| 0.0        |\r\n| 0.0        |\r\n| 0.0        |\r\n| 0.0        |\r\n| 0.0        |\r\n| 0.0        |\r\n+------------+\r\n10 rows selected (0.083 seconds)\r\n0: jdbc:drill:schema=dfs.drillTestDirMondrian> select t.sfa[0] from `complex.json` t where t.sfa[0] <> 0.0;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n+------------+\r\nNo rows selected (15.616 seconds)\r\n{code}\r\n\r\nThe max value of this field in a group by query gives wrong results.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirMondrian> select mod(trunc(t.sfa[1]), 10) sfamod, max(t.sfa[0]) sfamax from `complex.json` t group by mod(trunc(t.sfa[1]), 10);\r\n+------------+------------+\r\n|   sfamod   |   sfamax   |\r\n+------------+------------+\r\n| 1.0        | 4.9E-324   |\r\n| 2.0        | 4.9E-324   |\r\n| 3.0        | 4.9E-324   |\r\n| 4.0        | 4.9E-324   |\r\n| 5.0        | 4.9E-324   |\r\n| 6.0        | 4.9E-324   |\r\n| 7.0        | 4.9E-324   |\r\n| 8.0        | 4.9E-324   |\r\n| 9.0        | 4.9E-324   |\r\n| 0.0        | 4.9E-324   |\r\n+------------+------------+\r\n10 rows selected (14.474 seconds)\r\n{code}\r\n\r\nsimple max() on the field also gives the same wrong result.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirMondrian> select max(t.sfa[0]) sfamax from `complex.json` t;\r\n+------------+\r\n|   sfamax   |\r\n+------------+\r\n| 4.9E-324   |\r\n+------------+\r\n1 row selected (15.666 seconds)\r\n{code}"
    ],
    [
        "DRILL-3174",
        "DRILL-3955",
        "Calcite blocks queries whose type-missmatch can be resolved by Drill's Implicit casting {code}\r\nselect a from ... union all select b from ...\r\n{code}\r\n\r\nwhere a is int, and b is a bunch of integers in varchar types. Drill-Calcite interrupts this query by the column types. Since Drill has its own ways of handling type-mismatch, can we let Calcite ignore type checking?\r\n\r\n",
        "Possible bug in creation of Drill columns for HBase column families If all of the rows read by a given {{HBaseRecordReader}} have no HBase columns in a given HBase column family, {{HBaseRecordReader}} doesn't create a Drill column for that HBase column family.\r\n\r\nLater, in a {{ProjectRecordBatch}}'s {{setupNewSchema}}, because no Drill column exists for that HBase column family, that {{setupNewSchema}} creates a dummy Drill column using the usual {{NullableIntVector}} type.  In particular, it is not a map vector as {{HBaseRecordReader}} creates when it sees an HBase column family.\r\n\r\nShould {{HBaseRecordReader}} and/or something around setting up for reading HBase (including setting up that {{ProjectRecordBatch}}) make sure that all HBase column families are represented with map vectors so that {{setupNewSchema}} doesn't create a dummy field of type {{NullableIntVector}}?\r\n\r\n\r\nThe problem is that, currently, when an HBase table is read in two separate fragments, one fragment (seeing rows with columns in the column family) can get a map vector for the column family while the other (seeing only rows with no columns in the column familar) can get the {{NullableIntVector}}.  Downstream code that receives the two batches ends up with an unresolved conflict, yielding IndexOutOfBoundsExceptions as in DRILL-3954.\r\n\r\nIt's not clear whether there is only one bug\\--that downstream code doesn't resolve {{NullableIntValue}} dummy fields right (DRILL-TBD)\\--or two\\--that the HBase reading code should set up a Drill column for every HBase column family (regardless of whether it has any columns in the rows that were read) and that downstream code doesn't resolve {{NullableIntValue}} dummy fields (resolution is applicable to sources other than just HBase).\r\n\r\n\r\n\r\n\r\n"
    ],
    [
        "DRILL-2667",
        "DRILL-4216",
        "Document Date/Time functions plus misc fixes, including Bridget's fix of sandbox broken link ",
        "Aggregate Window Function COUNT() With GROUP BY Clause expected: range(0, 32768) *When column is row_key,it work well !*\r\n0: jdbc:drill:> select count(row_key) over() from hbase.web_initial_20151222 wi group by row_key limit 3;\r\n+---------+\r\n| EXPR$0  |\r\n+---------+\r\n| 102906  |\r\n| 102906  |\r\n| 102906  |\r\n+---------+\r\n3 rows selected (1.645 seconds)\r\n\r\n*When column is Hbase.Talbename.ColumnFamily.Qualify, and count(column) less than 32768,it work well !*\r\n0: jdbc:drill:> select count(wi.cf1.q5) over() from hbase.web_initial_20151214 wi group by wi.cf1.q5 limit 3;\r\n+---------+\r\n| EXPR$0  |\r\n+---------+\r\n| 10383   |\r\n| 10383   |\r\n| 10383   |\r\n+---------+\r\n3 rows selected (1.044 seconds)\r\n\r\n{color:red}\r\n    When column is Hbase.Talbename.ColumnFamily.Qualify, and count(column) more than 32768,IndexOutOfBoundsException\r\n{color}\r\n\r\n0: jdbc:drill:> select count(wi.cf1.q5) over() from hbase.web_initial_20151222 wi group by wi.cf1.q5 limit 3;\r\nError: SYSTEM ERROR: IndexOutOfBoundsException: index: 0, length: 62784 (expected: range(0, 32768))\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 77406a8a-8389-4f1b-af6c-d26d811379b7 on slave4.hadoop:31010] (state=,code=0)\r\njava.sql.SQLException: SYSTEM ERROR: IndexOutOfBoundsException: index: 0, length: 62784 (expected: range(0, 32768))\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 77406a8a-8389-4f1b-af6c-d26d811379b7 on slave4.hadoop:31010]\r\n\tat org.apache.drill.jdbc.impl.DrillCursor.nextRowInternally(DrillCursor.java:247)\r\n\tat org.apache.drill.jdbc.impl.DrillCursor.next(DrillCursor.java:320)\r\n\tat net.hydromatic.avatica.AvaticaResultSet.next(AvaticaResultSet.java:187)\r\n\tat org.apache.drill.jdbc.impl.DrillResultSetImpl.next(DrillResultSetImpl.java:160)\r\n\tat sqlline.IncrementalRows.hasNext(IncrementalRows.java:62)\r\n\tat sqlline.TableOutputFormat$ResizingRowsProvider.next(TableOutputFormat.java:87)\r\n\tat sqlline.TableOutputFormat.print(TableOutputFormat.java:118)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1593)\r\n\tat sqlline.Commands.execute(Commands.java:852)\r\n\tat sqlline.Commands.sql(Commands.java:751)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:746)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:621)\r\n\tat sqlline.SqlLine.start(SqlLine.java:375)\r\n\tat sqlline.SqlLine.main(SqlLine.java:268)\r\nCaused by: org.apache.drill.common.exceptions.UserRemoteException: SYSTEM ERROR: IndexOutOfBoundsException: index: 0, length: 62784 (expected: range(0, 32768))\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 77406a8a-8389-4f1b-af6c-d26d811379b7 on slave4.hadoop:31010]\r\n\tat org.apache.drill.exec.rpc.user.QueryResultHandler.resultArrived(QueryResultHandler.java:118)\r\n\tat org.apache.drill.exec.rpc.user.UserClient.handleReponse(UserClient.java:112)\r\n\tat org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:47)\r\n\tat org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:32)\r\n\tat org.apache.drill.exec.rpc.RpcBus.handle(RpcBus.java:69)\r\n\tat org.apache.drill.exec.rpc.RpcBus$RequestEvent.run(RpcBus.java:400)\r\n\tat org.apache.drill.common.SerializedExecutor$RunnableProcessor.run(SerializedExecutor.java:105)\r\n\tat org.apache.drill.exec.rpc.RpcBus$SameExecutor.execute(RpcBus.java:264)\r\n\tat org.apache.drill.common.SerializedExecutor.execute(SerializedExecutor.java:142)\r\n\tat org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:298)\r\n\tat org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:269)\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n\tat io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)\r\n\tat io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:618)\r\n\tat io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:329)\r\n\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:250)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n"
    ],
    [
        "DRILL-2084",
        "DRILL-277",
        "Order by on date, time, timestamp and boolean columns produces wrong results if column contains null values Nulls are neither sorted at the end or beginning of the table, they appear in the middle.\r\nI created table using parquet format from t1.csv file.\r\n\r\nHere is create table statement I used to create table that is used in the queries:\r\n{code}\r\ncreate table t1(c_varchar, c_integer, c_bigint, c_smalldecimal, c_bigdecimal, c_float, c_date, c_time, c_timestamp, c_boolean) as\r\nselect\r\n        case when columns[0] = '' then cast(null as varchar(255)) else cast(columns[0] as varchar(255)) end,\r\n        case when columns[1] = '' then cast(null as integer) else cast(columns[1] as integer) end,\r\n        case when columns[2] = '' then cast(null as bigint) else cast(columns[2] as bigint) end,\r\n        case when columns[3] = '' then cast(null as decimal(18,4)) else cast(columns[3] as decimal(18, 4)) end,\r\n        case when columns[4] = '' then cast(null as decimal(38,4)) else cast(columns[4] as decimal(38, 4)) end,\r\n        case when columns[5] = '' then cast(null as float) else cast(columns[5] as float) end,\r\n        case when columns[6] = '' then cast(null as date) else cast(columns[6] as date) end,\r\n        case when columns[7] = '' then cast(null as time) else cast(columns[7] as time) end,\r\n        case when columns[8] = '' then cast(null as timestamp) else cast(columns[8] as timestamp) end,\r\n        case when columns[9] = '' then cast(null as boolean) else cast(columns[9] as boolean) end\r\nfrom `t1.csv`;\r\n{code}\r\n\r\nFailing queries are:\r\n{code}\r\nselect c_date from t1 order by c_date;\r\nselect c_time from t1 order by c_time;\r\nselect c_timestamp from t1 order by c_timestamp;\r\nselect c_boolean from t1 order by c_boolean;\r\n{code}\r\n\r\nIf you add 'IS NULL' or 'IS NOT NULL' predicates to the queries above, you will get correct results.\r\nI will attach all the relevant files to this bug.\r\n",
        "Plus operator with float fails The following query fails\r\nselect cast(_MAP['R_REGIONKEY'] as int) + 1.2 from \"/Users/mbaid/incubator-drill/sample-data/region.parquet\";\r\n\r\nError: \"Screen received stop request sent. < SchemaChangeException:[ Failure while trying to materialize incoming schema.  Errors:\\n \\nError in expression at index 1.  Error: Unexpected argument type. Actual type: minor_type: FLOAT8\\nmode: REQUIRED\\n, Index: 1.  Full expression: null.. ]\""
    ],
    [
        "DRILL-3421",
        "DRILL-2453",
        "Add new outputformat=json ouputformat=vertical is nearly a json output format. Adding a json format option here, where the field names are quoted etc.. would make it dead simple to generate json output from query results. ",
        "hbase queries in certain env result in NPE at FragmentWritableBatch.getEmptyBatchWithSchema() Sounds similar to Drill-1932\r\nBut seems to be from a different place.\r\n\r\nStacktrace:\r\n{code}\r\njava.lang.NullPointerException: null\r\n        at org.apache.drill.exec.record.FragmentWritableBatch.getEmptyBatchWithSchema(FragmentWritableBatch.java:86) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.partitionsender.PartitionSenderRootExec.sendEmptyBatch(PartitionSenderRootExec.java:276) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.j\r\nar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.partitionsender.PartitionSenderRootExec.innerNext(PartitionSenderRootExec.java:133) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.\r\n8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:57) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:121) ~[drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.WorkManager$RunnableWrapper.run(WorkManager.java:303) [drill-java-exec-0.8.0-SNAPSHOT-rebuffed.jar:0.8.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_51]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]\r\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]\r\n2015-03-13 13:41:50,740 [2afcb471-b3a1-f719-a4ce-bbd75c36637a:frag:2:2] ERROR o.a.drill.exec.ops.FragmentContext - Fragment Context received failure.\r\n{code}"
    ],
    [
        "DRILL-999",
        "DRILL-1728",
        "TPCH 9 returns less rows than expected for SF 0.01 TPCH 9 on drill returns 63 rows in drill vs 175 rows for postgres.\r\n\r\n/root/drillAutomation/testing/framework/resources/tpch-complete/testcases/09.sql : -- tpch9 using 1395599672 as a seed to the RNG\r\nselect\r\n  nation,\r\n  o_year,\r\n  sum(amount) as sum_profit\r\nfrom\r\n  (\r\n    select\r\n      n.n_name as nation,\r\n      extract(year from o.o_orderdate) as o_year,\r\n      l.l_extendedprice * (1 - l.l_discount) - ps.ps_supplycost * l.l_quantity as amount\r\n    from\r\n      part p,\r\n      supplier s,\r\n      lineitem l,\r\n      partsupp ps,\r\n      orders o,\r\n      nation n\r\n    where\r\n      s.s_suppkey = l.l_suppkey\r\n      and ps.ps_suppkey = l.l_suppkey\r\n      and ps.ps_partkey = l.l_partkey\r\n      and p.p_partkey = l.l_partkey\r\n      and o.o_orderkey = l.l_orderkey\r\n      and s.s_nationkey = n.n_nationkey\r\n      and p.p_name like '%yellow%'\r\n  ) as profit\r\ngroup by\r\n  nation,\r\n  o_year\r\norder by\r\n  nation,\r\n  o_year desc\r\n\r\n",
        "Better error messages on Drill JSON read error {code}\r\n0: jdbc:drill:zk=localhost:2181> SELECT * FROM dfs.root.`Users/tshiran/Development/demo/data/yelp/business.json` WHERE true and REPEATED_CONTAINS(categories, 'Australian');\r\n+-------------+--------------+------------+------------+------------+------------+--------------+------------+------------+------------+------------+------------+------------+------------+---------------+\r\n| business_id | full_address |   hours    |    open    | categories |    city    | review_count |    name    | longitude  |   state    |   stars    |  latitude  | attributes |    type    | neighborhoods |\r\n+-------------+--------------+------------+------------+------------+------------+--------------+------------+------------+------------+------------+------------+------------+------------+---------------+\r\nQuery failed: Query stopeed., You tried to start when you are using a ValueWriter of type NullableBitWriterImpl. [ e5bafa1e-6226-443d-80fd-51e18f330899 on 172.17.3.132:31010 ]\r\n\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n{code}"
    ],
    [
        "DRILL-915",
        "DRILL-442",
        "Extracting year from a time literal should fail When we try to extract year, month or day from time, drill seems to be returning 1970, 1, 1 respectively (1970-01-01 is a valid date).  When we try to extract hour, minute, or second from any date literal, drill gives 0's. \r\n\r\nIn both cases we should fail the query with a proper message. Postgres fails the query in both the cases.\r\n\r\n\r\nselect extract (day from time '02:22:12') from basic;\r\nselect extract (hour from date '2000-04-21') from basic;",
        "Implement text format plugin Implement a format plugin to read text files, e.g. csv, tsv. This will be able to read compressed and uncompressed data.\r\n\r\nThe record reader will return a single column of type RepeatedVarChar. Compression and delimiter type will be determined from the file extensions."
    ],
    [
        "DRILL-2601",
        "DRILL-1618",
        "Print SQL query text along with query id in drillbit.log This is a request to print text of a query into drillbit.log  in the default non verbose output. It includes all the changes of a session level parameters and anything else that might help reproducing issue on a customer site.",
        "Drill allows creation of tables/view with backslash even though its not queryable I can create a view with a backslash in the name and it shows up in show files as well.\r\n{code}\r\n0: jdbc:drill:zk=localhost:5181> create view `dfs.tests`.`test\\_backslash` as select * from `hive43.default`.`emp`;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | View 'test\\_backslash' created successfully in 'dfs.tests' schema |\r\n+------------+------------+\r\n1 row selected (0.296 seconds)\r\n{code}\r\n\r\nBut when I try and query it, it doesn't work and the backslash seems to be removed:\r\n{code}\r\n0: jdbc:drill:zk=localhost:5181> select * from `dfs.tests`.`test\\_backslash`;\r\nQuery failed: Failure while parsing sql. Table 'dfs.tests.test_backslash' not found [02011f5b-6a33-440b-aae1-c72be2762d2c]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n{code}\r\n\r\nI would expect Drill to disallow the creation of such tables/views."
    ],
    [
        "DRILL-3199",
        "DRILL-22",
        "GenericAccessor doesn't support isNull GenericAccessor throws an UnsupportedOperationException when isNull() is called. However for other methods it delegates to its ValueVector's accessor. I think it should do the same for isNull().\r\n\r\n",
        "Aggregation of entire stream should be possible by specifying empty list of keys The aggregate operator should support an empty list of keys to allow aggregation across the entire input something like this:\r\n{code}\r\n      { op: \"aggregate\", type: \"simple\", keys: [], \r\n        aggregations: [ref: \"average\", expr: \"AVG(y)\"]}\r\n{code}"
    ],
    [
        "DRILL-2414",
        "DRILL-821",
        "Union-All on SELECT * FROM schema-less data source will throw exception Union-All on SELECT * (wildcard symbol) is supported only for the cases where schema (i.e., hive, view) is available. \r\nFor detailed design documentation, please refer to DRILL-2207.",
        "verification failure for hash join 2014-05-22 16:04:18 INFO  QuerySubmitter:93 - Submitting query:\r\nalter session set `planner.enable_mergejoin` = false\r\n2014-05-22 16:04:18 INFO  QuerySubmitter:93 - Submitting query:\r\nselect o.O_TOTALPRICE\r\n    from customer c, orders o\r\n    where c.C_CUSTKEY = o.O_CUSTKEY\r\n2014-05-22 16:04:18 INFO  QuerySubmitter:93 - Submitting query:\r\nalter session set `planner.enable_mergejoin` = true\r\n2014-05-22 16:04:18 INFO  DrillTestBase:212 - Query submit end time: 2014/05/22 16:04:18.0018\r\n2014-05-22 16:04:18 INFO  DrillTestBase:213 - The execution time for the query: 0 seconds.\r\n2014-05-22 16:04:19 INFO  TestVerifier:203 - These rows are not expected:\r\n2014-05-22 16:04:19 INFO  TestVerifier:206 -    93645.3 : 1 time(s).\r\n2014-05-22 16:04:19 INFO  TestVerifier:206 -    79836.5 : 1 time(s).\r\n2014-05-22 16:04:19 INFO  TestVerifier:206 -    161850.81 : 1 time(s).\r\n2014-05-22 16:04:19 INFO  TestVerifier:206 -    115961.79 : 1 time(s).\r\n2014-05-22 16:04:19 INFO  TestVerifier:206 -    15203.11 : 1 time(s).\r\n2014-05-22 16:04:19 INFO  TestVerifier:206 -    161112.03 : 1 time(s).\r\n2014-05-22 16:04:19 INFO  TestVerifier:206 -    4736.16 : 1 time(s).\r\n2014-05-22 16:04:19 INFO  TestVerifier:206 -    148993.75 : 1 time(s).\r\n2014-05-22 16:04:19 INFO  TestVerifier:206 -    138691.03 : 1 time(s).\r\n2014-05-22 16:04:19 INFO  TestVerifier:206 -    315724.81 : 1 time(s).\r\n2014-05-22 16:04:19 INFO  TestVerifier:213 - Total number of unexpected rows: 14996\r\n"
    ],
    [
        "DRILL-1533",
        "DRILL-759",
        "C++ Drill Client always sets hasSchemaChanged to true for every new record batch hasSchemaChanged is always set as true for all record batches except the first one regardless of whether the schema has changed or not, including cases where specific columns are projected, which should never happen.",
        "Drill does not support date + interval Right now drill only support date addition/subtraction via date_add function.  It should also support date + interval x as this method is used by other databases such as postgres and oracle."
    ],
    [
        "DRILL-76",
        "DRILL-374",
        "Parquet did not have a repository specified in the project POM file The POM file for the project included a dependency for parquet, but did not specify a repository for it. ",
        "Add configuration parameter for HazelCast subnets On machines with multiple network interfaces, it's possible that the the hazelcast instances will select different subnets, and will not communicate with each other.\r\n\r\nWe should allow configuring hazelcast to use a particular subnet to avoid this problem"
    ],
    [
        "DRILL-2228",
        "DRILL-1558",
        "Projecting '*' returns all nulls when we have flatten in a filter and order by git.commit.id.abbrev=3d863b5\r\n\r\nThe below query returns all nulls currently :\r\n{code}\r\n0: jdbc:drill:schema=dfs_eea> select * from `data.json` where 2 in (select flatten(lst_lst[1]) from `data.json`) order by flatten(lst_lst[1]);\r\n+------------+\r\n|     *      |\r\n+------------+\r\n| null       |\r\n| null       |\r\n| null       |\r\n| null       |\r\n| null       |\r\n| null       |\r\n| null       |\r\n| null       |\r\n| null       |\r\n| null       |\r\n+------------+\r\n{code}\r\n\r\nThere seems to be another issue here since the no of records returned also does not look right. I will raise a separate JIRA for that.\r\n\r\n\r\nThe issue goes away, if we do an order by without the flatten. Below query works\r\n{code}\r\nselect * from `data.json` where 2 in (select flatten(lst_lst[1]) from `data.json`) order by uid;\r\n{code}\r\n\r\nAttached the data files",
        "Count(<NestedDataInColumn>) does not work on JSON (and JSON-converted-Parquet) Works:\r\n> select count(`type`) from yelp_academic_dataset_review;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 1125458    |\r\n+------------+\r\n1 row selected (0.065 seconds)\r\n\r\nFails:\r\n\r\n> select count(`votes`) from yelp_academic_dataset_review;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 0          |\r\n+------------+\r\n\r\n> select count(`votes`) from `yelp_academic_dataset_review.json`;\r\nQuery failed: Screen received stop request sent. Failure while materializing expression. \r\nError in expression at index -1.  Error: Missing function implementation: [count(MAP-REQUIRED)].  Full expression: --UNKNOWN EXPRESSION. [e4cdd655-cc87-4803-9b25-555779161ec6]\r\n\r\nError: exception while executing query: Failure while trying to get next result batch. (state=,code=0)\r\n0: jdbc:drill:zk=10.10.103.34:5181> select count(`text`) from `yelp_academic_dataset_review.json`;\r\n\r\n\r\nSample data:\r\n+------------+------------+------------+------------+------------+------------+------------+-------------+\r\n|   votes    |  user_id   | review_id  |   stars    |    date    |    text    |    type    | business_id |\r\n+------------+------------+------------+------------+------------+------------+------------+-------------+\r\n| {\"funny\":0,\"useful\":0,\"cool\":0} | 3MSa_fdxgsaY9yF9vqmeUg | ag1fnnEmc2yernTW2ur2eg | 5          | 2010-09-07 | Great food and service | review     | KPoTixdjoJxSqRSEApSAGg |\r\n| {\"funny\":0,\"useful\":0,\"cool\":0} | f3LA83yEEBMj9q92H28O7w | LNW2d7TDLB7XHT-V8k-6Jg | 4          | 2010-09-08 | I haven't been in a while but I remember being impressed.  Nice amb |\r\n| {\"funny\":0,\"useful\":0,\"cool\":0} | SaC2fZjZZfWV-8wYb6o6fw | 449K7FhWs5Hn1xB1Qv8KPw | 4          | 2010-09-18 | Finally got to eat at this place after driving by so many times.  I |\r\n| {\"funny\":0,\"useful\":0,\"cool\":0} | Sg9eHImUp1DMYhY7gA2img | yjEOpYaOnahtzCHBzY6KUg | 3          | 2010-09-27 | It was fine.  A fine thai meal to have before a rock show downtown. |\r\n"
    ],
    [
        "DRILL-3764",
        "DRILL-2390",
        "Support the ability to identify and/or skip records when a function evaluation fails Drill can point out the filename and location of corrupted records in a file but it does not have a good mechanism to deal with the following scenario: \r\n\r\nConsider a text file with 2 records:\r\n{code}\r\n$ cat t4.csv\r\n10,2001\r\n11,http://www.cnn.com\r\n{code}\r\n\r\n{code}\r\n0: jdbc:drill:zk=local> alter session set `exec.errors.verbose` = true;\r\n\r\n0: jdbc:drill:zk=local> select cast(columns[0] as init), cast(columns[1] as bigint) from dfs.`t4.csv`;\r\n\r\nError: SYSTEM ERROR: NumberFormatException: http://www.cnn.com\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 72aad22c-a345-4100-9a57-dcd8436105f7 on 10.250.56.140:31010]\r\n\r\n  (java.lang.NumberFormatException) http://www.cnn.com\r\n    org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.nfeL():91\r\n    org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.varCharToLong():62\r\n    org.apache.drill.exec.test.generated.ProjectorGen1.doEval():62\r\n    org.apache.drill.exec.test.generated.ProjectorGen1.projectRecords():62\r\n    org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.doWork():172\r\n{code}\r\n\r\nThe problem is user does not have the context of where the error occurred -either the file name or the record number.   This becomes a pain point especially when CTAS is being used to do data conversion from (say) text format to Parquet format.  The CTAS may be accessing thousands of files and 1 such casting (or another function) failure aborts the query. \r\n\r\nIt would substantially improve the user experience if we provided: \r\n1) the filename and record number where  this failure occurred\r\n2) the ability to skip such records depending on a session option\r\n3) the ability to write such records to a staging table for future ingestion\r\n\r\nPlease see discussion on dev list: \r\nhttp://mail-archives.apache.org/mod_mbox/drill-dev/201509.mbox/%3cCAFyDVvLuPLgTNZ56S6=J=9Vb=aBs=pDw7NRHKkdUPbdxGFAdcg@mail.gmail.com%3e",
        "regression: MergeJoinBatch size exceeds 64k limit #Wed Mar 04 01:23:42 EST 2015\r\ngit.commit.id.abbrev=71b6bfe\r\n\r\nEnable merge join, following query hits the batch size limit issue.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> alter session set `planner.enable_mergejoin` = true;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | planner.enable_mergejoin updated. |\r\n+------------+------------+\r\n1 row selected (0.03 seconds)\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> alter session set `planner.enable_hashjoin` = false;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| true       | planner.enable_hashjoin updated. |\r\n+------------+------------+\r\n1 row selected (0.025 seconds)\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> select count(a.id) from `complex100k.json` a inner join `complex100k.json` b on a.gbyi=b.gbyi;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\nQuery failed: RemoteRpcException: Failure while running fragment., Incoming batch of org.apache.drill.exec.physical.impl.join.MergeJoinBatch has size 3276800, which is beyond the limit of 65536 [ e26545df-a8c3-4cd6-b02a-1872db4ac41f on qa-node120.qa.lab:31010 ]\r\n[ e26545df-a8c3-4cd6-b02a-1872db4ac41f on qa-node120.qa.lab:31010 ]\r\n\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n{code}\r\n\r\nThis worked before."
    ],
    [
        "DRILL-1626",
        "DRILL-4072",
        "sqlline echoes statements with unnecessary wrapping I type a command in sqlline, and when it is echoed, it is unnecessarily wrapped (making it hard to read, as well as casting uncertainty on what I was doing). Example:\r\n\r\n0: jdbc:drill:zk=local> create table donuts_parquet as select * from `donuts.json`;\r\ncreate table donuts_parquet as select * from `donuts.jso \r\nn`;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 5                         |\r\n\r\nThat's sqlline adding a newline after jso and before n in the echoed command.",
        "Hive partition pruning not working with avro serde's git.commit.id.abbrev=e78e286\r\n\r\nThe below plan indicates that partition pruning is not happening\r\n\r\n{code}\r\nexplain plan for select * from hive.episodes_partitioned where doctor > 4;\r\n+------+------+\r\n| text | json |\r\n+------+------+\r\n| 00-00    Screen\r\n00-01      Project(title=[$0], air_date=[$1], doctor=[$2], doctor_pt=[$3])\r\n00-02        Project(title=[$0], air_date=[$1], doctor=[$2], doctor_pt=[$3])\r\n00-03          SelectionVectorRemover\r\n00-04            Filter(condition=[>($2, 4)])\r\n00-05              Scan(groupscan=[HiveScan [table=Table(dbName:default, tableName:episodes_partitioned), inputSplits=[maprfs:///user/hive/warehouse/episodes_partitioned/doctor_pt=1/000000_0:0+367, maprfs:///user/hive/warehouse/episodes_partitioned/doctor_pt=11/000000_0:0+393, maprfs:///user/hive/warehouse/episodes_partitioned/doctor_pt=2/000000_0:0+371, maprfs:///user/hive/warehouse/episodes_partitioned/doctor_pt=4/000000_0:0+368, maprfs:///user/hive/warehouse/episodes_partitioned/doctor_pt=5/000000_0:0+357, maprfs:///user/hive/warehouse/episodes_partitioned/doctor_pt=6/000000_0:0+370, maprfs:///user/hive/warehouse/episodes_partitioned/doctor_pt=9/000000_0:0+350], columns=[`*`], numPartitions=7, partitions= [Partition(values:[1]), Partition(values:[11]), Partition(values:[2]), Partition(values:[4]), Partition(values:[5]), Partition(values:[6]), Partition(values:[9])]]])\r\n{code}\r\n\r\nI attached the data file and the hql required. Let me know if anything else is needed"
    ],
    [
        "DRILL-2075",
        "DRILL-2186",
        "Subquery not projecting the order by column is causing issues when used along with flatten git.commit.id.abbrev=3c6d0ef\r\n\r\nData Set :\r\n{code}\r\n{\r\n  \"uid\" : 1,\r\n  \"uid_str\" : \"01\",\r\n  \"type\" : \"web\",\r\n  \"events\" : [\r\n        { \"evnt_id\":\"e1\", \"campaign_id\":\"c1\", \"event_name\":\"e1_name\", \"event_time\":1000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e2\", \"campaign_id\":\"c1\", \"event_name\":\"e2_name\", \"event_time\":2000000, \"type\" : \"cmpgn4\"},\r\n        { \"evnt_id\":\"e3\", \"campaign_id\":\"c1\", \"event_name\":\"e3_name\", \"event_time\":3000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e4\", \"campaign_id\":\"c1\", \"event_name\":\"e4_name\", \"event_time\":4000000, \"type\" : \"cmpgn1\"},\r\n        { \"evnt_id\":\"e5\", \"campaign_id\":\"c2\", \"event_name\":\"e5_name\", \"event_time\":5000000, \"type\" : \"cmpgn3\"},\r\n        { \"evnt_id\":\"e6\", \"campaign_id\":\"c1\", \"event_name\":\"e6_name\", \"event_time\":6000000, \"type\" : \"cmpgn9\"},\r\n        { \"evnt_id\":\"e7\", \"campaign_id\":\"c1\", \"event_name\":\"e7_name\", \"event_time\":7000000, \"type\" : \"cmpgn3\"},\r\n        { \"evnt_id\":\"e8\", \"campaign_id\":\"c2\", \"event_name\":\"e8_name\", \"event_time\":8000000, \"type\" : \"cmpgn2\"},\r\n        { \"evnt_id\":\"e9\", \"campaign_id\":\"c2\", \"event_name\":\"e9_name\", \"event_time\":9000000, \"type\" : \"cmpgn4\"}\r\n  ]\r\n}\r\n{code}\r\n\r\nQuery with flatten :\r\n{code}\r\nselect s.type  from (select d.type type, flatten(d.events) evnts from `temp.json` d where d.type='web' order by d.uid) s where s.type='web';\r\nQuery failed: IndexOutOfBoundsException: index (2) must be less than size (1)\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nQuery without flatten :\r\n{code}\r\nselect s.type  from (select d.type type from `temp.json` d where d.type='web' order by d.uid) s where s.type='web';\r\n+------------+\r\n|    type    |\r\n+------------+\r\n| web        |\r\n+------------+\r\n{code}\r\n\r\nHowever even the above query fails when we remove the filter and this could be related to DRILL-1302\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select s.type  from (select d.type type from `temp.json` d where d.type='web' order by d.uid) s;\r\nQuery failed: ArrayIndexOutOfBoundsException: -1\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}",
        "select * is not working with flatten git.commit.id.abbrev=c54bd6a\r\n\r\nData Set :\r\n{code}\r\n{\r\n  \"id\" : 1,\r\n  \"lst\" : [1,2,3]\r\n}\r\n{code}\r\n\r\nThe below queries incorrectly return null\r\n{code}\r\n0: jdbc:drill:schema=dfs> select *, flatten(lst) from `temp.json`;\r\n+------------+------------+\r\n|     *      |   EXPR$1   |\r\n+------------+------------+\r\n| null       | 1          |\r\n| null       | 2          |\r\n| null       | 3          |\r\n+------------+------------+\r\n{code}\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs> select * from `temp.json` order by flatten(lst);\r\n+------------+\r\n|     *      |\r\n+------------+\r\n| null       |\r\n| null       |\r\n| null       |\r\n+------------+\r\n{code}"
    ],
    [
        "DRILL-1422",
        "DRILL-3622",
        "Include drillbit location in non-verbose error message Currently, in non-verbose mode, a short error message along with the error id string are included, e.g.\r\n\r\nQuery failed: Failure while running fragment. null [e5e75fa2-6d10-4469-92d0-355a1155c7ae]\r\n\r\nThe error id can be used to lookup the stacktrace of the error, but without the node where it occurred, it is difficult to figure out which logs to look in. We should include the node information even when non-verbose.",
        "With user authentication enabled, only admin users should be able to change system options "
    ],
    [
        "DRILL-2906",
        "DRILL-2752",
        "Json reader with extended json adds extra column Performing a CTAS with 'store.format' = 'json' and querying the table results in projecting an addition field '*' will null values. Below is a simple repro\r\n\r\n0: jdbc:drill:zk=local> create table t as select timestamp '1980-10-01 00:00:00' from cp.`employee.json` limit 1;\r\n+------------+---------------------------+\r\n|  Fragment  | Number of records written |\r\n+------------+---------------------------+\r\n| 0_0        | 1                         |\r\n+------------+---------------------------+\r\n1 row selected (0.314 seconds)\r\n0: jdbc:drill:zk=local> select * from t;\r\n+------------+------------+\r\n|   EXPR$0   |     *      |\r\n+------------+------------+\r\n| 1980-10-01 00:00:00.0 | null       |\r\n+------------+------------+\r\n\r\nNotice in the above result set we get an extra column '*' with null value.",
        "Error message must be improved when incompatible types are used in an expression Query with a filter expression containing incompatible types must fail gracefully. \r\n\r\ncolumn ss_net_profit  is of type double.\r\n{code:sql}\r\n\r\n> select * from store_sales  where ss_net_profit = 'abc' limit 1;\r\nQuery failed: RemoteRpcException: Failure while running fragment., abc [ 2206cb8a-1f5f-4c4e-9906-8b84237291c4 on abhi8.qa.lab:31010 ]\r\n[ 2206cb8a-1f5f-4c4e-9906-8b84237291c4 on abhi8.qa.lab:31010 ]\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n{code}\r\n\r\nError message must indicate that the value provided is of incompatible type. \r\n"
    ],
    [
        "DRILL-1818",
        "DRILL-1298",
        "Parquet files generated by Drill ignore field names when nested elements are queried I observed this with this parquet file and a more comprehensive testing might be needed here. The issue is that Drill seem to simply ignore field names at the leaf level and accessing data in a positional fashion.\r\n\r\nBelow is the repro.\r\n1. Generate  a parquet file using Drill. Input is the JSON doc below\r\n\r\ncreate  table dfs.tmp.sampleparquet as (select trans_id, cast(`date` as date) transdate,cast(`time` as time) transtime, cast(amount as double) amount,`user_info`,`marketing_info`, `trans_info` from dfs.`/Users/nrentachintala/Downloads/sample.json` )\r\n\r\n\r\n2. Now do queries. \r\nNote in query below, there is no field name called 'keywords' in trans_info, but data is just positionally returned (the data returned from prod_id column).\r\n0: jdbc:drill:zk=local> select t.`trans_info`.keywords from dfs.tmp.sampleparquet t where t.`trans_info`.keywords is not null;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| [16]       |\r\n| []         |\r\n| [293,90]   |\r\n| [173,18,121,84,115,226,464,525,35,11,94,45] |\r\n| [311,29,5,41] |\r\n\r\n0: jdbc:drill:zk=local> select t.`marketing_info`.keywords from dfs.tmp.sampleparquet t;\r\n\r\nNote in the query below, it is trying to return the first element in marketing_Info which is camp_id which is of int type for keywords columns. But keywords schema is string, so it throws error with type mismatch.\r\n\r\nQuery failed: Query failed: Failure while running fragment., You tried to write a VarChar type when you are using a ValueWriter of type NullableBigIntWriterImpl. [ c3761403-b8c5-43c1-8e90-2c4918d1f85c on 10.0.0.20:31010 ]\r\n[ c3761403-b8c5-43c1-8e90-2c4918d1f85c on 10.0.0.20:31010 ]\r\n\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\n0: jdbc:drill:zk=local> select t.`marketing_info`.`camp_id`,t.`marketing_info`.keywords from dfs.tmp.sampleparquet t;\r\n+------------+------------+\r\n|   EXPR$0   |   EXPR$1   |\r\n+------------+------------+\r\n| 4          | [\"go\",\"to\",\"thing\",\"watch\",\"made\",\"laughing\",\"might\",\"pay\",\"in\",\"your\",\"hold\"] |\r\n| 6          | [\"pronounce\",\"tree\",\"instead\",\"games\",\"sigh\"] |\r\n| 17         | []         |\r\n| 17         | [\"it's\"]   |\r\n| 8          | [\"fallout\"] |\r\n+------------+------------+\r\n\r\n\r\nSample.json is below\r\n{\"trans_id\":0,\"date\":\"2013-07-26\",\"time\":\"04:56:59\",\"amount\":80.5,\"user_info\":{\"cust_id\":28,\"device\":\"IOS5\",\"state\":\"mt\"},\"marketing_info\":{\"camp_id\":4,\"keywords\":[\"go\",\"to\",\"thing\",\"watch\",\"made\",\"laughing\",\"might\",\"pay\",\"in\",\"your\",\"hold\"]},\"trans_info\":{\"prod_id\":[16],\"purch_flag\":\"false\"}}\r\n\r\n{\"trans_id\":1,\"date\":\"2013-05-16\",\"time\":\"07:31:54\",\"amount\":100.40,\r\n\"user_info\":{\"cust_id\":86623,\"device\":\"AOS4.2\",\"state\":\"mi\"},\"marketing_info\":{\"camp_id\":6,\"keywords\":[\"pronounce\",\"tree\",\"instead\",\"games\",\"sigh\"]},\"trans_info\":{\"prod_id\":[],\"purch_flag\":\"false\"}}\r\n\r\n{\"trans_id\":2,\"date\":\"2013-06-09\",\"time\":\"15:31:45\",\"amount\":20.25,\r\n\"user_info\":{\"cust_id\":11,\"device\":\"IOS5\",\"state\":\"la\"},\"marketing_info\":{\"camp_id\":17,\"keywords\":[]},\"trans_info\":{\"prod_id\":[293,90],\"purch_flag\":\"true\"}}\r\n\r\n{\"trans_id\":3,\"date\":\"2013-07-19\",\"time\":\"11:24:22\",\"amount\":500.75,\r\n\"user_info\":{\"cust_id\":666,\"device\":\"IOS5\",\"state\":\"nj\"},\"marketing_info\":{\"camp_id\":17,\"keywords\":[\"it's\"]},\"trans_info\":{\"prod_id\":[173,18,121,84,115,226,464,525,35,11,94,45],\"purch_flag\":\"false\"}}\r\n\r\n{\"trans_id\":4,\"date\":\"2013-07-21\",\"time\":\"08:01:13\",\"amount\":34.20,\"user_info\":{\"cust_id\":999,\"device\":\"IOS7\",\"state\":\"ct\"},\"marketing_info\":{\"camp_id\":8,\"keywords\":[\"fallout\"]},\"trans_info\":{\"prod_id\":[311,29,5,41],\"purch_flag\":\"false\"}}\r\n",
        "Generic queues and UI "
    ],
    [
        "DRILL-586",
        "DRILL-1628",
        "Order by on select * fails 0: jdbc:drill:schema=dfs.drillTestDirP1> explain plan for select * from student order by age;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"e2eb9f49-d82b-41f3-a518-8ab698ed5812\"\r\nendpoint {\r\n  address: \"drillats3.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while setting up Foreman. < AssertionError:[ Internal error: Conversion to relational algebra failed to preserve datatypes:\\nvalidated type:\\nRecordType(ANY NOT NULL *) NOT NULL\\nconverted type:\\nRecordType(ANY NOT NULL *, ANY NOT NULL age) NOT NULL\\nrel:\\nProjectRel(*=[$0], age=[$1])\\n  SortRel(sort0=[$1], dir0=[ASC])\\n    ProjectRel(*=[$0], age=[$1], age0=[$1])\\n      TableAccessRel(table=[[dfs, drillTestDirP1, student]])\\n ]\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n\r\nThe query fails with the same error.",
        "Need To Accommodate Longer Identifier Length Need to be able to accommodate identifier lengths above the current default of 128"
    ],
    [
        "DRILL-2947",
        "DRILL-1516",
        "AllocationHelper.allocateNew() doesn't have a consistent behavior when it can't allocate AllocationHelper.allocateNew(...) has the following implementation:\r\n\r\n{code}\r\npublic static boolean allocateNew(ValueVector v, int valueCount){\r\n    if (v instanceof  FixedWidthVector) {\r\n      ((FixedWidthVector) v).allocateNew(valueCount);\r\n      return true;\r\n    } else {\r\n      return v.allocateNewSafe();\r\n    }\r\n  }\r\n{code}\r\n\r\nIf it can't allocate the memory it will either return false or throw an _OutOfMemoryRuntimeException_ depending on the class of v.\r\n\r\nSome operators that use this method, like _NestedLoopJoinBatch_ don't check if _allocateNew()_ returns false assuming it will throw an exception instead.",
        "Does not find java executable on Ubuntu when openjdk-7-doc is installed Original bug found by Takeshi Matsukura ( tmatsukura@maprtech.com )"
    ],
    [
        "DRILL-103",
        "DRILL-3482",
        "S3 support with range indexing ",
        "LIMIT not terminating early enough when concurrent queries are running This behavior was observed on a 30 node cluster, width.max_per_node = 23.  Run a long-running CTAS query that does joins, aggregations and takes about 5 minutes.  While this query is running, submit the following type of query: \r\n{code}\r\nSELECT * FROM table LIMIT 10;\r\n{code}\r\nThe table consists of 2500 parquet files and the total row count of this table is about 4 billion rows. \r\n\r\nThe elapsed time of this query was seen to be more than 2 minutes.   There were 690 minor fragments doing the Scan and the majority of them finished in a few seconds while a small number took 2 mins.   \r\n\r\nWhen the same query was run on an idle system, the query took about 6 seconds to complete.  It appears that when concurrent queries are running, the early termination for Limit is not getting processed fast enough on all the minor fragments. "
    ],
    [
        "DRILL-3468",
        "DRILL-2530",
        "CTAS IOB I am seeing a IOB when I use same table name in CTAS, after deleting the previously create parquet file.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> CREATE TABLE tbl_allData AS SELECT CAST(columns[0] as INT ), CAST(columns[1] as BIGINT ), CAST(columns[2] as CHAR(2) ), CAST(columns[3] as VARCHAR(52) ), CAST(columns[4] as TIMESTAMP ), CAST(columns[5] as DATE ), CAST(columns[6] as BOOLEAN ), CAST(columns[7] as DOUBLE), CAST( columns[8] as TIME) FROM `allData.csv`;\r\n+-----------+----------------------------+\r\n| Fragment  | Number of records written  |\r\n+-----------+----------------------------+\r\n| 0_0       | 11196                      |\r\n+-----------+----------------------------+\r\n1 row selected (1.864 seconds)\r\n{code}\r\n\r\nRemove the parquet file that was created by the above CTAS.\r\n\r\n{code}\r\n[root@centos-01 aggregates]# hadoop fs -ls /tmp/tbl_allData\r\nFound 1 items\r\n-rwxr-xr-x   3 mapr mapr     397868 2015-07-07 21:08 /tmp/tbl_allData/0_0_0.parquet\r\n[root@centos-01 aggregates]# hadoop fs -rm /tmp/tbl_allData/0_0_0.parquet\r\n15/07/07 21:10:47 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\r\n15/07/07 21:10:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\nDeleted /tmp/tbl_allData/0_0_0.parquet\r\n{code}\r\n\r\nI see a IOB when I CTAS with same table name as the one that was removed in the above step.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> CREATE TABLE tbl_allData AS SELECT CAST(columns[0] as INT ), CAST(columns[1] as BIGINT ), CAST(columns[2] as CHAR(2) ), CAST(columns[3] as VARCHAR(52) ), CAST(columns[4] as TIMESTAMP ), CAST(columns[5] as DATE ), CAST(columns[6] as BOOLEAN ), CAST(columns[7] as DOUBLE), CAST( columns[8] as TIME) FROM `lessData.csv`;\r\nError: SYSTEM ERROR: IndexOutOfBoundsException: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 6d6df8e9-699c-4475-8ad3-183c0a91dc99 on centos-02.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nstack trace from drillbit.log\r\n\r\n{code}\r\norg.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: Failure while trying to check if a table or view with given name [tbl_allData] already exists in schema [dfs.tmp]: Index: 0, Size: 0\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:253) [drill-java-exec-1.1.0.jar:1.1.0]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\nCaused by: org.apache.drill.common.exceptions.DrillRuntimeException: Failure while trying to check if a table or view with given name [tbl_allData] already exists in schema [dfs.tmp]: Index: 0, Size: 0\r\n        at org.apache.drill.exec.planner.sql.handlers.SqlHandlerUtil.getTableFromSchema(SqlHandlerUtil.java:222) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.planner.sql.handlers.CreateTableHandler.getPlan(CreateTableHandler.java:88) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:178) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:903) [drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:242) [drill-java-exec-1.1.0.jar:1.1.0]\r\n        ... 3 common frames omitted\r\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n        at java.util.ArrayList.rangeCheck(ArrayList.java:635) ~[na:1.7.0_45]\r\n        at java.util.ArrayList.get(ArrayList.java:411) ~[na:1.7.0_45]\r\n        at org.apache.drill.exec.store.dfs.FileSelection.getFirstPath(FileSelection.java:100) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.store.dfs.BasicFormatMatcher.isReadable(BasicFormatMatcher.java:75) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.create(WorkspaceSchemaFactory.java:303) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.create(WorkspaceSchemaFactory.java:118) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.getNewEntry(ExpandingConcurrentMap.java:96) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.get(ExpandingConcurrentMap.java:90) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.getTable(WorkspaceSchemaFactory.java:241) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.planner.sql.handlers.SqlHandlerUtil.getTableFromSchema(SqlHandlerUtil.java:219) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        ... 7 common frames omitted\r\n2015-07-07 21:12:14,643 [2a63bf51-09ff-467b-1aab-df2b5030e9b9:foreman] INFO  o.a.drill.exec.work.foreman.Foreman - foreman cleaning up.\r\n\r\n2015-07-07 21:12:14,648 [2a63bf51-09ff-467b-1aab-df2b5030e9b9:foreman] ERROR o.a.drill.exec.work.foreman.Foreman - SYSTEM ERROR: IndexOutOfBoundsException: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 6d6df8e9-699c-4475-8ad3-183c0a91dc99 on centos-02.qa.lab:31010]\r\norg.apache.drill.common.exceptions.UserException: SYSTEM ERROR: IndexOutOfBoundsException: Index: 0, Size: 0\r\n\r\n\r\n[Error Id: 6d6df8e9-699c-4475-8ad3-183c0a91dc99 on centos-02.qa.lab:31010]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:523) ~[drill-common-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman$ForemanResult.close(Foreman.java:737) [drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:839) [drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:781) [drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.common.EventProcessor.sendEvent(EventProcessor.java:73) [drill-common-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.moveToState(Foreman.java:783) [drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.moveToState(Foreman.java:892) [drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:253) [drill-java-exec-1.1.0.jar:1.1.0]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\nCaused by: org.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: Failure while trying to check if a table or view with given name [tbl_allData] already exists in schema [dfs.tmp]: Index: 0, Size: 0\r\n        ... 4 common frames omitted\r\nCaused by: org.apache.drill.common.exceptions.DrillRuntimeException: Failure while trying to check if a table or view with given name [tbl_allData] already exists in schema [dfs.tmp]: Index: 0, Size: 0\r\n        at org.apache.drill.exec.planner.sql.handlers.SqlHandlerUtil.getTableFromSchema(SqlHandlerUtil.java:222) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.planner.sql.handlers.CreateTableHandler.getPlan(CreateTableHandler.java:88) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:178) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:903) [drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:242) [drill-java-exec-1.1.0.jar:1.1.0]\r\n        ... 3 common frames omitted\r\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n        at java.util.ArrayList.rangeCheck(ArrayList.java:635) ~[na:1.7.0_45]\r\n        at java.util.ArrayList.get(ArrayList.java:411) ~[na:1.7.0_45]\r\n        at org.apache.drill.exec.store.dfs.FileSelection.getFirstPath(FileSelection.java:100) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.store.dfs.BasicFormatMatcher.isReadable(BasicFormatMatcher.java:75) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.create(WorkspaceSchemaFactory.java:303) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.create(WorkspaceSchemaFactory.java:118) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.getNewEntry(ExpandingConcurrentMap.java:96) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.planner.sql.ExpandingConcurrentMap.get(ExpandingConcurrentMap.java:90) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory$WorkspaceSchema.getTable(WorkspaceSchemaFactory.java:241) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        at org.apache.drill.exec.planner.sql.handlers.SqlHandlerUtil.getTableFromSchema(SqlHandlerUtil.java:219) ~[drill-java-exec-1.1.0.jar:1.1.0]\r\n        ... 7 common frames omitted\r\n{code}",
        "getColumns() doesn't return right COLUMN_SIZE for INTERVAL types "
    ],
    [
        "DRILL-2804",
        "DRILL-1775",
        "IOB Exception : Project non-existent column from compressed CSV file (.tgz) Query that projects a non-existent column from a CSV file that has an extension, (.tgz) results in IOB Exception. Whereas, the same query that projects a non-existent column (columns[8]) in the test query, does not return the exception, and we see the expected results where null is returned for a non-existent column in a CSV file that is NOT compressed in (.tgz) compression format. Tests were performed on 4 node cluster on CentOS.\r\n\r\n{code}\r\n\r\n0: jdbc:drill:> select columns[8] from `deletions.tgz` limit 4;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| null       |\r\n| null       |\r\n| null       |\r\n| null       |\r\nQuery failed: RemoteRpcException: Failure while running fragment., index: 18990, length: 1 (expected: range(0, 16384)) [ 1f8b0c2e-ffc1-4384-874b-fae2cc4e129b on centos-04.qa.lab:31010 ]\r\n[ 1f8b0c2e-ffc1-4384-874b-fae2cc4e129b on centos-04.qa.lab:31010 ]\r\n\r\n\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while executing query.\r\n\tat sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n\tat sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n\tat sqlline.SqlLine.print(SqlLine.java:1809)\r\n\tat sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n\tat sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n\tat sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n\tat sqlline.SqlLine.begin(SqlLine.java:763)\r\n\tat sqlline.SqlLine.start(SqlLine.java:498)\r\n\tat sqlline.SqlLine.main(SqlLine.java:460)\r\n0: jdbc:drill:> select columns[8] from `deletions/deletions-00000-of-00020.csv` limit 4;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| null       |\r\n| null       |\r\n| null       |\r\n| null       |\r\n+------------+\r\n4 rows selected (0.388 seconds):\r\n\r\nstack trace from drillbit.log file\r\n\r\n2015-04-16 15:15:58,363 [2ad02dd2-9990-de56-4d94-f901ebe8db3c:frag:1:5] WARN  o.a.d.e.w.fragment.FragmentExecutor - Error while initializing or executing fragment\r\njava.lang.IndexOutOfBoundsException: index: 18990, length: 1 (expected: range(0, 16384))\r\n        at io.netty.buffer.DrillBuf.checkIndexD(DrillBuf.java:187) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n        at io.netty.buffer.DrillBuf.chk(DrillBuf.java:209) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n        at io.netty.buffer.DrillBuf.setByte(DrillBuf.java:608) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:4.0.24.Final]\r\n        at org.apache.drill.exec.vector.UInt1Vector$Mutator.set(UInt1Vector.java:359) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.vector.UInt1Vector$Mutator.setSafe(UInt1Vector.java:366) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.vector.NullableVarCharVector$Mutator.setSafe(NullableVarCharVector.java:500) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.test.generated.ProjectorGen0.doEval(ProjectorTemplate.java:25) ~[na:na]\r\n        at org.apache.drill.exec.test.generated.ProjectorGen0.projectRecords(ProjectorTemplate.java:62) ~[na:na]\r\n        at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.doWork(ProjectRecordBatch.java:174) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:93) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:134) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:142) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:118) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:68) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.SingleSenderCreator$SingleSenderRootExec.innerNext(SingleSenderCreator.java:99) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:58) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:163) ~[drill-java-exec-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-0.9.0-SNAPSHOT-rebuffed.jar:0.9.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]\r\n\r\n\r\n{code}",
        "Binary ENUM read failure in Parquet "
    ],
    [
        "DRILL-2899",
        "DRILL-1228",
        "Update sort memory max to allow any positive number ",
        "Connecting to Tableau live returns null values for dimensions Connecting to Tableau using live queries returns null dimensions. Connecting using Import All will return as normal. Both queries run fine in SQLLine and Drill Explorer. Using an older drillbit (from July 21st), everything ran fine.\r\n\r\nLive query:\r\n{code}\r\nSELECT `varchar_table`.`column1` AS `none_columnB_nk`\r\nFROM `hive43.default`.`varchar_table` `varchar_table`\r\nGROUP BY `varchar_table`.`column1`\r\n{code}\r\n\r\nImport All query:\r\n{code}\r\nSELECT 1 AS `Number_of_Records`,\r\n  `varchar_table`.`column1` AS `columnB`,\r\n  `varchar_table`.`keycolumn` AS `keycolumn`\r\nFROM `hive43.default`.`varchar_table` `varchar_table`\r\n{code}"
    ],
    [
        "DRILL-587",
        "DRILL-2625",
        "Order by on projected columns fails 0: jdbc:drill:schema=dfs.drillTestDirP1> explain plan for select name,age from student order by age;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| ScreenPrel: rowcount = 100.0, cumulative cost = {508.4136148790474 rows, 161.0 cpu, 0.0 io}, id = 12142\r\n  SingleMergeExchangePrel(sort0=[1 ASC]): rowcount = 100.0, cumulative cost = {498.4136148790474 rows, 151.0 cpu, 0.0 io}, id = 12141\r\n    SortPrel(sort0=[$1], dir |\r\n+------------+------------+\r\n1 row selected (0.064 seconds)\r\n0: jdbc:drill:schema=dfs.drillTestDirP1> select name,age from student order by age;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"50315910-bb12-496f-ae39-cb4844dd0301\"\r\nendpoint {\r\n  address: \"drillats3.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < ClassCastException:[ org.apache.drill.exec.vector.VarBinaryVector cannot be cast to org.apache.drill.exec.vector.IntVector ]\"\r\n]\r\nError: exception while executing query (state=,code=0)",
        "org.apache.drill.common.StackTrace should follow standard stacktrace format org.apache.drill.common.StackTrace uses a different textual format than JDK's standard format for stack traces.\r\n\r\nIt should probably use the standard format so that its stack trace output can be used by tools that already can parse the standard format to provide functionality such as displaying the corresponding source.\r\n\r\n(After correcting for DRILL-2624, StackTrace formats stack traces like this:\r\n\r\norg.apache.drill.common.StackTrace.<init>:1\r\norg.apache.drill.exec.server.Drillbit.run:20\r\norg.apache.drill.jdbc.DrillConnectionImpl.<init>:232\r\n\r\nThe normal form is like this:\r\n{noformat}\r\n\tat org.apache.drill.exec.memory.TopLevelAllocator.close(TopLevelAllocator.java:162)\r\n\tat org.apache.drill.exec.server.BootStrapContext.close(BootStrapContext.java:75)\r\n\tat com.google.common.io.Closeables.close(Closeables.java:77)\r\n{noformat}\r\n)\r\n\r\n"
    ],
    [
        "DRILL-932",
        "DRILL-2325",
        "Add bracketless Syntax support for complex queries right now, for complex types you have to do a['b'][4]['c'].  a.b[4].c should also be allowed.",
        "conf/drill-override-example.conf is outdated The conf/drill-override-example.conf file is outdated.  Properties have been added (e.g., compile), removed (e.g., cache.hazel.subnets) or otherwise modified.\r\n\r\nThe file is statically tracked in distribution/src/resources/drill-override-example.conf.  Ideally there should be a way to update the file programmatically when things change."
    ],
    [
        "DRILL-1314",
        "DRILL-2830",
        "Parquet Reader for compressed parquet files fails When querying compressed Parquet files, an error is reported for the RLE stream:\r\nselect\r\nl_orderkey, l_partkey, l_suppkey, l_linenumber, l_quantity, l_extendedprice, l_discount, l_tax, l_returnflag, l_linestatus, l_shipdate, l_commitdate, l_receiptdate, l_shipinstruct, l_shipmode, l_comment\r\nfrom\r\n        lineitem_imp100\r\nwhere\r\n        l_orderkey > 0\r\n        and l_partkey >= 0\r\n        and l_suppkey >= 0\r\n        and l_linenumber >= 0\r\n        and l_quantity >= 0\r\n        and l_extendedprice >= 0\r\n        and l_discount >= 0\r\n        and l_tax >= 0\r\n        and length(l_returnflag) > 0\r\n        and length(l_linestatus) > 0\r\n        and length(l_shipdate) > 0\r\n        and length(l_commitdate) > 0\r\n        and length(l_receiptdate) > 0\r\n        and length(l_shipinstruct) > 0\r\n        and length(l_shipmode) > length(l_comment);\r\n+------------+------------+------------+--------------+------------+-----------------+------------+------------+--------------+--------------+------------+--------------+---------------+----------------+------------+------------+\r\n| l_orderkey | l_partkey  | l_suppkey  | l_linenumber | l_quantity | l_extendedprice | l_discount |   l_tax    | l_returnflag | l_linestatus | l_shipdate | l_commitdate | l_receiptdate | l_shipinstruct | l_shipmode | l_comment  |\r\n+------------+------------+------------+--------------+------------+-----------------+------------+------------+--------------+--------------+------------+--------------+---------------+----------------+------------+------------+\r\nQuery failed: Failure while running fragment. Reading past RLE/BitPacking stream. [3a999343-a881-4193-9853-27e24aaf768b]\r\njava.lang.RuntimeException: java.sql.SQLException: Failure while trying to get next result batch.\r\n        at sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2514)\r\n        at sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2148)\r\n        at sqlline.SqlLine.print(SqlLine.java:1809)\r\n        at sqlline.SqlLine$Commands.execute(SqlLine.java:3766)\r\n        at sqlline.SqlLine$Commands.sql(SqlLine.java:3663)\r\n        at sqlline.SqlLine.dispatch(SqlLine.java:889)\r\n        at sqlline.SqlLine.begin(SqlLine.java:763)\r\n        at sqlline.SqlLine.start(SqlLine.java:498)\r\n        at sqlline.SqlLine.main(SqlLine.java:460)\r\n\r\nThe JDBC client reports the stack trace:\r\njava.sql.SQLException: exception while executing query: Failure while trying to get next result batch.\r\n        at net.hydromatic.avatica.Helper.createException(Helper.java:40)\r\n        at net.hydromatic.avatica.AvaticaConnection.executeQueryInternal(AvaticaConnection.java:406)\r\n        at net.hydromatic.avatica.AvaticaStatement.executeQueryInternal(AvaticaStatement.java:351)\r\n        at net.hydromatic.avatica.AvaticaStatement.executeQuery(AvaticaStatement.java:78)\r\n        at PipSQueak.executeQuery(PipSQueak.java:243)\r\n        at PipSQueak.runTest(PipSQueak.java:81)\r\n        at PipSQueak.main(PipSQueak.java:404)\r\nCaused by: java.sql.SQLException: Failure while trying to get next result batch.\r\n        at org.apache.drill.jdbc.DrillCursor.next(DrillCursor.java:110)\r\n        at org.apache.drill.jdbc.DrillResultSet.execute(DrillResultSet.java:90)\r\n        at org.apache.drill.jdbc.DrillResultSet.execute(DrillResultSet.java:44)\r\n        at net.hydromatic.avatica.AvaticaConnection.executeQueryInternal(AvaticaConnection.java:404)\r\n        ... 5 more\r\nCaused by: org.apache.drill.exec.rpc.RpcException: Failure while running fragment. Reading past RLE/BitPacking stream. [2cc52105-15ac-49ba-a93b-37cf8d47e3c8]\r\n\r\n        at org.apache.drill.exec.rpc.user.QueryResultHandler.batchArrived(QueryResultHandler.java:77)\r\n        at org.apache.drill.exec.rpc.user.UserClient.handleReponse(UserClient.java:90)\r\n        at org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:52)\r\n        at org.apache.drill.exec.rpc.BasicClientWithConnection.handle(BasicClientWithConnection.java:34)\r\n        at org.apache.drill.exec.rpc.RpcBus.handle(RpcBus.java:60)\r\n        at org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:181)\r\n        at org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:165)\r\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:332)\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:318)\r\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:332)\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:318)\r\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:163)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:332)\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:318)\r\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:332)\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:318)\r\n        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787)\r\n        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:125)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:507)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:464)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:378)\r\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:350)\r\n        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)\r\n        at java.lang.Thread.run(Thread.java:744)\r\n",
        "Self-Join via view gives wrong results Create a view by:\r\n{code}\r\ncreate view v1(x, y) as select n_regionkey,  n_nationkey from cp.`tpch/nation.parquet`;\r\n{code}\r\n\r\nAnd join this view with the file which created the view with condition t.n_nationkey  = v1.y (where 'y' is n_nationkey)\r\n\r\n{code}\r\nselect t.n_nationkey from cp.`tpch/nation.parquet` t inner join v1 on t.n_nationkey  = v1.y;\r\n\r\n+-------------+\r\n| n_nationkey |\r\n+-------------+\r\n| 0           |\r\n| 1           |\r\n| 1           |\r\n| 1           |\r\n| 4           |\r\n| 0           |\r\n| 3           |\r\n| 3           |\r\n| 2           |\r\n| 2           |\r\n| 4           |\r\n| 4           |\r\n| 2           |\r\n| 4           |\r\n| 0           |\r\n| 0           |\r\n| 0           |\r\n| 1           |\r\n| 2           |\r\n| 3           |\r\n| 4           |\r\n| 2           |\r\n| 3           |\r\n| 3           |\r\n| 1           |\r\n+-------------+\r\n25 rows selected (0.153 seconds)\r\n{code}\r\n\r\nAfter investigating the plan, I found out that the result was produced as if the join condition was t.n_nationkey  = v1.x (where x is 'n_regionkey')"
    ],
    [
        "DRILL-3817",
        "DRILL-3010",
        "Refresh metadata does not work when used with sub schema   refresh table metadata dfs.tmp.`lineitem` does not work, hit the following exception\r\n\r\norg.apache.drill.common.exceptions.UserRemoteException: PARSE ERROR: org.apache.calcite.sql.SqlBasicCall cannot be cast to org.apache.calcite.sql.SqlIdentifier\r\n\r\nIf the sub schema is removed it works.\r\nrefresh table metadata dfs.`/tmp/lineitem`",
        "Convert bad command error messages into UserExceptions in SqlHandlers Currently SqlHandlers such as CreateTable or ViewHandler send the error messages as bad command records.  Instead we should throw a UserException.\r\n\r\n{code}\r\n0: jdbc:drill:zk=local> create table t1 as select * from cp.`region.json`;\r\n+------------+------------+\r\n|     ok     |  summary   |\r\n+------------+------------+\r\n| false      | Unable to create table. Schema [dfs.default] is immutable.  |\r\n+------------+------------+\r\n1 row selected (0.103 seconds)\r\n{code}\r\n\r\nInstead it should be like:\r\n\r\n{code}\r\n0: jdbc:drill:zk=10.10.30.143:5181> create table t1 as select * from cp.`region.json`;\r\nError: PARSE ERROR: Unable to create or drop tables/views. Schema [dfs.default] is immutable.\r\n\r\n\r\n[Error Id: 3a92d026-3df7-4e8b-8988-2300463fa00b on centos64-30146.qa.lab:31010] (state=,code=0)\r\n{code}"
    ],
    [
        "DRILL-3167",
        "DRILL-3822",
        "When a query fails, Foreman should wait for all fragments to finish cleaning up before sending a FAILED state to the client TestDrillbitResilience.foreman_runTryEnd() exposes this problem intermittently\r\n\r\nThe query fails and the Foreman reports the failure to the client which removes the results listener associated to the failed query. \r\nSometimes, a data batch reaches the client after the FAILED state already arrived, the client doesn't handle this properly and the corresponding buffer is never released.\r\n\r\nMaking the Foreman wait for all fragments to finish before sending the final state should help avoid such scenarios.",
        "PathScanner fails to find jdbc-all's drill-module.conf in SQuirreL git.commit.id.abbrev=3c89b30\r\n\r\nI used the latest drill-jdbc-all-1.2.0-SNAPSHOT.jar against the SQuirreL SQL application.  I got the following error when trying to connect to the drill data source:\r\n\r\n{noformat}\r\nERROR net.sourceforge.squirrel_sql.client.gui.db.ConnectToAliasCallBack  - Unexpected Error occurred attempting to open an SQL connection.\r\njava.util.concurrent.ExecutionException: java.lang.RuntimeException: oadd.com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'drill.exec'\r\nFull error message is in the attached file. \r\n{noformat}\r\n\r\nWe turned on logging and found that the jdbc-all Jar file's {{drill-module.conf}} file was not being found (explaining why the configuration key {{drill.exec}} wasn't found).\r\n\r\nAfter further investigation, it seems that {{PathScanner}} directly uses the system class loader, bypassing the context class loader.\r\n\r\n(After drill-jdbc-all-1.2.0-SNAPSHOT.jar was changed from being listed in SQuirreL's \"additional class paths\" (presumably being loaded by a special class loader) to being copied into SQuirreL's Jar file directory (and therefore loaded by the system class loader), SQuirreL worked. (Apparently, {{PathScanner}} was then able to find  {{drill-module.conf}} in the JDBC-all Jar file and load it, so the later reference to {{drill.exec}} no longer failed.) \r\n\r\nAlso, SQuirreL works correctly with drill-1.1's JDBC-all Jar file, and there were some recent changes to {{PathScanner}} related to class loaders.)\r\n"
    ],
    [
        "DRILL-258",
        "DRILL-1564",
        "Explore moving code generation from source code to ByteCode generation Look at using Julien Le Dem's bytecode generation library Brennus\r\n\r\nhttps://github.com/julienledem/brennus\r\n\r\nExamples can be seen here: \r\n\r\nhttps://github.com/Parquet/parquet-mr/tree/fsa_codegen2\r\n\r\nby running mvn test\r\n\r\nThis could have meaningful impact in very short queries. ",
        "C++ Client. Assert failed for querySubmitter for a specific query The following query has assertion failed:\r\n\r\n{code}\r\n./querySubmitter connectStr=\"zk=127.0.0.1:2181/drill/drillbits1\" type=sql api=async query=\"select * from sys.options limit 1\"                                                        \r\nConnected!\r\n\r\nAssertion failed: (length>0), function SlicedByteBuf, file /Users/mx/drill-workspace/incubator-drill/contrib/native/client/src/clientlib/../include/drill/recordBatch.hpp, line 84.\r\n[1]    58337 abort      ./querySubmitter connectStr=\"zk=127.0.0.1:2181/drill/drillbits1\" type=sql\r\n{code}\r\n\r\nBut  running the following it is OK:\r\n{code}\r\n$ ./querySubmitter connectStr=\"zk=127.0.0.1:2181/drill/drillbits1\" type=sql api=async query=\"select * from sys.options limit 2\"                                                                 Connected!\r\n\r\nname    kind    type    num_val    string_val    bool_val    float_val\r\ndrill.exec.rpc.bit.server.retry.delay    LONG    BOOT    500    null    null    null\r\njava.awt.graphicsenv    null    BOOT    null    \"sun.awt.CGraphicsEnvironment\"    null    null\r\n\r\nINFO: [30030]Received query_state: COMPLETED.\r\n{code}\r\n\r\nIt is not related to `limit` operator though.  \r\nThe query\r\n{code}\r\nselect * from INFORMATION_SCHEMA.SCHEMATA limit 1\r\n{code}\r\nalso works fine."
    ],
    [
        "DRILL-1349",
        "DRILL-3860",
        "Could not find schemas when using drill ODBC connecting to Tableau 8.2 Documentation is based on Tableau 8.1:\r\nhttp://doc.mapr.com/display/MapR/Tableau+Examples\r\n\r\nHowever when I am using Tableau 8.2, after connecting to drill ODBC DSN, Tableau could not find any schemas. \r\nHowever this DSN works fine in Drill explorer.\r\nSee attached pictures for details.\r\n\r\nAsk:\r\n1. Does drill ODBC support/verify Tableau desktop 8.2 or not?\r\n2. If so, could we add the new steps in the documentation?\r\n3. If not, where could we find the support matrix between Drill ODBC and other BI tools?\r\n",
        "Delimited identifier `*` breaks in select list--acts like plain asterisk token At least when it appears in a SELECT list, a delimited identifier whose body consists of a single asterisk (\"{{`\\*`}}\") is not treated consistently with other delimited identifiers (that is, specifying a column whose name matches the body (\"{{\\*}}\").)\r\n\r\nFor example, in the following, notice how in the first two queries, each select list delimited identifier selects the one expected column, but in the third query, instead of selecting the one expected column, it selected all columns (list the regular \"{{*}}\" in the fourth query):\r\n\r\n{noformat}\r\n0: jdbc:drill:zk=local> SELECT `a` FROM (VALUES (1, 2, 3)) AS T(a, `.`, `*`);\r\n+----+\r\n| a  |\r\n+----+\r\n| 1  |\r\n+----+\r\n1 row selected (0.132 seconds)\r\n0: jdbc:drill:zk=local> SELECT `.` FROM (VALUES (1, 2, 3)) AS T(a, `.`, `*`);\r\n+----+\r\n| .  |\r\n+----+\r\n| 2  |\r\n+----+\r\n1 row selected (0.152 seconds)\r\n0: jdbc:drill:zk=local> SELECT `*` FROM (VALUES (1, 2, 3)) AS T(a, `.`, `*`);\r\n+----+----+----+\r\n| a  | .  | *  |\r\n+----+----+----+\r\n| 1  | 2  | 3  |\r\n+----+----+----+\r\n1 row selected (0.136 seconds)\r\n0: jdbc:drill:zk=local> SELECT * FROM (VALUES (1, 2, 3)) AS T(a, `.`, `*`);\r\n+----+----+----+\r\n| a  | .  | *  |\r\n+----+----+----+\r\n| 1  | 2  | 3  |\r\n+----+----+----+\r\n1 row selected (0.128 seconds)\r\n0: jdbc:drill:zk=local> \r\n{noformat}\r\n\r\nAlthough this acts the same as if the SQL parser treated the delimited identifier {{`\\*`}} as a plain asterisk token, that does not seem to be the actual mechanism for this behavior.  (The problem seems to be further downstream.)\r\n\r\n"
    ],
    [
        "DRILL-2782",
        "DRILL-548",
        "Decide, implement behavior for transaction-related JDBC methods Officially, JDBC requires transaction support. Because of that, the JDBC specification (PDF document and Javadoc) addresses the behavior of transaction-related methods only for the case in which transactions are supported.\r\n\r\nIn particular, JDBC does not specify the behavior when transactions are not supported.\r\n\r\nTherefore, it is not clear what behavior a JDBC client tool would expect, or be programmed to handle, from a JDBC driver and back end that do not support transactions (i.e., Drill).\r\n\r\nIn turn, that means that it is not clear exactly what Drill's JDBC driver's transaction-related methods should do. \r\n\r\nFor example, if a tool tries to call setAutoCommit(false), issue a create-view query, and call commit():\r\n- Should Drill throw an exception at setAutoCommit(false) (because Drill's behavior, which is effectively auto-commit mode, can't be disabled)?  If so, would tools likely be able to handle that exception, specifically, by switching to using auto-commit mode, not calling commit() after the query? \r\n- Should Drill silently accept the setAutoCommit(false) even though it can't really implement it?  If so, should it silently accept the commit(), to make things look \"normal\" to calling tools? If so, then what about a call to rollback()? \r\n\r\nOne datapoint: We've seen Spotfire call setAutoCommit(false), issue a query, and call rollback() (presumably to make sure to avoid making unintended changes).",
        "Tpch13 is flapping with OOB exception On both TpchSingle and TpchDistributed, Tpch13 complete succesfully some of the time.  Example error:\r\n\r\njava.lang.IndexOutOfBoundsException: writerIndex: 2037150496 (expected: readerIndex(0) <= writerIndex <= capacity(3407820))\r\n\tio.netty.buffer.AbstractByteBuf.writerIndex(AbstractByteBuf.java:87) ~[netty-buffer-4.0.7.Final.jar:na]\r\n\torg.apache.drill.exec.memory.AccountingByteBuf.writerIndex(AccountingByteBuf.java:137) ~[classes/:na]\r\n\torg.apache.drill.exec.vector.VarBinaryVector$Mutator.setValueCount(VarBinaryVector.java:414) ~[test-classes/:na]\r\n\torg.apache.drill.exec.physical.impl.filter.FilterRecordBatch.doWork(FilterRecordBatch.java:83) ~[classes/:na]\r\n\torg.apache.drill.exec.record.AbstractSingleRecordBatch.next(AbstractSingleRecordBatch.java:62) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:111) ~[classes/:na]\r\n\torg.apache.drill.exec.record.AbstractSingleRecordBatch.next(AbstractSingleRecordBatch.java:42) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:111) ~[classes/:na]\r\n\torg.apache.drill.exec.record.AbstractSingleRecordBatch.next(AbstractSingleRecordBatch.java:42) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:111) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.aggregate.StreamingAggBatch.next(StreamingAggBatch.java:83) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:111) ~[classes/:na]\r\n\torg.apache.drill.exec.record.AbstractSingleRecordBatch.next(AbstractSingleRecordBatch.java:42) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:111) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.sort.SortBatch.next(SortBatch.java:112) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:111) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.aggregate.StreamingAggBatch.next(StreamingAggBatch.java:83) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:111) ~[classes/:na]\r\n\torg.apache.drill.exec.record.AbstractSingleRecordBatch.next(AbstractSingleRecordBatch.java:42) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:111) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.sort.SortBatch.next(SortBatch.java:112) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:111) ~[classes/:na]\r\n\torg.apache.drill.exec.record.AbstractSingleRecordBatch.next(AbstractSingleRecordBatch.java:42) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator.next(IteratorValidatorBatchIterator.java:111) ~[classes/:na]\r\n\torg.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.next(ScreenCreator.java:85) ~[classes/:na]\r\n\torg.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:83) ~[classes/:na]\r\n\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) [na:1.7.0_09]\r\n\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) [na:1.7.0_09]\r\n\tjava.lang.Thread.run(Thread.java:722) [na:1.7.0_09]\r\n"
    ],
    [
        "DRILL-3733",
        "DRILL-1931",
        "erro message fix - NTILE function Can we have the message read this way\r\n\"NTILE only accepts positive (non-zero) integer argument\"\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select col7 , col9 , ntile(0) over(order by col0) lastVal_col9 from FEWRWSPQQ_101;\r\nError: FUNCTION ERROR: NTILE only accepts positive integer argument\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 650d44a8-73d0-4675-8c18-67609f43a962 on centos-04.qa.lab:31010] (state=,code=0)\r\n{code}",
        "byte_substr() does not set the 'start' field of output holder correctly "
    ],
    [
        "DRILL-2284",
        "DRILL-2158",
        "Describe table returns no rows for Parquet / JSON / Text 'Describe' table should either support displaying schema information or fail indicating the lack of support for Parquet / JSON / Text \r\n\r\nCurrently no rows are returned, which might confuse an end user\r\n\r\n*Parquet/JSON/Text:*\r\n> describe store_sales;\r\n+--+\r\n|  |\r\n+--+\r\n+--+\r\nNo rows selected (0.07 seconds)\r\n\r\n*Hive:* \r\n> describe region;\r\n+-------------+------------+-------------+\r\n| COLUMN_NAME | DATA_TYPE  | IS_NULLABLE |\r\n+-------------+------------+-------------+\r\n| r_regionkey | INTEGER    | YES         |\r\n| r_name      | VARCHAR    | YES         |\r\n| r_comment   | VARCHAR    | YES         |\r\n+-------------+------------+-------------+\r\n\r\n*Views:*\r\n> describe ship_mode_par_view;\r\n+-------------+------------+-------------+\r\n| COLUMN_NAME | DATA_TYPE  | IS_NULLABLE |\r\n+-------------+------------+-------------+\r\n| *           | ANY        | NO          |\r\n+-------------+------------+-------------+\r\n\r\n",
        " Failure while attempting to start Drillbit in embedded mode.  First, I install my drill according to \u201chttps://cwiki.apache.org/confluence/display/DRILL/Apache+Drill+in+10+Minutes\u201d.\r\nWhen to start my drill via \"bin/sqlline -u jdbc:drill:zk=local -n admin -p admin\",\r\nIt shows \r\n\"Error: Failure while attempting to start Drillbit in embedded mode. (state=,code=0)\r\nsqlline version 1.1.6\r\n0: jdbc:drill:zk=local>\"\r\n\r\nThen I install my drill with maven according to \"INSTALL.md\" in the source from github. But the same result like above.\r\n\r\nFinally , in the path \"tmp/drill/\", there's nothing, do I need to create by myself?\r\nIs it necessary to build a distributed system for example hadoop?\r\nMuch apperaite!"
    ],
    [
        "DRILL-348",
        "DRILL-3958",
        "Improve parquet scanner to read nullable data source If you use Pig to convert your data source to parquet format, currently Drill will have trouble reading the converted file. By default, Pig will convert your data into nullable type. Drill will throw indexoutofboundsexception when reading the file. ",
        "Improve error message when JDBC driver not found When setting up a storage definition for JDBC in the Drill web UI, the appropriate driver has to be available in the 3rdparty folder before defining the storage, otherwise an error is displayed.\r\n\r\nThe error message refers to a JSON mapping error which is completely inappropriate in this case, because the error is the missing JDBC driver in the 3rdparty folder and not the JSON mapping.\r\n\r\nI request to change the error message to something appropriate that the class/driver referred to could not be found (like for example: com.mysql.jdbc.Driver)"
    ],
    [
        "DRILL-3077",
        "DRILL-598",
        "sqlline's return code is 0 even when it force exits due to failed sql command My SQL script looks like this:\r\n\r\n{code}\r\nselect * from sys.options limit 1;\r\nselect * sys.options; <--- from clause is missing\r\nselect * from sys.options limit 1;\r\n{code}\r\n\r\nsqlline correctly exists (--force is set to true by default).\r\nHowever, return code is '0', which makes scripting challenging.\r\nIt should be set to 1.\r\n{code}\r\n[Wed May 13 17:49:39 root@~ ] # ${DRILL_HOME}/bin/sqlline -u \"jdbc:drill:schema=dfs.ctas_parquet\"  --run=/root/script.sql\r\n1/5          select * from sys.options limit 1;\r\n+------------+------------+------------+------------+------------+------------+------------+------------+\r\n|    name    |    kind    |    type    |   status   |  num_val   | string_val |  bool_val  | float_val  |\r\n+------------+------------+------------+------------+------------+------------+------------+------------+\r\n| drill.exec.rpc.bit.server.retry.delay | LONG       | BOOT       | BOOT       | 500        | null       | null       | null       |\r\n+------------+------------+------------+------------+------------+------------+------------+------------+\r\n1 row selected (0.247 seconds)\r\n2/5          \r\n3/5          select * sys.options;\r\nError: PARSE ERROR: Encountered \".\" at line 1, column 13.\r\nWas expecting one of:\r\n    \"FROM\" ...\r\n    \",\" ...\r\n[Error Id: 9da00514-6a96-4d9a-b90a-c903d006c060 on atsqa4-133.qa.lab:31010] (state=,code=0)\r\nAborting command set because \"force\" is false and command failed: \"select * sys.options;\"\r\nClosing: org.apache.drill.jdbc.DrillJdbc41Factory$DrillJdbc41Connection\r\nsqlline version 1.1.6\r\n[Wed May 13 17:53:56 root@~ ] # echo $?\r\n0\r\n{code}",
        "should we support timestamptz data type? 0: jdbc:drill:schema=dfs> select cast(c_date as timestamptz) from data;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"7b141866-e832-42e1-a669-aa9c65ef4fb0\"\r\nendpoint {\r\n  address: \"qa-node119.qa.lab\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while parsing sql. < ValidationException:[ org.eigenbase.util.EigenbaseContextException: From line 1, column 23 to line 1, column 33 ] < EigenbaseContextException:[ From line 1, column 23 to line 1, column 33 ] < SqlValidatorException:[ Unknown datatype name \\'timestamptz\\' ]\"\r\n]\r\nError: exception while executing query (state=,code=0)"
    ],
    [
        "DRILL-872",
        "DRILL-2714",
        "Validate column names for strongly-typed tables such as parquet Currently, parquet tables, like other filesystem tables, are treated as schema-less tables. But it would make sense to get the schema by reading and merging the footers from all of the parquet files. This will allow drill to validate whether a requested column exists, and throw an error before running the query. It will also remove the need to qualify columns with the table name or alias when doing a join.",
        "Exchange should be removed if Drill scans a local system table For the query: \r\n\"select t1.name, t1.kind, t2.n_nationkey from (select * from sys.options) t1 join (select * from cp.`tpch/nation.parquet`) t2 on t1.name = t2.n_name;\" (TestStarQueries.testSelStarJoinSchemaWithSchemaLess)\r\n\r\nIf broadcast_join is disabled, the following plan will be generated:\r\n{code}\r\ntext\tjson\r\n00-00    Screen\r\n00-01      ProjectAllowDup(name=[$0], kind=[$1], n_nationkey=[$2])\r\n00-02        Project(name=[$0], kind=[$1], n_nationkey=[ITEM($7, 'n_nationkey')])\r\n00-03          HashJoin(condition=[=($0, $8)], joinType=[inner])\r\n00-04            Project(T0\u00a6\u00a6*=[$0], $f1=[ITEM($0, 'n_name')])\r\n00-06              Project(T0\u00a6\u00a6*=[$0])\r\n00-08                Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=/tpch/nation.parquet]], selectionRoot=/tpch/nation.parquet, numFiles=1, columns=[`*`]]])\r\n00-05            Project(name=[$0], kind=[$1], type=[$2], num_val=[$3], string_val=[$4], bool_val=[$5], float_val=[$6])\r\n00-07              HashToRandomExchange(dist0=[[$0]])\r\n01-01                UnorderedMuxExchange\r\n02-01                  Project(name=[$0], kind=[$1], type=[$2], num_val=[$3], string_val=[$4], bool_val=[$5], float_val=[$6], E_X_P_R_H_A_S_H_F_I_E_L_D=[castINT(hash64($0))])\r\n02-02                    Scan(groupscan=[SystemTableScan [table=OPTION, distributed=false]])\r\n{code}\r\n\r\nThe exchange on the SystemTableScan does not get removed. This will result in error (see the attachment). \r\n\r\n"
    ],
    [
        "DRILL-2275",
        "DRILL-3205",
        "need implementations of sys tables for drill memory and threads profiles In order to check drill state information, the following tables are to be implemented:\r\n\r\n1. Memory: a query such as\r\n\r\nselect * from sys.drillmemory;\r\n\r\nshould return a result set like the following:\r\n{code}\r\n+------------+------------+--------------+------------+------------+\r\n|    drillbit    | total_sys_memory   |heap_size | direct_alloc_memory |\r\n+------------+------------+--------------+------------+------------+\r\n| node1:port1       | 24596676k         | 15200420k         | 1012372k   |\r\n+------------+------------+--------------+------------+------------+\r\n| node2:port2       | 24596676k         | 15200420k         | 2012372k   |\r\n+------------+------------+--------------+------------+------------+\r\n{code}\r\n2. Threads:\r\nFor each node in a cluster, we need counts of threads of the drillbits.  A query like this:\r\n\r\nselect * from sys.drillbitthreads;\r\n\r\nshould return a result set like the following:\r\n{code}\r\n+------------+------------+--------------+------------+------------+\r\n|    drillbit    | pool_name   | total_threads | busy_threads |\r\n+------------+------------+--------------+------------+------------+\r\n| node1:port1       | pool1         | 8         | 2   |\r\n+------------+------------+--------------+------------+------------+\r\n| node2:port2       | pool2         | 10         | 5   |\r\n+------------+------------+--------------+------------+------------+\r\n{code}",
        "Test framework should report actual values in case of failures Test framework seems to report expected results alone. This makes it particularly hard to investigate and reason about the problem. Even in case where test result matcher is un-ordered we can report actual results if returned number of rows & columns is manageable (say r x c \u2264 100)\r\n\r\n{code:title=Sample test failure that reports expected result but not the actual value}\r\nTests run: 43, Failures: 0, Errors: 1, Skipped: 3, Time elapsed: 30.168 sec <<< FAILURE! - in org.apache.drill.TestFunctionsQuery\r\ntestToCharFunction(org.apache.drill.TestFunctionsQuery)  Time elapsed: 0.112 sec  <<< ERROR!\r\njava.lang.Exception: Did not find expected record in result set: `DEC28_1` : 12,345,678,912,345,678,912.5567, `DEC38_1` : 999999999999999999999999999.5, `DEC9_1` : 1,234.56, `DEC18_1` : 99999912399.9567, `FLOAT8_1` : 1,234.56, `FLOAT8_2` : $1,234.50, \r\n\r\n\tat org.apache.drill.DrillTestWrapper.compareResults(DrillTestWrapper.java:541)\r\n\tat org.apache.drill.DrillTestWrapper.compareUnorderedResults(DrillTestWrapper.java:295)\r\n\tat org.apache.drill.DrillTestWrapper.run(DrillTestWrapper.java:119)\r\n\tat org.apache.drill.TestBuilder.go(TestBuilder.java:125)\r\n\tat org.apache.drill.TestFunctionsQuery.testToCharFunction(TestFunctionsQuery.java:517)\r\n{code}"
    ],
    [
        "DRILL-1050",
        "DRILL-3293",
        "Tpch Query 3 fails with a verification error for a scale factor of 100 git.commit.id.abbrev=894037a\r\nBuild ID : 26156-1\r\n\r\nNo of nodes in cluster : 22\r\nplanner.width.max_per_node : 4\r\n\r\nBelow are the details of the execution. Let me know if you need more information\r\n\r\nSize of expected result set: 10\r\nSize of result set from Drill: 10\r\nTotal number of unexpected rows: 4\r\n\r\n*****************ACTUAL******************\r\n277818180       457185.6052     1995-03-13      0\r\n255100807       458115.5767     1995-03-08      0\r\n233189124       457750.4448     1995-03-16      0\r\n258432482       456372.2707     1995-03-11      0\r\n\r\n\r\n****************EXPECTED***************\r\n493528132       468488.9059     1995-02-26      0 \r\n271880324       464526.73529999994      1995-03-02      0\r\n191691776       472636.1952000001       1995-03-14      0 \r\n501322081       496480.35899999994      1995-02-04      0 ",
        "CTAS with window function fails with UnsupportedOperationException {code}\r\n0: jdbc:drill:schema=dfs> create table wf_t1 as select sum(a1) over(partition by a1) from t1;\r\nError: SYSTEM ERROR:\r\n\r\nFragment 0:0\r\n\r\n[Error Id: 96897b46-70c0-4373-9d85-ca7501cb1479 on atsqa4-133.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\ndrillbit.log\r\n{code}\r\n[Error Id: bde0d90b-7eaa-4772-9316-9c58a46b01d2 on atsqa4-133.qa.lab:31010]\r\norg.apache.drill.common.exceptions.UserException: SYSTEM ERROR:\r\n\r\nFragment 0:0\r\n\r\n[Error Id: bde0d90b-7eaa-4772-9316-9c58a46b01d2 on atsqa4-133.qa.lab:31010]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:522) ~[drill-common-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.sendFinalState(FragmentExecutor.java:325) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.cleanup(FragmentExecutor.java:181) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:294) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_71]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]\r\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]\r\nCaused by: java.lang.UnsupportedOperationException: null\r\n        at org.apache.drill.exec.expr.TypeHelper.getValueVectorClass(TypeHelper.java:674) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.VectorContainer.addOrGet(VectorContainer.java:82) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.setupNewSchema(ProjectRecordBatch.java:421) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:78) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:146) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:105) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:95) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:146) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:105) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:95) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.WriterRecordBatch.innerNext(WriterRecordBatch.java:92) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:146) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:105) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:95) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:51) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:146) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:83) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:79) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:73) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor$1.run(FragmentExecutor.java:260) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor$1.run(FragmentExecutor.java:254) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at java.security.AccessController.doPrivileged(Native Method) ~[na:1.7.0_71]\r\n        at javax.security.auth.Subject.doAs(Subject.java:415) ~[na:1.7.0_71]\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1566) ~[hadoop-common-2.5.1-mapr-1503.jar:na]\r\n        at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:254) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        ... 4 common frames omitted\r\n2015-06-15 17:18:04,909 [BitServer-4] INFO  o.a.drill.exec.work.foreman.Foreman - State change requested.  RUNNING --> FAILED\r\norg.apache.drill.common.exceptions.UserRemoteException: SYSTEM ERROR:\r\n\r\nFragment 0:0\r\n{code}"
    ],
    [
        "DRILL-2641",
        "DRILL-4289",
        "Move unrelated tests in exec/jdbc module into appropriate modules Move following unreleated tests out of \"exec/jdbc\" into appropriate modules.\r\n\r\n{{jdbc.TestHiveStoreage.java}} into \"contrib/storage-hive/core\"\r\nSplit {{jdbc.TestMetadataDDL.java}} into \"exec/java-exec\" and \"contrib/storage-hive/core\" modules.\r\nRemove redundant tests {{TestHiveScalarUDFs.java}}",
        "window functions give different results if star is used in inner query The following queries give different results, although they are similar:\r\n{noformat}\r\nSELECT position_id, COUNT(*) OVER w AS `count` FROM (SELECT position_id FROM `/b1p2tbl` ORDER BY employee_id, sub) WINDOW w AS (PARTITION BY position_id);\r\n+--------------+--------+\r\n| position_id  | count  |\r\n+--------------+--------+\r\n| 1            | 10     |\r\n| 1            | 10     |\r\n| 1            | 10     |\r\n| 1            | 10     |\r\n| 1            | 10     |\r\n| 1            | 10     |\r\n| 1            | 10     |\r\n| 1            | 10     |\r\n| 1            | 10     |\r\n| 1            | 10     |\r\n| 2            | 10     |\r\n| 2            | 10     |\r\n| 2            | 10     |\r\n| 2            | 10     |\r\n| 2            | 10     |\r\n| 2            | 10     |\r\n| 2            | 10     |\r\n| 2            | 10     |\r\n| 2            | 10     |\r\n| 2            | 10     |\r\n+--------------+--------+\r\n{noformat}\r\n\r\n{noformat}\r\nSELECT position_id, COUNT(*) OVER w AS `count` FROM (SELECT * FROM dfs.data.`b1p2tbl` ORDER BY employee_id, sub) WINDOW w AS (PARTITION BY position_id);\r\n+--------------+--------+\r\n| position_id  | count  |\r\n+--------------+--------+\r\n| 1            | 20     |\r\n| 1            | 20     |\r\n| 1            | 20     |\r\n| 1            | 20     |\r\n| 1            | 20     |\r\n| 1            | 20     |\r\n| 1            | 20     |\r\n| 1            | 20     |\r\n| 1            | 20     |\r\n| 1            | 20     |\r\n| 2            | 20     |\r\n| 2            | 20     |\r\n| 2            | 20     |\r\n| 2            | 20     |\r\n| 2            | 20     |\r\n| 2            | 20     |\r\n| 2            | 20     |\r\n| 2            | 20     |\r\n| 2            | 20     |\r\n| 2            | 20     |\r\n+--------------+--------+\r\n{noformat}\r\n\r\nthe results of the second query are incorrect."
    ],
    [
        "DRILL-2829",
        "DRILL-946",
        "Info. schema hygiene (for upcoming fixes) ",
        "sum(decimal18) only give precession 9 result #Mon Jun 09 10:22:40 PDT 2014\r\ngit.commit.id.abbrev=fb2091a\r\n\r\nWhen I sum() a decimal18, I used to get result with P18, with this build, I only get P9. Looks like a regression.\r\n\r\nHere is the query through drill:\r\n\r\n0: jdbc:drill:schema=dfs> select sum(cast(c_decimal9 as decimal(18,9))) from data;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 1.23582883E8 |\r\n+------------+\r\n\r\nAnd I used to get\r\n\r\n1.2358288447063999E8"
    ],
    [
        "DRILL-911",
        "DRILL-3171",
        "Group by queries don't work with hbase Group by queries for an hbase table return UnsupportedOperationException.\r\n{code}\r\n0: jdbc:drill:zk=localhost:5181> select `account` from `hbase`.`students` group by `account`;\r\nQuery failed: org.apache.drill.exec.rpc.RpcException: Remote failure while running query.[error_id: \"9e89db54-6f31-4247-9db1-c1cb804d9948\"\r\nendpoint {\r\n  address: \"192.168.39.43\"\r\n  user_port: 31010\r\n  control_port: 31011\r\n  data_port: 31012\r\n}\r\nerror_type: 0\r\nmessage: \"Failure while running fragment. < UnsupportedOperationException\"\r\n]\r\nError: exception while executing query (state=,code=0)\r\n{code}",
        "Storage Plugins : Two processes tried to update the storage plugin at the same time Commit Id# : bd8ac4fca03ad5043bca27fbc7e0dec5a35ac474\r\n\r\nWe have seen this issue happen with the below steps\r\n   1. Clear out the zookeeper\r\n   2. Update the storage plugin using the rest API on one of the node\r\n   3. Submit 10 queries concurrently\r\n\r\nWith randomized foreman node selection, the node executing the query might not have the updated storage plugins info. This could be causing the issue.\r\n\r\n- Rahul"
    ],
    [
        "DRILL-1566",
        "DRILL-1701",
        "C++Client does not handle incoming record batches with zero records A simple query like 'select * from cp.`tpch/lineitem.parquet` where L_ORDERKEY=0 will result in a record batchsent back from the server with only the schema information set correctly. The C++ client still tries to go ahead and build the record batch and the building of value vectors asserts.\r\n\r\n",
        "Drill chooses the optimized reader that only supports flat data, rather than the one that supports complex in the case where there is a simple non-repeated map type "
    ],
    [
        "DRILL-996",
        "DRILL-3671",
        "Build - allow other versions of Hadoop to be specified on command-line Right now, in order to change the version of Hadoop that Drill is built against, you have to edit the POM. However, for automated build systems that build Drill against a variety of possible Hadoop versions, it would be much cleaner to be able to specify it on the command-line.",
        "UNION infinite PENDING status Querying a View containing more than 7 UNION clause on the same table, leads the query to remain infinitely in PENDING status. The Physical Plan is not created. \r\n\r\ndata_balance_sheet.csv :\r\naccount|m1|m2|m3|m4|m5|m6|m7|m8|m9|m10|m11|m12\r\nA|3058.77|450.12|257390.92|58104.74|9376.08|109.28|13.24|2149.25|1962.30|1076.59|530.98|44918.63\r\n\r\nSELECT columns[0] FROM dfs.tmp.`data_balance_sheet.csv` \r\n=>\r\n+---------+\r\n| EXPR$0  |\r\n+---------+\r\n| A      |\r\n+---------+\r\n\r\nView:\r\nCREATE OR REPLACE VIEW dfs.tmp.view_balance_sheet AS (\r\nSELECT CAST(columns[0] AS Varchar(20)) account, '01' Period, CAST(columns[1] AS Varchar(20)) Val FROM dfs.tmp.`data_balance_sheet.csv`\r\nUNION \r\nSELECT CAST(columns[0] AS Varchar(20)) account, '02' Period, CAST(columns[2] AS Varchar(20)) Val FROM dfs.tmp.`data_balance_sheet.csv`\r\nUNION \r\nSELECT CAST(columns[0] AS Varchar(20)) account, '03' Period, CAST(columns[3] AS Varchar(20)) Val FROM dfs.tmp.`data_balance_sheet.csv`\r\nUNION \r\nSELECT CAST(columns[0] AS Varchar(20)) account, '04' Period, CAST(columns[4] AS Varchar(20)) Val FROM dfs.tmp.`data_balance_sheet.csv`\r\nUNION \r\nSELECT CAST(columns[0] AS Varchar(20)) account, '05' Period, CAST(columns[5] AS Varchar(20)) Val FROM dfs.tmp.`data_balance_sheet.csv`\r\nUNION \r\nSELECT CAST(columns[0] AS Varchar(20)) account, '06' Period, CAST(columns[6] AS Varchar(20)) Val FROM dfs.tmp.`data_balance_sheet.csv`\r\nUNION \r\nSELECT CAST(columns[0] AS Varchar(20)) account, '07' Period, CAST(columns[7] AS Varchar(20)) Val FROM dfs.tmp.`data_balance_sheet.csv`\r\nUNION \r\nSELECT CAST(columns[0] AS Varchar(20)) account, '08' Period, CAST(columns[8] AS Varchar(20)) Val FROM dfs.tmp.`data_balance_sheet.csv`\r\nUNION \r\nSELECT CAST(columns[0] AS Varchar(20)) account, '09' Period, CAST(columns[9] AS Varchar(20)) Val FROM dfs.tmp.`data_balance_sheet.csv`\r\nUNION \r\nSELECT CAST(columns[0] AS Varchar(20)) account, '10' Period, CAST(columns[10] AS Varchar(20)) Val FROM dfs.tmp.`data_balance_sheet.csv`\r\nUNION \r\nSELECT CAST(columns[0] AS Varchar(20)) account, '11' Period, CAST(columns[11] AS Varchar(20)) Val FROM dfs.tmp.`data_balance_sheet.csv`\r\nUNION \r\nSELECT CAST(columns[0] AS Varchar(20)) account, '12' Period, CAST(columns[12] AS Varchar(20)) Val FROM dfs.tmp.`data_balance_sheet.csv`\r\n);\r\n\r\n=>\r\nView 'view_balance_sheet' replaced successfully in 'dfs.tmp' schema\r\n\r\nSELECT * FROM dfs.tmp.view_balance_sheet;\r\n=> Nothing appends, status remains PENDING, no Physical Plan is created\r\n\r\n\r\n\r\n\r\n"
    ],
    [
        "DRILL-3286",
        "DRILL-3151",
        "IN clause with null in it results in AssertionError: Error while applying rule DrillValuesRule Query that uses IN clause and there is a null as a value specified inside the IN clause we see an UnsupportedOperationException and AssertionError: Internal error: Error while applying rule DrillValuesRule.\r\nTest was executed on 4 node cluster on CentOS.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select * from tblWnulls where c2 in ('a','b','c',null);\r\nError: SYSTEM ERROR: java.lang.UnsupportedOperationException: Unable to convert the value of null and type ANY to a Drill constant expression.\r\n\r\n\r\n[Error Id: ecd34f5c-ca9e-46a1-87bb-f7257b155de4 on centos-01.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nData in the table (it is coming from a Parquet file)\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.tmp> select c1, c2 from tblWnulls;\r\n+-------------+-------+\r\n|     c1      |  c2   |\r\n+-------------+-------+\r\n| 1           | a     |\r\n| 2           | b     |\r\n| 13          | c     |\r\n| 4           | c     |\r\n| 5           | a     |\r\n| 6           | c     |\r\n| null        | d     |\r\n| 17          | b     |\r\n| 8           | c     |\r\n| 9           | b     |\r\n| 10          | d     |\r\n| 2147483647  | d     |\r\n| 10          | a     |\r\n| 11          | a     |\r\n| null        | c     |\r\n| 11          | d     |\r\n| 12          | c     |\r\n| 19          | null  |\r\n| 13          | b     |\r\n| 14          | a     |\r\n| 13          | c     |\r\n| 15          | e     |\r\n| -1          | e     |\r\n| 0           | a     |\r\n| 2147483647  | d     |\r\n| null        | d     |\r\n| 65536       | null  |\r\n| 1000000     | null  |\r\n| null        | null  |\r\n| 11111       | a     |\r\n+-------------+-------+\r\n30 rows selected (0.169 seconds)\r\n{code}\r\n\r\nStack trace from dill bit.log\r\n{code}\r\n[Error Id: ecd34f5c-ca9e-46a1-87bb-f7257b155de4 on centos-01.qa.lab:31010]\r\norg.apache.drill.common.exceptions.UserException: SYSTEM ERROR: java.lang.UnsupportedOperationException: Unable to convert the value of null and type ANY to a Drill constant expression.\r\n\r\n\r\n[Error Id: ecd34f5c-ca9e-46a1-87bb-f7257b155de4 on centos-01.qa.lab:31010]\r\n        at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:522) ~[drill-common-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$ForemanResult.close(Foreman.java:738) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:840) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.processEvent(Foreman.java:782) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.common.EventProcessor.sendEvent(EventProcessor.java:73) [drill-common-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman$StateSwitch.moveToState(Foreman.java:784) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.moveToState(Foreman.java:893) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:253) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]\r\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]\r\nCaused by: org.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: Internal error: Error while applying rule DrillValuesRule, args [rel#5149:LogicalValues.NONE.ANY([]).[[0]](type=RecordType(ANY ROW_VALUE),tuples=[{ 'a' }, { 'b' }, { 'c' }, { null }])]\r\n        ... 4 common frames omitted\r\nCaused by: java.lang.AssertionError: Internal error: Error while applying rule DrillValuesRule, args [rel#5149:LogicalValues.NONE.ANY([]).[[0]](type=RecordType(ANY ROW_VALUE),tuples=[{ 'a' }, { 'b' }, { 'c' }, { null }])]\r\n        at org.apache.calcite.util.Util.newInternal(Util.java:790) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:251) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:795) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:303) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.calcite.prepare.PlannerImpl.transform(PlannerImpl.java:316) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.logicalPlanningVolcanoAndLopt(DefaultSqlHandler.java:507) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.convertToDrel(DefaultSqlHandler.java:281) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.handlers.DefaultSqlHandler.getPlan(DefaultSqlHandler.java:187) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.sql.DrillSqlWorker.getPlan(DrillSqlWorker.java:178) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.runSQL(Foreman.java:904) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.work.foreman.Foreman.run(Foreman.java:242) [drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        ... 3 common frames omitted\r\nCaused by: java.lang.UnsupportedOperationException: Unable to convert the value of null and type ANY to a Drill constant expression.\r\n        at org.apache.drill.exec.planner.logical.DrillValuesRel.writeLiteral(DrillValuesRel.java:294) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.logical.DrillValuesRel.convertToJsonNode(DrillValuesRel.java:152) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.logical.DrillValuesRel.<init>(DrillValuesRel.java:77) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.drill.exec.planner.logical.DrillValuesRule.onMatch(DrillValuesRule.java:40) ~[drill-java-exec-1.1.0-SNAPSHOT-rebuffed.jar:1.1.0-SNAPSHOT]\r\n        at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:228) ~[calcite-core-1.1.0-drill-r7.jar:1.1.0-drill-r7]\r\n        ... 12 common frames omitted\r\n{code}",
        "ResultSetMetaData not as specified by JDBC (null/dummy value, not \"\"/etc.) In Drill's JDBC driver, some ResultSetMetaData methods don't return what JDBC specifies they should return.\r\n\r\nSome cases:\r\n\r\n{{getTableName(int)}}:\r\n- (JDBC says: {{table name or \"\" if not applicable}})\r\n- Drill returns {{null}} (instead of empty string or table name)\r\n- (Drill indicates \"not applicable\" even when from named table, e.g., for  \"{{SELECT * FROM INFORMATION_SCHEMA.CATALOGS}}\".)\r\n\r\n{{getSchemaName(int)}}:\r\n- (JDBC says: {{schema name or \"\" if not applicable}})\r\n- Drill returns \"{{\\-\\-UNKNOWN--}}\" (instead of empty string or schema name)\r\n- (Drill indicates \"not applicable\" even when from named table, e.g., for  \"{{SELECT * FROM INFORMATION_SCHEMA.CATALOGS}}\".)\r\n\r\n{{getCatalogName(int)}}:\r\n- (JDBC says: {{the name of the catalog for the table in which the given column appears or \"\" if not applicable}})\r\n- Drill returns \"{{\\-\\-UNKNOWN--}}\" (instead of empty string or catalog name)\r\n- (Drill indicates \"not applicable\" even when from named table, e.g., for  \"{{SELECT * FROM INFORMATION_SCHEMA.CATALOGS}}\".)\r\n\r\n{{isSearchable(int)}}:\r\n- (JDBC says:  {{Indicates whether the designated column can be used in a where clause.}})\r\n- Drill returns {{false}}.\r\n\r\n{{getColumnClassName(int}}:\r\n- (JDBC says: {{the fully-qualified name of the class in the Java programming language that would be used by the method ResultSet.getObject to retrieve the value in the specified column. This is the class name used for custom mapping.}})\r\n- Drill returns \"{{none}}\" (instead of the correct class name).\r\n\r\nMore cases:\r\n\r\n{{getColumnDisplaySize}}\r\n- (JDBC says (quite ambiguously): {{the normal maximum number of characters allowed as the width of the designated column}})\r\n- Drill always returns {{10}}!\r\n"
    ],
    [
        "DRILL-4099",
        "DRILL-3326",
        "DRILL QUERY LIMIT ERROR I query `select attr from table limit 65536`, then I get error : org.apache.drill.common.exceptions.UserRemoteException: SYSTEM ERROR: IndexOutOfBoundsException: index: 131072, length: 2 (expected: range(0, 131072)) Fragment 0:0",
        "Query with unsupported windows function containing \"AS\" blocks correct error message The following query contains \"AS\"  using un-supported function first_value gives incorrect error:\r\n\r\nselect FIRST_VALUE(voter_id) OVER (PARTITION BY age ORDER BY contributions ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING) as t from voter_hive;\r\nError: SYSTEM ERROR: org.apache.drill.exec.exception.SchemaChangeException: Failure while materializing expression. \r\nError in expression at index -1.  Error: Missing function implementation: [first_value(INT-OPTIONAL)].  Full expression: --UNKNOWN EXPRESSION--.\r\n\r\nAfter I remove the \"AS\" keyword, the correct error message is displayed:\r\nselect FIRST_VALUE(voter_id) OVER (PARTITION BY age ORDER BY contributions ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING) from voter_hive;\r\nError: UNSUPPORTED_OPERATION ERROR: The window function FIRST_VALUE is not supported\r\nSee Apache Drill JIRA: DRILL-3195"
    ],
    [
        "DRILL-1671",
        "DRILL-1913",
        "Incorrect results reported by drill when we have more than  10 flattens (2048 records) git.commit.id.abbrev=60aa446\r\nI ran the below test against the private branch of Jason which has some patches for bugs related to flatten which are not yet merged into the master.\r\n\r\nThe data is in such a way that each array within the record contains only 2 records. So with each flatten added to the query the no of rows should get doubled\r\n\r\nThe below query works as expected\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir>select count(*) from (select id, flatten(evnts1), flatten(evnts2), flatten(evnts3), flatten(evnts4), flatten(evnts5), flatten(evnts6), flatten(evnts7), flatten(evnts8), flatten(evnts9), flatten(evnts10) from `json_kvgenflatten/many-arrays-50.json`) ;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 1024       |\r\n+------------+\r\n{code}\r\n\r\nHowever the below query reports incorrect results. The correct output is 2048.\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDir> select count(*) from (select id, flatten(evnts1), flatten(evnts2), flatten(evnts3), flatten(evnts4), flatten(evnts5), flatten(evnts6), flatten(evnts7), flatten(evnts8), flatten(evnts9), flatten(evnts10), flatten(evnts11) from `json_kvgenflatten/many-arrays-50.json`) ;\r\n+------------+\r\n|   EXPR$0   |\r\n+------------+\r\n| 2047       |\r\n+------------+\r\n{code}\r\n\r\nFrom here on no matter how many flattens we add to the query, the output still remains the same. However the duration of the query seems to more and more with each new flatten added.\r\n\r\nI attached the data file. Let me know if you have any questions.",
        "Drill did not correctly preserve output field's case sensitivity For the following query,\r\n\r\n{code}\r\nselect EMPLOYEE_ID from cp.`employee.json` limit 2;\r\n+-------------+\r\n| employee_id |\r\n+-------------+\r\n| 1           |\r\n| 2           |\r\n+-------------+\r\n{code}\r\n\r\nEven though upper-case EMPLOYEE_ID matches the field 'employee_id' in the input JSON file, Drill did not preset the upper-case in the output result, which seems to be not right.\r\n\r\nThe plan coming out of optiq/calcite seems right, as it has a project which will do the renaming. But Drill's logical planning will remove such project, hence not being able to preserve the upper-case in the output.\r\n"
    ],
    [
        "DRILL-3037",
        "DRILL-2351",
        "Unable to query on hdfs after moving to 0.9.0 version I  recently moved from drill 0.8.0 to 0.9.0. Since then,I am unable to query on json files present in hadoop file system( queries working fine on other storage plugins cp and dfs) \r\n\r\nI had registered a storage plugin named hadoop in drill. I had all my files in /user/hadoop in hdfs. \r\n\r\nWhen I query \"use hadoop;\" it returns \"true\". But from drill when I query \"show files in hadoop.`/user/hadoop/`;\" gives me following error\r\n\r\nQuery failed: SYSTEM ERROR: Failure handling SQL.\r\nError: exception while executing query: Failure while executing query. (state=,code=0)\r\n\r\nAnd when I do query \"select * from hadoop.`/user/hadoop/donuts.json`;\" gives me the following error\r\n\r\nQuery failed: PARSE ERROR: From line 1, column 15 to line 1, column 20: Table 'hadoop./home/hadoop/donuts.json' not found\r\n\r\nSame queries worked fine when I use apache drill 0.8.0.\r\n\r\nSo are there any extra things I should configure in drill 0.9.0 or is it a bug?",
        "Fix TestParquetWriter  In TestParquetWriter we have a utility method runTestAndValidate() to create a parquet table and validate the results with a subsequent query. However we  only seem to be creating the test builder required for the validation and never actually run it. I tried modifying runTestAndValidate() to run validation and it causes memory leaks and other exceptions. \r\n\r\nNeed to fix this class so that we actually perform the validation of the CTAS."
    ],
    [
        "DRILL-2188",
        "DRILL-3194",
        "JDBC should default to getting complex data as JSON Currently the ODBC driver gets complex data as a JSON string while the JDBC driver gets complex data as a complex type which it then converts to JSON. The conversion to JSON in the JDBC path uses an expensive method that also consumes excessive amounts of CPU.\r\nSince client applications are unable to consume complex data, the default should be to get JSON data and there should be a client side setting (session paramater) to revert to getting  complex data.",
        "TestDrillbitResilience#memoryLeaksWhenFailed hangs TestDrillbitResilience#memoryLeaksWhenFailed hangs and fails when run multiple times. This might be related to DRILL-3163."
    ],
    [
        "DRILL-312",
        "DRILL-3227",
        "Modularize org.apache.drill.exec.physical.impl.ImplCreator using operator creator registry The class \"org.apache.drill.exec.physical.impl.ImplCreator\" currently implements multiple methods which can be replaced by a single visitOp() using some sort of operator creator registry.\r\n\r\nThis will also enable plugging external PhysicalOperator.",
        "Planning Error : Self join between a view and underlying file fails to plan when the view contains missing columns git.commit.id.abbrev=6f54223\r\n\r\nThe below view is created with non-existent columns\r\n{code}\r\ncreate or replace view v1(x,y) as select col1, col2 from `a.json`;\r\n+-------+-------------------------------------------------------------------+\r\n|  ok   |                              summary                              |\r\n+-------+-------------------------------------------------------------------+\r\n| true  | View 'v1' created successfully in 'dfs.drillTestDirViews' schema  |\r\n+-------+-------------------------------------------------------------------+\r\n1 row selected (0.142 seconds)\r\n0: jdbc:drill:schema=dfs_eea> select * from v1;\r\n+-------+-------+\r\n|   x   |   y   |\r\n+-------+-------+\r\n| null  | null  |\r\n+-------+-------+\r\n1 row selected (0.129 seconds)\r\n{code}\r\n\r\nNow when I try to join the view with the underlying file, I get a planning error\r\n{code}\r\n0: jdbc:drill:schema=dfs_eea> select c1 from `a.json` a inner join v1 on a.c1 = v1.x;\r\nError: SYSTEM ERROR: org.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: null\r\n\r\n\r\n[Error Id: d38b4e61-d56f-4cd6-97dd-787deaebd33f on qa-node191.qa.lab:31010] (state=,code=0)\r\n\r\nexplain plan for select c1 from `a.json` a inner join v1 on a.c1 = v1.x;\r\nError: SYSTEM ERROR: org.apache.drill.exec.work.foreman.ForemanException: Unexpected exception during fragment initialization: null\r\n\r\n\r\n[Error Id: af10a933-ddf2-4bca-94a7-7b0383c9a1d4 on qa-node191.qa.lab:31010] (state=,code=0)\r\n{code}\r\n\r\nI attached the data and the error log"
    ],
    [
        "DRILL-2309",
        "DRILL-120",
        "Selecting count(), avg() of nullable columns causes wrong results #Thu Feb 19 18:40:10 EST 2015\r\ngit.commit.id.abbrev=1ceddff\r\n\r\nThe following query returns correct count involving columns that contains null value.\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> select tt.gbyi, count(tt.nul) from (select t.id, t.gbyi, t.fl, t.nul from `complex.json` t) tt group by tt.gbyi order by tt.gbyi;\r\n+------------+------------+\r\n|    gbyi    |   EXPR$1   |\r\n+------------+------------+\r\n| 0          | 33580      |\r\n| 1          | 33317      |\r\n| 2          | 33438      |\r\n| 3          | 33535      |\r\n| 4          | 33369      |\r\n| 5          | 32990      |\r\n| 6          | 33661      |\r\n| 7          | 33130      |\r\n| 8          | 33362      |\r\n| 9          | 33364      |\r\n| 10         | 33229      |\r\n| 11         | 33567      |\r\n| 12         | 33379      |\r\n| 13         | 33045      |\r\n| 14         | 33305      |\r\n+------------+------------+\r\n{code}\r\n\r\nBut if you add more aggregation to the query, the returned count is wrong (pay attention to the last column). \r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> select tt.gbyi, sum(tt.id), avg(tt.fl), count(tt.nul) from (select t.id, t.gbyi, t.fl, t.nul from `complex.json` t) tt group by tt.gbyi order by tt.gbyi;\r\n+------------+------------+------------+------------+\r\n|    gbyi    |   EXPR$1   |   EXPR$2   |   EXPR$3   |\r\n+------------+------------+------------+------------+\r\n| 0          | 33445554017 | 499613.0956877819 | 66943      |\r\n| 1          | 33209358334 | 500760.0252919893 | 66318      |\r\n| 2          | 33369118041 | 498091.82200273 | 66994      |\r\n| 3          | 33254533860 | 498696.5063226428 | 66683      |\r\n| 4          | 33393965595 | 501125.64656145993 | 66638      |\r\n| 5          | 33216885506 | 499961.32710397616 | 66439      |\r\n| 6          | 33380205950 | 498875.3923256599 | 66911      |\r\n| 7          | 33405849390 | 501093.43067788356 | 66666      |\r\n| 8          | 33136951190 | 498458.1044031481 | 66479      |\r\n| 9          | 33319291474 | 499967.5392457864 | 66643      |\r\n| 10         | 33339388887 | 499190.47462408233 | 66787      |\r\n| 11         | 33571590550 | 502095.86682194035 | 66863      |\r\n| 12         | 33437342090 | 501708.8141502653 | 66647      |\r\n| 13         | 33071800925 | 498896.453904129 | 66290      |\r\n| 14         | 33448664191 | 501487.4206955959 | 66699      |\r\n+------------+------------+------------+------------+\r\n[code}\r\n\r\nplan for the query returned the wrong result:\r\n\r\n{code}\r\n0: jdbc:drill:schema=dfs.drillTestDirComplexJ> explain plan for select tt.gbyi, sum(tt.id), avg(tt.fl), count(tt.nul) from (select t.id, t.gbyi, t.fl, t.nul from `complex.json` t) tt group by tt.gbyi order by tt.gbyi;\r\n+------------+------------+\r\n|    text    |    json    |\r\n+------------+------------+\r\n| 00-00    Screen\r\n00-01      Project(gbyi=[$0], EXPR$1=[$1], EXPR$2=[$2], EXPR$3=[$3])\r\n00-02        SingleMergeExchange(sort0=[0 ASC])\r\n01-01          SelectionVectorRemover\r\n01-02            Sort(sort0=[$0], dir0=[ASC])\r\n01-03              Project(gbyi=[$0], EXPR$1=[CASE(=($2, 0), null, $1)], EXPR$2=[CAST(/(CastHigh(CASE(=($4, 0), null, $3)), $4)):ANY], EXPR$3=[$5])\r\n01-04                HashAgg(group=[{0}], agg#0=[$SUM0($1)], agg#1=[$SUM0($2)], agg#2=[$SUM0($3)], agg#3=[$SUM0($4)], EXPR$3=[$SUM0($5)])\r\n01-05                  HashToRandomExchange(dist0=[[$0]])\r\n02-01                    HashAgg(group=[{0}], agg#0=[$SUM0($1)], agg#1=[COUNT($1)], agg#2=[$SUM0($2)], agg#3=[COUNT($2)], EXPR$3=[COUNT()])\r\n02-02                      Project(gbyi=[$3], id=[$2], fl=[$1], nul=[$0])\r\n02-03                        Scan(groupscan=[EasyGroupScan [selectionRoot=/drill/testdata/complex_type/json/complex.json, numFiles=1, columns=[`gbyi`, `id`, `fl`, `nul`], files=[maprfs:/drill/testdata/complex_type/json/complex.json]]])\r\n{code}\r\n",
        "Record Batch Level UDF support "
    ]
]