[
    [
        "HBASE-187",
        "HBASE-141",
        "The hbase  DFSAbort test is failing causing the complete test to fail. When I do 'ant test' or 'ant test-contrib' the tests fail just because of the DFSAbort timeout. The logs do not convey any useful information.",
        "[hbase] When hdfs is yanked out from under hbase, hbase should go down gracefully Currenlty, if hdfs is shutdown under a running hbase, hbase just hangs there trying to relocate its missing filesystem.    hbase should go down as graciously as possible."
    ],
    [
        "HBASE-206",
        "HBASE-209",
        "hbase table filename problem running ver 0.15.0 \n\nI store web pages in hbase and use the urls as row keys like google does with bigtable but it seams that with foward slashes ( / ) as the row key breaks the path for the hbase filenames example\n\nstarting off one of my tables has this file name\nhregion_webdata,,-3862545529986602998\n\nbut when it trys to split it will have these file names\nhregion_webdata,,-3862545529986602998\nhregion_webdata,com.tripod.beifaust/robots.txt:http,837745221057372860\n\nThe / in the row key from the url is breaking the path name to the hregion file. This causes the region server to exit. I assume it would kill all region servers in a pool by assigning each one the table and each would die on trying the split the table.\n\nEasy solution for this would be the key/filename need to be escaped but I am not sure how thats done in java.",
        "[hbase] HLog generates incorrect file name when splitting a log, race  condition also contributes In Hadoop-Nightly #277 TestRegionServerExit failed with a timeout.\n\nThe reason for this was a race in the Master in which checkAssigned (run from either the root or meta scanner)  will immediately try to split the log and then assign a region which has invalid server info.\n\nThe scenario went something like this:\n\n1. region server aborted\n2. root region was written on optional cache flush\nlease timed out on aborted server which removes it from serversToServerInfo and queues a PendingServerShutdown operation\n3. root scanner runs and finds server info incorrect (it is in the root region but the server is not in serversToServerInfo\n4. checkAssigned starts splitting the log but because the log name is incorrect it can't finish\n5. PendingServerShutdown fires and really gums up the works.\n\nSo there are two problems:\n\n1. HLog.splitLog needs to generate the correct log file name.\n2. PendingServerShutdown and/or leaseExpired need to cooperate with checkAssigned so that there are not two concurrent attempts to recover the log.\n"
    ],
    [
        "HBASE-394",
        "HBASE-392",
        "[hbase] Updating a row on non-existent table runs all the retries and timeouts instead of failing fast If you try to access row in non-existent table, the client hangs waiting on all timeouts and retries.  Rather it should be able to fail fast if no such table.\n\n\n\n\n\n\n\n\n",
        "[hbase] Return quickly from HTable constructor if table does not exist In HConnectionManager.findServersForTable, if both the root and meta tables are accessible and a user table cannot be found in the meta table, it should throw a TableNotFoundException and not retry."
    ],
    [
        "HBASE-344",
        "HBASE-288",
        "[hbase] Performance - add a block cache A block cache would cache fixed size blocks (default 64k) of data read from HDFS by the MapFile. It would help read performance for data close to recently read data (see Bigtable paper, section 6). It would be configurable on a per-column family basis.",
        "Add in-memory caching of data Bigtable provides two in-memory caches: one for row/column data and one for disk block caches.\n\nThe size of each cache should be configurable, data should be loaded lazily, and the cache managed by an LRU mechanism.\n\nOne complication of the block cache is that all data is read through a SequenceFile.Reader which ultimately reads data off of disk via a RPC proxy for ClientProtocol. This would imply that the block caching would have to be pushed down to either the DFSClient or SequenceFile.Reader\n"
    ],
    [
        "HBASE-362",
        "HBASE-288",
        "[Hbase] Caching for read performance * Use two level of caching to improve read performance\n\n* Scan cache\n** Higher-level cache\n*** Caches the K,V pairs returned by the SSTable(HStore?) interface to the region server code\n** Most useful for applications that tend to read the same data repeatedly\n\n* Block cache\n** Lower-level cache\n*** Caches SSTables blocks that were read from HDFS\n** Useful for applications that read data close to the data that they recently read\n*** E.g. Sequential read or random read of different column in same locality group within a hot row\n",
        "Add in-memory caching of data Bigtable provides two in-memory caches: one for row/column data and one for disk block caches.\n\nThe size of each cache should be configurable, data should be loaded lazily, and the cache managed by an LRU mechanism.\n\nOne complication of the block cache is that all data is read through a SequenceFile.Reader which ultimately reads data off of disk via a RPC proxy for ClientProtocol. This would imply that the block caching would have to be pushed down to either the DFSClient or SequenceFile.Reader\n"
    ],
    [
        "HBASE-362",
        "HBASE-344",
        "[Hbase] Caching for read performance * Use two level of caching to improve read performance\n\n* Scan cache\n** Higher-level cache\n*** Caches the K,V pairs returned by the SSTable(HStore?) interface to the region server code\n** Most useful for applications that tend to read the same data repeatedly\n\n* Block cache\n** Lower-level cache\n*** Caches SSTables blocks that were read from HDFS\n** Useful for applications that read data close to the data that they recently read\n*** E.g. Sequential read or random read of different column in same locality group within a hot row\n",
        "[hbase] Performance - add a block cache A block cache would cache fixed size blocks (default 64k) of data read from HDFS by the MapFile. It would help read performance for data close to recently read data (see Bigtable paper, section 6). It would be configurable on a per-column family basis."
    ],
    [
        "HBASE-589",
        "HBASE-502",
        "Remove references to deprecated methods in Hadoop once hadoop-0.17.0 is released A number of methods in Hadoop have been deprecated for release 0.17.0. Once 0.17.0 is released, use preferred alternate.",
        "When deleting a directory, use FileUtil.fullyDelete instead of FileSystem.delete FileUtil.fullyDelete properly deletes a directory by deleting its contents first. While FileSystem.delete works on HDFS, it does not work on local file systems that do not permit a directory to be deleted if it is not empty."
    ],
    [
        "HBASE-1060",
        "HBASE-616",
        "RegionServer lease expire  due to  LogRoller take long time and the RegionServer  cannot recovery Usually, after tens GB of data, following exception occurs, it seems LogRoller take long time and the RegionServer cannot report to master, and the lease expire. But it cannot recovery.\n---------\n2008-12-13 11:13:39,347 INFO org.apache.hadoop.hbase.regionserver.LogRoller: Rolling hlog. Number of entries: 30015\n2008-12-13 11:13:39,354 INFO org.apache.hadoop.hbase.regionserver.HLog: New log writer created at /hbase/log_10.24.1.14_1229053900800_60020/hlog.dat.1229138019352\n2008-12-13 11:13:47,991 INFO org.apache.hadoop.hbase.regionserver.LogRoller: Rolling hlog. Number of entries: 30015\n2008-12-13 11:13:49,138 INFO org.apache.hadoop.hbase.regionserver.HLog: New log writer created at /hbase/log_10.24.1.14_1229053900800_60020/hlog.dat.1229138029136\n2008-12-13 11:15:49,867 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 120318ms, ten times longer than scheduled: 3000\n2008-12-13 11:15:50,175 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: unable to report to master for 120343 milliseconds - retrying\n2008-12-13 11:15:50,271 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_CALL_SERVER_STARTUP\n2008-12-13 11:15:50,419 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 60020, call get([B@4b2a82b3, [B@53eadc52, [B@6026b688, -1, -1) from 10.24.1.18:58581: error: org.apache.hadoop.hbase.NotServingRegionException: TableDes,,1228799531753\norg.apache.hadoop.hbase.NotServingRegionException: TableDes,,1228799531753\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:1859)\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1321)\n        at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:554)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)\n2008-12-13 11:15:50,434 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 60020, call get([B@26ee4c87, [B@66e24708, [B@11a592fd, -1, -1) from 10.24.1.12:46262: error: org.apache.hadoop.hbase.NotServingRegionException: TableDes,,1228799531753\norg.apache.hadoop.hbase.NotServingRegionException: TableDes,,1228799531753\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:1859)\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1321)\n        at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:554)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)\n2008-12-13 11:15:50,431 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 60020, call get([B@519182c2, [B@281fb864, [B@3a82c1d0, -1, -1) from 10.24.1.16:34950: error: org.apache.hadoop.hbase.NotServingRegionException: TableDes,,1228799531753\norg.apache.hadoop.hbase.NotServingRegionException: TableDes,,122879953175\n.................many exceptions.......................",
        "\" We slept XXXXXX ms, ten times longer than scheduled: 3000\" happens frequently. Just saw the below in a log... all in a row on the one server.\r\n\r\n{code}\r\n   4493 2008-05-05 18:08:17,512 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 34557ms, ten times longer than scheduled: 3000\r\n   4494 2008-05-05 18:11:08,879 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 30576ms, ten times longer than scheduled: 3000\r\n   4495 2008-05-05 18:30:45,056 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 1091720ms, ten times longer than scheduled: 3000\r\n   4496 2008-05-05 18:30:45,056 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 1094209ms, ten times longer than scheduled: 10000\r\n   4497 2008-05-05 18:30:45,429 FATAL org.apache.hadoop.hbase.HRegionServer: unable to report to master for 1092093 milliseconds - aborting server\r\n{code}\r\n\r\nWe're seeing these kinda outages pretty frequently.  In the case above, it was small cluster that was using TableReduce to insert.  The MR, HDFS and HBase were all running on same nodes."
    ],
    [
        "HBASE-2280",
        "HBASE-2063",
        "HFileOutputFormat writes output to \"unsafe\" directory HFileOutputFormat writes data direct to output folder. It's incorrect as failed (or killed, or interrupted) reducers leaves inconsistent files in output folder.\r\n\r\nThe convinient way to ouput data from OutputFormat is to use \"working directory\". The content of this directory is being moved to output directory at the end of reducer process if only reducer succeeded (this process is called \"output commit\" or \"atomic commit\").\r\n\r\nIf means that instead of\r\n\r\n final Path outputdir = FileOutputFormat.getOutputPath(context);\r\n\r\nhbase should use\r\n\r\n final Path outputdir = FileOutputFormat.getWorkOutputPath(context);\r\n",
        "For hfileoutputformat, on timeout/failure/kill clean up half-written hfile Below is from mailing list.  Read from bottom to top:\r\n\r\n{code}\r\n I was going to write that perhaps you needed to turn mapred.reduce.tasks.speculative.execution off, but if enabling it and things work, that would seem to indicate that a our reducer first takes longer than the task timeout maximum and secondly, on failure, we should clean up the hfile.\r\n\r\nOn the first issue, you are using KeyValueSortReducer?  Are your values large?  We set reducer status every 100 values.  Maybe this is not enough?  We should set status more frequently?  If you call context setstatus more frequently, do things work w/o speculative execution?\r\n \r\nOn the second, HFileOutputFormat close will set the metadata on the hfile and then close it.  On kill, this code is not being called.   Let me see if can do something about that (e.g. register a shutdown hook to clean away incomplete files -- ).\r\n\r\nThanks,\r\nSt.Ack\r\n\r\n\r\nOn Sun, Dec 20, 2009 at 11:26 PM, ChingShen <chingshenchen@gmail.com> wrote:\r\nI think I found a way.\r\nI set the \"mapred.reduce.tasks.speculative.execution\" to true and output\r\nhfiles again, then successfully load hfiles into hbase.\r\nIs it best solution? or HFileOutputFormat bug?\r\n\r\nShen\r\n\r\nOn Mon, Dec 21, 2009 at 8:25 AM, ChingShen <chingshenchen@gmail.com> wrote:\r\n\r\n> Thanks, stack.\r\n>\r\n> I checked this file that isn't empty. But I found that as long as the\r\n> \"Killed Task Attempts\" > 0 in reduce phase, and run the loadtable.rb script\r\n> to load hfiles then failed.\r\n> How to avoid this problem?\r\n>\r\n> Thanks.\r\n>\r\n> Shen\r\n>\r\n>\r\n> On Sat, Dec 19, 2009 at 3:49 AM, stack <stack@duboce.net> wrote:\r\n>\r\n>> Check the\r\n>> file\r\n>> hdfs://domU-12-31-39-09-C5-54.compute-1.internal/osm2_hfile/Level4/197894389945760574.\r\n>>  Is it empty?  Was there an error during running of your MR job?  Perhaps\r\n>> a\r\n>> task failed?\r\n>>\r\n>> St.Ack\r\n>>\r\n>>\r\n>>\r\n>> On Thu, Dec 17, 2009 at 9:46 PM, ChingShen <chingshenchen@gmail.com>\r\n>> wrote:\r\n>>\r\n>> > Hi,\r\n>> >  I use the script loadtable.rb to load my hfiles into hbase, but I got\r\n>> an\r\n>> > exception as below.\r\n>> >  Does anyone have any suggestions?\r\n>> >\r\n>> > 09/12/17 23:59:33 INFO loadtable: 18 read firstkey of -3.9290_52.5534\r\n>> from\r\n>> >\r\n>> >\r\n>> hdfs://domU-12-31-39-09-C5-54.compute-1.internal/osm2_hfile/Level4/1978943899457605747\r\n>> > org/apache/hadoop/hbase/io/hfile/HFile.java:1335:in `deserialize':\r\n>> > java.io.IOException: Trailer 'header' is wrong; does the trailer size\r\n>> match\r\n>> > content? (NativeException)\r\n>> >    from org/apache/hadoop/hbase/io/hfile/HFile.java:813:in `readTrailer'\r\n>> >    from org/apache/hadoop/hbase/io/hfile/HFile.java:758:in\r\n>> `loadFileInfo'\r\n>> >    from sun.reflect.GeneratedMethodAccessor7:-1:in `invoke'\r\n>> >    from sun/reflect/DelegatingMethodAccessorImpl.java:25:in `invoke'\r\n>> >    from java/lang/reflect/Method.java:597:in `invoke'\r\n>> >    from org/jruby/javasupport/JavaMethod.java:298:in\r\n>> > `invokeWithExceptionHandling'\r\n>> >    from org/jruby/javasupport/JavaMethod.java:259:in `invoke'\r\n>> >    from org/jruby/java/invokers/InstanceMethodInvoker.java:36:in `call'\r\n>> >     ... 18 levels...\r\n>> >    from org/jruby/Main.java:94:in `main'\r\n>> >    from loadtable.rb:83:in `each'\r\n>> >    from loadtable.rb:83\r\n>> > Complete Java stackTrace\r\n>> > java.io.IOException: Trailer 'header' is wrong; does the trailer size\r\n>> match\r\n>> > content?\r\n>> >    at\r\n>> >\r\n>> >\r\n>> org.apache.hadoop.hbase.io.hfile.HFile$FixedFileTrailer.deserialize(HFile.java:1335)\r\n>> >    at\r\n>> >\r\n>> org.apache.hadoop.hbase.io.hfile.HFile$Reader.readTrailer(HFile.java:813)\r\n>> >    at\r\n>> >\r\n>> org.apache.hadoop.hbase.io.hfile.HFile$Reader.loadFileInfo(HFile.java:758)\r\n>> >    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\r\n>> >    at\r\n>> >\r\n>> >\r\n>> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n>> >    at java.lang.reflect.Method.invoke(Method.java:597)\r\n>> >    at\r\n>> >\r\n>> >\r\n>> org.jruby.javasupport.JavaMethod.invokeWithExceptionHandling(JavaMethod.java:298)\r\n>> >    at org.jruby.javasupport.JavaMethod.invoke(JavaMethod.java:259)\r\n>> >    at\r\n>> >\r\n>> >\r\n>> org.jruby.java.invokers.InstanceMethodInvoker.call(InstanceMethodInvoker.java:36)\r\n>> >    at\r\n>> > org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:70)\r\n>> >    at loadtable.ensure_1$RUBY$__ensure___2(loadtable.rb:86)\r\n>> >    at loadtable.block_0$RUBY$__for__(loadtable.rb:85)\r\n>> >    at loadtableBlockCallback$block_0$RUBY$__for__xx1.call(Unknown\r\n>> Source)\r\n>> >    at org.jruby.runtime.CompiledBlock.yield(CompiledBlock.java:102)\r\n>> >    at org.jruby.runtime.Block.yield(Block.java:100)\r\n>> >    at\r\n>> org.jruby.java.proxies.ArrayJavaProxy.each(ArrayJavaProxy.java:112)\r\n>> >    at\r\n>> >\r\n>> >\r\n>> org.jruby.java.proxies.ArrayJavaProxy$i_method_0_0$RUBYINVOKER$each.call(org/jruby/java/proxies/ArrayJavaProxy$i_method_0_0$RUBYINVOKER$each.gen)\r\n>> >    at\r\n>> >\r\n>> >\r\n>> org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:263)\r\n>> >    at\r\n>> >\r\n>> >\r\n>> org.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite.java:81)\r\n>> >    at\r\n>> >\r\n>> >\r\n>> org.jruby.runtime.callsite.CachingCallSite.callIter(CachingCallSite.java:96)\r\n>> >    at loadtable.__file__(loadtable.rb:83)\r\n>> >    at loadtable.load(loadtable.rb)\r\n>> >    at org.jruby.Ruby.runScript(Ruby.java:577)\r\n>> >    at org.jruby.Ruby.runNormally(Ruby.java:480)\r\n>> >    at org.jruby.Ruby.runFromMain(Ruby.java:354)\r\n>> >    at org.jruby.Main.run(Main.java:229)\r\n>> >    at org.jruby.Main.run(Main.java:110)\r\n>> >    at org.jruby.Main.main(Main.java:94)\r\n>> >\r\n>>\r\n>\r\n>\r\n>\r\n>\r\n\r\n\r\n--\r\n*****************************************************\r\nChing-Shen Chen\r\nAdvanced Technology Center,\r\nInformation & Communications Research Lab.\r\nE-mail: chenchingshen@itri.org.tw\r\nTel:+886-3-5915542\r\n*****************************************************\r\n\r\n{code}"
    ],
    [
        "HBASE-2627",
        "HBASE-2616",
        "testWritesWhileGetting flakey on branch at least. See http://hudson.hbase.org/job/hbase-branch-0.20/26/testReport/org.apache.hadoop.hbase.regionserver/TestHRegion/testWritesWhileGetting/ \r\n\r\n0.20 builds are failing on this test pretty regularly (hudson.hbase.apache.org is the TM hosted hudson.... runs the tests on an ec2 node).",
        "TestHRegion.testWritesWhileGetting flaky on trunk Saw this failure on my internal hudson:\r\n\r\njunit.framework.AssertionFailedError: expected:<\\x00\\x00\\x00\\x96> but was:<\\x00\\x00\\x01\\x00>\r\n\tat org.apache.hadoop.hbase.HBaseTestCase.assertEquals(HBaseTestCase.java:684)\r\n\tat org.apache.hadoop.hbase.regionserver.TestHRegion.testWritesWhileGetting(TestHRegion.java:2334)\r\n"
    ],
    [
        "HBASE-2942",
        "HBASE-2666",
        "Custom filters should not require registration in HbaseObjectWritable Some of the filter RPC serialization still requires that code -> class mappings be added to HbaseObjectWritable.  FilterList in particular requires this for it's child filters, since it calls HbaseObjectWritable.writeObject() on each.  This makes developing custom filters a big pain, as HbaseObjectWritable must be modified and the hbase core jar re-staged to the cluster.\r\n\r\nWe should fix this so that all filters can be written as a class name + data if no code exists.",
        "Why do we have to add new custom filters to HbaseObjectWritable? I though that if no code for a param type, we'd fall back to passing param name as String.  Check it out.  That devs have to add to HbaseObjectWritable is broke."
    ],
    [
        "HBASE-2984",
        "HBASE-2960",
        "[shell] Altering a family shouldn't reset to default unchanged attributes I changed the replication on a family that was also VERSIONS => 1 and COMPRESSION => LZO. I forgot that you have to respecify everything everytime you alter a family, so both were reset to 3 and NONE. Then the regions were compacted... and it has been splitting for about 20 minutes now. Fortunately this is our MR environment so our web site isn't affected, but it's still a major pain. Oh and also the table cannot be disabled to be re-altered since split parents are always present (I hope it'll stop splitting before midnight).\r\n\r\nThe shell should use the old values for attributes that aren't changed.",
        "Allow Incremental Table Alterations As per the HBase shell help, the alter command will \"Alter column family schema;  pass table name and a dictionary  specifying new column family schema.\" The assumption here seems to be that the new column family schema must be completely specified. In other words, if a certain attribute is not specified in the column family schema, then it is effectively defaulted. Is this side-effect by design? \r\n\r\nI for one assumed (wrongly apparently) that I can alter a table in \"increments\". Case in point, the following commands should've resulted in the final value of the VERSIONS attribute of my table to stay put at 1, but instead it got defaulted to 3. I guess there's no right or wrong answer here, but what should alter do by default? My expectation is that it only changes those attributes that were specified in the \"alter\" command, leaving the unspecified attributes untouched.\r\n\r\nhbase(main):003:0> create 't1', {NAME => 'f1', VERSIONS => 1}\r\n0 row(s) in 1.7230 seconds\r\nhbase(main):004:0> describe 't1'\r\nDESCRIPTION                                                            \r\n {NAME => 't1', FAMILIES => [{NAME => 'f1', COMPRESSION => 'NONE', VERSIONS => '1', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => ' false', BLOCKCACHE => 'true'}]}\r\n1 row(s) in 0.2030 seconds\r\nhbase(main):006:0> disable 't1'\r\n0 row(s) in 0.1140 seconds\r\nhbase(main):007:0> alter 't1', {NAME => 'f1', IN_MEMORY => 'true'}\r\n0 row(s) in 0.0160 seconds\r\nhbase(main):009:0> describe 't1'\r\nDESCRIPTION                                                            \r\n {NAME => 't1', FAMILIES => [{NAME => 'f1', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => ' true', BLOCKCACHE => 'true'}]}\r\n1 row(s) in 0.1280 seconds"
    ],
    [
        "HBASE-3379",
        "HBASE-3285",
        "Log splitting slowed by repeated attempts at connecting to downed datanode Testing if I kill RS and DN on a node, log splitting takes longer as we doggedly try connecting to the downed DN to get WAL blocks.  Here's the cycle I see:\r\n\r\n{code}\r\n2010-12-21 17:34:48,239 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_900551257176291912_1203821 failed  because recovery from primary datanode 10.20.20.182:10010 failed 5 times.    Pipeline was 10.20.20.184:10010, 10.20.20.186:10010, 10.20.20.182:10010. Will retry...\r\n2010-12-21 17:34:50,240 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.20.20.182:10020. Already tried 0 time(s).\r\n2010-12-21 17:34:51,241 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.20.20.182:10020. Already tried 1 time(s).\r\n2010-12-21 17:34:52,241 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.20.20.182:10020. Already tried 2 time(s).\r\n2010-12-21 17:34:53,242 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.20.20.182:10020. Already tried 3 time(s).\r\n2010-12-21 17:34:54,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.20.20.182:10020. Already tried 4 time(s).\r\n2010-12-21 17:34:55,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.20.20.182:10020. Already tried 5 time(s).\r\n2010-12-21 17:34:56,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.20.20.182:10020. Already tried 6 time(s).\r\n2010-12-21 17:34:57,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.20.20.182:10020. Already tried 7 time(s).\r\n2010-12-21 17:34:58,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.20.20.182:10020. Already tried 8 time(s).\r\n2010-12-21 17:34:59,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.20.20.182:10020. Already tried 9 time(s).\r\n2010-12-21 17:34:59,246 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 10.20.20.182:10010\r\njava.net.ConnectException: Call to /10.20.20.182:10020 failed on connection exception: java.net.ConnectException: Connection refused\r\n    at org.apache.hadoop.ipc.Client.wrapException(Client.java:767)\r\n    at org.apache.hadoop.ipc.Client.call(Client.java:743)\r\n    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)\r\n    at $Proxy8.getProtocolVersion(Unknown Source)\r\n    at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)\r\n    at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:346)\r\n    at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:383)\r\n...\r\n{code}\r\n\r\n\"because recovery from primary datanode\" is done 5 times (hardcoded).  Within these retries we'll do\r\n{code}\r\nthis.maxRetries = conf.getInt(\"ipc.client.connect.max.retries\", 10);\r\n{code}\r\n\r\nThe hardcoding of 5 attempts we should get fixed and we should doc the ipc.client.connect.max.retries as important config.  We should recommend bringing it down from default.",
        "Hlog recovery takes too much time Currently HBase uses append to trigger the close of HLog during Hlog split. Append is a very expensive operation, which involves not only NameNode operations but creating a writing pipeline. If one of datanodes on the pipeline has a problem, this recovery may takes minutes. I'd like implement a lightweight NameNode operation to trigger lease recovery and make HBase to use this instead."
    ],
    [
        "HBASE-3483",
        "HBASE-2158",
        "No soft flush trigger on global memstore limit I think this is the reason people see long blocking periods under write load.\r\n\r\nCurrently when we hit the global memstore limit, we call reclaimMemStoreMemory() which is synchronized - thus everyone has to wait until the memory has flushed down to the low water mark. This causes every writer to block for 10-15 seconds on a large heap.\r\n\r\nInstead we should start triggering flushes (in another thread) whenever we're above the low water mark. Then only block writers when we're above the high water mark.",
        "Change how high/low global limit works; start taking on writes as soon as we dip below high limit rather than block until low limit as we currently do. A Ryan Rawson suggestion.  See HBASE-2149 for more context."
    ],
    [
        "HBASE-3604",
        "HBASE-2231",
        "Two region servers think that they own the same region: data loss I observed this on a 100 node cluster that is constantly doing about 500K ops/second.\r\n\r\nThe region server on machine A was servicing IOs for a particular region. Then the machine went into a bad state where it is ping-able but not ssh-able. The master detected that there is a problem with machine A and reassigned the region to machine B. The regionserver on machine B opened the region and opened all the required HFiles for this region. After two hours, the NameNode received a delete request for one of the HFiles from machine A and happily renamed the file to HDFS-Trash. After another 3 hours or so, the regionserver on machine B tried to read contents from that HFile but failed because the file was renamed earlier. The region server on B in now stuck, and possible data loss. \r\n\r\nThe problems stems from the fact that although the master-and-ZK reassigned the region, the old regionserver was not possibly dead.\r\n",
        "Compaction events should be written to HLog The sequence for a compaction should look like this:\r\n# Compact region to \"new\" files\r\n# Write a \"Compacted Region\" entry to the HLog\r\n# Delete \"old\" files\r\n\r\nThis deals with a case where the RS has paused between step 1 and 2 and the regions have since been reassigned."
    ],
    [
        "HBASE-3636",
        "HBASE-3007",
        "a bug about deciding whether this key is a new key for the ROWCOL bloomfilter When ROWCOL bloomfilter needs to decide whether this key is a new key or not,\r\nit will call the matchingRowColumn function, which will compare the timestamp offset between this kv and last kv.\r\nBut when checking the timestamp offset, it didn't deduct the original offset of the keyvalue itself.\r\n\r\nFor example, when 2 keyvalue objects have the same row key and col key, but from different storefiles. It is highly likely that these 2 keyvalue objects have different offset value. So the timestamp offset of these 2 objects are totally different. They will be regard as new keys to add into bloomfilters.\r\nSo after compaction, the key count of bloomfilter will increase immediately, which is almost equal to the number of entries.\r\n\r\nThe solution is straightforward. Just compare the relevant timestamp offset, which is the timestamp offset - key_value offset.\r\n\r\nThis also may explain this jira: https://issues.apache.org/jira/browse/HBASE-3007",
        "StoreFile Blooms are Being Stored Undersized While looking through error logs today, I noticed the following line.\r\n\r\n{code}\r\n 2010-09-16 04:12:26,401 INFO org.apache.hadoop.hbase.regionserver.StoreFile: Bloom added to HFile.  9600B, 10292/6933 (148%) \r\n{code}\r\n\r\nThe last 3 numbers are: # of keys in bloom, # of keys preallocated for bloom, % full.  The last number should never be > 100%.  Oversized blooms will cause increased false positives.  Note that this occurred on a Row+Col bloom.  Will provide more details after further instrumentation."
    ],
    [
        "HBASE-3796",
        "HBASE-3051",
        "Per-Store Entries in Compaction Queue Although compaction is decided on a per-store basis, right now the CompactSplitThread only deals at the Region level for queueing.  Store-level compaction queue entries will give us more visibility into compaction workload + allow us to stop summarizing priorities.",
        "Compaction at the granularity of a column-family Currently, when compactions are requested, they are being done at a granularity of a region. So, when a particular column-family store\r\nexceeds the threshold, for example, we go and perform the compaction for the entire region -- consisting of all the column-families.\r\n\r\nWould like to add support for performing compactions per column-family. "
    ],
    [
        "HBASE-3920",
        "HBASE-3885",
        "HLog hbase.regionserver.flushlogentries no longer supported While searching for config options on syncing the HLog, I was a bit confused by hbase.regionserver.flushlogentries which is still in the code and in hbase-default.xml but isn't actually used.",
        "Remove hbase.regionserver.flushlogentries, its not used "
    ],
    [
        "HBASE-3939",
        "HBASE-1913",
        "Some crossports of Hadoop IPC fixes A few fixes from Hadoop IPC that we should probably cross-port into our copy:\r\n- HADOOP-7227: remove the protocol version check at call time\r\n- HADOOP-7146: fix a socket leak in server\r\n- HADOOP-7121: fix behavior when response serialization throws an exception\r\n- HADOOP-7346: send back nicer error response when client is using an out of date IPC version",
        "Regionserver accepts connections, doesn't handle them after bad filter request I deployed some new regionservers but forgot to include a library that one of my filters used.  When a client used that filter, the HBaseServer listener thread attempted to deserialize it, and threw a NoClassDefFoundError.  This killed the listener thread without cleaning up the socket (only Exception and OOME are caught, not other Error subclasses).  New clients continued to successfully open TCP connections, but the regionserver would never handle them, never shutdown, and the master would never expire it, so all of its regions were effectively unavailable until we intervened."
    ],
    [
        "HBASE-4834",
        "HBASE-3497",
        "CopyTable: Cannot have ZK source to destination During a Copy Table, involving --peer.adr, we found the following block of code:\r\n\r\nif (address != null) {\r\n        ZKUtil.applyClusterKeyToConf(this.conf, address);\r\n   }\r\n\r\nWhen we set ZK conf in setConf method, that also gets called in frontend when MR initializes TOF, so there's no way now to have two ZK points for a single job, cause source gets reset before job is submitted.",
        "TableMapReduceUtil.initTableReducerJob broken due to setConf method in TableOutputFormat setConf() method in TableOutputFormat gets called and it replaces the hbase.zookeeper.quorum address in the job conf xml when you run a CopyTable job from one cluster to another. The conf gets set to the peer.addr that is specified, which makes the job read and write from/to the peer cluster instead of reading from the original cluster and writing to the peer.\r\n\r\nPossibly caused due to the change in https://issues.apache.org/jira/browse/HBASE-3111"
    ],
    [
        "HBASE-5054",
        "HBASE-4854",
        "hadoop's classpath takes precedence over hbase's classpath Since hbase shares the metrics framework with core hadoop, and they both use 'hadoop-metrics.properties' file on the classpath for configuration, the ordering causes hbase's directories to be shadowed by hadoop's.  What this means is that for me to set hbase's hadoop-metrics.properties, I have to do it in /etc/hadoop/conf since the one in /etc/hbase/conf is later in the classpath.\r\n\r\nRunning hbase classpath confirms the ordering:\r\n{quote}\r\n% hbase classpath\r\n{color:red}/usr/lib/hadoop-0.20/conf{color}:/usr/lib/hadoop-0.20/hadoop-core-0.20.2-cdh3u2.jar:/usr/lib/hadoop-0.20/lib/ant-contrib-1.0b3.jar:/usr/lib/hadoop-0.20/lib/aspectjrt-1.6.5.jar:/usr/lib/hadoop-0.20/lib/aspectjtools-1.6.5.jar:/usr/lib/hadoop-0.20/lib/commons-cli-1.2.jar:/usr/lib/hadoop-0.20/lib/commons-codec-1.4.jar:/usr/lib/hadoop-0.20/lib/commons-daemon-1.0.1.jar:/usr/lib/hadoop-0.20/lib/commons-el-1.0.jar:/usr/lib/hadoop-0.20/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-0.20/lib/commons-logging-1.0.4.jar:/usr/lib/hadoop-0.20/lib/commons-logging-api-1.0.4.jar:/usr/lib/hadoop-0.20/lib/commons-net-1.4.1.jar:/usr/lib/hadoop-0.20/lib/core-3.1.1.jar:/usr/lib/hadoop-0.20/lib/hadoop-fairscheduler-0.20.2-cdh3u2.jar:/usr/lib/hadoop-0.20/lib/hadoop-lzo-0.4.6.jar:/usr/lib/hadoop-0.20/lib/hsqldb-1.8.0.10.jar:/usr/lib/hadoop-0.20/lib/jackson-core-asl-1.5.2.jar:/usr/lib/hadoop-0.20/lib/jackson-mapper-asl-1.5.2.jar:/usr/lib/hadoop-0.20/lib/jasper-compiler-5.5.12.jar:/usr/lib/hadoop-0.20/lib/jasper-runtime-5.5.12.jar:/usr/lib/hadoop-0.20/lib/jets3t-0.6.1.jar:/usr/lib/hadoop-0.20/lib/jetty-6.1.26.cloudera.1.jar:/usr/lib/hadoop-0.20/lib/jetty-servlet-tester-6.1.26.cloudera.1.jar:/usr/lib/hadoop-0.20/lib/jetty-util-6.1.26.cloudera.1.jar:/usr/lib/hadoop-0.20/lib/jsch-0.1.42.jar:/usr/lib/hadoop-0.20/lib/junit-4.5.jar:/usr/lib/hadoop-0.20/lib/kfs-0.2.2.jar:/usr/lib/hadoop-0.20/lib/log4j-1.2.15.jar:/usr/lib/hadoop-0.20/lib/mockito-all-1.8.2.jar:/usr/lib/hadoop-0.20/lib/oro-2.0.8.jar:/usr/lib/hadoop-0.20/lib/servlet-api-2.5-20081211.jar:/usr/lib/hadoop-0.20/lib/servlet-api-2.5-6.1.14.jar:/usr/lib/hadoop-0.20/lib/slf4j-api-1.4.3.jar:/usr/lib/hadoop-0.20/lib/slf4j-log4j12-1.4.3.jar:/usr/lib/hadoop-0.20/lib/xmlenc-0.52.jar:{color:red}/usr/lib/hbase/bin/../conf{color}:/usr/java/jdk1.6.0_29/lib/tools.jar:/usr/lib/hbase/bin/..:/usr/lib/hbase/bin/../hbase-0.90.4-cdh3u2.jar:/usr/lib/hbase/bin/../hbase-0.90.4-cdh3u2-tests.jar:/usr/lib/hbase/bin/../lib/activation-1.1.jar:/usr/lib/hbase/bin/../lib/asm-3.1.jar:/usr/lib/hbase/bin/../lib/avro-1.5.4.jar:/usr/lib/hbase/bin/../lib/avro-ipc-1.5.4.jar:/usr/lib/hbase/bin/../lib/commons-cli-1.2.jar:/usr/lib/hbase/bin/../lib/commons-codec-1.4.jar:/usr/lib/hbase/bin/../lib/commons-el-1.0.jar:/usr/lib/hbase/bin/../lib/commons-httpclient-3.1.jar:/usr/lib/hbase/bin/../lib/commons-lang-2.5.jar:/usr/lib/hbase/bin/../lib/commons-logging-1.1.1.jar:/usr/lib/hbase/bin/../lib/commons-net-1.4.1.jar:/usr/lib/hbase/bin/../lib/core-3.1.1.jar:/usr/lib/hbase/bin/../lib/guava-r06.jar:/usr/lib/hbase/bin/../lib/hadoop-core.jar:/usr/lib/hbase/bin/../lib/jackson-core-asl-1.5.2.jar:/usr/lib/hbase/bin/../lib/jackson-jaxrs-1.5.5.jar:/usr/lib/hbase/bin/../lib/jackson-mapper-asl-1.5.2.jar:/usr/lib/hbase/bin/../lib/jackson-xc-1.5.5.jar:/usr/lib/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/usr/lib/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/usr/lib/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/usr/lib/hbase/bin/../lib/jaxb-api-2.1.jar:/usr/lib/hbase/bin/../lib/jaxb-impl-2.1.12.jar:/usr/lib/hbase/bin/../lib/jersey-core-1.4.jar:/usr/lib/hbase/bin/../lib/jersey-json-1.4.jar:/usr/lib/hbase/bin/../lib/jersey-server-1.4.jar:/usr/lib/hbase/bin/../lib/jettison-1.1.jar:/usr/lib/hbase/bin/../lib/jetty-6.1.26.jar:/usr/lib/hbase/bin/../lib/jetty-util-6.1.26.jar:/usr/lib/hbase/bin/../lib/jruby-complete-1.6.0.jar:/usr/lib/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/usr/lib/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/usr/lib/hbase/bin/../lib/jsp-api-2.1.jar:/usr/lib/hbase/bin/../lib/jsr311-api-1.1.1.jar:/usr/lib/hbase/bin/../lib/log4j-1.2.16.jar:/usr/lib/hbase/bin/../lib/netty-3.2.4.Final.jar:/usr/lib/hbase/bin/../lib/protobuf-java-2.3.0.jar:/usr/lib/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/usr/lib/hbase/bin/../lib/servlet-api-2.5.jar:/usr/lib/hbase/bin/../lib/slf4j-api-1.5.8.jar:/usr/lib/hbase/bin/../lib/slf4j-log4j12-1.5.8.jar:/usr/lib/hbase/bin/../lib/snappy-java-1.0.3.2.jar:/usr/lib/hbase/bin/../lib/stax-api-1.0.1.jar:/usr/lib/hbase/bin/../lib/thrift-0.2.0.jar:/usr/lib/hbase/bin/../lib/velocity-1.5.jar:/usr/lib/hbase/bin/../lib/xmlenc-0.52.jar:/usr/lib/hbase/bin/../lib/zookeeper.jar:/etc/zookeeper:/etc/hadoop-0.20/conf:/usr/lib/hadoop-0.20/\\*:/usr/lib/hadoop-0.20/lib/\\*:/usr/lib/zookeeper/\\*:/usr/lib/zookeeper/lib/\\*:\r\n{quote}\r\n\r\nWhile I discovered this trying to figure out why changed in /etc/hbase/conf were not reflected in the running application, this also extends to ANY class or library hbase uses.  It is very non-obvious that if there are duplicate classes/property files/etc that the hadoop ones would take precedence.\r\n\r\nSorry if this is a dupe, but searching for 'classpath' in the jira's is like searching for 'Exception' in a log ;)",
        "it seems that CLASSPATH elements coming from Hadoop change HBase behaviour It looks like HBASE-3465 introduced a slight change in behavior. The ordering of classpath elements makes Hadoop ones go before the HBase ones, which leads to log4j properties picked up from the wrong place, etc. It seems that the easies way to fix that would be to revert the ordering of classpath."
    ],
    [
        "HBASE-5062",
        "HBASE-4099",
        "Missing logons if security is enabled Somehow the attached changes are missing from the security integration. ",
        "Authentication for ThriftServer clients The current implementation of HBase client authentication only works with the Java API.  Alternate access gateways, like Thrift and REST are left out and will not work.\r\n\r\nFor the ThriftServer to be able to fully interoperate with the security implementation:\r\n# the ThriftServer should be able to login from a keytab file with it's own server principal on startup\r\n# thrift clients should be able to authenticate securely when connecting to the server\r\n# the ThriftServer should be able to act as a proxy for those clients so that the RPCs it issues will be correctly authorized as the original client identities\r\n\r\nThere is already some support for step 3 in UserGroupInformation and related classes.\r\n\r\nFor step #2, we really need to look at what thrift itself supports.\r\n\r\nAt a bare minimum, we need to implement step #1.  If we do this, even without steps 2 & 3, this would at least allow deployments to use a ThriftServer per application user, and have the server login as that user on startup.  Thrift clients may not be directly authenticated, but authorization checks for HBase could still be handled correctly this way."
    ],
    [
        "HBASE-5062",
        "HBASE-4100",
        "Missing logons if security is enabled Somehow the attached changes are missing from the security integration. ",
        "Authentication for REST clients Like Thrift, the REST gateway is not currently integrated into the authentication used for HBase RPC.  Currently this means the REST gateway cannot even be used when HBase security is active.\r\n\r\nFor the REST gateway to be able to interoperate with HBase security:\r\n# the REST server needs to be able to login from a keytab on startup with its own server principal\r\n# REST clients need to be able to authenticate security with the REST server\r\n# the REST server needs to be able to act as a trusted proxy for the original client identities, so that the HBase authorization checks can be performed against the original client request\r\n\r\nLike Thrift, implementing step #1 as a bare minimum would at least allow deploying a REST server configured to login as the application user on startup.  Even without authenticating REST clients, this would allow the gateway to work when HBase security is active.\r\n\r\nFor step #2, we can make use of SPNEGO to provide Kerberos/GSSAPI authentication of clients over HTTP.  The Alfredo library from Cloudera would hopefully make this relatively easy to do:\r\nhttp://cloudera.github.com/alfredo/docs/latest/index.html\r\n"
    ],
    [
        "HBASE-5759",
        "HBASE-5725",
        "HBaseClient throws NullPointerException when EOFException should be used. When a RPC data input stream is closed, protobuf doesn't raise an EOFException, it returns a null RpcResponse object.\r\n\r\nWe need to check if the response is null before trying to access it.",
        "HBaseClient throws NPE while in Connection#receiveResponse call. When i am running TestSplitTransactionOnCluster, it is throwing NPE from HBaseClient \r\n\r\nHBaseClient:\r\n \r\nRpcResponse response = RpcResponse.parseDelimitedFrom(in);\r\n int id = response.getCallId();\r\n\r\nThe above code throws NPE.\r\n"
    ],
    [
        "HBASE-5834",
        "HBASE-4931",
        "CopyTable usage is incorrect The example given here is outdated:\r\nhttp://hbase.apache.org/book/ops_mgt.html#copytable\r\n\r\nThe classes for rs.class and rs.impl don't exist.\r\n\r\nExample in Java code needs to be updated as well.\r\n",
        "CopyTable instructions could be improved. The book and the usage instructions could be improved to include more details, things caveats and to better explain usage.\r\n\r\nOne example in particular, could be updated to refer to ReplicationRegionInterface and ReplicationRegionServer in thier current locations (o.a.h.h.client.replication and o.a.h.h.replication.regionserver), and better explain why one would use particular arguments.\r\n\r\n{code}\r\n$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable\r\n--rs.class=org.apache.hadoop.hbase.ipc.ReplicationRegionInterface\r\n--rs.impl=org.apache.hadoop.hbase.regionserver.replication.ReplicationRegionServer\r\n--starttime=1265875194289 --endtime=1265878794289\r\n--peer.adr=server1,server2,server3:2181:/hbase TestTable\r\n{code}"
    ],
    [
        "HBASE-5968",
        "HBASE-1299",
        "Proper html escaping for region names I noticed that we are not doing html escaping for the rs/master web interfaces, so you can end up generating html like: \r\n{code}\r\n<tr>\r\n  <td>ci,,\\xEEp/<T\\xBE\\xC0,1336471826990.fc5a943e75ce8521b1ccdaf72d2c96c8.</td>\r\n  \r\n  <td>\r\n    <a href=\"hostname\">hostname</a>\r\n  </td>\r\n  \r\n  <td>,\\xEEp/<T\\xBE\\xC0</td>\r\n  <td>-n\\xA8\\xE0\\x15\\xDD\\x80!</td>\r\n  <td>2966724</td>\r\n</tr>\r\n{code}\r\n\r\nThis obviously does not render properly. \r\n\r\nAlso, my crazy theory is that it can be a security risk. Since the region name is computed from table rows, which are most of the time user input. Thus if  the rows contain a \"<script onload=\" or similar, then that will be executed on the developer's browser having possibly access to dev environment. \r\n",
        "JSPs don't HTML escape literals (ie: table names, region names, start & end keys) similar to HBASE-1298, the various JSPs included with HBase for monitoring the system don't seem to do any HTML escaping when displaying user entered data which may contain special characters: table names, region names, start Keys, or end Keys"
    ],
    [
        "HBASE-6151",
        "HBASE-4470",
        "Master can die if RegionServer throws ServerNotRunningYet See, for example:\r\n\r\n{noformat}\r\n2012-05-23 16:49:22,745 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.\r\norg.apache.hadoop.hbase.ipc.ServerNotRunningException: org.apache.hadoop.hbase.ipc.ServerNotRunningException: Server is not running yet\r\n\tat org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1038)\r\n\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\r\n\tat org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:96)\r\n\tat org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1240)\r\n\tat org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:444)\r\n\tat org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:343)\r\n\tat org.apache.hadoop.hbase.catalog.CatalogTracker.verifyMetaRegionLocation(CatalogTracker.java:540)\r\n\tat org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:474)\r\n\tat org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:412)\r\n{noformat}\r\n\r\nThe HRegionServer calls HBaseServer:\r\n{code}\r\n  public void start() {\r\n    startThreads();\r\n    openServer();\r\n  }\r\n{code}\r\n\r\nbut the server can start accepting RPCs once the threads have been started, but if they do, they throw ServerNotRunningException until openServer runs.  We should probably\r\n1) Catch the remote exception and retry on the master\r\n2) Look into whether the start() behavior of HBaseServer makes any sense.  Why would you start accepting RPCs only to throw back ServerNotRunningException?\r\n",
        "ServerNotRunningException coming out of assignRootAndMeta kills the Master I'm surprised we still have issues like that and I didn't get a hit while googling so forgive me if there's already a jira about it.\r\n\r\nWhen the master starts it verifies the locations of root and meta before assigning them, if the server is started but not running you'll get this:\r\n\r\n{quote}\r\n2011-09-23 04:47:44,859 WARN org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: RemoteException connecting to RS\r\norg.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.ipc.ServerNotRunningException: Server is not running yet\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1038)\r\n\r\n        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:771)\r\n        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)\r\n        at $Proxy6.getProtocolVersion(Unknown Source)\r\n        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:419)\r\n        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:393)\r\n        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:444)\r\n        at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:349)\r\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:969)\r\n        at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:388)\r\n        at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:287)\r\n        at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyMetaRegionLocation(CatalogTracker.java:484)\r\n        at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:441)\r\n        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:388)\r\n        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:282)\r\n{quote}\r\n\r\nI hit that 3-4 times this week while debugging something else. The worst is that when you restart the master it sees that as a failover, but none of the regions are assigned so it takes an eternity to get back fully online."
    ],
    [
        "HBASE-6196",
        "HBASE-5876",
        "MR testcases TestImportExport does not run in Trunk with hadoop2.0 TEstImportExport test cases does not run in trunk when compiled with hadoop 2.0\r\n",
        "TestImportExport has been failing against hadoop 0.23 profile TestImportExport has been failing against hadoop 0.23 profile"
    ],
    [
        "HBASE-6263",
        "HBASE-6166",
        "Use default mode for HBase Thrift gateway if not specified The Thrift gateway should start with a default mode if one is not selected. Currently, instead we see:\r\n\r\n{noformat}\r\nException in thread \"main\" java.lang.AssertionError: Exactly one option out of [-hsha, -nonblocking, -threadpool, -threadedselector] has to be specified\r\n\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$ImplType.setServerImpl(ThriftServerRunner.java:201)\r\n\tat org.apache.hadoop.hbase.thrift.ThriftServer.processOptions(ThriftServer.java:169)\r\n\tat org.apache.hadoop.hbase.thrift.ThriftServer.doMain(ThriftServer.java:85)\r\n\tat org.apache.hadoop.hbase.thrift.ThriftServer.main(ThriftServer.java:192)\r\n{noformat}\r\n\r\nSee also BIGTOP-648. ",
        "Allow thrift to start wih the server type specified in config Currently the thrift server type must be specified on the command line.  If it's already in config it shouldn't be needed."
    ],
    [
        "HBASE-6529",
        "HBASE-5640",
        "With HFile v2, the region server will always perform an extra copy of source files With HFile v2 implementation in HBase 0.94 & 0.96, the region server will use HFileSystem as its {color:blue}fs{color}. When it performs bulk load in Store.bulkLoadHFile(), it checks if its {color:blue}fs{color} is the same as {color:blue}srcFs{color}, which however will be DistributedFileSystem. Consequently, it will always perform an extra copy of source files.",
        "bulk load runs slowly than before I am loading data from an external system into hbase. There are many prints of the form. This is possibly a regression caused by a recent patch.\r\n\r\n....on different filesystem than destination store - moving to this filesystem"
    ],
    [
        "HBASE-6534",
        "HBASE-6533",
        "[replication] replication will be block if WAL compress set differently in master and slave configuration as we know in hbase 0.94.0 we have a configuration below\r\n  <property>\r\n    <name>hbase.regionserver.wal.enablecompression</name>\r\n         <value>true</value>\r\n  </property>\r\nif we enable it in master cluster and disable it in slave cluster . Then replication will not work. It will throw unwrapRemoteException again and again in master cluster.\r\n\r\n2012-08-09 12:49:55,892 WARN org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Can't replicate because of an error\r\n on the remote cluster: \r\njava.io.IOException: IPC server unable to read call parameters: Error in readFields\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)\r\n        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)\r\n        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:635)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:365)\r\nCaused by: org.apache.hadoop.ipc.RemoteException: IPC server unable to read call parameters: Error in readFields\r\n        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:921)\r\n        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:151)\r\n        at $Proxy13.replicateLogEntries(Unknown Source)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:616)\r\n        ... 1 more \r\n\r\n\r\nThis is because Slave cluster can not parse the hlog entry .\r\n\r\n2012-08-09 14:46:05,891 WARN org.apache.hadoop.ipc.HBaseServer: Unable to read call parameters for client 10.232.98.89\r\njava.io.IOException: Error in readFields\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:685)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:586)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:635)\r\n        at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:125)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:1292)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:1207)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:735)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:524)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:499)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n        at java.lang.Thread.run(Thread.java:662)\r\nCaused by: java.io.EOFException\r\n        at java.io.DataInputStream.readFully(DataInputStream.java:180)\r\n        at org.apache.hadoop.hbase.KeyValue.readFields(KeyValue.java:2254)\r\n        at org.apache.hadoop.hbase.regionserver.wal.WALEdit.readFields(WALEdit.java:146)\r\n        at org.apache.hadoop.hbase.regionserver.wal.HLog$Entry.readFields(HLog.java:1767)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:682)\r\n        ... 11 more \r\n",
        "[replication] replication will block if WAL compress set differently in master and slave configuration as we know in hbase 0.94.0 we have a configuration below\r\n  <property>\r\n    <name>hbase.regionserver.wal.enablecompression</name>\r\n         <value>true</value>\r\n  </property>\r\nif we enable it in master cluster and disable it in slave cluster . Then replication will not work. It will throw unwrapRemoteException again and again in master cluster.\r\n\r\n\r\n\r\n2012-08-09 12:49:55,892 WARN org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Can't replicate because of an error\r\n on the remote cluster: \r\njava.io.IOException: IPC server unable to read call parameters: Error in readFields\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)\r\n        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)\r\n        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:635)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:365)\r\nCaused by: org.apache.hadoop.ipc.RemoteException: IPC server unable to read call parameters: Error in readFields\r\n        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:921)\r\n        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:151)\r\n        at $Proxy13.replicateLogEntries(Unknown Source)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:616)\r\n        ... 1 more \r\n\r\n\r\nThis is because Slave cluster can not parse the hlog entry .\r\n\r\n2012-08-09 14:46:05,891 WARN org.apache.hadoop.ipc.HBaseServer: Unable to read call parameters for client 10.232.98.89\r\njava.io.IOException: Error in readFields\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:685)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:586)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:635)\r\n        at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:125)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:1292)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:1207)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:735)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:524)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:499)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n        at java.lang.Thread.run(Thread.java:662)\r\nCaused by: java.io.EOFException\r\n        at java.io.DataInputStream.readFully(DataInputStream.java:180)\r\n        at org.apache.hadoop.hbase.KeyValue.readFields(KeyValue.java:2254)\r\n        at org.apache.hadoop.hbase.regionserver.wal.WALEdit.readFields(WALEdit.java:146)\r\n        at org.apache.hadoop.hbase.regionserver.wal.HLog$Entry.readFields(HLog.java:1767)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:682)\r\n        ... 11 more \r\n\r\n\r\n"
    ],
    [
        "HBASE-6535",
        "HBASE-6533",
        "[replication] replication will be block if WAL compress set differently in master and slave cluster configuration As we know in hbase 0.94.0 we have a configuration below\r\n  <property>\r\n    <name>hbase.regionserver.wal.enablecompression</name>\r\n         <value>true</value>\r\n  </property>\r\nif we enable it in master cluster and disable it in slave cluster . Then replication will not work. It will throw unwrapRemoteException again and again in master cluster because slave can not parse the hlog entry buffer.",
        "[replication] replication will block if WAL compress set differently in master and slave configuration as we know in hbase 0.94.0 we have a configuration below\r\n  <property>\r\n    <name>hbase.regionserver.wal.enablecompression</name>\r\n         <value>true</value>\r\n  </property>\r\nif we enable it in master cluster and disable it in slave cluster . Then replication will not work. It will throw unwrapRemoteException again and again in master cluster.\r\n\r\n\r\n\r\n2012-08-09 12:49:55,892 WARN org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Can't replicate because of an error\r\n on the remote cluster: \r\njava.io.IOException: IPC server unable to read call parameters: Error in readFields\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)\r\n        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)\r\n        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:635)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:365)\r\nCaused by: org.apache.hadoop.ipc.RemoteException: IPC server unable to read call parameters: Error in readFields\r\n        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:921)\r\n        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:151)\r\n        at $Proxy13.replicateLogEntries(Unknown Source)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:616)\r\n        ... 1 more \r\n\r\n\r\nThis is because Slave cluster can not parse the hlog entry .\r\n\r\n2012-08-09 14:46:05,891 WARN org.apache.hadoop.ipc.HBaseServer: Unable to read call parameters for client 10.232.98.89\r\njava.io.IOException: Error in readFields\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:685)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:586)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:635)\r\n        at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:125)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:1292)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:1207)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:735)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:524)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:499)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n        at java.lang.Thread.run(Thread.java:662)\r\nCaused by: java.io.EOFException\r\n        at java.io.DataInputStream.readFully(DataInputStream.java:180)\r\n        at org.apache.hadoop.hbase.KeyValue.readFields(KeyValue.java:2254)\r\n        at org.apache.hadoop.hbase.regionserver.wal.WALEdit.readFields(WALEdit.java:146)\r\n        at org.apache.hadoop.hbase.regionserver.wal.HLog$Entry.readFields(HLog.java:1767)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:682)\r\n        ... 11 more \r\n\r\n\r\n"
    ],
    [
        "HBASE-6535",
        "HBASE-6534",
        "[replication] replication will be block if WAL compress set differently in master and slave cluster configuration As we know in hbase 0.94.0 we have a configuration below\r\n  <property>\r\n    <name>hbase.regionserver.wal.enablecompression</name>\r\n         <value>true</value>\r\n  </property>\r\nif we enable it in master cluster and disable it in slave cluster . Then replication will not work. It will throw unwrapRemoteException again and again in master cluster because slave can not parse the hlog entry buffer.",
        "[replication] replication will be block if WAL compress set differently in master and slave configuration as we know in hbase 0.94.0 we have a configuration below\r\n  <property>\r\n    <name>hbase.regionserver.wal.enablecompression</name>\r\n         <value>true</value>\r\n  </property>\r\nif we enable it in master cluster and disable it in slave cluster . Then replication will not work. It will throw unwrapRemoteException again and again in master cluster.\r\n\r\n2012-08-09 12:49:55,892 WARN org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Can't replicate because of an error\r\n on the remote cluster: \r\njava.io.IOException: IPC server unable to read call parameters: Error in readFields\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)\r\n        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)\r\n        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:635)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:365)\r\nCaused by: org.apache.hadoop.ipc.RemoteException: IPC server unable to read call parameters: Error in readFields\r\n        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:921)\r\n        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:151)\r\n        at $Proxy13.replicateLogEntries(Unknown Source)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:616)\r\n        ... 1 more \r\n\r\n\r\nThis is because Slave cluster can not parse the hlog entry .\r\n\r\n2012-08-09 14:46:05,891 WARN org.apache.hadoop.ipc.HBaseServer: Unable to read call parameters for client 10.232.98.89\r\njava.io.IOException: Error in readFields\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:685)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:586)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:635)\r\n        at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:125)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:1292)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:1207)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:735)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:524)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:499)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n        at java.lang.Thread.run(Thread.java:662)\r\nCaused by: java.io.EOFException\r\n        at java.io.DataInputStream.readFully(DataInputStream.java:180)\r\n        at org.apache.hadoop.hbase.KeyValue.readFields(KeyValue.java:2254)\r\n        at org.apache.hadoop.hbase.regionserver.wal.WALEdit.readFields(WALEdit.java:146)\r\n        at org.apache.hadoop.hbase.regionserver.wal.HLog$Entry.readFields(HLog.java:1767)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:682)\r\n        ... 11 more \r\n"
    ],
    [
        "HBASE-6536",
        "HBASE-6533",
        "[replication] replication will be block if WAL compress set differently in master and slave cluster configuration As we know in hbase 0.94.0 we have a configuration below\r\n  <property>\r\n    <name>hbase.regionserver.wal.enablecompression</name>\r\n         <value>true</value>\r\n  </property>\r\nif we enable it in master cluster and disable it in slave cluster . Then replication will not work. It will throw unwrapRemoteException again and again in master cluster because slave can not parse the hlog entry buffer.",
        "[replication] replication will block if WAL compress set differently in master and slave configuration as we know in hbase 0.94.0 we have a configuration below\r\n  <property>\r\n    <name>hbase.regionserver.wal.enablecompression</name>\r\n         <value>true</value>\r\n  </property>\r\nif we enable it in master cluster and disable it in slave cluster . Then replication will not work. It will throw unwrapRemoteException again and again in master cluster.\r\n\r\n\r\n\r\n2012-08-09 12:49:55,892 WARN org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Can't replicate because of an error\r\n on the remote cluster: \r\njava.io.IOException: IPC server unable to read call parameters: Error in readFields\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)\r\n        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)\r\n        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:635)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:365)\r\nCaused by: org.apache.hadoop.ipc.RemoteException: IPC server unable to read call parameters: Error in readFields\r\n        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:921)\r\n        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:151)\r\n        at $Proxy13.replicateLogEntries(Unknown Source)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:616)\r\n        ... 1 more \r\n\r\n\r\nThis is because Slave cluster can not parse the hlog entry .\r\n\r\n2012-08-09 14:46:05,891 WARN org.apache.hadoop.ipc.HBaseServer: Unable to read call parameters for client 10.232.98.89\r\njava.io.IOException: Error in readFields\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:685)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:586)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:635)\r\n        at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:125)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:1292)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:1207)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:735)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:524)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:499)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n        at java.lang.Thread.run(Thread.java:662)\r\nCaused by: java.io.EOFException\r\n        at java.io.DataInputStream.readFully(DataInputStream.java:180)\r\n        at org.apache.hadoop.hbase.KeyValue.readFields(KeyValue.java:2254)\r\n        at org.apache.hadoop.hbase.regionserver.wal.WALEdit.readFields(WALEdit.java:146)\r\n        at org.apache.hadoop.hbase.regionserver.wal.HLog$Entry.readFields(HLog.java:1767)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:682)\r\n        ... 11 more \r\n\r\n\r\n"
    ],
    [
        "HBASE-6536",
        "HBASE-6534",
        "[replication] replication will be block if WAL compress set differently in master and slave cluster configuration As we know in hbase 0.94.0 we have a configuration below\r\n  <property>\r\n    <name>hbase.regionserver.wal.enablecompression</name>\r\n         <value>true</value>\r\n  </property>\r\nif we enable it in master cluster and disable it in slave cluster . Then replication will not work. It will throw unwrapRemoteException again and again in master cluster because slave can not parse the hlog entry buffer.",
        "[replication] replication will be block if WAL compress set differently in master and slave configuration as we know in hbase 0.94.0 we have a configuration below\r\n  <property>\r\n    <name>hbase.regionserver.wal.enablecompression</name>\r\n         <value>true</value>\r\n  </property>\r\nif we enable it in master cluster and disable it in slave cluster . Then replication will not work. It will throw unwrapRemoteException again and again in master cluster.\r\n\r\n2012-08-09 12:49:55,892 WARN org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Can't replicate because of an error\r\n on the remote cluster: \r\njava.io.IOException: IPC server unable to read call parameters: Error in readFields\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)\r\n        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)\r\n        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:635)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:365)\r\nCaused by: org.apache.hadoop.ipc.RemoteException: IPC server unable to read call parameters: Error in readFields\r\n        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:921)\r\n        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:151)\r\n        at $Proxy13.replicateLogEntries(Unknown Source)\r\n        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:616)\r\n        ... 1 more \r\n\r\n\r\nThis is because Slave cluster can not parse the hlog entry .\r\n\r\n2012-08-09 14:46:05,891 WARN org.apache.hadoop.ipc.HBaseServer: Unable to read call parameters for client 10.232.98.89\r\njava.io.IOException: Error in readFields\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:685)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:586)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:635)\r\n        at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:125)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:1292)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:1207)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:735)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:524)\r\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:499)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n        at java.lang.Thread.run(Thread.java:662)\r\nCaused by: java.io.EOFException\r\n        at java.io.DataInputStream.readFully(DataInputStream.java:180)\r\n        at org.apache.hadoop.hbase.KeyValue.readFields(KeyValue.java:2254)\r\n        at org.apache.hadoop.hbase.regionserver.wal.WALEdit.readFields(WALEdit.java:146)\r\n        at org.apache.hadoop.hbase.regionserver.wal.HLog$Entry.readFields(HLog.java:1767)\r\n        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:682)\r\n        ... 11 more \r\n"
    ],
    [
        "HBASE-6536",
        "HBASE-6535",
        "[replication] replication will be block if WAL compress set differently in master and slave cluster configuration As we know in hbase 0.94.0 we have a configuration below\r\n  <property>\r\n    <name>hbase.regionserver.wal.enablecompression</name>\r\n         <value>true</value>\r\n  </property>\r\nif we enable it in master cluster and disable it in slave cluster . Then replication will not work. It will throw unwrapRemoteException again and again in master cluster because slave can not parse the hlog entry buffer.",
        "[replication] replication will be block if WAL compress set differently in master and slave cluster configuration As we know in hbase 0.94.0 we have a configuration below\r\n  <property>\r\n    <name>hbase.regionserver.wal.enablecompression</name>\r\n         <value>true</value>\r\n  </property>\r\nif we enable it in master cluster and disable it in slave cluster . Then replication will not work. It will throw unwrapRemoteException again and again in master cluster because slave can not parse the hlog entry buffer."
    ],
    [
        "HBASE-6537",
        "HBASE-3152",
        "Race between balancer and disable table can lead to inconsistent cluster Appear in 94. trunk is ok for the issue\r\nBalancer will collect the regionplans to move(unassign and then assign).\r\nbefore unassign, disable table appears, \r\nafter close the region in rs, master will delete the znode, romove region from RIT,\r\nand then clean the region from the online regions.\r\n\r\nDuring romoving region from RIT and cleaning out the region from the online regions. \r\nbalancer begins to unassign, it will get a NotServingRegionException and if the table is disabling, it will deal with the state in master and delete the znode . However the table is disabled now, so the RIT and znode will remain. TimeoutMonitor draws a blank on it.\r\n\r\nIt will hold back enabling the table or balancer unless restart\r\n",
        "Disabling table stuck on regions-in-transition after stress testing with only ICV and trying to disable table hbase run into regions-in-transition loop. I have logs and threads dumps. This region is on db2a regionserver, more logs as attachments\r\n\r\n2010-10-26 09:32:00,838 INFO org.apache.hadoop.hbase.master.AssignmentManager: Waiting on NGolden_AC,,1287997478278.85b04e1097c676ef52b16d49305e0ab0. state=PENDING_CLOSE, ts=1288078080792 to clear regions-in-transition               \r\n2010-10-26 09:32:01,838 INFO org.apache.hadoop.hbase.master.AssignmentManager: Waiting on NGolden_AC,,1287997478278.85b04e1097c676ef52b16d49305e0ab0. state=PENDING_CLOSE, ts=1288078080792 to clear regions-in-transition               \r\n2010-10-26 09:32:02,838 INFO org.apache.hadoop.hbase.master.AssignmentManager: Waiting on NGolden_AC,,1287997478278.85b04e1097c676ef52b16d49305e0ab0. state=PENDING_CLOSE, ts=1288078080792 to clear regions-in-transition               \r\n2010-10-26 09:32:02,842 INFO org.apache.hadoop.hbase.master.ServerManager: regionservers=2, averageload=1803                                                                                                                             \r\n2010-10-26 09:32:03,104 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  NGolden_AC,,1287997478278.85b04e1097c676ef52b16d49305e0ab0. state=PENDING_CLOSE, ts=1288078080792                       \r\n2010-10-26 09:32:03,104 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE or CLOSING for too long, running forced unassign again on region=NGolden_AC,,1287997478278.85b04e1097c676ef52b16d49305e0ab0.\r\n2010-10-26 09:32:03,104 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x12be24df47b0000 Deleting existing unassigned node for 85b04e1097c676ef52b16d49305e0ab0 that is in expected state RS_ZK_REGION_CLOSING           \r\n2010-10-26 09:32:03,104 WARN org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x12be24df47b0000 Attempting to delete unassigned node in RS_ZK_REGION_CLOSING state but node is in RS_ZK_REGION_CLOSED state                      \r\n2010-10-26 09:32:03,838 INFO org.apache.hadoop.hbase.master.AssignmentManager: Waiting on NGolden_AC,,1287997478278.85b04e1097c676ef52b16d49305e0ab0. state=PENDING_CLOSE, ts=1288078080792 to clear regions-in-transition               \r\n2010-10-26 09:32:04,838 INFO org.apache.hadoop.hbase.master.AssignmentManager: Waiting on NGolden_AC,,1287997478278.85b04e1097c676ef52b16d49305e0ab0. state=PENDING_CLOSE, ts=1288078080792 to clear regions-in-transition               \r\n2010-10-26 09:32:05,838 INFO org.apache.hadoop.hbase.master.AssignmentManager: Waiting on NGolden_AC,,1287997478278.85b04e1097c676ef52b16d49305e0ab0. state=PENDING_CLOSE, ts=1288078080792 to clear regions-in-transition               "
    ],
    [
        "HBASE-6565",
        "HBASE-6280",
        "Coprocessor exec result Map is not thread safe I develop a coprocessor program ,but found some different results in repeated tests.for example,normally,the result's size is 10.but sometimes it appears 9.\r\nI read the HTable.java code,found a TreeMap(thread-unsafe) be used in multithreading environment.It cause the bug happened",
        "why using treeMap at default implement with class  Batch.Callback public <T extends CoprocessorProtocol, R> Map<byte[],R> coprocessorExec(\r\n      Class<T> protocol, byte[] startKey, byte[] endKey,\r\n      Batch.Call<T,R> callable)\r\n      throws IOException, Throwable {\r\n\r\n    final Map<byte[],R> results = new TreeMap<byte[],R>(\r\n        Bytes.BYTES_COMPARATOR);\r\n    coprocessorExec(protocol, startKey, endKey, callable,\r\n        new Batch.Callback<R>(){\r\n      public void update(byte[] region, byte[] row, R value) {\r\n        results.put(region, value);\r\n      }\r\n    });\r\n    return results;\r\n  }\r\n\r\n\r\n\r\n\r\n\r\nwhen mulit region  call the Batch.Callback ,the treemap should lockup.\r\nwe meet this situation after we run 3 month."
    ],
    [
        "HBASE-6707",
        "HBASE-6690",
        "TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps https://builds.apache.org/job/HBase-TRUNK/3293/\r\n\r\nError Message\r\n\r\nArchived HFiles (hdfs://localhost:59986/user/jenkins/hbase/.archive/otherTable/01ced3b55d7220a9c460273a4a57b198/fam) should have gotten deleted, but didn't, remaining files:[hdfs://localhost:59986/user/jenkins/hbase/.archive/otherTable/01ced3b55d7220a9c460273a4a57b198/fam/fc872572a1f5443eb55b6e2567cfeb1c]\r\n\r\nStacktrace\r\n\r\njava.lang.AssertionError: Archived HFiles (hdfs://localhost:59986/user/jenkins/hbase/.archive/otherTable/01ced3b55d7220a9c460273a4a57b198/fam) should have gotten deleted, but didn't, remaining files:[hdfs://localhost:59986/user/jenkins/hbase/.archive/otherTable/01ced3b55d7220a9c460273a4a57b198/fam/fc872572a1f5443eb55b6e2567cfeb1c]\r\n\tat org.junit.Assert.fail(Assert.java:93)\r\n\tat org.junit.Assert.assertTrue(Assert.java:43)\r\n\tat org.junit.Assert.assertNull(Assert.java:551)\r\n\tat org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables(TestZooKeeperTableArchiveClient.java:291)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
        "TestZooKeeperTableArchiveClient.testMultipleTables is flapping TestZooKeeperTableArchiveClient.testMultipleTables is a flapping test. It is complaining that some archived HFiles were not deleted.\r\n\r\nTest history: https://builds.apache.org/job/HBase-TRUNK/3293/testReport/junit/org.apache.hadoop.hbase.backup.example/TestZooKeeperTableArchiveClient/testMultipleTables/history/\r\n\r\nError message:\r\nArchived HFiles (hdfs://localhost:59986/user/jenkins/hbase/.archive/otherTable/01ced3b55d7220a9c460273a4a57b198/fam) should have gotten deleted, but didn't, remaining files:\\[hdfs://localhost:59986/user/jenkins/hbase/.archive/otherTable/01ced3b55d7220a9c460273a4a57b198/fam/fc872572a1f5443eb55b6e2567cfeb1c\\]\r\n"
    ],
    [
        "HBASE-6762",
        "HBASE-6761",
        "HBASE-6340 broke SecureRPCEngine {noformat}\r\n$ mvn -Psecurity -DskipTests clean install\r\n[...]\r\n[ERROR] /usr/src/Hadoop/HBase/hbase-0.94/security/src/main/java/org/apache/hadoop/hbase/ipc/SecureRpcEngine.java:[165,20] cannot find symbol\r\n[ERROR] symbol  : constructor Invocation(java.lang.reflect.Method,java.lang.Object[])\r\n[ERROR] location: class org.apache.hadoop.hbase.ipc.Invocation\r\n[ERROR] /usr/src/Hadoop/HBase/hbase-0.94/security/src/main/java/org/apache/hadoop/hbase/ipc/SecureRpcEngine.java:[237,23] cannot find symbol\r\n[ERROR] symbol  : constructor Invocation(java.lang.reflect.Method,java.lang.Object[])\r\n[ERROR] location: class org.apache.hadoop.hbase.ipc.Invocation\r\n\r\n{noformat}",
        "secure build is failing: \"cannot find symbol symbol  : constructor Invocation(java.lang.reflect.Method,java.lang.Object[])\" HBASE-6340 udpated Invocation.java but did not to update SecureRpcEngine with the signature changes."
    ],
    [
        "HBASE-6924",
        "HBASE-4458",
        "HBase Master spews into a loop if a user attempts to create a snappy table when snappy isn't properly configured If a user attempts to create a table, for instance\r\n create 't1',{ NAME=>'c1', COMPRESSION=>'snappy'}\r\non stock HBase(Without snappy setup), \r\nthe master will spew this error in a loop\r\n\r\n12/10/02 12:41:38 INFO handler.OpenRegionHandler: Opening of region {NAME => 't1,,1349206881317.2d34e32205ffe677496b03faa7e66063.', STARTKEY => '', ENDKEY => '', ENCODED => 2d34e32205ffe677496b03faa7e66063,} failed, marking as FAILED_OPEN in ZK\r\n12/10/02 12:41:38 INFO regionserver.HRegionServer: Received request to open region: t1,,1349206881317.2d34e32205ffe677496b03faa7e66063.\r\n12/10/02 12:41:38 INFO regionserver.HRegion: Setting up tabledescriptor config now ...\r\n12/10/02 12:41:38 ERROR handler.OpenRegionHandler: Failed open of region=t1,,1349206881317.2d34e32205ffe677496b03faa7e66063., starting to roll back the global memstore size.\r\njava.io.IOException: Compression algorithm 'snappy' previously failed test.\r\n        at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:78)\r\n        at org.apache.hadoop.hbase.regionserver.HRegion.checkCompressionCodecs(HRegion.java:3822)\r\n        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3811)\r\n        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3761)\r\n        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:332)\r\n        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:108)\r\n        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:169)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n        at java.lang.Thread.run(Thread.java:662)\r\nEven after the shell is killed.\r\n\r\nIn fact, even after a hbase reboot we will endlessly spew this painful and high overhead error.\r\n\r\n",
        "HBase should give actionable information when a region is compressed with a codec that is not available. A cluster that previously used LZO codec was upgraded with the intent of moving away from the codec to another.  Several regions failed to deploy because the LZO codec was no longer present.  However, there was little indication that this as the problem.\r\n\r\nIdeally, the master web ui or hbck would detect these problems and provide why it fails to deploy and also provide an actionable error message.\r\n\r\n"
    ],
    [
        "HBASE-7122",
        "HBASE-6446",
        "Proper warning message when opening a log file with no entries (idle cluster) In case the cluster is idle and the log has rolled (offset to 0), replicationSource tries to open the log and gets an EOF exception. This gets printed after every 10 sec until an entry is inserted in it.\r\n{code}\r\n2012-11-07 15:47:40,924 DEBUG regionserver.ReplicationSource (ReplicationSource.java:openReader(487)) - Opening log for replication c0315.hal.cloudera.com%2C40020%2C1352324202860.1352327804874 at 0\r\n2012-11-07 15:47:40,926 WARN  regionserver.ReplicationSource (ReplicationSource.java:openReader(543)) - 1 Got: \r\njava.io.EOFException\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:180)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:152)\r\n\tat org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1508)\r\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1486)\r\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1475)\r\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1470)\r\n\tat org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader$WALReader.<init>(SequenceFileLogReader.java:55)\r\n\tat org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.init(SequenceFileLogReader.java:175)\r\n\tat org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(HLog.java:716)\r\n\tat org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.openReader(ReplicationSource.java:491)\r\n\tat org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:290)\r\n2012-11-07 15:47:40,927 WARN  regionserver.ReplicationSource (ReplicationSource.java:openReader(547)) - Waited too long for this file, considering dumping\r\n2012-11-07 15:47:40,927 DEBUG regionserver.ReplicationSource (ReplicationSource.java:sleepForRetries(562)) - Unable to open a reader, sleeping 1000 times 10\r\n\r\n{code}\r\nWe should reduce the log spewing in this case (or some informative message, based on the offset).",
        "Replication source will throw EOF exception when hlog size is 0 when master cluster startup new hlog which size is 0 will be created. if we start replication, replication source will print many EOF exception when openreader. I think we need to ignore this case and do not print so many exception warning log ."
    ],
    [
        "HBASE-7476",
        "HBASE-6262",
        "HBase shell count command doesn't escape binary output When running the the count command in the HBase shell, the row key is printed each time a count interval is reached. However, the key is printed verbatim, meaning that non-printable characters are directly printed to the terminal. This can cause confusing results, or even leave the terminal in an unusable state.",
        "Row Count hangs due to printing raw without filtering for control codes/non-displayable characters The problem is the key is printed raw without filtering for control codes/non-displayable characters. Consequently, row 264000's row key includes the data <ESC>P (hex: 1B 50) which initiates a VT100/ANSI DCS sequence, which normally must be terminated by a DCS string terminator code, <ESC>\\ (hex: 1B 5C) (cf.\r\n\r\nAt row 264000, <ESC>P appears, but is not followed by the <ESC>\\ termination code -- so instead of data being output to the display, it is getting buffered by the terminal to complete the DCS command. Result: the data is not displayed (xterminal) or the terminal locks up (gnome-terminal).\r\n\r\nNote that this is a potential for abusing this \"feature\" to hide/alter information displayed on the screen, since an arbitrary terminal control codes could potentially be constructed and injected into a rowkey.\r\n\r\nIf you open a VT100-emulating terminal window can cat the attached file, as-is, it would hang after displaying the starting portion of the line for row 264000."
    ],
    [
        "HBASE-7489",
        "HBASE-7473",
        "TestHCM is racy And the patch fixes this. Pure test issue.",
        "TestHCM#testRegionCaching fails intermittently in trunk builds In trunk build #3683, I saw:\r\n\r\n  testRegionCaching(org.apache.hadoop.hbase.client.TestHCM): test timed out after 60000 milliseconds"
    ],
    [
        "HBASE-7507",
        "HBASE-7385",
        "Make memstore flush be able to retry after exception We will abort regionserver if memstore flush throws exception.\r\n\r\nI thinks we could do retry to make regionserver more stable because file system may be not ok in a transient time. e.g. Switching namenode in the NamenodeHA environment\r\n\r\n\r\n\r\n{code}\r\nHRegion#internalFlushcache(){\r\n\r\n...\r\ntry {\r\n...\r\n}catch(Throwable t){\r\nDroppedSnapshotException dse = new DroppedSnapshotException(\"region: \" +\r\n          Bytes.toStringBinary(getRegionName()));\r\ndse.initCause(t);\r\nthrow dse;\r\n}\r\n...\r\n\r\n}\r\n\r\nMemStoreFlusher#flushRegion(){\r\n...\r\nregion.flushcache();\r\n...\r\n try {\r\n}catch(DroppedSnapshotException ex){\r\nserver.abort(\"Replay of HLog required. Forcing server shutdown\", ex);\r\n}\r\n\r\n...\r\n}\r\n{code}",
        "Do not abort regionserver if StoreFlusher.flushCache() fails A rare NN failover may cause RS abort, in the following sequence of events: \r\n - RS tries to flush the memstore\r\n - Create a file, start block, and acquire a lease\r\n - Block is complete, lease removed, but before we send the RPC response back to the client, NN is killed.\r\n - New NN comes up, client retries the block complete again, the new NN throws lease expired since the block was already complete.\r\n - RS receives the exception, and aborts.\r\n\r\nThis is actually a NN+DFSClient issue that, the dfs client from RS does not receive the rpc response about the block close, and upon retry on the new NN, it gets the exception, since the file was already closed. However, although this is DFS client specific, we can also make RS more resilient by not aborting the RS upon exception from the flushCache(). We can change StoreFlusher so that: \r\n\r\nStoreFlusher.prepare() will become idempotent (so will Memstore.snapshot())\r\nStoreFlusher.flushCache() will throw with IOException upon DFS exception, but we catch IOException, and just abort the flush request (not RS).\r\nStoreFlusher.commit() still cause RS abort on exception. This is also debatable. If dfs is alive, and we can undo the flush changes, than we should not abort. \r\n\r\nlogs: \r\n{code}\r\norg.apache.hadoop.hbase.DroppedSnapshotException: region: loadtest_ha,e6666658,1355820729877.298bcbd550b80507a379fe67eefbe5ea.\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1485)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1364)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:896)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:845)\r\n\tat org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:119)\r\n\tat org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:169)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n\tat java.lang.Thread.run(Thread.java:662)\r\nCaused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /apps/hbase/data/loadtest_ha/298bcbd550b80507a379fe67eefbe5ea/.tmp/5cf8951ee12449ce8e4e6dd0bf1645c2 File is not open for writing. [Lease.  Holder: DFSClient_hb_rs_XXX,60020,1355813552066_203591774_25, pendingcreates: 1]\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1724)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1707)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:1762)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:1750)\r\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.complete(NameNode.java:779)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n\tat java.lang.reflect.Method.invoke(Method.java:597)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:578)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1393)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1389)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1387)\r\n\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1107)\r\n\tat org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)\r\n\tat $Proxy10.complete(Unknown Source)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n\tat java.lang.reflect.Method.invoke(Method.java:597)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)\r\n\tat $Proxy10.complete(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:4087)\r\n\tat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3988)\r\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61)\r\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86)\r\n\tat org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.finishClose(AbstractHFileWriter.java:255)\r\n\tat org.apache.hadoop.hbase.io.hfile.HFileWriterV2.close(HFileWriterV2.java:432)\r\n\tat org.apache.hadoop.hbase.regionserver.StoreFile$Writer.close(StoreFile.java:1214)\r\n\tat org.apache.hadoop.hbase.regionserver.Store.internalFlushCache(Store.java:762)\r\n\tat org.apache.hadoop.hbase.regionserver.Store.flushCache(Store.java:674)\r\n\tat org.apache.hadoop.hbase.regionserver.Store.access$400(Store.java:109)\r\n\tat org.apache.hadoop.hbase.regionserver.Store$StoreFlusherImpl.flushCache(Store.java:2286)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1460)\r\n{code}"
    ],
    [
        "HBASE-7581",
        "HBASE-7580",
        "TestAccessController depends on the execution order ",
        "TestAccessController fails in trunk It failed in build #3756.\r\nI can reproduce the failure locally:\r\n{code}\r\ntestReadWrite(org.apache.hadoop.hbase.security.access.TestAccessController)  Time elapsed: 39.306 sec  <<< ERROR!\r\norg.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=10, exceptions:\r\nWed Jan 16 04:31:13 PST 2013, org.apache.hadoop.hbase.client.HTable$10@124d998e, java.io.IOException: java.io.IOException: java.lang.NullPointerException\r\nWed Jan 16 04:31:14 PST 2013, org.apache.hadoop.hbase.client.HTable$10@124d998e, java.io.IOException: java.io.IOException: java.lang.NullPointerException\r\nWed Jan 16 04:31:16 PST 2013, org.apache.hadoop.hbase.client.HTable$10@124d998e, java.io.IOException: java.io.IOException: java.lang.NullPointerException\r\nWed Jan 16 04:31:17 PST 2013, org.apache.hadoop.hbase.client.HTable$10@124d998e, java.io.IOException: java.io.IOException: java.lang.NullPointerException\r\nWed Jan 16 04:31:19 PST 2013, org.apache.hadoop.hbase.client.HTable$10@124d998e, java.io.IOException: java.io.IOException: java.lang.NullPointerException\r\nWed Jan 16 04:31:21 PST 2013, org.apache.hadoop.hbase.client.HTable$10@124d998e, java.io.IOException: java.io.IOException: java.lang.NullPointerException\r\nWed Jan 16 04:31:25 PST 2013, org.apache.hadoop.hbase.client.HTable$10@124d998e, java.io.IOException: java.io.IOException: java.lang.NullPointerException\r\nWed Jan 16 04:31:29 PST 2013, org.apache.hadoop.hbase.client.HTable$10@124d998e, java.io.IOException: java.io.IOException: java.lang.NullPointerException\r\nWed Jan 16 04:31:37 PST 2013, org.apache.hadoop.hbase.client.HTable$10@124d998e, java.io.IOException: java.io.IOException: java.lang.NullPointerException\r\nWed Jan 16 04:31:53 PST 2013, org.apache.hadoop.hbase.client.HTable$10@124d998e, java.io.IOException: java.io.IOException: java.lang.NullPointerException\r\n\r\n  at org.apache.hadoop.hbase.client.ServerCallable.withRetries(ServerCallable.java:186)\r\n  at org.apache.hadoop.hbase.client.HTable.checkAndDelete(HTable.java:843)\r\n  at org.apache.hadoop.hbase.security.access.TestAccessController$27.run(TestAccessController.java:668)\r\n{code}"
    ],
    [
        "HBASE-7724",
        "HBASE-6060",
        "[0.94] Just OPENED regions are ignored by ServerShutdownHandler and go unassigned for ever Visiting a user today, I came across following interesting case (0.94.2 HBase).\r\n\r\nA server was added to cluster.  It was assigned regions by the balancer.  A bunch opened on the regionserver and just after the open, the regionserver was manually shutdown.  There was a lag processing the zk region open events in the master (because clean shutdown, there was no zk activity when the regions were closed on the shutdown regionserver).  Processing the server shutdown, we do this for a good few of the regions that had just been opened on the regionserver:\r\n\r\n2013-01-30 02:41:19,917 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Skip assigning region OBFUSCATED_TABLE,OBFUSCATED_STARTROW,1344723216908.55e9cb551edeea0b52bb91af7c2de199. state=OPEN, ts=1359513674715, server=XX.XX.18.40,10304,1359513445136\r\n\r\nSeems like outright bug that'd we'd skip a region that is in transition that is in the OPEN state.\r\n\r\nMore detail to follow.",
        "Regions's in OPENING state from failed regionservers takes a long time to recover we have seen a pattern in tests, that the regions are stuck in OPENING state for a very long time when the region server who is opening the region fails. My understanding of the process: \r\n \r\n - master calls rs to open the region. If rs is offline, a new plan is generated (a new rs is chosen). RegionState is set to PENDING_OPEN (only in master memory, zk still shows OFFLINE). See HRegionServer.openRegion(), HMaster.assign()\r\n - RegionServer, starts opening a region, changes the state in znode. But that znode is not ephemeral. (see ZkAssign)\r\n - Rs transitions zk node from OFFLINE to OPENING. See OpenRegionHandler.process()\r\n - rs then opens the region, and changes znode from OPENING to OPENED\r\n - when rs is killed between OPENING and OPENED states, then zk shows OPENING state, and the master just waits for rs to change the region state, but since rs is down, that wont happen. \r\n - There is a AssignmentManager.TimeoutMonitor, which does exactly guard against these kind of conditions. It periodically checks (every 10 sec by default) the regions in transition to see whether they timedout (hbase.master.assignment.timeoutmonitor.timeout). Default timeout is 30 min, which explains what you and I are seeing. \r\n - ServerShutdownHandler in Master does not reassign regions in OPENING state, although it handles other states. \r\n\r\nLowering that threshold from the configuration is one option, but still I think we can do better. \r\n\r\nWill investigate more. "
    ],
    [
        "HBASE-7774",
        "HBASE-6265",
        "RegionObserver.prePut() cannot rely on the Put's timestamps, can even cause data loss We had a user that had code that looked like this in a coprocessor's prePut():\r\n\r\n{code}\r\nif (put.has(expectedKv))\r\n  put.add(kvSayingIFoundIt);\r\nelse\r\n  put.add(kvSayingNotFound);\r\n{code}\r\n\r\nIf you have MSLAB turned *off*, and you have the {{expectedKv}} in your {{Put}}, doing a {{Get}} following your insert will only return {{kvSayingIFoundIt}} and not the KV you were actually inserting.\r\n\r\nMore so, if you only do {{put.has(expectedKv)}}, you will not get anything back. Your data seems to be gone.\r\n\r\nThe reason is that in {{prePut()}} the timestamp hasn't been set yet, so calling {{kv.getTimestamp()}} during the comparisons in {{put.has()}} will populate {{kv.timestampCache}} with {{Long.MAX_VALUE}}. Then it will stay in the {{MemStore}} with that big timestamp and be filtered out because {{TimeRange}} will compare {{Long.MAX_VALUE}} >= {{Long.MAX_VALUE}} and return {{SKIP}}.\r\n\r\nAnd the reason it works correctly with MSLAB *on* is that the KV is cloned in {{maybeCloneWithAllocator()}} and the cache is reset.\r\n\r\nNow, I think this has bigger implications. Basically, you can't rely on the timestamp at all in {{prePut()}}. I'm sure this can screw someone else in a creative way later.",
        "Calling getTimestamp() on a KV in cp.prePut() causes KV not to be flushed There is an issue when you call getTimestamp() on any KV handed into a Coprocessor's prePut(). It initializes the internal \"timestampCache\" variable. \r\n\r\nWhen you then pass it to the normal processing, the region server sets the time to the server time in case you have left it unset from the client side (updateLatestStamp() call). \r\n\r\nThe TimeRangeTracker then calls getTimestamp() later on to see if it has to include the KV, but instead of getting the proper time it sees the cached timestamp from the prePut() call."
    ],
    [
        "HBASE-7896",
        "HBASE-643",
        "make rename_table working in 92/94 The rename_table function is very useful for our customers. However, rename_table.rb does not work for 92/94. It has several bugs. It will be useful to fix them so that users can solve their problems. ",
        "Rename tables It would be nice to be able to rename tables, if this is possible.  Some of our internal users are doing things like: upload table mytable -> realize they screwed up -> upload table mytable_2 -> decide mytable_2 looks better -> have to go on using mytable_2 instead of originally desired table name."
    ],
    [
        "HBASE-7906",
        "HBASE-7859",
        "OfflineCallBack in bulk assignment does not work as expected This is as part of discussion in HBASE-7799.\r\nThe OfflineCallBack does not bother if the znode already exists or not.  Infact the catch block AM.asyncSetOfflineInZooKeeper() is a dead code.",
        "errors from creating ZK offline node are not handled Quote from HBASE-7799\r\n{quote}\r\nWhat i observed was\r\n\r\n{code}\r\ntry {\r\n      ZKAssign.asyncCreateNodeOffline(watcher, state.getRegion(),\r\n        destination, cb, state);\r\n    } catch (KeeperException e) {\r\n      if (e instanceof NodeExistsException) {\r\n        LOG.warn(\"Node for \" + state.getRegion() + \" already exists\");\r\n      } else {\r\n        server.abort(\"Unexpected ZK exception creating/setting node OFFLINE\", e);\r\n      }\r\n      return false;\r\n    }\r\nreturn true;\r\n{code}\r\n\r\nThe asyncCreateNodeOffline just always returns true because it does not wait for the callback to take action. Also the callback does not throw NodeExistsException.\r\n{quote}\r\nIn short the catch block is a dead code. "
    ],
    [
        "HBASE-8136",
        "HBASE-5492",
        "coprocessor service requires .meta. to be available all the time. \r\nHTable#getRegionLocations does not use a cache: all the calls to this function go to .META.\r\n\r\nSo:\r\n- we're missing an opportunity to reuse/update the location cache in the HConnection.\r\n- this method is called by the coprocessor service. So, for people using this features, they have .meta. on their execution path, and it's not good for performances, scalability and reliability.\r\n\r\nI'm not totally clear on the fix. I think it should be possible to use the cache to see if we have all regions for the table. But it means we won't always have the last version when calling getRegionLocations.\r\n\r\nAny thought on this?",
        "Caching StartKeys and EndKeys of Regions Each call for HTable.getStartEndKeys will read meta table.\r\n\r\nIn particular, \r\nin the case of client side multi-threaded concurrency statistics, \r\nwe must call HTable.coprocessorExec== > getStartKeysInRange ==> getStartEndKeys,\r\nresulting in the need to always scan the meta table.\r\n\r\nThis is not necessary,\r\nwe can implement the HConnectionManager.HConnectionImplementation.locateRegions(byte[] tableName) method,\r\n\r\nthen, get the StartKeys and EndKeys from the cachedRegionLocations of HConnectionImplementation.\r\n\r\nCombined with https://issues.apache.org/jira/browse/HBASE-5491, can improve the performance of statistical\r\n"
    ],
    [
        "HBASE-8136",
        "HBASE-6870",
        "coprocessor service requires .meta. to be available all the time. \r\nHTable#getRegionLocations does not use a cache: all the calls to this function go to .META.\r\n\r\nSo:\r\n- we're missing an opportunity to reuse/update the location cache in the HConnection.\r\n- this method is called by the coprocessor service. So, for people using this features, they have .meta. on their execution path, and it's not good for performances, scalability and reliability.\r\n\r\nI'm not totally clear on the fix. I think it should be possible to use the cache to see if we have all regions for the table. But it means we won't always have the last version when calling getRegionLocations.\r\n\r\nAny thought on this?",
        "HTable#coprocessorExec always scan the whole table  In current logic, HTable#coprocessorExec always scans the entire META table, loading it into memory and then filters the keys to return only those that fall in specified range.  The version after the patch only scans the portions of meta that are in the specified key range, and returns them.  Put simply -- before we did a load-all-then-filter; afterwards we only-scan-what-is-needed.\r\n\r\nThe former has low efficiency and greatly impacts the Regionserver carrying .META. when there are many coprocessorExec requests.\r\n\r\n"
    ],
    [
        "HBASE-8212",
        "HBASE-8207",
        "Introduce a new separator instead of hyphen('-') for renaming recovered queues' znodes hyphen is frequently used in the HostName. Likes we have one regionserver named \"160-172-0-1\", so under this scenario, 160-172-0-1 will be splited to 4 Strings and will be considered for 4 possible dead servers.\r\nIt won't find all the logs for \"160-172-0-1\" any more, so causes data-loss.",
        "Replication could have data loss when machine name contains hyphen \"-\" In the recent test case TestReplication* failures, I'm finally able to find the cause(or one of causes) for its intermittent failures.\r\n\r\nWhen a machine name contains \"-\", it breaks the function ReplicationSource.checkIfQueueRecovered. It causes the following issue:\r\n\r\ndeadRegionServers list is way off so that replication doesn't wait for log splitting finish for a wal file and move on to the next one(data loss)\r\n\r\nYou can see that replication use those weird paths constructed from deadRegionServers to check a file existence\r\n{code}\r\n2013-03-26 21:26:51,385 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/1.compute.internal,52170,1364333181125/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540\r\n2013-03-26 21:26:51,386 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/1.compute.internal,52170,1364333181125-splitting/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540\r\n2013-03-26 21:26:51,387 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/west/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540\r\n2013-03-26 21:26:51,389 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/west-splitting/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540\r\n2013-03-26 21:26:51,391 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/156.us/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540\r\n2013-03-26 21:26:51,394 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/156.us-splitting/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540\r\n2013-03-26 21:26:51,396 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/0/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540\r\n2013-03-26 21:26:51,398 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/0-splitting/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540\r\n{code}\r\n\r\nThis happened in the recent test failure in http://54.241.6.143/job/HBase-0.94/org.apache.hbase$hbase/21/testReport/junit/org.apache.hadoop.hbase.replication/TestReplicationQueueFailover/queueFailover/?auto_refresh=false\r\n\r\nSearch for \r\n{code}\r\nFile does not exist: hdfs://localhost:52882/user/ec2-user/hbase/.oldlogs/ip-10-197-0-156.us-west-1.compute.internal%2C52170%2C1364333181125.1364333199540\r\n{code}\r\n\r\nAfter 10 times retries, replication source gave up and move on to the next file. Data loss happens. \r\n\r\nSince lots of EC2 machine names contain \"-\" including our Jenkin servers, this is a high impact issue."
    ],
    [
        "HBASE-8251",
        "HBASE-7824",
        "enable SSH before assign META on Master startup I think HBASE-5918 could not fix this issue. In HMaster#assignRootAndMeta:\r\n1. Assign ROOT.\r\n2. Block until ROOT be opened.\r\n3. Assign META.\r\n4. Block until META be opened.\r\n\r\nSSH is enabled after step 4. So if the RS who host ROOT dies before step 4, master will be blocked.\r\n",
        "Improve master start up time when there is log splitting work When there is log split work going on, master start up waits till all log split work completes even though the log split has nothing to do with meta region servers.\r\n\r\nIt's a bad behavior considering a master node can run when log split is happening while its start up is blocking by log split work. \r\n\r\nSince master is kind of single point of failure, we should start it ASAP.\r\n"
    ],
    [
        "HBASE-8277",
        "HBASE-8275",
        "[API Compatibility]: Create tool to analyze changes in the HBase Java public API API incompatibilities can be frustrating for customers. Therefore, it is important to identify incompatibilities and correct them.\r\n\r\nThere is a tool called JDiff which will compare two public API descriptions. Having it pull from a git repo it will make it very useful.",
        "Tool to test binary compatibility Stack and I were discussing of ways to make binary compatibility easier to test than doing it completely by hand.\r\n\r\nOne idea would be to have a tool that uses reflection to generate code that calls all the public methods from a list of classes. You would then compile this code against the current version you are on, then try it out with different HBase jars without recompiling."
    ],
    [
        "HBASE-8428",
        "HBASE-8297",
        "Tighten up IntegrationTestsDriver filter Currently, filter that looks for IntegrationTests is broad.  Reports loads of errors as we try to parse classes we don't care about.  Let me tighten it up so it doesn't scare folks away.\r\n\r\nIt is particular bad when being run against a distribute cluster when the test context is not all present; here there are lots of ERROR reports about classes not found.",
        "ClassFinder/IntegrationTestDriver errors are not helpful ClassFinder logs a bunch of errors about some random classes it cannot load. This is not useful. Ideally it should only log the classes if they satisfy the other criteria, such as the name filter, but as far as I remember the organization of code that is checked after the class is loaded. \r\nAlso maybe IntegrationTestTool should have a list command that will list the tests that can be run, and tell ClassFinder to list the JARs/directories examined."
    ],
    [
        "HBASE-8490",
        "HBASE-5356",
        "Region mover script can get stuck in infinite loop if UnknownRegionException {code}\r\n13/05/02 23:56:55 INFO region_mover: Moving 1 region(s) from procyon-8.nova.cloudera.com,60020,1367530634629 during this cycle\r\n13/05/02 23:56:55 INFO region_mover: Moving region 588ccd887c699c7c23188a79a3d1807f (0 of 1) to server=procyon-5.nova.cloudera.com,60020,1367542172830\r\n13/05/02 23:56:55 INFO region_mover: Exception moving 588ccd887c699c7c23188a79a3d1807f; split/moved? Continuing: java.lang.reflect.UndeclaredThrowableException: org.apache.hadoop.hbase.UnknownRegionException: 588ccd887c699c7c23188a79a3d1807f\r\n\tat org.apache.hadoop.hbase.master.HMaster.move(HMaster.java:1015)\r\n\tat sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n\tat java.lang.reflect.Method.invoke(Method.java:597)\r\n\tat org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)\r\n\tat org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1345)\r\n\r\n{code}\r\n\r\nThis was triggered due {{IllegalStateException}} while trying to open an unexisting region:\r\n\r\n{code}\r\n2013-05-02 16:23:06,558 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received request to open region: TestTable,,1367530404417.588ccd887c699c7c23188a79a3d1\r\n807f.\r\n2013-05-02 16:23:06,583 INFO org.apache.hadoop.hbase.util.FSUtils: hdfs://nameservice1/hbase/TestTable doesn't exist\r\n2013-05-02 16:23:06,586 INFO org.apache.hadoop.hbase.util.FSUtils: hdfs://nameservice1/hbase/TestTable doesn't exist\r\n2013-05-02 16:23:06,586 WARN org.apache.hadoop.hbase.util.FSTableDescriptors: The following folder is in HBase's root directory and doesn't contain a table descriptor,\r\n do consider deleting it: TestTable\r\n2013-05-02 16:23:06,666 ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed open of region=TestTable,,1367530404417.588ccd887c699c7c23188a79a3\r\nd1807f.\r\njava.lang.IllegalStateException: Could not instantiate a region instance.\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(HRegion.java:3123)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3254)\r\n\tat org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:331)\r\n\tat org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:107)\r\n\tat org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:169)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n\tat java.lang.Thread.run(Thread.java:662)\r\nCaused by: java.lang.reflect.InvocationTargetException\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(HRegion.java:3120)\r\n\t... 7 more\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.loadTableCoprocessors(RegionCoprocessorHost.java:133)\r\n\tat org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.<init>(RegionCoprocessorHost.java:125)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.<init>(HRegion.java:376)\r\n\t... 12 more\r\n{code}\r\n\r\n",
        "region_mover.rb can hang if table region it belongs to is deleted. I was testing the region_mover.rb script on a loaded hbase and noticed that it can hang (thus hanging graceful shutdown) if a region that it is attempting to move gets deleted (by a table delete operation).\r\n\r\nHere's the start of the relevent stack dump\r\n{code}\r\n12/02/08 13:27:13 WARN client.HConnectionManager$HConnectionImplementation: Encountered problems when prefetch META table:\r\norg.apache.hadoop.hbase.TableNotFoundException: Cannot find row in .META. for table: TestLoadAndVerify_1328735001040, row=TestLoadAnd\\\r\nVerify_1328735001040,yC^P\\xD7\\x945\\xD4,99999999999999\r\n        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:136)\r\n        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:95)\r\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:64\\\r\n9)\r\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:703\\\r\n)\r\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:594)\r\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:565)\r\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:416)\r\n        at org.apache.hadoop.hbase.client.ServerCallable.instantiateServer(ServerCallable.java:57)\r\n        at org.apache.hadoop.hbase.client.ScannerCallable.instantiateServer(ScannerCallable.java:63)\r\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.\\\r\njava:1018)\r\n        at org.apache.hadoop.hbase.client.HTable$ClientScanner.nextScanner(HTable.java:1104)\r\n        at org.apache.hadoop.hbase.client.HTable$ClientScanner.initialize(HTable.java:1027)\r\n        at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:535)\r\n        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n        at java.lang.reflect.Method.invoke(Method.java:597)\r\n        at org.jruby.javasupport.JavaMethod.invokeDirectWithExceptionHandling(JavaMethod.java:525)\r\n        at org.jruby.javasupport.JavaMethod.invokeDirect(JavaMethod.java:380)\r\n        at org.jruby.java.invokers.InstanceMethodInvoker.call(InstanceMethodInvoker.java:58)\r\n        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:137)\r\n        at usr.lib.hbase.bin.region_mover.method__7$RUBY$isSuccessfulScan(/usr/lib/hbase/bin/region_mover.rb:133)\r\n        at usr$lib$hbase$bin$region_mover#method__7$RUBY$isSuccessfulScan.call(usr$lib$hbase$bin$region_mover#method__7$RUBY$isSucces\\\r\nsfulScan:65535)\r\n        at usr$lib$hbase$bin$region_mover#method__7$RUBY$isSuccessfulScan.call(usr$lib$hbase$bin$region_mover#method__7$RUBY$isSucces\\\r\nsfulScan:65535)\r\n        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:171)\r\n        at usr.lib.hbase.bin.region_mover.block_4$RUBY$__for__(/usr/lib/hbase/bin/region_mover.rb:326)\r\n        at usr$lib$hbase$bin$region_mover#block_4$RUBY$__for__.call(usr$lib$hbase$bin$region_mover#block_4$RUBY$__for__:65535)\r\n        at org.jruby.runtime.CompiledBlock.yield(CompiledBlock.java:133)\r\n        at org.jruby.runtime.BlockBody.call(BlockBody.java:73)\r\n        at org.jruby.runtime.Block.call(Block.java:89)\r\n        at org.jruby.RubyProc.call(RubyProc.java:268)\r\n        at org.jruby.RubyProc.call(RubyProc.java:228)\r\n        at org.jruby.RubyProc$i$0$0$call.call(RubyProc$i$0$0$call.gen:65535)\r\n        at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:209)\r\n        at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:205)\r\n        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:137)\r\n        at org.jruby.ast.CallOneArgNode.interpret(CallOneArgNode.java:57)\r\n        at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:103)\r\n        at org.jruby.ast.WhileNode.interpret(WhileNode.java:131)\r\n        at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:103)\r\n        at org.jruby.ast.BlockNode.interpret(BlockNode.java:71)\r\n        at org.jruby.evaluator.ASTInterpreter.INTERPRET_METHOD(ASTInterpreter.java:74)\r\n        at org.jruby.internal.runtime.methods.InterpretedMethod.call(InterpretedMethod.java:169)\r\n        at org.jruby.internal.runtime.methods.DefaultMethod.call(DefaultMethod.java:171)\r\n        at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:272)\r\n        at org.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite.java:114)\r\n        at org.jruby.runtime.callsite.CachingCallSite.callIter(CachingCallSite.java:123)\r\n        at usr.lib.hbase.bin.region_mover.chained_26_rescue_4$RUBY$SYNTHETICunloadRegions(/usr/lib/hbase/bin/region_mover.rb:319)\r\n        at usr.lib.hbase.bin.region_mover.method__25$RUBY$unloadRegions(/usr/lib/hbase/bin/region_mover.rb:313)\r\n        at usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions.call(usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions:65535)\r\n        at usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions.call(usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions:65535)\r\n        at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:302)\r\n        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:173)\r\n        at usr.lib.hbase.bin.region_mover.__file__(/usr/lib/hbase/bin/region_mover.rb:430)\r\n        at usr.lib.hbase.bin.region_mover.load(/usr/lib/hbase/bin/region_mover.rb)\r\n        at org.jruby.Ruby.runScript(Ruby.java:670)\r\n        at org.jruby.Ruby.runNormally(Ruby.java:574)\r\n        at org.jruby.Ruby.runFromMain(Ruby.java:423)\r\n        at org.jruby.Main.doRunFromMain(Main.java:278)\r\n        at org.jruby.Main.internalRun(Main.java:198)\r\n        at org.jruby.Main.run(Main.java:164)\r\n        at org.jruby.Main.run(Main.java:148)\r\n        at org.jruby.Main.main(Main.java:128)\r\n{code}"
    ],
    [
        "HBASE-8612",
        "HBASE-8590",
        "Fix TestMetaScanner.testConcurrentMetaScannerAndCatalogJanitor failure Got this test failure:\r\nREGRESSION:  org.apache.hadoop.hbase.client.TestMetaScanner.testConcurrentMetaScannerAndCatalogJanitor\r\n\r\nError Message:\r\nSplit daughter region testConcurrentMetaScannerAndCatalogJanitor,q\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF,1369373178944.aa8d1dc3daf7fae3ec55a940f9848e42. cannot be found in META.\r\n\r\nStack Trace:\r\norg.apache.hadoop.hbase.client.RegionOfflineException: Split daughter region testConcurrentMetaScannerAndCatalogJanitor,q\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF,1369373178944.aa8d1dc3daf7fae3ec55a940f9848e42. cannot be found in META.\r\n        at org.apache.hadoop.hbase.client.MetaScanner$BlockingMetaScannerVisitor.processRow(MetaScanner.java:433)\r\n        at org.apache.hadoop.hbase.client.MetaScanner$TableMetaScannerVisitor.processRow(MetaScanner.java:495)\r\n        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:224)\r\n        at org.apache.hadoop.hbase.client.MetaScanner.access$000(MetaScanner.java:54)\r\n        at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:133)\r\n        at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:130)\r\n        at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:383)\r\n        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:130)\r\n        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:105)\r\n        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:83)\r\n        at org.apache.hadoop.hbase.client.MetaScanner.allTableRegions(MetaScanner.java:323)\r\n        at org.apache.hadoop.hbase.client.TestMetaScanner$1MetaScannerVerifier.run(TestMetaScanner.java:194)\r\n        at java.lang.Thread.run(Thread.java:662)\r\n",
        "[0.94] BlockingMetaScannerVisitor should check for parent meta entry while waiting for split daughter This was discovered after HBASE-8505 went in, which introduces a test sporadically triggering this bug. \r\n\r\nFrom comments at HBASE-8505: \r\nFrom the logs at https://builds.apache.org/job/HBase-0.94-security/ws/trunk/target/surefire-reports/org.apache.hadoop.hbase.client.TestMetaScanner-output.txt, I think I understand what is going on: \r\nBlockingMetaScannerVisitor blocks and wait for the split daughter to appear when it sees a parent region (HBASE-5986). CatalogJanitor on the other hand will order the regions in a (kind-of) topological sort (based on parent child relation) so that it will guarantee parents are not GC'd before daughters.\r\nWhat is happening in this issue is not related to the patch in this jira, but the test triggers this extremely rare case by running concurrent catalogjanitor, splits and metascanners. We have parent, splita and splitb regions, and catalogjanitor decides to delete parent first and splitb in one run. While there is a concurrent metascanner which will go over the parent, and sees that it is split, but before being able to read the split daughter, catalog janitor will delete both the parent and the child, which will lead to metascanner blocking until timeout and failing the test.\r\nOn solution might be to also check whether the parent is still there in BlockingMetaScannerVisitor while we are blocking for the daughter.\r\nGood thing is that with HBASE-7721, we don't need any of this in trunk."
    ],
    [
        "HBASE-8992",
        "HBASE-8991",
        "TestMasterFailover.testMasterFailoverWithMockedRITOnDeadRS fails This one fails rare enough.  It is a very long test.\r\n\r\nhttp://54.241.6.143/job/HBase-0.95-Hadoop-2/org.apache.hbase$hbase-server/645/testReport/org.apache.hadoop.hbase.master/TestMasterFailover/testMasterFailoverWithMockedRITOnDeadRS/\r\n\r\nFails like this:\r\n\r\n{code}\r\njava.lang.AssertionError: region=enabledTable,e\\xDC\\xB4,1374137106469.efad8aaaf052f839fdbf5abc2e04af4c., [{ENCODED => 1028785192, NAME => '.META.,,1', STARTKEY => '', ENDKEY => ''}, {ENCODED => 1e14e83d175bb4dc82a6eac427b0397c, NAME => 'disabledTable,,1374137107108.1e14e83d175bb4dc82a6eac427b0397c.', STARTKEY => '', ENDKEY => 'aaa'}, {ENCODED => 2ce3188d9e655c74aeed8627699d198b, NAME => 'disabledTable,aaa,1374137107131.2ce3188d9e655c74aeed8627699d198b.', STARTKEY => 'aaa', ENDKEY => 'bF\\xD8'}, {ENCODED => a87139be050d4e945fc4523cf2bd1467, NAME => 'disabledTable,bF\\xD8,1374137107163.a87139be050d4e945fc4523cf2bd1467.', STARTKEY => 'bF\\xD8', ENDKEY => 'c,O'}, {ENCODED => 822ce4ce69d81d4e1d6e5e8200887240, NAME => 'disabledTable,c,O,1374137107170.822ce4ce69d81d4e1d6e5e8200887240.', STARTKEY => 'c,O', ENDKEY => 'd\\x11\\xC6'}, {ENCODED => ad4209578ff51f67e975135f23b7ac13, NAME => 'disabledTable,d\\x11\\xC6,1374137107176.ad4209578ff51f67e975135f23b7ac13.', STARTKEY => 'd\\x11\\xC6', ENDKEY => 'd\\xF7='}, {ENCODED => 20c540d9d7213cf8dbf4b4b90007e7ad, NAME => 'disabledTable,d\\xF7=,1374137107182.20c540d9d7213cf8dbf4b4b90007e7ad.', STARTKEY => 'd\\xF7=', ENDKEY => 'e\\xDC\\xB4'}, {ENCODED => 04d860ad0b8e0ae5a0147a6bccea46f4, NAME => 'enabledTable,,1374137106279.04d860ad0b8e0ae5a0147a6bccea46f4.', STARTKEY => '', ENDKEY => 'aaa'}, {ENCODED => b5486e3cf48aa50ca1cba30d0d6ab662, NAME => 'enabledTable,aaa,1374137106428.b5486e3cf48aa50ca1cba30d0d6ab662.', STARTKEY => 'aaa', ENDKEY => 'bF\\xD8'}, {ENCODED => c11af1f6d1aa422916caf6e6ac17eeeb, NAME => 'enabledTable,bF\\xD8,1374137106437.c11af1f6d1aa422916caf6e6ac17eeeb.', STARTKEY => 'bF\\xD8', ENDKEY => 'c,O'}, {ENCODED => 5e4dffd62115d6745b589e40136179c7, NAME => 'enabledTable,c,O,1374137106444.5e4dffd62115d6745b589e40136179c7.', STARTKEY => 'c,O', ENDKEY => 'd\\x11\\xC6'}, {ENCODED => 0a7dc5f03fa7db89e2ed3aa7512bc6cd, NAME => 'enabledTable,d\\x11\\xC6,1374137106455.0a7dc5f03fa7db89e2ed3aa7512bc6cd.', STARTKEY => 'd\\x11\\xC6', ENDKEY => 'd\\xF7='}, {ENCODED => 8f406facf525870443e93345a3172fbb, NAME => 'enabledTable,d\\xF7=,1374137106463.8f406facf525870443e93345a3172fbb.', STARTKEY => 'd\\xF7=', ENDKEY => 'e\\xDC\\xB4'}, {ENCODED => 9daab73408eb240c4780493b6661eb7b, NAME => 'enabledTable,f\\xC2+,1374137106475.9daab73408eb240c4780493b6661eb7b.', STARTKEY => 'f\\xC2+', ENDKEY => 'g\\xA7\\xA2'}, {ENCODED => 47911855a8362112a57c3767d6864ced, NAME => 'enabledTable,g\\xA7\\xA2,1374137106482.47911855a8362112a57c3767d6864ced.', STARTKEY => 'g\\xA7\\xA2', ENDKEY => 'h\\x8D\\x19'}, {ENCODED => e6c71e364bfc9766c1336c9c912b9466, NAME => 'enabledTable,h\\x8D\\x19,1374137106488.e6c71e364bfc9766c1336c9c912b9466.', STARTKEY => 'h\\x8D\\x19', ENDKEY => 'ir\\x90'}, {ENCODED => 63ae8de95f8149c04a18c90ac0ba99db, NAME => 'enabledTable,ir\\x90,1374137106494.63ae8de95f8149c04a18c90ac0ba99db.', STARTKEY => 'ir\\x90', ENDKEY => 'jX\\x07'}, {ENCODED => f477cc9a5378ef9ac464c5ffccf5d295, NAME => 'enabledTable,jX\\x07,1374137106500.f477cc9a5378ef9ac464c5ffccf5d295.', STARTKEY => 'jX\\x07', ENDKEY => 'k=~'}, {ENCODED => d366f300dffeae98f6c0ace8222dc3e4, NAME => 'enabledTable,k=~,1374137106506.d366f300dffeae98f6c0ace8222dc3e4.', STARTKEY => 'k=~', ENDKEY => 'l\"\\xF5'}, {ENCODED => ca8b5a676edb14a42bf80de8c6eb2f84, NAME => 'enabledTable,m\\x08l,1374137106518.ca8b5a676edb14a42bf80de8c6eb2f84.', STARTKEY => 'm\\x08l', ENDKEY => 'm\\xED\\xE3'}]\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.junit.Assert.assertTrue(Assert.java:41)\r\n\tat org.apache.hadoop.hbase.master.TestMasterFailover.testMasterFailoverWithMockedRITOnDeadRS(TestMasterFailover.java:815)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n\tat java.lang.reflect.Method.invoke(Method.java:597)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n...\r\n{code}\r\n\r\nAm tempted to disable.  This test should be broken into smaller pieces.  Leaving for now.  Will keep an eye on it.",
        "TestMasterFailover.testMasterFailoverWithMockedRITOnDeadRS failed again It failed again http://54.241.6.143/job/HBase-0.95-Hadoop-2/org.apache.hbase$hbase-server/645/testReport/junit/org.apache.hadoop.hbase.master/TestMasterFailover/testMasterFailoverWithMockedRITOnDeadRS/\r\n\r\n{noformat}\r\nStacktrace\r\n\r\njava.lang.AssertionError: region=enabledTable,e\\xDC\\xB4,1374137106469.efad8aaaf052f839fdbf5abc2e04af4c., [{ENCODED => 1028785192, NAME => '.META.,,1', STARTKEY => '', ENDKEY => ''}, {ENCODED => 1e14e83d175bb4dc82a6eac427b0397c, NAME => 'disabledTable,,1374137107108.1e14e83d175bb4dc82a6eac427b0397c.', STARTKEY => '', ENDKEY => 'aaa'}, {ENCODED => 2ce3188d9e655c74aeed8627699d198b, NAME => 'disabledTable,aaa,1374137107131.2ce3188d9e655c74aeed8627699d198b.', STARTKEY => 'aaa', ENDKEY => 'bF\\xD8'}, {ENCODED => a87139be050d4e945fc4523cf2bd1467, NAME => 'disabledTable,bF\\xD8,1374137107163.a87139be050d4e945fc4523cf2bd1467.', STARTKEY => 'bF\\xD8', ENDKEY => 'c,O'}, {ENCODED => 822ce4ce69d81d4e1d6e5e8200887240, NAME => 'disabledTable,c,O,1374137107170.822ce4ce69d81d4e1d6e5e8200887240.', STARTKEY => 'c,O', ENDKEY => 'd\\x11\\xC6'}, {ENCODED => ad4209578ff51f67e975135f23b7ac13, NAME => 'disabledTable,d\\x11\\xC6,1374137107176.ad4209578ff51f67e975135f23b7ac13.', STARTKEY => 'd\\x11\\xC6', ENDKEY => 'd\\xF7='}, {ENCODED => 20c540d9d7213cf8dbf4b4b90007e7ad, NAME => 'disabledTable,d\\xF7=,1374137107182.20c540d9d7213cf8dbf4b4b90007e7ad.', STARTKEY => 'd\\xF7=', ENDKEY => 'e\\xDC\\xB4'}, {ENCODED => 04d860ad0b8e0ae5a0147a6bccea46f4, NAME => 'enabledTable,,1374137106279.04d860ad0b8e0ae5a0147a6bccea46f4.', STARTKEY => '', ENDKEY => 'aaa'}, {ENCODED => b5486e3cf48aa50ca1cba30d0d6ab662, NAME => 'enabledTable,aaa,1374137106428.b5486e3cf48aa50ca1cba30d0d6ab662.', STARTKEY => 'aaa', ENDKEY => 'bF\\xD8'}, {ENCODED => c11af1f6d1aa422916caf6e6ac17eeeb, NAME => 'enabledTable,bF\\xD8,1374137106437.c11af1f6d1aa422916caf6e6ac17eeeb.', STARTKEY => 'bF\\xD8', ENDKEY => 'c,O'}, {ENCODED => 5e4dffd62115d6745b589e40136179c7, NAME => 'enabledTable,c,O,1374137106444.5e4dffd62115d6745b589e40136179c7.', STARTKEY => 'c,O', ENDKEY => 'd\\x11\\xC6'}, {ENCODED => 0a7dc5f03fa7db89e2ed3aa7512bc6cd, NAME => 'enabledTable,d\\x11\\xC6,1374137106455.0a7dc5f03fa7db89e2ed3aa7512bc6cd.', STARTKEY => 'd\\x11\\xC6', ENDKEY => 'd\\xF7='}, {ENCODED => 8f406facf525870443e93345a3172fbb, NAME => 'enabledTable,d\\xF7=,1374137106463.8f406facf525870443e93345a3172fbb.', STARTKEY => 'd\\xF7=', ENDKEY => 'e\\xDC\\xB4'}, {ENCODED => 9daab73408eb240c4780493b6661eb7b, NAME => 'enabledTable,f\\xC2+,1374137106475.9daab73408eb240c4780493b6661eb7b.', STARTKEY => 'f\\xC2+', ENDKEY => 'g\\xA7\\xA2'}, {ENCODED => 47911855a8362112a57c3767d6864ced, NAME => 'enabledTable,g\\xA7\\xA2,1374137106482.47911855a8362112a57c3767d6864ced.', STARTKEY => 'g\\xA7\\xA2', ENDKEY => 'h\\x8D\\x19'}, {ENCODED => e6c71e364bfc9766c1336c9c912b9466, NAME => 'enabledTable,h\\x8D\\x19,1374137106488.e6c71e364bfc9766c1336c9c912b9466.', STARTKEY => 'h\\x8D\\x19', ENDKEY => 'ir\\x90'}, {ENCODED => 63ae8de95f8149c04a18c90ac0ba99db, NAME => 'enabledTable,ir\\x90,1374137106494.63ae8de95f8149c04a18c90ac0ba99db.', STARTKEY => 'ir\\x90', ENDKEY => 'jX\\x07'}, {ENCODED => f477cc9a5378ef9ac464c5ffccf5d295, NAME => 'enabledTable,jX\\x07,1374137106500.f477cc9a5378ef9ac464c5ffccf5d295.', STARTKEY => 'jX\\x07', ENDKEY => 'k=~'}, {ENCODED => d366f300dffeae98f6c0ace8222dc3e4, NAME => 'enabledTable,k=~,1374137106506.d366f300dffeae98f6c0ace8222dc3e4.', STARTKEY => 'k=~', ENDKEY => 'l\"\\xF5'}, {ENCODED => ca8b5a676edb14a42bf80de8c6eb2f84, NAME => 'enabledTable,m\\x08l,1374137106518.ca8b5a676edb14a42bf80de8c6eb2f84.', STARTKEY => 'm\\x08l', ENDKEY => 'm\\xED\\xE3'}]\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.junit.Assert.assertTrue(Assert.java:41)\r\n\tat org.apache.hadoop.hbase.master.TestMasterFailover.testMasterFailoverWithMockedRITOnDeadRS(TestMasterFailover.java:815)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\n\tat java.lang.reflect.Method.invoke(Method.java:597)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n{noformat}"
    ],
    [
        "HBASE-9245",
        "HBASE-9176",
        "Remove dead or deprecated code from hbase 0.96 This is an umbrella issue that will cover the removal or refactoring of dangling dead code and cruft.  Some can make it into 0.96, some may have to wait for an 0.98.  The \"great culling\" of code will be grouped patches that are logically related.",
        "Purge deprecated APIs (and code) Making a place-holder to remove deprecated APIs.  Could get rid of some code too."
    ],
    [
        "HBASE-9427",
        "HBASE-8781",
        "Copy constructor of ImmutableBytesWritable needs to consider the offset A simple test below\r\n{code}\r\n    byte[] bytes = {'a','b','c','d','e','f'};\r\n    ImmutableBytesWritable writable1 = new ImmutableBytesWritable(bytes, 1, bytes.length);\r\n    ImmutableBytesWritable writable2 = new ImmutableBytesWritable(writable1);\r\n    Assert.assertTrue(\"Mismatch\", writable1.equals(writable2));\r\n{code}\r\nwould fail with AssertionFailedError.\r\n\r\nThe reason for this is \r\n\r\n{code}\r\n  public ImmutableBytesWritable(final ImmutableBytesWritable ibw) {\r\n    this(ibw.get(), 0, ibw.getSize());\r\n  }\r\n{code}\r\n\r\nthe constructor would always assume 0 as the offset while it can get it from ibw.getOffset() method.",
        "ImmutableBytesWritable constructor with another IBW as param need to consider the offset of the passed IBW {code}\r\n/**\r\n   * Set the new ImmutableBytesWritable to the contents of the passed\r\n   * <code>ibw</code>.\r\n   * @param ibw the value to set this ImmutableBytesWritable to.\r\n   */\r\n  public ImmutableBytesWritable(final ImmutableBytesWritable ibw) {\r\n    this(ibw.get(), 0, ibw.getSize());\r\n  }\r\n{code}\r\n\r\n\r\nIt should be this(ibw.get(), ibw.getOffset(), ibw.getSize());\r\n"
    ],
    [
        "HBASE-9545",
        "HBASE-9498",
        "NPE when trying to get cluster status on an hbase cluster that isn't there As part of some fault injection testing, I'm trying to talk to an HBaseCluster that isn't there, opening a connection and expecting things to fail. It turns out you can create an {{HBaseAdmin}} instance, but when you ask for its cluster status the NPE surfaces\r\n\r\n",
        "NPE in HBaseAdmin if master not running It is caused because master is not up.  The prepare fails:\r\n\r\n{code}\r\n2:47:46.573 PM \tERROR \tcom.cloudera.cmon.firehose.AbstractHBasePoller \t\r\n\r\nError polling HBASE-1, error: java.io.IOException: Can't get master address from ZooKeeper; znode data == null\r\norg.apache.hadoop.hbase.MasterNotRunningException: java.io.IOException: Can't get master address from ZooKeeper; znode data == null\r\n\tat org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$StubMaker.makeStub(HConnectionManager.java:1641)\r\n\tat org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$MasterMonitorServiceStubMaker.makeStub(HConnectionManager.java:1667)\r\n\tat org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getKeepAliveMasterMonitorService(HConnectionManager.java:2152)\r\n\tat org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.listTables(HConnectionManager.java:2629)\r\n\tat org.apache.hadoop.hbase.client.HBaseAdmin.listTables(HBaseAdmin.java:290)\r\n\tat com.cloudera.cmf.cdh5client.hbase.HBaseAdminImpl.listTables(HBaseAdminImpl.java:60)\r\n\tat com.cloudera.cmon.firehose.HBaseRegionHealthCanaryPoller$CanaryPollerRunnable.run(HBaseRegionHealthCanaryPoller.java:213)\r\n\tat com.cloudera.cmon.firehose.HBaseRegionHealthCanaryPoller$CanaryPollerRunnable.run(HBaseRegionHealthCanaryPoller.java:177)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1477)\r\n\tat com.cloudera.cmf.cdh5client.security.UserGroupInformationImpl.doAs(UserGroupInformationImpl.java:29)\r\n\tat com.cloudera.cmon.firehose.HBaseRegionHealthCanaryPoller.performPoll(HBaseRegionHealthCanaryPoller.java:116)\r\n\tat com.cloudera.cmon.firehose.AbstractHBasePoller.handleHbaseService(AbstractHBasePoller.java:339)\r\n\tat com.cloudera.cmon.firehose.AbstractHBasePoller.runWithTracking(AbstractHBasePoller.java:215)\r\n\tat com.cloudera.cmon.firehose.AbstractHBasePoller.run(AbstractHBasePoller.java:140)\r\n\tat com.cloudera.enterprise.PeriodicEnterpriseService$UnexceptionablePeriodicRunnable.doWork(PeriodicEnterpriseService.java:116)\r\n\tat com.cloudera.enterprise.PeriodicEnterpriseService$UnexceptionablePeriodicRunnable.run(PeriodicEnterpriseService.java:65)\r\n\tat com.cloudera.enterprise.AbstractCDHVersionAwarePeriodicService$4.run(AbstractCDHVersionAwarePeriodicService.java:116)\r\n\tat com.cloudera.cmf.cdh5client.CDH5TaskRunner.run(CDH5TaskRunner.java:45)\r\n\tat java.lang.Thread.run(Thread.java:724)\r\nCaused by: java.io.IOException: Can't get master address from ZooKeeper; znode data == null\r\n\tat org.apache.hadoop.hbase.zookeeper.MasterAddressTracker.getMasterAddress(MasterAddressTracker.java:108)\r\n\tat org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$StubMaker.makeStubNoRetries(HConnectionManager.java:1567)\r\n\tat org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$StubMaker.makeStub(HConnectionManager.java:1612)\r\n\t... 20 more\r\n\r\n{code}\r\n\r\nThen when done we do the call:\r\n\r\n{code}\r\n2:47:51.587 PM \tERROR \tcom.cloudera.cmon.firehose.HBasePoller \t\r\n\r\nEncountered exception null\r\njava.lang.NullPointerException\r\n\tat org.apache.hadoop.hbase.client.HBaseAdmin$MasterMonitorCallable.close(HBaseAdmin.java:3053)\r\n\tat org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:3089)\r\n\tat org.apache.hadoop.hbase.client.HBaseAdmin.getClusterStatus(HBaseAdmin.java:2081)\r\n\tat com.cloudera.cmf.cdh5client.hbase.HConnectionImpl.getClusterStatus(HConnectionImpl.java:69)\r\n\tat com.cloudera.cmon.firehose.HbaseServicePolledStatus.update(HbaseServicePolledStatus.java:137)\r\n\tat com.cloudera.cmon.firehose.HBasePoller$1.run(HBasePoller.java:95)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1477)\r\n\tat com.cloudera.cmf.cdh5client.security.UserGroupInformationImpl.doAs(UserGroupInformationImpl.java:29)\r\n\tat com.cloudera.cmon.firehose.HBasePoller.performPoll(HBasePoller.java:84)\r\n\tat com.cloudera.cmon.firehose.AbstractHBasePoller.handleHbaseService(AbstractHBasePoller.java:339)\r\n\tat com.cloudera.cmon.firehose.AbstractHBasePoller.runWithTracking(AbstractHBasePoller.java:215)\r\n\tat com.cloudera.cmon.firehose.AbstractHBasePoller.run(AbstractHBasePoller.java:140)\r\n\tat com.cloudera.enterprise.PeriodicEnterpriseService$UnexceptionablePeriodicRunnable.doWork(PeriodicEnterpriseService.java:116)\r\n\tat com.cloudera.enterprise.PeriodicEnterpriseService$UnexceptionablePeriodicRunnable.run(PeriodicEnterpriseService.java:65)\r\n\tat com.cloudera.enterprise.AbstractCDHVersionAwarePeriodicService$4.run(AbstractCDHVersionAwarePeriodicService.java:116)\r\n\tat com.cloudera.cmf.cdh5client.CDH5TaskRunner.run(CDH5TaskRunner.java:45)\r\n\tat java.lang.Thread.run(Thread.java:724)\r\n\r\n{code}\r\n\r\nLet me fix.  Let me make sure there aren't other prepare/close's that have this issue while at it."
    ],
    [
        "HBASE-9760",
        "HBASE-8859",
        "truncate_preserve uses duplicate split keys {code}\r\nhbase(main):003:0> truncate_preserve \"IntegrationTestBigLinkedList\"\r\nTruncating 'IntegrationTestBigLinkedList' table (it may take a while):\r\n - Disabling table...\r\n - Dropping table...\r\n - Creating table with region boundaries...\r\n\r\nERROR: All split keys must be unique, found duplicate: \\xEF\\xBF\\xBD, \\xEF\\xBF\\xBD\r\n\r\nHere is some help for this command:\r\n  Disables, drops and recreates the specified table while still maintaing the previous region boundaries.\r\n{code}",
        "truncate_preserve should get table split keys as it is instead of converting them to string type and then again to bytes If we take int,long or double bytes as split keys then we are not creating table with same split keys because converting them to strings directly and to bytes is giving different split keys, sometimes getting IllegalArgument exception because of same split keys(converted). Instead we can get split keys directly from HTable and pass them while creating table.\r\n{code}\r\n      h_table = org.apache.hadoop.hbase.client.HTable.new(conf, table_name)\r\n      splits = h_table.getRegionLocations().keys().map{|i| i.getStartKey} :byte\r\n      splits = org.apache.hadoop.hbase.util.Bytes.toByteArrays(splits)\r\n{code}\r\n\r\n{code}\r\nTruncating 'emp3' table (it may take a while):\r\n - Disabling table...\r\n - Dropping table...\r\n - Creating table with region boundaries...\r\n\r\nERROR: java.lang.IllegalArgumentException: All split keys must be unique, found duplicate: B\\x11S\\xEF\\xBF\\xBD\\xEF\\xBF\\xBD\\xEF\\xBF\\xBD\\x00\\x00, B\\x11S\\xEF\\xBF\\xBD\\xEF\\xBF\\xBD\\xEF\\xBF\\xBD\\x00\\x00\r\n\r\n{code}"
    ],
    [
        "HBASE-9798",
        "HBASE-9666",
        "Include dependency hamcrest-core We exclude hamcrest-code dependency from junit: \r\n{code}\r\n<dependencyManagement>\r\n   <dependency>\r\n        <groupId>junit</groupId>\r\n        <artifactId>junit</artifactId>\r\n        <version>${junit.version}</version>\r\n        <exclusions>\r\n          <exclusion>\r\n            <groupId>org.hamcrest</groupId>\r\n            <artifactId>hamcrest-core</artifactId>\r\n          </exclusion>\r\n        </exclusions>\r\n      </dependency>\r\n{code}\r\n\r\nHowever, running ITTD fails with hadoop1 and 2 tarballs: \r\n{code}\r\nbin/hbase org.apache.hadoop.hbase.IntegrationTestsDriver\r\n...\r\nException in thread \"main\" java.lang.NoClassDefFoundError: org/hamcrest/SelfDescribing\r\n\tat java.lang.ClassLoader.defineClass1(Native Method)\r\n{code}\r\n\r\n",
        "Integration Test Driver is broken {code}\r\nbin/hbase org.apache.hadoop.hbase.IntegrationTestsDriver -r IntegrationTestSendTraceRequests\r\n{code}\r\n\r\nResults in :\r\n\r\n{code}\r\nException in thread \"main\" java.lang.NoClassDefFoundError: org/hamcrest/SelfDescribing\r\n\tat java.lang.ClassLoader.defineClass1(Native Method)\r\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:792)\r\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\r\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:449)\r\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:71)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:361)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.junit.runner.Computer.getSuite(Computer.java:28)\r\n\tat org.junit.runner.Request.classes(Request.java:75)\r\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:117)\r\n\tat org.apache.hadoop.hbase.IntegrationTestsDriver.doWork(IntegrationTestsDriver.java:110)\r\n\tat org.apache.hadoop.hbase.util.AbstractHBaseTool.run(AbstractHBaseTool.java:112)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\r\n\tat org.apache.hadoop.hbase.IntegrationTestsDriver.main(IntegrationTestsDriver.java:46)\r\nCaused by: java.lang.ClassNotFoundException: org.hamcrest.SelfDescribing\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\t... 20 more\r\n{code}"
    ],
    [
        "HBASE-10349",
        "HBASE-9738",
        "Table became unusable when master balanced its region after table was dropped 0.98 was used.\r\nThis was sequence of events:\r\n\r\ncreate 'tablethree_mod'\r\nsnapshot 'tablethree_mod', 'snapshot_tablethree_mod'\r\ndisable 'tablethree_mod'\r\n2014-01-15 09:34:51,749   restore_snapshot 'snapshot_tablethree_mod'\r\n2014-01-15 09:35:07,210   enable 'tablethree_mod'\r\n2014-01-15 09:35:46,134   delete_snapshot 'snapshot_tablethree_mod'\r\n2014-01-15 09:41:42,210   disable 'tablethree_mod'\r\n2014-01-15 09:41:43,610   drop 'tablethree_mod'\r\ncreate 'tablethree_mod'\r\n\r\nFor the last table creation request:\r\n{code} \r\n2014-01-15 10:03:52,999|beaver.component.hbase|INFO| 'create 'tablethree_mod',\r\n{NAME => 'f1', VERSIONS => 3}\r\n\r\n,\r\n{NAME => 'f2', VERSIONS => 3}\r\n\r\n,\r\n{NAME => 'f3', VERSIONS => 3}\r\n\r\n'\r\n2014-01-15 10:03:52,999|beaver.component.hbase|INFO| 'exists 'tablethree_mod''\r\n2014-01-15 10:03:52,999|beaver.component.hbase|INFO| 'put 'tablethree_mod', '0', 'f1:q1', 'value-0', 10'\r\n2014-01-15 10:03:52,999|beaver.component.hbase|INFO| 'put 'tablethree_mod', '1', 'f1:q1', 'value-1', 20'\r\n2014-01-15 10:03:53,000|beaver.component.hbase|INFO| 'put 'tablethree_mod', '2', 'f2:q2', 'value-2', 30'\r\n2014-01-15 10:03:53,000|beaver.component.hbase|INFO| 'put 'tablethree_mod', '3', 'f3:q3', 'value-3', 40'\r\n2014-01-15 10:03:53,000|beaver.component.hbase|INFO| 'put 'tablethree_mod', '4', 'f3:q3', 'value-4', 50'\r\n2014-01-15 10:03:53,000|beaver.component.hbase|INFO|Done writing commands to file. Will execute them now.\r\n2014-01-15 10:03:53,000|beaver.machine|INFO|RUNNING: /usr/lib/hbase/bin/hbase shell /grid/0/tmp/hwqe/artifacts/tmp-471142\r\n2014-01-15 10:03:55,878|beaver.machine|INFO|2014-01-15 10:03:55,878 INFO [main] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available\r\n2014-01-15 10:03:57,283|beaver.machine|INFO|2014-01-15 10:03:57,283 WARN [main] conf.Configuration: hbase-site.xml:an attempt to override final parameter: dfs.support.append; Ignoring.\r\n2014-01-15 10:03:57,669|beaver.machine|INFO|2014-01-15 10:03:57,669 WARN [main] conf.Configuration: hbase-site.xml:an attempt to override final parameter: dfs.support.append; Ignoring.\r\n2014-01-15 10:03:57,720|beaver.machine|INFO|2014-01-15 10:03:57,720 WARN [main] conf.Configuration: hbase-site.xml:an attempt to override final parameter: dfs.support.append; Ignoring.\r\n2014-01-15 10:03:57,997|beaver.machine|INFO|\r\n2014-01-15 10:03:57,997|beaver.machine|INFO|ERROR: Table already exists: tablethree_mod!\r\n2014-01-15 10:03:57,997|beaver.machine|INFO|\r\n{code}\r\nThis was an intermittent issue after using Snapshots, a table is not properly dropped / and not able to properly re-create with the same name. And a HRegion is empty or null Error occurs. (When you try to drop the table it says it does not exist, and when you try to create the table it says that it does already exist).\r\n{code}\r\n2014-01-15 10:04:02,462|beaver.machine|INFO|ERROR: HRegionInfo was null or empty in hbase:meta, row=keyvalues=\r\n{tablethree_mod,,1389778226606.afc82d1ceabbaca36a504b83b65fc0c9./info:seqnumDuringOpen/1389778905355/Put/vlen=8/mvcc=0, tablethree_mod,,1389778226606.afc82d1ceabbaca36a504b83b65fc0c9./info:server/1389778905355/Put/vlen=32/mvcc=0, tablethree_mod,,1389778226606.afc82d1ceabbaca36a504b83b65fc0c9./info:serverstartcode/1389778905355/Put/vlen=8/mvcc=0} \r\n{code}\r\nThanks to Huned who discovered this issue.",
        "Delete table and loadbalancer interference I have noticed that when the balancer is computing a plan for region moves, and a delete table is issued, there is some interference.\r\n\r\n1. At time t1, user deleted the table.\r\n\r\n2. This led to the master updating the meta table to remove the line for the regioninfo for a region f2a9e2e9d70894c03f54ee5902bebee6.\r\n{noformat}\r\n2013-10-04 08:42:52,495 INFO  [MASTER_TABLE_OPERATIONS-hor15n05:60000-0] catalog.MetaEditor: Deleted [{ENCODED => f2a9e2e9d70894c03f54ee5902bebee6, NAME => 'usertable,,1380876170581.f2a9e2e9d70894c03f54ee5902bebee6.', STARTKEY => '', ENDKEY => ''}]\r\n{noformat}\r\n\r\n3. However around the same time, the balancer kicked in, and reassigned the region and made it online somewhere. It didn't check the fact (nor anyone else did) that the table was indeed deleted.\r\n{noformat}\r\n2013-10-04 08:42:53,215 INFO  [hor15n05.gq1.ygridcore.net,60000,1380869262259-BalancerChore] master.HMaster: balance hri=usertable,,1380876170581.f2a9e2e9d70894c03f54ee5902bebee6., src=hor15n09.gq1.ygridcore.net,60020,1380869263722, dest=hor15n11.gq1.ygridcore.net,60020,1380869263682\r\n{noformat}\r\n.....\r\n{noformat}\r\n2013-10-04 08:42:53,592 INFO  [AM.ZK.Worker-pool2-t829] master.RegionStates: Onlined f2a9e2e9d70894c03f54ee5902bebee6 on hor15n11.gq1.ygridcore.net,60020,1380869263682\r\n{noformat}\r\n\r\n4. Henceforth, all the drop tables started giving warnings like\r\n{noformat}\r\n2013-10-04 08:45:17,587 INFO  [RpcServer.handler=8,port=60000] master.HMaster: Client=hrt_qa//68.142.246.151 delete usertable\r\n2013-10-04 08:45:17,631 DEBUG [RpcServer.handler=8,port=60000] lock.ZKInterProcessLockBase: Acquired a lock for /hbase/table-lock/usertable/write-master:600000000000000\r\n2013-10-04 08:45:17,637 WARN  [RpcServer.handler=8,port=60000] catalog.MetaReader: No serialized HRegionInfo in keyvalues={usertable,,1380876170581.f2a9e2e9d70894c03f54ee5902bebee6./info:seqnumDuringOpen/1380876173509/Put/vlen=8/mvcc=0, usertable,,1380876170581.f2a9e2e9d70894c03f54ee5902bebee6./info:server/1380876173509/Put/vlen=32/mvcc=0, usertable,,1380876170581.f2a9e2e9d70894c03f54ee5902bebee6./info:serverstartcode/1380876173509/Put/vlen=8/mvcc=0}\r\n{noformat}\r\n\r\n5. The create of the same table also fails since there is still state (reincarnated, maybe) about the table in the master."
    ],
    [
        "HBASE-10371",
        "HBASE-6749",
        "Compaction creates empty hfile, then selects this file for compaction and creates empty hfile and over again (1) Select HFile for compaction\r\n{code}\r\n2014-01-16 01:01:25,111 INFO org.apache.hadoop.hbase.regionserver.compactions.CompactSelection: Deleting the expired store file by compaction: hdfs://dump002002.cm6:9000/hbase-0.90/storagetable/7d8941661904fb99a41f79a1fce47767/a/f3e38d10d579420494079e17a2557f0b whose maxTimeStamp is -1 while the max expired timestamp is 1389632485111\r\n{code}\r\n(2) Compact\r\n{code}\r\n2014-01-16 01:01:26,042 DEBUG org.apache.hadoop.hbase.regionserver.Compactor: Compacting hdfs://dump002002.cm6:9000/hbase-0.90/storagetable/7d8941661904fb99a41f79a1fce47767/a/f3e38d10d579420494079e17a2557f0b, keycount=0, bloomtype=NONE, size=534, encoding=NONE\r\n2014-01-16 01:01:26,045 DEBUG org.apache.hadoop.hbase.util.FSUtils: Creating file=hdfs://dump002002.cm6:9000/hbase-0.90/storagetable/7d8941661904fb99a41f79a1fce47767/.tmp/40de5d79f80e4fb197e409fb99ab0fd8 with permission=rwxrwxrwx\r\n2014-01-16 01:01:26,076 INFO org.apache.hadoop.hbase.regionserver.Store: Renaming compacted file at hdfs://dump002002.cm6:9000/hbase-0.90/storagetable/7d8941661904fb99a41f79a1fce47767/.tmp/40de5d79f80e4fb197e409fb99ab0fd8 to hdfs://dump002002.cm6:9000/hbase-0.90/storagetable/7d8941661904fb99a41f79a1fce47767/a/40de5d79f80e4fb197e409fb99ab0fd8\r\n2014-01-16 01:01:26,142 INFO org.apache.hadoop.hbase.regionserver.Store: Completed compaction of 1 file(s) in a of storagetable,01:,1369377609136.7d8941661904fb99a41f79a1fce47767. into 40de5d79f80e4fb197e409fb99ab0fd8, size=534; total size for store is 399.0 M\r\n2014-01-16 01:01:26,142 INFO org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest: completed compaction: regionName=storagetable,01:,1369377609136.7d8941661904fb99a41f79a1fce47767., storeName=a, fileCount=1, fileSize=534, priority=16, time=18280340606333745; duration=0sec\r\n{code}\r\n(3) Select HFile for compaction\r\n{code}\r\n2014-01-16 03:48:05,120 INFO org.apache.hadoop.hbase.regionserver.compactions.CompactSelection: Deleting the expired store file by compaction: hdfs://dump002002.cm6:9000/hbase-0.90/storagetable/7d8941661904fb99a41f79a1fce47767/a/40de5d79f80e4fb197e409fb99ab0fd8 whose maxTimeStamp is -1 while the max expired timestamp is 1389642485120\r\n{code}\r\n(4) Compact\r\n{code}\r\n2014-01-16 03:50:17,731 DEBUG org.apache.hadoop.hbase.regionserver.Compactor: Compacting hdfs://dump002002.cm6:9000/hbase-0.90/storagetable/7d8941661904fb99a41f79a1fce47767/a/40de5d79f80e4fb197e409fb99ab0fd8, keycount=0, bloomtype=NONE, size=534, encoding=NONE\r\n2014-01-16 03:50:17,732 DEBUG org.apache.hadoop.hbase.util.FSUtils: Creating file=hdfs://dump002002.cm6:9000/hbase-0.90\r\n{code}\r\n... \r\nthis loop for ever.\r\n",
        "Compact one expired HFile all the time It's an interesting issue. We found there's 1 HFile keeped changing its name all the time. After dig in more, we found one strange behavior in compaction flow.\r\n\r\nHere's the problem(We set the TTL property in our table):\r\n\r\nThere were 10 HFiles and only 1 expired HFile when this problem occured:\r\n{noformat}\r\n2012-09-07 02:21:05,298 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://hacluster/hbase/fhtest/97ac7ea6f732b7c10fe504a54ab02441/value/221f56905cbd4bf09bd4d5d9dceb113a.3ed2e43476ca2c614e33d0e1255c79a9, isReference=true, isBulkLoadResult=false, seqid=118730, majorCompaction=false\r\n2012-09-07 02:21:05,309 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://hacluster/hbase/fhtest/97ac7ea6f732b7c10fe504a54ab02441/value/297b45a6c5f541dca05105ab098dab8d, isReference=false, isBulkLoadResult=false, seqid=122018, majorCompaction=false\r\n2012-09-07 02:21:05,326 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://hacluster/hbase/fhtest/97ac7ea6f732b7c10fe504a54ab02441/value/4a4a4598bc0443c9be087812052d6796.3ed2e43476ca2c614e33d0e1255c79a9, isReference=true, isBulkLoadResult=false, seqid=119850, majorCompaction=false\r\n2012-09-07 02:21:05,348 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://hacluster/hbase/fhtest/97ac7ea6f732b7c10fe504a54ab02441/value/8c6d56c9bafb4b0eb0dd6e04e41ca5b7, isReference=false, isBulkLoadResult=false, seqid=123135, majorCompaction=false\r\n2012-09-07 02:21:05,357 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://hacluster/hbase/fhtest/97ac7ea6f732b7c10fe504a54ab02441/value/a6c2873a646c425f87a6a8a271a9904e, isReference=false, isBulkLoadResult=false, seqid=76561, majorCompaction=false\r\n2012-09-07 02:21:05,370 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://hacluster/hbase/fhtest/97ac7ea6f732b7c10fe504a54ab02441/value/a712a2f79bd247f48405d2c6a91757ab.3ed2e43476ca2c614e33d0e1255c79a9, isReference=true, isBulkLoadResult=false, seqid=120951, majorCompaction=false\r\n2012-09-07 02:21:05,381 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://hacluster/hbase/fhtest/97ac7ea6f732b7c10fe504a54ab02441/value/a717c63214534cb0aaf8e695147fde46, isReference=false, isBulkLoadResult=false, seqid=122763, majorCompaction=false\r\n2012-09-07 02:21:05,431 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://hacluster/hbase/fhtest/97ac7ea6f732b7c10fe504a54ab02441/value/c456f3831b094ac3a0590678acbf27a5.3ed2e43476ca2c614e33d0e1255c79a9, isReference=true, isBulkLoadResult=false, seqid=120579, majorCompaction=false\r\n2012-09-07 02:21:05,518 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://hacluster/hbase/fhtest/97ac7ea6f732b7c10fe504a54ab02441/value/dde4e56b131a4ffdaec8f9574bffa5ab.3ed2e43476ca2c614e33d0e1255c79a9, isReference=true, isBulkLoadResult=false, seqid=121651, majorCompaction=false\r\n2012-09-07 02:21:05,593 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://hacluster/hbase/fhtest/97ac7ea6f732b7c10fe504a54ab02441/value/ee62844594f2474e88186dbde673c802.3ed2e43476ca2c614e33d0e1255c79a9, isReference=true, isBulkLoadResult=false, seqid=119478, majorCompaction=false\r\n{noformat} \r\nCompaction was triggered during Region-Opening . As we know, compaction should choose the expired HFiles to compact first, so that 1 expired HFile was choosed. \r\nSince no KeyValue was there, compaction deleted the old HFile and created a new one with minimumTimestamp = -1 && maximumTimestamp = -1.\r\nSo after the first compaction, there were still 10 HFiles. It triggered compaction again and again.....\r\n{noformat}\r\n2012-09-07 02:21:06,079 INFO org.apache.hadoop.hbase.regionserver.compactions.CompactSelection: Deleting the expired store file by compaction: hdfs://hacluster/hbase/fhtest/97ac7ea6f732b7c10fe504a54ab02441/value/72468dff1cd94c4fb9cf9196cc3183b7 whose maxTimeStamp is -1 while the max expired timestamp is 1344824466079\r\n....\r\n2012-09-07 02:21:06,080 DEBUG org.apache.hadoop.hbase.regionserver.Store: Compacting hdfs://hacluster/hbase/fhtest/97ac7ea6f732b7c10fe504a54ab02441/value/72468dff1cd94c4fb9cf9196cc3183b7, keycount=0, bloomtype=NONE, size=558, encoding=NONE\r\n2012-09-07 02:21:06,082 DEBUG org.apache.hadoop.hbase.util.FSUtils: Creating file:hdfs://hacluster/hbase/fhtest/97ac7ea6f732b7c10fe504a54ab02441/.tmp/8239019ff92f49bfab26b02ca43bc26a with permission:rwxrwxrwx\r\n{noformat}"
    ],
    [
        "HBASE-10448",
        "HBASE-8937",
        "ZKUtil create and watch methods don't set watch in some cases While using the ZKUtil methods during testing, I found that watch was not set when it should be set based on the methods and method comments:\r\ncreateNodeIfNotExistsAndWatch\r\ncreateEphemeralNodeAndWatch\r\n\r\nFor example, in createNodeIfNotExistsAndWatch():\r\n\r\n{code}\r\n public static boolean createNodeIfNotExistsAndWatch(\r\n      ZooKeeperWatcher zkw, String znode, byte [] data)\r\n  throws KeeperException {\r\n    try {\r\n      zkw.getRecoverableZooKeeper().create(znode, data, createACL(zkw, znode),\r\n          CreateMode.PERSISTENT);\r\n    } catch (KeeperException.NodeExistsException nee) {\r\n      try {\r\n        zkw.getRecoverableZooKeeper().exists(znode, zkw);\r\n      } catch (InterruptedException e) {\r\n        zkw.interruptedException(e);\r\n        return false;\r\n      }\r\n      return false;\r\n    } catch (InterruptedException e) {\r\n      zkw.interruptedException(e);\r\n      return false;\r\n    }\r\n    return true;\r\n  }\r\n{code}\r\n\r\nThe watch is only set via exists() call when the node already exists.\r\nSimilarly in createEphemeralNodeAndWatch():\r\n{code}\r\n  public static boolean createEphemeralNodeAndWatch(ZooKeeperWatcher zkw,\r\n      String znode, byte [] data)\r\n  throws KeeperException {\r\n    try {\r\n      zkw.getRecoverableZooKeeper().create(znode, data, createACL(zkw, znode),\r\n          CreateMode.EPHEMERAL);\r\n    } catch (KeeperException.NodeExistsException nee) {\r\n      if(!watchAndCheckExists(zkw, znode)) {\r\n        // It did exist but now it doesn't, try again\r\n        return createEphemeralNodeAndWatch(zkw, znode, data);\r\n      }\r\n      return false;\r\n    } catch (InterruptedException e) {\r\n      LOG.info(\"Interrupted\", e);\r\n      Thread.currentThread().interrupt();\r\n    }\r\n    return true;\r\n  }\r\n{code}\r\n",
        "createEphemeralNodeAndWatch don't set watcher if the node is created successfully CreateEphemeralNodeAndWatch in zkUtil don't set watcher if the node is created successfully. This is not consistent with the comment and may causes the ActiveMasterManager cannot get events that master node is deleted or changed.\r\n\r\n{code}\r\n  public static boolean createEphemeralNodeAndWatch(ZooKeeperWatcher zkw,\r\n      String znode, byte [] data)\r\n  throws KeeperException {\r\n    try {\r\n      zkw.getRecoverableZooKeeper().create(znode, data, createACL(zkw, znode),\r\n          CreateMode.EPHEMERAL);\r\n    } catch (KeeperException.NodeExistsException nee) {\r\n      if(!watchAndCheckExists(zkw, znode)) {\r\n        // It did exist but now it doesn't, try again\r\n        return createEphemeralNodeAndWatch(zkw, znode, data);\r\n      }\r\n      return false;\r\n    } catch (InterruptedException e) {\r\n      LOG.info(\"Interrupted\", e);\r\n      Thread.currentThread().interrupt();\r\n    }\r\n    return true;\r\n  }\r\n{code}\r\n\r\n\r\n"
    ],
    [
        "HBASE-10533",
        "HBASE-5841",
        "commands.rb is giving wrong error messages on exceptions 1) Clone into existing table name is printing snapshot name instead of table name.\r\n{code}\r\nhbase(main):004:0> clone_snapshot 'myTableSnapshot-122112','table'\r\n\r\nERROR: Table already exists: myTableSnapshot-122112!\r\n{code}\r\nThe reason for this is we are printing first argument instead of exception message.\r\n{code}\r\n        if cause.kind_of?(org.apache.hadoop.hbase.TableExistsException) then\r\n          raise \"Table already exists: #{args.first}!\"\r\n        end\r\n{code}\r\n2) If we give wrong column family in put or delete. Expectation is to print actual column families in the table but instead throwing the exception.\r\n{code}\r\nhbase(main):002:0> put 't1','r','unkwown_cf','value'\r\n2014-02-14 15:51:10,037 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n2014-02-14 15:51:10,640 INFO  [main] hdfs.PeerCache: SocketCache disabled.\r\n\r\nERROR: Failed 1 action: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family unkwown_cf does not exist in region t1,eeeeeeee,1392118273512.c7230b923c58f1af406a6d84930e40c1. in table 't1', {NAME => 'f1', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', COMPRESSION => 'NONE', VERSIONS => '6', TTL => '2147483647', MIN_VERSIONS => '0', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}\r\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.doBatchOp(HRegionServer.java:4206)\r\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.doNonAtomicRegionMutation(HRegionServer.java:3441)\r\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3345)\r\n        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:28460)\r\n        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2008)\r\n        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:92)\r\n        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)\r\n        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)\r\n        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)\r\n        at java.lang.Thread.run(Thread.java:662)\r\n: 1 time,\r\n\r\n{code}\r\n\r\nThe reason for this is server will not throw NoSuchColumnFamilyException directly, instead RetriesExhaustedWithDetailsException will be thrown.\r\n",
        "hbase shell translate_hbase_exceptions() rely on table name as first argument shell/commands.rb translate_hbase_exceptions() rely on the fact that the table name is the first argument.\r\n\r\nThis is true for many of the commands but for example:\r\n - grant(user, rights, table_name, family=nil, qualifier=nil\r\n - revoke(user, table_name, family=nil, qualifier=nil)\r\n\r\nhas user as first argument, so if you specify a table that doesn't exists, or where you don't have access you end up with a message like \"Unknown table {username}\" and so on..."
    ],
    [
        "HBASE-10913",
        "HBASE-10622",
        "Print exception of why a copy failed during ExportSnapshot Currently we print a vague \"Failed to copy the snapshot directory from X to Y\" whenever X pre-exists on Y. Users have to figure this out by themselves.",
        "Improve log and Exceptions in Export Snapshot  from the logs of export snapshot is not really clear what's going on,\r\nadding some extra information useful to debug, and in some places the real exception can be thrown"
    ],
    [
        "HBASE-11217",
        "HBASE-11036",
        "Race between SplitLogManager task creation + TimeoutMonitor Some time ago, we reported a test failure in HBASE-11036, which resulted in already-split and merged regions coming back to life, causing split brain for region boundaries and resulting in data loss. \r\n\r\nIt turns out that the root cause was not concurrent online schema change + region split/merge, but meta log splitting failing and the meta updates getting lost. This in turn causes the region split/merge information and assignment to be lost causing large scale data loss. \r\n\r\nLogs below shows that the split task for meta log is created, but before the znode is created, the timeout thread kicks in and sees the unassigned task. Then it does a get on znode which fails with NoNode (because the znode is not created yet). This causes the task to be marked complete (setDone(path, SUCCESS)) which means that the logs are lost. Meta is assigned elsewhere (and opened with the same seqId as previous) confirming data loss in meta. \r\n\r\n{code}\r\n2014-04-16 18:31:26,267 INFO  [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] handler.MetaServerShutdownHandler: Splitting hbase:meta logs for hor13n03.gq1.ygridcore.net,60020,1397672668647\r\n2014-04-16 18:31:26,274 DEBUG [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] master.MasterFileSystem: Renamed region directory: hdfs://hor13n01.gq1.ygridcore.net:8020/apps/hbase/data/WALs/hor13n03.gq1.ygridcore.net,60020,1397672668647-splitting\r\n2014-04-16 18:31:26,274 INFO  [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] master.SplitLogManager: dead splitlog workers [hor13n03.gq1.ygridcore.net,60020,1397672668647]\r\n2014-04-16 18:31:26,276 DEBUG [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] master.SplitLogManager: Scheduling batch of logs to split\r\n2014-04-16 18:31:26,276 INFO  [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] master.SplitLogManager: started splitting 1 logs in [hdfs://hor13n01.gq1.ygridcore.net:8020/apps/hbase/data/WALs/hor13n03.gq1.ygridcore.net,60020,1397672668647-splitting]\r\n2014-04-16 18:31:26,276 INFO  [hor13n02.gq1.ygridcore.net,60000,1397672191204.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 1 unassigned = 1 tasks={/hbase/splitWAL/WALs%2Fhor13n03.gq1.ygridcore.net%2C60020%2C1397672668647-splitting%2Fhor13n03.gq1.ygridcore.net%252C60020%252C1397672668647.1397672681632.meta=last_update = -1 last_version = -\r\n2014-04-16 18:31:26,276 DEBUG [hor13n02.gq1.ygridcore.net,60000,1397672191204.splitLogManagerTimeoutMonitor] master.SplitLogManager: resubmitting unassigned task(s) after timeout\r\n2014-04-16 18:31:26,277 WARN  [main-EventThread] master.SplitLogManager$GetDataAsyncCallback: task znode /hbase/splitWAL/WALs%2Fhor13n03.gq1.ygridcore.net%2C60020%2C1397672668647-splitting%2Fhor13n03.gq1.ygridcore.net%252C60020%252C1397672668647.1397672681632.meta vanished.\r\n2014-04-16 18:31:26,277 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fhor13n03.gq1.ygridcore.net%2C60020%2C1397672668647-splitting%2Fhor13n03.gq1.ygridcore.net%252C60020%252C1397672668647.1397672681632.meta\r\n2014-04-16 18:31:26,282 DEBUG [main-EventThread] master.SplitLogManager: put up splitlog task at znode /hbase/splitWAL/WALs%2Fhor13n03.gq1.ygridcore.net%2C60020%2C1397672668647-splitting%2Fhor13n03.gq1.ygridcore.net%252C60020%252C1397672668647.1397672681632.meta                                                                                                      \r\n2014-04-16 18:31:26,286 WARN  [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] master.SplitLogManager: returning success without actually splitting and deleting all the log files in path hdfs://hor13n01.gq1.ygridcore.net:8020/apps/hbase/data/WALs/hor13n03.gq1.ygridcore.net,60020,1397672668647-splitting\r\n2014-04-16 18:31:26,286 INFO  [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] master.SplitLogManager: finished splitting (more than or equal to) 9 bytes in 1 log files in [hdfs://hor13n01.gq1.ygridcore.net:8020/apps/hbase/data/WALs/hor13n03.gq1.ygridcore.net,60020,1397672668647-splitting] in 10ms\r\n2014-04-16 18:31:26,290 DEBUG [main-EventThread] master.SplitLogManager$DeleteAsyncCallback: deleted /hbase/splitWAL/WALs%2Fhor13n03.gq1.ygridcore.net%2C60020%2C1397672668647-splitting%2Fhor13n03.gq1.ygridcore.net%252C60020%252C1397672668647.1397672681632.meta\r\n2014-04-16 18:31:26,290 WARN  [main-EventThread] master.SplitLogManager$GetDataAsyncCallback: task znode /hbase/splitWAL/WALs%2Fhor13n03.gq1.ygridcore.net%2C60020%2C1397672668647-splitting%2Fhor13n03.gq1.ygridcore.net%252C60020%252C1397672668647.1397672681632.meta vanished.\r\n2014-04-16 18:31:26,290 DEBUG [main-EventThread] master.SplitLogManager: unacquired orphan task is done /hbase/splitWAL/WALs%2Fhor13n03.gq1.ygridcore.net%2C60020%2C1397672668647-splitting%2Fhor13n03.gq1.ygridcore.net%252C60020%252C1397672668647.1397672681632.meta\r\n2014-04-16 18:31:26,291 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/RESCAN0000000383 entered state: DONE hor13n02.gq1.ygridcore.net,60000,1397672191204\r\n2014-04-16 18:31:26,291 DEBUG [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] master.AssignmentManager: based on AM, current region=hbase:meta,,1.1588230740 is on server=hor13n03.gq1.ygridcore.net,60020,1397672668647 server being checked: hor13n03.gq1.ygridcore.net,60020,1397672668647\r\n2014-04-16 18:31:26,291 INFO  [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] handler.MetaServerShutdownHandler: Server hor13n03.gq1.ygridcore.net,60020,1397672668647 was carrying META. Trying to assign.\r\n2014-04-16 18:31:26,291 INFO  [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] master.RegionStates: Transitioned {1588230740 state=OPEN, ts=1397672681933, server=hor13n03.gq1.ygridcore.net,60020,1397672668647} to {1588230740 state=OFFLINE, ts=1397673086291, server=hor13n03.gq1.ygridcore.net,60020,1397672668647}\r\n2014-04-16 18:31:26,291 INFO  [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] master.RegionStates: Offlined 1588230740 from hor13n03.gq1.ygridcore.net,60020,1397672668647\r\n2014-04-16 18:31:26,299 INFO  [main-EventThread] master.SplitLogManager$DeleteAsyncCallback: /hbase/splitWAL/WALs%2Fhor13n03.gq1.ygridcore.net%2C60020%2C1397672668647-splitting%2Fhor13n03.gq1.ygridcore.net%252C60020%252C1397672668647.1397672681632.meta does not exist. Either was created but deleted behind our back by another pending delete OR was deleted in earl\r\n2014-04-16 18:31:26,299 DEBUG [main-EventThread] master.SplitLogManager: deleted task without in memory state /hbase/splitWAL/WALs%2Fhor13n03.gq1.ygridcore.net%2C60020%2C1397672668647-splitting%2Fhor13n03.gq1.ygridcore.net%252C60020%252C1397672668647.1397672681632.meta\r\n2014-04-16 18:31:26,299 DEBUG [main-EventThread] master.SplitLogManager$DeleteAsyncCallback: deleted /hbase/splitWAL/RESCAN0000000383\r\n2014-04-16 18:31:26,299 DEBUG [main-EventThread] master.SplitLogManager: deleted task without in memory state /hbase/splitWAL/RESCAN0000000383\r\n2014-04-16 18:31:26,301 INFO  [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] catalog.CatalogTracker: Failed verification of hbase:meta,,1 at address=hor13n03.gq1.ygridcore.net,60020,1397672668647, exception=java.net.ConnectException: Connection refused\r\n2014-04-16 18:31:26,301 INFO  [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] zookeeper.ZooKeeperNodeTracker: Unsetting hbase:meta region location in ZooKeeper\r\n2014-04-16 18:31:26,315 DEBUG [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] master.AssignmentManager: No previous transition plan found (or ignoring an existing plan) for hbase:meta,,1.1588230740; generated random plan=hri=hbase:meta,,1.1588230740, src=, dest=hor13n04.gq1.ygridcore.net,60020,1397672685370; 3 (online=3, available=3) available servers, forceNew\r\n2014-04-16 18:31:26,315 DEBUG [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] zookeeper.ZKAssign: master:60000-0x3456a48dd7d0223, quorum=hor13n04.gq1.ygridcore.net:2181,hor13n03.gq1.ygridcore.net:2181,hor13n20.gq1.ygridcore.net:2181, baseZNode=/hbase Creating (or updating) unassigned node 1588230740 with OFFLINE state\r\n2014-04-16 18:31:26,323 INFO  [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] master.AssignmentManager: Assigning hbase:meta,,1.1588230740 to hor13n04.gq1.ygridcore.net,60020,1397672685370\r\n2014-04-16 18:31:26,323 INFO  [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] master.RegionStates: Transitioned {1588230740 state=OFFLINE, ts=1397673086315, server=hor13n03.gq1.ygridcore.net,60020,1397672668647} to {1588230740 state=PENDING_OPEN, ts=1397673086323, server=hor13n04.gq1.ygridcore.net,60020,1397672685370}\r\n2014-04-16 18:31:28,337 DEBUG [MASTER_META_SERVER_OPERATIONS-hor13n02:60000-2] master.DeadServer: Finished processing hor13n03.gq1.ygridcore.net,60020,1397672668647\r\n{code}\r\n\r\n\r\n",
        "Online schema change with region merge may cause data loss  We have found out that online schema change and region merges may still cause issues about merged regions coming back online, and thus causing data loss. \r\n\r\nRecently ITBLL failed reporting 800K missing rows out of 720M. We've been running this test for some extended period of time, and this is the first we are seeing it, meaning that it is more rare. But it is still concerning. \r\n\r\nFrom master's log:\r\nThe merge has happened:\r\n{code}\r\n2014-04-16 18:26:37,247 INFO  [AM.ZK.Worker-pool2-t73] master.AssignmentManager: Handled MERGED event; merged=IntegrationTestBigLinkedList,\\xB2\\xFE\\x03s,1397672795119.80159738a0167e20a2e29fb2c46702f2., \t\r\n\tregion_a=IntegrationTestBigLinkedList,\\xB2\\xFE\\x03s,1397670978959.0ac116e4d7da87922d3a8f218ca21079., \r\n\tregion_b=IntegrationTestBigLinkedList,\\xB8\\x03\\x94\\x15,1397672587456.1265d06478082ced65dd9a0c1c2b63c2., on hor13n03.gq1.ygridcore.net,60020,1397672668647\r\n{code}\r\n\r\nThe region server shows the merge is complete: \r\n{code}\r\n2014-04-16 18:26:37,352 INFO  [regionserver60020-merges-1397672794972] regionserver.RegionMergeRequest: Regions merged, hbase:meta updated, and report to master. region_a=IntegrationTestBigLinkedList,\\xB2\\xFE\\x03s,1397670978959.0ac116e4d7da87922d3a8f218ca21079., region_b=IntegrationTestBigLinkedList,\\xB8\\x03\\x94\\x15,1397672587456.1265d06478082ced65dd9a0c1c2b63c2.. Region merge took 2sec\r\n{code}\r\n\r\nThe new region was online on the region server for some time: \r\n{code}\r\n2014-04-16 18:31:22,858 DEBUG [RS_OPEN_REGION-hor13n03:60020-1] handler.OpenRegionHandler: Opened IntegrationTestBigLinkedList,\\xB2\\xFE\\x03s,1397672795119.80159738a0167e20a2e29fb2c46702f2. on hor13n03.gq1.ygridcore.net,60020,1397672668647\r\n{code}\r\nThen the region server was killed around 2014-04-16 18:31:26,254. \r\n\r\nThe master started log splitting etc for the dead RS:\r\n{code}\r\n2014-04-16 18:31:28,942 INFO  [MASTER_SERVER_OPERATIONS-hor13n02:60000-3] handler.ServerShutdownHandler: Splitting logs for hor13n03.gq1.ygridcore.net,60020,1397672668647 before assignment.\r\n..\r\n2014-04-16 18:31:40,887 INFO  [MASTER_SERVER_OPERATIONS-hor13n02:60000-3] master.RegionStates: Transitioned {80159738a0167e20a2e29fb2c46702f2 state=OPEN, ts=1397673082874, server=hor13n03.gq1.ygridcore.net,60020,1397672668647} to {80159738a0167e20a2e29fb\r\n2014-04-16 18:31:40,887 INFO  [MASTER_SERVER_OPERATIONS-hor13n02:60000-3] master.RegionStates: Offlined 80159738a0167e20a2e29fb2c46702f2 from hor13n03.gq1.ygridcore.net,60020,1397672668647\r\n{code}\r\n\r\nBut this region was not assigned again at all. Instead, the master restarted shortly after, and reassigned the regions that were merged already: \r\n{code}\r\n2014-04-16 18:34:02,569 INFO  [master:hor13n02:60000] master.ActiveMasterManager: Registered Active Master=hor13n02.gq1.ygridcore.net,60000,1397673241215\r\n...\r\n2014-04-16 18:34:10,412 INFO  [master:hor13n02:60000] master.AssignmentManager: Found regions out on cluster or in RIT; presuming failover\r\n2014-04-16 18:34:10,412 WARN  [master:hor13n02:60000] master.ServerManager: Expiration of hor13n03.gq1.ygridcore.net,60020,1397671753021 but server not online\r\n..\r\n2014-04-16 18:34:10,880 INFO  [MASTER_SERVER_OPERATIONS-hor13n02:60000-3] master.AssignmentManager: Bulk assigning 28 region(s) across 4 server(s), round-robin=true\r\n..\r\n2014-04-16 18:34:10,882 DEBUG [hor13n02.gq1.ygridcore.net,60000,1397673241215-GeneralBulkAssigner-1] zookeeper.ZKAssign: master:60000-0x2456a4863640255, quorum=hor13n04.gq1.ygridcore.net:2181,hor13n03.gq1.ygridcore.net:2181,hor13n20.gq1.ygridcore.net:2181, baseZNode=/hbase Async create of unassigned node 0ac116e4d7da8792..\r\n..\r\n2014-04-16 18:34:13,077 INFO  [AM.ZK.Worker-pool2-t7] master.RegionStates: Onlined 0ac116e4d7da87922d3a8f218ca21079 on hor13n04.gq1.ygridcore.net,60020,1397672685370\r\n{code}\r\n\r\n\r\nBefore the master went down, there were other logs that indicate something funky: \r\n{code}\r\n2014-04-16 18:32:42,113 DEBUG [hor13n02.gq1.ygridcore.net,60000,1397672191204-org.apache.hadoop.hbase.master.BulkReOpen-0] master.AssignmentManager: Starting unassign of IntegrationTestBigLinkedList,\\xB2\\xFE\\x03s,1397670978959.0ac116e4d7da87922d3a8f218ca\r\n2014-04-16 18:32:42,113 INFO  [hor13n02.gq1.ygridcore.net,60000,1397672191204-org.apache.hadoop.hbase.master.BulkReOpen-0] master.AssignmentManager: Attempting to unassign {0ac116e4d7da87922d3a8f218ca21079 state=MERGED, ts=1397672797223, server=hor13n03.gq1.ygridcore.net,60020,1397672668647}, ignored\r\n{code}\r\n\r\nThis above log is due to bulk reopening because of online schema change: \r\n{code}\r\n2014-04-16 18:32:32,081 DEBUG [FifoRpcScheduler.handler1-thread-27] handler.TableEventHandler: Ignoring table not disabled exception for supporting online schema changes.\r\n2014-04-16 18:32:32,082 INFO  [FifoRpcScheduler.handler1-thread-27] handler.TableEventHandler: Handling table operation C_M_MODIFY_TABLE on table IntegrationTestBigLinkedList\r\n2014-04-16 18:32:32,150 DEBUG [FifoRpcScheduler.handler1-thread-27] util.FSTableDescriptors: Wrote descriptor into: hdfs://hor13n01.gq1.ygridcore.net:8020/apps/hbase/data/data/default/IntegrationTestBigLinkedList/.tabledesc/.tableinfo.0000000051\r\n2014-04-16 18:32:32,159 DEBUG [FifoRpcScheduler.handler1-thread-27] util.FSTableDescriptors: Deleted table descriptor at hdfs://hor13n01.gq1.ygridcore.net:8020/apps/hbase/data/data/default/IntegrationTestBigLinkedList/.tabledesc/.tableinfo.0000000050\r\n2014-04-16 18:32:32,159 INFO  [FifoRpcScheduler.handler1-thread-27] util.FSTableDescriptors: Updated tableinfo=hdfs://hor13n01.gq1.ygridcore.net:8020/apps/hbase/data/data/default/IntegrationTestBigLinkedList/.tabledesc/.tableinfo.0000000051\r\n2014-04-16 18:32:32,160 INFO  [FifoRpcScheduler.handler1-thread-27] handler.TableEventHandler: Bucketing regions by region server...\r\n2014-04-16 18:32:32,191 INFO  [FifoRpcScheduler.handler1-thread-27] handler.TableEventHandler: Skip {ENCODED => e81668b7b64a9c92dd3689db96a3adb0, NAME => 'IntegrationTestBigLinkedList,\\xC0\\x03O,1397669732121.e81668b7b64a9c92dd3689db96a3adb0.', STARTKEY =\r\n2014-04-16 18:32:32,191 INFO  [FifoRpcScheduler.handler1-thread-27] handler.TableEventHandler: Skip {ENCODED => a51376af5b2aaef5bde980d48b05d75d, NAME => 'IntegrationTestBigLinkedList,\\xE8\\x01\\x99,,1397671676090.a51376af5b2aaef5bde980d48b05d75d.', STARTK\r\n2014-04-16 18:32:32,191 INFO  [FifoRpcScheduler.handler1-thread-27] handler.TableEventHandler: Reopening 51 regions on 5 region servers.\r\n{code}\r\n\r\nMy theory is that this may have left the regions in RIT state, and when the master was restarted it saw those in RIT and assigned. \r\n\r\nAlso notice that this is not an issue about a concurrent merge and online schema change that can be prevented with table locks. The merge happened minutes ago, however the online schema change still wanted to reassign the merged regions. "
    ],
    [
        "HBASE-11360",
        "HBASE-11322",
        "SnapshotFileCache causes too many cache refreshes Right now we decide whether to refresh the cache based on the lastModified timestamp of all the snapshots and those \"running\" snapshots which is located in the /hbase/.hbase-snapshot/.tmp/<snapshot> directory\r\n\r\nWe ran a ExportSnapshot job which takes around 7 minutes between creating the directory and copying all the files. \r\n\r\nThus the modified time for the \r\n/hbase/.hbase-snapshot/.tmp directory was 7 minutes earlier than the modified time of the\r\n/hbase/.hbase-snapshot/.tmp/<snapshot> directory\r\n\r\nThus the cache refresh happens and doesn't pick up all the files but thinks its up to date as the modified time of the .tmp directory never changes.\r\n\r\nThis is a bug as when the export job starts the cache never contains the files for the \"running\" snapshot and will fail.\r\n",
        "SnapshotHFileCleaner makes the wrong check for lastModified time thus causing too many cache refreshes The SnapshotHFileCleaner calls the SnapshotFileCache if a particular HFile in question is part of a snapshot.\r\nIf the HFile is not in the cache, we then refresh the cache and check again.\r\n\r\nBut the cache refresh checks to see if anything has been modified since the last cache refresh but this logic is incorrect in certain scenarios.\r\n\r\nThe last modified time is done via this operation:\r\n{code}\r\nthis.lastModifiedTime = Math.min(dirStatus.getModificationTime(),\r\n                                     tempStatus.getModificationTime());\r\n{code}\r\n\r\nand the check to see if the snapshot directories have been modified:\r\n{code}\r\n// if the snapshot directory wasn't modified since we last check, we are done\r\n    if (dirStatus.getModificationTime() <= lastModifiedTime &&\r\n        tempStatus.getModificationTime() <= lastModifiedTime) {\r\n      return;\r\n    }\r\n{code}\r\n\r\nSuppose the following happens:\r\ndirStatus modified 6-1-2014\r\ntempStatus modified 6-2-2014\r\n\r\nlastModifiedTime = 6-1-2014\r\n\r\nprovided these two directories don't get modified again all subsequent checks wont exit early, like they should.\r\n\r\nIn our cluster, this was a huge performance hit.  The cleaner chain fell behind, thus almost filling up dfs and our namenode heap.\r\n\r\nIts a simple fix, instead of Math.min we use Math.max for the lastModified, I believe that will be correct.\r\n\r\nI'll apply a patch for you guys.\r\n\r\n"
    ],
    [
        "HBASE-11420",
        "HBASE-11052",
        "ThriftServer (version 1) may crash on OOME When using ThriftServer as a gateway for php & c/c++ applications, I found it very easy to crash of OOME. I analyzed the jprof file and found that the ThriftServer had a 1.2G size byte array before it crashed. It seems to be a memory leak. But when did it happen? I checked the huge byte array and realized that it was a HTTP request string. That means a request in protocols other than Thrift may cause memory allocation exception. \r\nWe cound easily recur the bug by wget/curl the ThriftServer. And we can check the memory usage infomation using TOP command.",
        "Sending random data crashes thrift service Upstream thrift library has a know issue (THRIFT-601) causing the thrift server to crash with an Out-of-Memory Error when bogus requests are sent.\r\n\r\nThis reproduces when a very large request size is sent in the request header, making the thrift server to allocate a large memory segment leading to OOM.\r\n\r\nLoadBalancer health checks are the first \"candidate\" for bogus requests\r\nThrift developers admit this is a known issue with TBinaryProtocol and their recommandation is to use TCompactProtocol/TFramedTransport but this requires all thrift clients to be updated (might not be feasible atm)\r\n\r\nSo we need a fix similar to CASSANDRA-475.\r\n"
    ],
    [
        "HBASE-11612",
        "HBASE-11333",
        "Remove MetaMigrationConvertingToPB  Can MetaMigrationConvertingToPB be removed now from 0.98 onwards? The javadoc says it should be removed on a major release after 0.96. It does a full Meta scan which takes quite some time especially if there are around 1M regions.",
        "Remove deprecated class MetaMigrationConvertingToPB MetaMigrationConvertingToPB is marked deprecated and to be deleted next major release after 0.96. Is that the time?"
    ],
    [
        "HBASE-11613",
        "HBASE-10728",
        "\"get_counter\" shell command is not displaying the result for counter columns. perform the following opertions in HBase shell prompt.\r\n1. create a table with one column family.\r\n2. insert some amount of data into the table.\r\n3. then perform increment operation on any column qualifier.\r\neg: incr 't', 'r1', 'f:c1'\r\n4. then queried the get counter query,\r\nit is throwing nocounter found message to the user.\r\n{code}\r\neg:\r\n hbase(main):010:0> get_counter 't', 'r1', 'f', 'c1'\r\n No counter found at specified coordinates\r\n{code}\r\n=====================================================\r\nand wrong message is throwing to user, while executing the get_counter query.\r\n{code}\r\nhbase(main):009:0> get_counter 't', 'r1', 'f'\r\n\r\nERROR: wrong number of arguments (3 for 4)\r\n\r\nHere is some help for this command:\r\nReturn a counter cell value at specified table/row/column coordinates.\r\nA cell cell should be managed with atomic increment function oh HBase\r\nand the data should be binary encoded. Example:\r\n\r\n  hbase> get_counter 'ns1:t1', 'r1', 'c1'\r\n  hbase> get_counter 't1', 'r1', 'c1'\r\n\r\nThe same commands also can be run on a table reference. Suppose you had a reference\r\nt to table 't1', the corresponding command would be:\r\n\r\n  hbase> t.get_counter 'r1', 'c1'\r\n{code}\r\n{code}\r\nproblem:\r\n   In example they given 3 arguments but asking 4 arguments\r\n   If run with 3 arguments it will throw error.\r\n   if run with 4 arguments \"No counter found at specified coordinates\" message is throwing even though counter is specified.\r\n{code}",
        "get_counter value is never used. "
    ],
    [
        "HBASE-11622",
        "HBASE-11609",
        "completebulkload/loadIncrementalHFiles cannot specify table with namespace I'm using completebulkload to load 500GB of data to a table (presplitted). However, it reports the following errors:\r\n\r\nLooks like completebulkload didn't recognize the namespace part (namespace:table).\r\n\r\nCaused by: java.net.URISyntaxException: Relative path in absolute URI: grapple:vertices,37.bottom\r\n        at java.net.URI.checkPath(URI.java:1804)\r\n        at java.net.URI.<init>(URI.java:752)\r\n        at org.apache.hadoop.fs.Path.initialize(Path.java:203)\r\n\r\nBy looking at the source code of LoadIncrementalHFiles.java, it seems the temporary path created for splitting will contain ':',\r\n\r\nThe error part should be this:\r\nString uniqueName = getUniqueName(table.getName());\r\n    HColumnDescriptor familyDesc = table.getTableDescriptor().getFamily(item.family);\r\n    Path botOut = new Path(tmpDir, uniqueName + \".bottom\");\r\n    Path topOut = new Path(tmpDir, uniqueName + \".top\");\r\n    splitStoreFile(getConf(), hfilePath, familyDesc, splitKey,\r\n        botOut, topOut);\r\n\r\nuniqueName will be \"namespce:table\" so \"new Path(...)\" will fail.\r\n\r\nA bug?\r\n\r\nP.S.\r\nComment from Matteo Bertozzi:\r\n\r\nwe don't need the name to be related to the table name.\r\nWe can replace the getUniqueName() using something like this\r\n\r\n  String getUniqueName(TableName tableName) {\r\n    String name = UUID.randomUUID().toString().replaceAll(\"-\", \"\") +\r\n                  \",\" + regionCount.incrementAndGet();\r\n    return name;\r\n  }\r\n",
        "LoadIncrementalHFiles fails if the namespace is specified from Jianshi Huang on the user list\r\ntrying to bulk load a table in a namespace, like:\r\n$ hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles test/ foo:testtb\r\n\r\nwe get an exception\r\n{code}\r\n2014-07-29 19:59:53,373 ERROR [main] mapreduce.LoadIncrementalHFiles: Unexpected execution exception during splitting\r\njava.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: foo:testtb,1.bottom\r\n        at java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n        at java.util.concurrent.FutureTask.get(FutureTask.java:188)\r\n        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.groupOrSplitPhase(LoadIncrementalHFiles.java:449)\r\n        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.doBulkLoad(LoadIncrementalHFiles.java:304)\r\n        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.run(LoadIncrementalHFiles.java:899)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\r\n{code}\r\n\r\nThe problem is related to the ':' symbol going to the file path. the simple fix is to replace the current LoadIncrementalHFiles.getUniqueName()"
    ],
    [
        "HBASE-11669",
        "HBASE-11575",
        "The HMaster uses the same port with HRegionServer HMaster uses the same port with the HRegionServer.\r\nIn the constructor of HMaster, it has to run super(conf, csm) which is HRegionServer. In this method, the rpc service is instantiated, and HMaster and HRegionServer will use the same rpc configuration. If users start the HMaster and HRegionServer in the same server, the error \"Address already in use\" occurs when HRegionServer starts up.",
        "Pseudo distributed mode does not work as documented  After master-RS colocation, now the pseudo dist-mode does not work as documented since you cannot start a region server in the same port 16020. \r\n\r\nI think we can either select a random port (and info port) for the master's region server, or document how to do a pseudo-distributed setup in the book. \r\n\r\n[~jxiang] wdyt? "
    ],
    [
        "HBASE-11700",
        "HBASE-11648",
        "Fix typo of hbase.store.compaction.ratio (should be hbase.hstore.compaction.ratio) in Ref Guide Hey, noticed that in the doc it lists out the following option: hbase.store.compaction.ratio but it should be hbase.hstore.compaction.ratio. Doc looks really good though! I will hopefully have more feedback for you soon. ",
        "Typo of config: hbase.hstore.compaction.ratio in book.xml When looking at the parameters used by compaction algorithm in http://hbase.apache.org/book/regions.arch.html, we found there is typo.\r\nIn hbase code, the config key for compaction ratio is hbase.hstore.compaction.ratio.\r\nBut in the hbase book it's hbase.store.compaction.ratio.\r\nCompactSelection.java#66\r\n{code}\r\n    this.conf = conf;\r\n    this.compactRatio = conf.getFloat(\"hbase.hstore.compaction.ratio\", 1.2F);\r\n    this.compactRatioOffPeak = conf.getFloat(\"hbase.hstore.compaction.ratio.offpeak\", 5.0F);\r\n{code}\r\n\r\nJust fix it to avoid misleading.\r\n\r\n"
    ],
    [
        "HBASE-11714",
        "HBASE-11374",
        "RpcRetryingCaller#callWithoutRetries set rpc timeout to 2 seconds incorrectly Discussed on the user@hbase mailing list (http://markmail.org/thread/w3cqjxwo2smkn2jw)\r\n{quote}\r\n\"Recently switched from 0.94 and 0.98, and finding that periodically things\r\nare having issues - lots of retry exceptions\" :\r\n{quote}\r\n\r\nclient log:\r\n{quote}\r\n2014-08-08 17:22:43 o.a.h.h.c.AsyncProcess [INFO] #105158,\r\ntable=rt_global_monthly_campaign_deliveries, attempt=10/35 failed 500 ops,\r\nlast exception: java.net.SocketTimeoutException: Call to\r\nip-10-201-128-23.us-west-1.compute.internal/10.201.128.23:60020 failed\r\nbecause java.net.SocketTimeoutException: 2000 millis timeout while waiting\r\nfor channel to be ready for read. ch :\r\njava.nio.channels.SocketChannel[connected local=/10.248.130.152:46014\r\nremote=ip-10-201-128-23.us-west-1.compute.internal/10.201.128.23:60020] on\r\nip-10-201-128-23.us-west-1.compute.internal,60020,1405642103651, tracking\r\nstarted Fri Aug 08 17:21:55 UTC 2014, retrying after 10043 ms, replay 500\r\nops.\r\n{quote}\r\n\r\nanalysis:\r\nthere are 2 methods in RpcRetryingCaller: callWithRetries and callWithoutRetries.\r\nit looks the timeout setup of callWithRetries is good, while callWithoutRetries is wrong(multi RPC for this user): caller cannot specify a valid timeout, but callWithoutRetries still calls beforeCall, which looks a method for callWithRetries only,  to set timeout. since RpcRetryingCaller#callTimeout  is not set, thread local timeout is set to 2s(MIN_RPC_TIMEOUT) via RpcClient.setRpcTimeout, which is the final pinginterval set to the socket.\r\n\r\nwhen there are heavy write workload and the rpc cannot complete in 2s, the client close the connection, so the server side connection is reset and finally exposes the problem in HBASE-11705",
        "RpcRetryingCaller#callWithoutRetries has a timeout of zero This code is called by the client on the \"multi\" path.\r\nAs zero is detected as infinite value, we fallback to 2 seconds, which may not may correct.\r\n\r\nTypically, you can see this kind of message in the client (see the SocketTimeoutException: 2000)\r\n{noformat}\r\n2014-08-08 17:22:43 o.a.h.h.c.AsyncProcess [INFO] #105158,\r\ntable=rt_global_monthly_campaign_deliveries, attempt=10/35 failed 500 ops,\r\nlast exception: java.net.SocketTimeoutException: Call to\r\nip-10-201-128-23.us-west-1.compute.internal/10.201.128.23:60020 failed\r\nbecause java.net.SocketTimeoutException: 2000 millis timeout while waiting\r\nfor channel to be ready for read. ch :\r\njava.nio.channels.SocketChannel[connected local=/10.248.130.152:46014\r\nremote=ip-10-201-128-23.us-west-1.compute.internal/10.201.128.23:60020] on\r\nip-10-201-128-23.us-west-1.compute.internal,60020,1405642103651, tracking\r\nstarted Fri Aug 08 17:21:55 UTC 2014, retrying after 10043 ms, replay 500\r\nops.\r\n{noformat}"
    ],
    [
        "HBASE-12006",
        "HBASE-11542",
        "[JDK 8] KeyStoreTestUtil#generateCertificate fails due to \"subject class type invalid\" Running tests on Java 8. All unit tests for branch 0.98 pass. On master branch some variation in the security API is causing a failure in TestSSLHttpServer:\r\n\r\n{noformat}\r\nTests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 0.181 sec <<< FAILURE! - in org.apache.hadoop.hbase.http.TestSSLHttpServer\r\norg.apache.hadoop.hbase.http.TestSSLHttpServer  Time elapsed: 0.181 sec  <<< ERROR!\r\njava.security.cert.CertificateException: Subject class type invalid.\r\n\tat sun.security.x509.X509CertInfo.setSubject(X509CertInfo.java:888)\r\n\tat sun.security.x509.X509CertInfo.set(X509CertInfo.java:415)\r\n\tat org.apache.hadoop.hbase.http.ssl.KeyStoreTestUtil.generateCertificate(KeyStoreTestUtil.java:94)\r\n\tat org.apache.hadoop.hbase.http.ssl.KeyStoreTestUtil.setupSSLConfig(KeyStoreTestUtil.java:246)\r\n\tat org.apache.hadoop.hbase.http.TestSSLHttpServer.setup(TestSSLHttpServer.java:72)\r\n\r\norg.apache.hadoop.hbase.http.TestSSLHttpServer  Time elapsed: 0.181 sec  <<< ERROR!\r\njava.lang.NullPointerException: null\r\n\tat org.apache.hadoop.hbase.http.TestSSLHttpServer.cleanup(TestSSLHttpServer.java:100)\r\n\r\nTests in error: \r\n  TestSSLHttpServer.setup:72 \u00bb Certificate Subject class type invalid.\r\n  TestSSLHttpServer.cleanup:100 NullPointer\r\n{noformat}",
        "Unit Test  KeyStoreTestUtil.java compilation failure in IBM JDK  In trunk,  jira HBase-10336 added a utility test KeyStoreTestUtil.java, which leverages the following sun classes:\r\n\u00a0\u00a0 import sun.security.x509.AlgorithmId;\r\n\u00a0\u00a0 import sun.security.x509.CertificateAlgorithmId;\r\n\u00a0 ....\r\n\r\nthis cause hbase compiler failure if using IBM JDK,  \r\nThere are similar classes like below in IBM jdk: \r\nimport com.ibm.security.x509.AlgorithmId;\r\nimport com.ibm.security.x509.CertificateAlgorithmId; \r\n\r\nThis jira is to add handling of the x509 references. \r\n  "
    ],
    [
        "HBASE-12018",
        "HBASE-11983",
        "HRegion.close neglects to close its HLog I found this while diagnosing leaking file handles after test runs in a build of 0.98. I haven't investigated subsequent branches. Perhaps someone more knowledgeable about region lifecycle can have a look.",
        "HRegion constructors should not create HLog  We should get rid of HRegion creating its own HLog. It should ALWAYS get the log from outside. \r\nI think this was added for unit tests, but we should refrain from such practice in the future (adding UT constructors always leads to weird and critical bugs down the road). See recent: HBASE-11982, HBASE-11654. \r\n\r\nGet rid of weird things like ignoreHLog:\r\n\r\n{code}\r\n  /**\r\n   * @param ignoreHLog - true to skip generate new hlog if it is null, mostly for createTable\r\n   */\r\n  public static HRegion createHRegion(final HRegionInfo info, final Path rootDir,\r\n                                      final Configuration conf,\r\n                                      final HTableDescriptor hTableDescriptor,\r\n                                      final HLog hlog,\r\n                                      final boolean initialize, final boolean ignoreHLog)\r\n{code}\r\n\r\nWe can unify all the createXX and newXX methods and separate creating a region in the file system vs opening a region. "
    ],
    [
        "HBASE-12129",
        "HBASE-12074",
        "ProtobufLogWriter throws NPE {noformat}\r\n2014-09-30 14:47:53,170 INFO  [sync.2] wal.FSHLog: Slow sync cost: 161 ms, current pipeline: []\r\n2014-09-30 14:47:53,171 ERROR [sync.0] wal.FSHLog: Error syncing, request close of hlog\r\njava.io.IOException: java.lang.NullPointerException\r\n        at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:170)\r\n        at org.apache.hadoop.hbase.regionserver.wal.FSHLog$SyncRunner.run(FSHLog.java:1302)\r\n        at java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.NullPointerException\r\n        at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:167)\r\n        ... 2 more\r\n{noformat}",
        "TestLogRollingNoCluster#testContendedLogRolling() failed TestLogRollingNoCluster#testContendedLogRolling() failed on a 0.98 run. I am trying to understand the context. \r\n\r\nThe failure is this: \r\n{code}\r\njava.lang.AssertionError\r\n\tat org.junit.Assert.fail(Assert.java:86)\r\n\tat org.junit.Assert.assertTrue(Assert.java:41)\r\n\tat org.junit.Assert.assertFalse(Assert.java:64)\r\n\tat org.junit.Assert.assertFalse(Assert.java:74)\r\n\tat org.apache.hadoop.hbase.regionserver.wal.TestLogRollingNoCluster.testContendedLogRolling(TestLogRollingNoCluster.java:80)\r\n{code}\r\n\r\nCaused because one of the Appenders calling FSHLog.sync() threw IOE because of concurrent close: \r\n\r\n{code}\r\n4-09-23 16:36:39,530 FATAL [pool-1-thread-1-WAL.AsyncSyncer0] wal.FSHLog$AsyncSyncer(1246): Error while AsyncSyncer sync, request close of hlog \r\njava.io.IOException: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:168)\r\n\tat org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1241)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:165)\r\n\t... 2 more\r\n2014-09-23 16:36:39,531 INFO  [32] wal.TestLogRollingNoCluster$Appender(137): Caught exception from Appender:32\r\njava.io.IOException: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:168)\r\n\tat org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1241)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:165)\r\n\t... 2 more\r\n2014-09-23 16:36:39,532 INFO  [19] wal.TestLogRollingNoCluster$Appender(137): Caught exception from Appender:19\r\njava.io.IOException: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:168)\r\n\tat org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1241)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:165)\r\n\t... 2 more\r\n{code}\r\n\r\nThe code is: \r\n{code}\r\n  public void sync() throws IOException {\r\n    try {\r\n      this.output.flush();\r\n      this.output.sync();\r\n    } catch (NullPointerException npe) {\r\n      // Concurrent close...\r\n      throw new IOException(npe);\r\n    }\r\n  }\r\n{code}\r\n\r\nI think the test case written exactly to catch this case: \r\n{code}\r\n   * Spin up a bunch of threads and have them all append to a WAL.  Roll the\r\n   * WAL frequently to try and trigger NPE.\r\n{code}\r\n\r\nThis is why I am reporting since I don't have much context. It may not be a test issue, but an actual bug. "
    ],
    [
        "HBASE-12241",
        "HBASE-8357",
        "The crash of regionServer when taking deadserver's replication queue breaks replication When a regionserver crash, another regionserver will try to take over the replication hlogs queue and help the the the dead regionserver to finish the replcation.See NodeFailoverWorker in ReplicationSourceManager\r\n\r\nCurrently hbase.zookeeper.useMulti is false in default configuration. The operation of taking over replication queue is not atomic. The ReplicationSourceManager firstly lock the replication node of dead regionserver and then copy the replication queue, and delete replication node of dead regionserver at last. The operation of the lockOtherRS just creates a persistent zk node named \"lock\" which prevent other regionserver taking over the replication queue.\r\nSee:\r\n{code}\r\n  public boolean lockOtherRS(String znode) {\r\n    try {\r\n      String parent = ZKUtil.joinZNode(this.rsZNode, znode);\r\n      if (parent.equals(rsServerNameZnode)) {\r\n        LOG.warn(\"Won't lock because this is us, we're dead!\");\r\n        return false;\r\n      }\r\n      String p = ZKUtil.joinZNode(parent, RS_LOCK_ZNODE);\r\n      ZKUtil.createAndWatch(this.zookeeper, p, Bytes.toBytes(rsServerNameZnode));\r\n    } catch (KeeperException e) {\r\n      ...\r\n      return false;\r\n    }\r\n    return true;\r\n  }\r\n{code}\r\n\r\nBut if a regionserver crashed after creating this \"lock\" zk node and before coping the replication queue to its replication queue, the \"lock\" zk node will be left forever and\r\nno other regionserver can take over the replication queue.\r\n\r\nIn out production cluster, we encounter this problem. We found the replication queue was there and no regionserver took over it and a \"lock\" zk node left there.\r\n{quote}\r\nhbase.32561.log:2014-09-24,14:09:28,790 INFO org.apache.hadoop.hbase.replication.ReplicationZookeeper: Won't transfer the queue, another RS took care of it because of: KeeperErrorCode = NoNode for /hbase/hhsrv-micloud/replication/rs/hh-hadoop-srv-st09.bj,12610,1410937824255/lock\r\nhbase.32561.log:2014-09-24,14:14:45,148 INFO org.apache.hadoop.hbase.replication.ReplicationZookeeper: Won't transfer the queue, another RS took care of it because of: KeeperErrorCode = NoNode for /hbase/hhsrv-micloud/replication/rs/hh-hadoop-srv-st10.bj,12600,1410937795685/lock\r\n{quote}\r\n\r\nA quick solution is that the lock operation just create an ephemeral \"lock\" zookeeper node and when the lock node is deleted, other regionserver will be notified to check if there are replication queue left.\r\n\r\nSuggestions are welcomed! Thanks.\r\n",
        "current region server failover mechanism for replication can lead to stale region server whose left hlogs can't be replicated by other region server consider this scenario: rs A/B/C, A dies, B and C race to lock A to help replicate A's left unreplicated hlogs, B wins and successfully creates lock under A's znode, but before B copies A's hlog queues to its own znode, B also dies, and C successfully creates lock under B's znode and helps replicate B's own left hlogs. But A's left hlogs can't be replicated by any other rs since B left back a lock under A's znode and B didn't transfer A's hlog queues to its own znode before B dies."
    ],
    [
        "HBASE-12299",
        "HBASE-12294",
        "Can't assemble tarballs after HBASE-12261 {{$ mvn -DskipTests clean install package assembly:single}}\r\n...\r\nERROR Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.4:single (default-cli) on project hbase-assembly: Failed to create assembly: Artifact: org.apache.hbase:hbase-checkstyle:jar:0.98.7 (included by module) does not have an artifact with a file. Please ensure the package phase is run before the assembly is generated.\r\n\r\n$ {{mvn -DskipTests clean install package && mvn -DskipTests assembly:single}}\r\n...\r\nERROR Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.4:single (default-cli) on project hbase-assembly: Failed to create assembly: Artifact: org.apache.hbase:hbase-annotations:jar:0.98.8-SNAPSHOT (included by module) does not have an artifact with a file. Please ensure the package phase is run before the assembly is generated.",
        "Regression from HBASE-12261: Can't build the docs after the hbase-checkstyle module was added Since the 15th, I have not been able to build the docs. I get these errors:\r\n{code}\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.3:stage (default-cli) on project hbase-checkstyle: Missing distribution management in project HBase - Checkstyle (org.apache.hbase:hbase-checkstyle:2.0.0-SNAPSHOT) -> [Help 1]\r\n{code}\r\n\r\n{code}\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.3:stage (default-cli) on project hbase-checkstyle: Missing distribution management in project HBase - Checkstyle (org.apache.hbase:hbase-checkstyle:2.0.0-SNAPSHOT)\r\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:216)\r\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)\r\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)\r\nat org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:108)\r\nat org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:76)\r\nat org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)\r\nat org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:116)\r\nat org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:361)\r\nat org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)\r\nat org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)\r\nat org.apache.maven.cli.MavenCli.doMain(MavenCli.java:213)\r\nat org.apache.maven.cli.MavenCli.main(MavenCli.java:157)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:606)\r\nat org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)\r\nat org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)\r\nat org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)\r\nat org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)\r\nCaused by: org.apache.maven.plugin.MojoExecutionException: Missing distribution management in project HBase - Checkstyle (org.apache.hbase:hbase-checkstyle:2.0.0-SNAPSHOT)\r\nat org.apache.maven.plugins.site.AbstractDeployMojo.getSite(AbstractDeployMojo.java:762)\r\nat org.apache.maven.plugins.site.AbstractDeployMojo.getDeployModuleDirectory(AbstractDeployMojo.java:249)\r\nat org.apache.maven.plugins.site.AbstractDeployMojo.deploy(AbstractDeployMojo.java:320)\r\nat org.apache.maven.plugins.site.AbstractDeployMojo.deployTo(AbstractDeployMojo.java:281)\r\nat org.apache.maven.plugins.site.AbstractDeployMojo.execute(AbstractDeployMojo.java:163)\r\nat org.apache.maven.plugins.site.SiteStageMojo.execute(SiteStageMojo.java:75)\r\nat org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:133)\r\nat org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)\r\n... 19 more\r\n{code}\r\n\r\nI'm able to resolve it by adding the attached patch to the POM. [~eclark], is there a specific reason you didn't use inheritance in the checkstyles module?"
    ],
    [
        "HBASE-12443",
        "HBASE-11419",
        "After increasing the TTL value of a Hbase Table , table gets inaccessible. Scan table not working. \r\nAfter increasing the TTL value of a Hbase Table , table gets inaccessible. Scan table not working.\r\n\r\nScan in hbase shell throws\r\n\r\njava.lang.IllegalStateException: Block index not loaded\r\nat com.google.common.base.Preconditions.checkState(Preconditions.java:145)\r\nat org.apache.hadoop.hbase.io.hfile.HFileReaderV1.blockContainingKey(HFileReaderV1.java:181)\r\nat org.apache.hadoop.hbase.io.hfile.HFileReaderV1$AbstractScannerV1.seekTo(HFileReaderV1.java:426)\r\nat org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:226)\r\nat org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:145)\r\nat org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:131)\r\nat org.apache.hadoop.hbase.regionserver.Store.getScanner(Store.java:2015)\r\nat org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.<init>(HRegion.java:3706)\r\nat org.apache.hadoop.hbase.regionserver.HRegion.instantiateRegionScanner(HRegion.java:1761)\r\nat org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1753)\r\nat org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1730)\r\nat org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:2409)\r\nat sun.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\nat java.lang.reflect.Method.invoke(Method.java:597)\r\nat org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:320)\r\nat org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1426)\r\n\r\n\r\n\r\n\r\nSTEPS to Reproduce:\r\n\r\n create 'debugger',{NAME => 'd',TTL => 15552000}\r\n put 'debugger','jdb','d:desc','Java debugger',1399699792000\r\n disable 'debugger'\r\nalter 'debugger',{NAME => 'd',TTL => 69120000}\r\nenable 'debugger'\r\nscan 'debugger'\r\n\r\n\r\n\r\nReason for the issue:\r\n\r\n   When inserting already expired data in debugger table, hbase creates a hfile with empty data \r\nblock and index block. On scanning table, StoreFile.Reader checks whether the TimeRangeTracker's maximum timestamp is greater than ttl value, so it skips the empty file.\r\n\r\n  But when ttl is changed, the maximum timestamp will be lesser than ttl value, so StoreFile.Reader tries to read index block from HFile leading to java.lang.IllegalStateException: Block index not loaded.\r\n\r\nSOLUTION:\r\n\r\nStoreFile.java \r\n\r\n       boolean passesTimerangeFilter(Scan scan, long oldestUnexpiredTS) {\r\n      if (timeRangeTracker == null) {\r\n        return true;\r\n      } else {\r\n        return timeRangeTracker.includesTimeRange(scan.getTimeRange()) &&\r\n            timeRangeTracker.getMaximumTimestamp() >= oldestUnexpiredTS;\r\n      }\r\n    }\r\n\r\nIn the above method, by checking whether there are entries in the hfile by using FixedFileTrailer\r\nblock we can skip scanning the empty hfile.\r\n\r\n// changed code will solve the issue\r\n\r\n     boolean passesTimerangeFilter(Scan scan, long oldestUnexpiredTS) {\r\n      if (timeRangeTracker == null) {\r\n        return true;\r\n      } else {\r\n        return timeRangeTracker.includesTimeRange(scan.getTimeRange()) &&\r\n            timeRangeTracker.getMaximumTimestamp() >= oldestUnexpiredTS && reader.getEntries()>0;\r\n      }\r\n    }\r\n\r\n\r\n\r\n\r\n",
        "After increasing TTL value of a hbase table having pre-split regions and decreasing TTL value, table becomes inaccessible. After increasing and decreasing the TTL value of a Hbase Table , table gets inaccessible. Scan table not working.\r\n\r\nScan in hbase shell throws\r\n\r\njava.lang.IllegalStateException: Block index not loaded\r\nat com.google.common.base.Preconditions.checkState(Preconditions.java:145)\r\nat org.apache.hadoop.hbase.io.hfile.HFileReaderV1.blockContainingKey(HFileReaderV1.java:181)\r\nat org.apache.hadoop.hbase.io.hfile.HFileReaderV1$AbstractScannerV1.seekTo(HFileReaderV1.java:426)\r\nat org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:226)\r\nat org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:145)\r\nat org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:131)\r\nat org.apache.hadoop.hbase.regionserver.Store.getScanner(Store.java:2015)\r\nat org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.<init>(HRegion.java:3706)\r\nat org.apache.hadoop.hbase.regionserver.HRegion.instantiateRegionScanner(HRegion.java:1761)\r\nat org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1753)\r\nat org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1730)\r\nat org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:2409)\r\nat sun.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\r\nat java.lang.reflect.Method.invoke(Method.java:597)\r\nat org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:320)\r\nat org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1426)\r\n\r\n"
    ],
    [
        "HBASE-12717",
        "HBASE-12716",
        "Pre-split algorithm in HBaseAdmin.create() can not find the split point \r\nWhen we set the start key and the end key in the function:\r\ncreateTable(HTableDescriptor desc, byte[] startKey, byte[] endKey, int numRegions)\r\nThe current pre-split algorithm could not find a split point between the keys like \"aaa\" and \"aab\", \"1111\" and \"1112\". \r\n\r\nExample Code for this bug:\r\nadmin.createTable(htd, Bytes.toBytes(\"aaa\"), Bytes.toBytes(\"aab\"), 4);\r\n\r\nwe will get the following ERROR:\r\nException in thread \"main\" java.lang.IllegalArgumentException: Unable to split key range into enough regions\r\n\tat org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:473)\r\n\tat test.JavaTest.main(JavaTest.java:28)\r\n\r\nWe hope this pre-split algorithm should be able to calculate the split point with an additional byte. for example:\r\n\"aaa\" and \"aab\", split point= \"aaaP\"\r\n\"1111\" and \"1112\", split point =\"1111P\" \r\n ",
        "A bug in RegionSplitter.UniformSplit algorithm Welcome to the review board: https://reviews.apache.org/r/29424/diff/#\r\n\r\nI`m working for another issues HBASE-12590 and trying to use the UniformSplit algorithm in RegionSplitter. When the last bytes of start key and end key are adjacent in alphabetical order or ASCII order, the UniformSplit algorithm meet an NPE.\r\nLike startkey: aaa, endkey :aab\r\n       startkey:1111 endkey: 1112\r\n\r\nFor example, we write this simple test code:\r\n{code}\r\nimport org.apache.hadoop.hbase.util.RegionSplitter.UniformSplit;\r\n......\r\nbyte[] a1 = { 'a', 'a', 'a' };\r\nbyte[] a2 = { 'a', 'a', 'b' };\r\nUniformSplit us = new UniformSplit();\r\nbyte[] mid = us.split(a1, a2);\r\n......\r\n{code}\r\nWe will get the ERROR:\r\n{code}\r\nException in thread \"main\" java.lang.NullPointerException\r\n\tat org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit.split(RegionSplitter.java:986)\r\n{code}\r\nWe hope this algorithm should be able to calculate the split point with an additional byte. for example:\r\n\"aaa\" and \"aab\", split point= \"aaaP\"\r\n\"1111\" and \"1112\", split point =\"1111P\" \r\n\r\nreview board:https://reviews.apache.org/r/29424/\r\n\r\n\r\n\r\n\r\n"
    ],
    [
        "HBASE-12791",
        "HBASE-12178",
        "HBase does not attempt to clean up an aborted split when the regionserver shutting down HBase not cleaning the daughter region directories from HDFS  if region server shut down after creating the daughter region directories during the split.\r\n\r\nHere the logs.\r\n\r\n-> RS shutdown after creating the daughter regions.\r\n{code}\r\n2014-12-31 09:05:41,406 DEBUG [regionserver60020-splits-1419996941385] zookeeper.ZKAssign: regionserver:60020-0x14a9701e53100d1, quorum=localhost:2181, baseZNode=/hbase Transitioned node 80c665138d4fa32da4d792d8ed13206f from RS_ZK_REQUEST_REGION_SPLIT to RS_ZK_REQUEST_REGION_SPLIT\r\n2014-12-31 09:05:41,514 DEBUG [regionserver60020-splits-1419996941385] regionserver.HRegion: Closing t,,1419996880699.80c665138d4fa32da4d792d8ed13206f.: disabling compactions & flushes\r\n2014-12-31 09:05:41,514 DEBUG [regionserver60020-splits-1419996941385] regionserver.HRegion: Updates disabled for region t,,1419996880699.80c665138d4fa32da4d792d8ed13206f.\r\n2014-12-31 09:05:41,516 INFO  [StoreCloserThread-t,,1419996880699.80c665138d4fa32da4d792d8ed13206f.-1] regionserver.HStore: Closed f\r\n2014-12-31 09:05:41,518 INFO  [regionserver60020-splits-1419996941385] regionserver.HRegion: Closed t,,1419996880699.80c665138d4fa32da4d792d8ed13206f.\r\n2014-12-31 09:05:49,922 DEBUG [regionserver60020-splits-1419996941385] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table t dd9731ee43b104da565257ca1539aa8c\r\n2014-12-31 09:05:49,922 DEBUG [regionserver60020-splits-1419996941385] regionserver.HRegion: Instantiated t,,1419996941401.dd9731ee43b104da565257ca1539aa8c.\r\n2014-12-31 09:05:49,929 DEBUG [regionserver60020-splits-1419996941385] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table t 2e40a44511c0e187d357d651f13a1dab\r\n2014-12-31 09:05:49,929 DEBUG [regionserver60020-splits-1419996941385] regionserver.HRegion: Instantiated t,row2,1419996941401.2e40a44511c0e187d357d651f13a1dab.\r\nWed Dec 31 09:06:30 IST 2014 Terminating regionserver\r\n2014-12-31 09:06:30,465 INFO  [Thread-8] regionserver.ShutdownHook: Shutdown hook starting; hbase.shutdown.hook=true; fsShutdownHook=org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer@42d2282e\r\n{code}\r\n-> Skipping rollback if RS stopped or stopping so we end up in dirty daughter regions in HDFS.\r\n{code}\r\n2014-12-31 09:07:49,547 INFO  [regionserver60020-splits-1419996941385] regionserver.SplitRequest: Skip rollback/cleanup of failed split of t,,1419996880699.80c665138d4fa32da4d792d8ed13206f. because server is stopped\r\njava.io.InterruptedIOException: Interrupted after 0 tries  on 350\r\n        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:156)\r\n{code}\r\n\r\nBecause of this hbck always showing inconsistencies. \r\n{code}\r\nERROR: Region { meta => null, hdfs => hdfs://localhost:9000/hbase/data/default/t/2e40a44511c0e187d357d651f13a1dab, deployed =>  } on HDFS, but not listed in hbase:meta or deployed on any region server\r\nERROR: Region { meta => null, hdfs => hdfs://localhost:9000/hbase/data/default/t/dd9731ee43b104da565257ca1539aa8c, deployed =>  } on HDFS, but not listed in hbase:meta or deployed on any region server\r\n{code}\r\n\r\nIf we try to repair then we end up in overlap regions in hbase:meta. and both daughter regions and parent are online.",
        "Failed splits are kept in transition When a region split fails ( took too long to write the reference files ) the region stays int SPLITTING_NEW state and in transition on the master forever. This will block balancer invocations."
    ],
    [
        "HBASE-12832",
        "HBASE-12821",
        "Describe table from shell no longer shows Table's attributes, only CF attributes When you describe a table with some attributes at the table level, it is not shown from shell any more:\r\n{code}\r\nhbase(main):010:0> create 'usertable2', 'family', {REGION_REPLICATION => 2, CONFIGURATION => {'hbase.hregion.scan.loadColumnFamiliesOnDemand' => 'true'}}\r\nhbase(main):011:0> describe 'usertable2' \r\nTable usertable2 is ENABLED                                                                                                                                                                                 \r\nCOLUMN FAMILIES DESCRIPTION                                                                                                                                                                                 \r\n{NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALS\r\nE', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}                                                                                                                                       \r\n1 row(s) in 0.0200 seconds\r\n{code}\r\n\r\nMaster UI shows: \r\n{code}\r\n'usertable2', {TABLE_ATTRIBUTES => {REGION_REPLICATION => '2', CONFIGURATION => {'hbase.hregion.scan.loadColumnFamiliesOnDemand' => 'true'}}, {NAME => 'family'}\r\n{code}\r\n\r\nHBASE-10082 changed the formatting from shell for one line per CF. We should add the table level attributes back to the formatting.",
        "Describe on table doesn't show table attributes on hbase shell 1) hbase(main):003:0> create 'test','CF'\r\n2) hbase(main):006:0> alter 'test', METADATA => {'TEST_PROPERTY' => 'TEST_VALUE'}\r\n3) hbase(main):007:0> describe 'test'\r\n{NAME => 'CF', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}   \r\n\r\nIssue : The added property , table attribute, isn't getting displayed.\r\n\r\nNote : If we check the table description from master page, we can see the changed property.\r\n'test', {TABLE_ATTRIBUTES => {METADATA => {'TEST_PROPERTY' => 'TEST_VALUE'}}, {NAME => 'CF'}  "
    ],
    [
        "HBASE-12832",
        "HBASE-12828",
        "Describe table from shell no longer shows Table's attributes, only CF attributes When you describe a table with some attributes at the table level, it is not shown from shell any more:\r\n{code}\r\nhbase(main):010:0> create 'usertable2', 'family', {REGION_REPLICATION => 2, CONFIGURATION => {'hbase.hregion.scan.loadColumnFamiliesOnDemand' => 'true'}}\r\nhbase(main):011:0> describe 'usertable2' \r\nTable usertable2 is ENABLED                                                                                                                                                                                 \r\nCOLUMN FAMILIES DESCRIPTION                                                                                                                                                                                 \r\n{NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALS\r\nE', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}                                                                                                                                       \r\n1 row(s) in 0.0200 seconds\r\n{code}\r\n\r\nMaster UI shows: \r\n{code}\r\n'usertable2', {TABLE_ATTRIBUTES => {REGION_REPLICATION => '2', CONFIGURATION => {'hbase.hregion.scan.loadColumnFamiliesOnDemand' => 'true'}}, {NAME => 'family'}\r\n{code}\r\n\r\nHBASE-10082 changed the formatting from shell for one line per CF. We should add the table level attributes back to the formatting.",
        "Add getTableAttributes API to HBaseAdmin class.  At present, there is no api/function to get table attributes. It would be nice to have a function that returns table attributes given the table name."
    ],
    [
        "HBASE-12897",
        "HBASE-9472",
        "Minimum memstore size is a percentage We have a cluster which is optimized for random reads.  Thus we have a large block cache and a small memstore.  Currently our heap is 20GB and we wanted to configure the memstore to take 4% or 800MB.  Right now the minimum memstore size is 5%.  What do you guys think about reducing the minimum size to 1%?  Suppose we log a warning if the memstore is below 5% but allow it?\r\n\r\nWhat do you folks think? \r\n\r\n",
        "If the memstore size is under .1 or greater than .9 the memstore size defaults to the default memstore size In HbaseConfiguration.checkForClusterFreeMemoryLimit it does a check to see if the blockCache + memstore > .8 this threshold ensures we do not run out of memory.\r\n\r\nBut MemStoreFlusher.getMemStoreLimit does this check:\r\n{code}\r\nif (limit >= 0.9f || limit < 0.1f) {\r\n      LOG.warn(\"Setting global memstore limit to default of \" + defaultLimit +\r\n        \" because supplied value outside allowed range of 0.1 -> 0.9\");\r\n      effectiveLimit = defaultLimit;\r\n    }\r\n{code}\r\n\r\nIn our cluster we had the block cache set to an upper limit of 0.76 and the memstore upper limit was set to 0.04.  We noticed the memstore size was exceeding the limit we had set and after looking at the getMemStoreLimit code it seems that the memstore upper limit is sized to the default value if the configuration value is less than .1 or greater than .9.  This now makes the block cache and memstore greater than our available heap.\r\n\r\nWe can remove the check for the greater than 90% of the heap as this can never happen due to the check in HbaseConfiguration.checkForClusterFreeMemoryLimit()\r\n\r\nThis check doesn't seem necessary anymore as we have the HbaseConfiguration class checking for the cluster free limit.  Am I correct in this assumption?\r\n\r\n"
    ],
    [
        "HBASE-12904",
        "HBASE-12875",
        "Threading issues in region_mover.rb We've seen various race conditions when using region_mover with multiple threads.\r\n\r\n{code}\r\nNoMethodError: undefined method `getScanner' for nil:NilClass\r\n  isSuccessfulScan at /home/sfdc/current//bigdata-hbase/hbase/hbase/bin/region_mover.rb:138\r\n     unloadRegions at /home/sfdc/current//bigdata-hbase/hbase/hbase/bin/region_mover.rb:360\r\n{code}\r\n\r\n{code}\r\nNoMethodError: undefined method `[]=' for nil:NilClass\r\n       getTable at /home/sfdc/current//bigdata-hbase/hbase/hbase/bin/region_mover.rb:64\r\n  unloadRegions at /home/sfdc/current//bigdata-hbase/hbase/hbase/bin/region_mover.rb:359\r\n{code}\r\n\r\nLooking at getTable, it's not thread safe. So the multithreaded that was added is incorrect.",
        "Region mover script region_mover.rb sometimes fail with NPE Moved region _IDX_EVENT.PLATFORM_EVENT_METRICS,\\x0B\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00,1421392135537.f058d58e0560a5a0d681d3345d778b71. cost: 1.643\r\n\r\nNoMethodError: undefined method `getScanner' for nil:NilClass\r\n  isSuccessfulScan at <>/bin/region_mover.rb:138\r\n     unloadRegions at <>/bin/region_mover.rb:360\r\n              call at org/jruby/RubyProc.java:270\r\n              call at org/jruby/RubyProc.java:220\r\n        initialize at<>/bin/thread-pool.rb:33\r\n              loop at org/jruby/RubyKernel.java:1439\r\n        initialize at <>/bin/thread-pool.rb:31\r\n             catch at org/jruby/RubyKernel.java:1212\r\n        initialize at <>/bin/thread-pool.rb:30\r\n              call at org/jruby/RubyProc.java:270\r\n              call at org/jruby/RubyProc.java:224"
    ],
    [
        "HBASE-12942",
        "HBASE-12393",
        "After disabling the hfile block cache, Regionserver UI is throwing java.lang.NullPointerException Regionserver UI throws java.lang.NullPointerException\r\nif hfile.block.cache.size is disabled\r\n\r\n\r\n{code}\r\njava.lang.NullPointerException\r\n        at org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmplImpl.__jamon_innerUnit__bc_stats(BlockCacheTmplImpl.java:121)\r\n        at org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmplImpl.renderNoFlush(BlockCacheTmplImpl.java:84)\r\n        at org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl.renderNoFlush(BlockCacheTmpl.java:140)\r\n        at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmplImpl.renderNoFlush(RSStatusTmplImpl.java:146)\r\n        at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.renderNoFlush(RSStatusTmpl.java:220)\r\n        at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.render(RSStatusTmpl.java:211)\r\n        at org.apache.hadoop.hbase.regionserver.RSStatusServlet.doGet(RSStatusServlet.java:58)\r\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\r\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\r\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\r\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\r\n        at org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:113)\r\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\r\n        at org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1351)\r\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\r\n        at org.apache.hadoop.hbase.http.NoCacheFilter.doFilter(NoCacheFilter.java:49)\r\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\r\n        at org.apache.hadoop.hbase.http.NoCacheFilter.doFilter(NoCacheFilter.java:49)\r\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\r\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\r\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\r\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\r\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\r\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\r\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\r\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\r\n        at org.mortbay.jetty.Server.handle(Server.java:326)\r\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\r\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\r\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\r\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\r\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\r\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\r\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\r\n{code}",
        "The regionserver web will throw exception if we disable block cache The CacheConfig.getBlockCache() will return the null point when we set hfile.block.cache.size to zero.\r\nThe BlockCacheTmpl.jamon doesn't make a check on null blockcache.\r\n\r\n{code}\r\n<%if cacheConfig == null %>\r\n<p>CacheConfig is null</p>\r\n<%else>\r\n<table class=\"table table-striped\">\r\n    <tr>\r\n        <th>Attribute</th>\r\n        <th>Value</th>\r\n        <th>Description</th>\r\n    </tr>\r\n    <tr>\r\n        <td>Size</td>\r\n        <td><% StringUtils.humanReadableInt(cacheConfig.getBlockCache().size()) %></td>\r\n        <td>Total size of Block Cache (bytes)</td>\r\n    </tr>\r\n    <tr>\r\n        <td>Free</td>\r\n        <td><% StringUtils.humanReadableInt(cacheConfig.getBlockCache().getFreeSize()) %></td>\r\n        <td>Free space in Block Cache (bytes)</td>\r\n    </tr>\r\n    <tr>\r\n        <td>Count</td>\r\n        <td><% String.format(\"%,d\", cacheConfig.getBlockCache().getBlockCount()) %></td>\r\n        <td>Number of blocks in Block Cache</td>\r\n    </tr>\r\n    <tr>\r\n        <td>Evicted</td>\r\n        <td><% String.format(\"%,d\", cacheConfig.getBlockCache().getStats().getEvictedCount()) %></td>\r\n        <td>Number of blocks evicted</td>\r\n    </tr>\r\n    <tr>\r\n        <td>Evictions</td>\r\n        <td><% String.format(\"%,d\", cacheConfig.getBlockCache().getStats().getEvictionCount()) %></td>\r\n        <td>Number of times an eviction occurred</td>\r\n    </tr>\r\n    <tr>\r\n        <td>Hits</td>\r\n        <td><% String.format(\"%,d\", cacheConfig.getBlockCache().getStats().getHitCount()) %></td>\r\n        <td>Number requests that were cache hits</td>\r\n    </tr>\r\n    <tr>\r\n        <td>Hits Caching</td>\r\n        <td><% String.format(\"%,d\", cacheConfig.getBlockCache().getStats().getHitCachingCount()) %></td>\r\n        <td>Cache hit block requests but only requests set to use Block Cache</td>\r\n    </tr>\r\n    <tr>\r\n        <td>Misses</td>\r\n        <td><% String.format(\"%,d\", cacheConfig.getBlockCache().getStats().getMissCount()) %></td>\r\n        <td>Number of requests that were cache misses</td>\r\n    </tr>\r\n    <tr>\r\n        <td>Misses Caching</td>\r\n        <td><% String.format(\"%,d\", cacheConfig.getBlockCache().getStats().getMissCount()) %></td>\r\n        <td>Block requests that were cache misses but only requests set to use Block Cache</td>\r\n    </tr>\r\n    <tr>\r\n        <td>Hit Ratio</td>\r\n        <td><% String.format(\"%,.2f\", cacheConfig.getBlockCache().getStats().getHitRatio() * 100) %><% \"%\" %></td>\r\n        <td>Hit Count divided by total requests count</td>\r\n    </tr>\r\n{code}\r\n\r\n"
    ],
    [
        "HBASE-13317",
        "HBASE-12813",
        "Region server reportForDuty stuck looping if there is a master change During cluster startup, region server reportForDuty gets stuck looping if there is a master change.\r\n\r\n{noformat}\r\n2015-03-22 11:15:16,186 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=bigaperf274,60000,1427045883965 with port=60020, startcode=1427048115174\r\n2015-03-22 11:15:16,272 WARN  [regionserver60020] regionserver.HRegionServer: error telling master we are up\r\ncom.google.protobuf.ServiceException: java.net.ConnectException: Connection refused\r\n\tat org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1678)\r\n\tat org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1719)\r\n\tat org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$BlockingStub.regionServerStartup(RegionServerStatusProtos.java:8277)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.reportForDuty(HRegionServer.java:2137)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:896)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n2015-03-22 11:15:16,274 WARN  [regionserver60020] regionserver.HRegionServer: reportForDuty failed; sleeping and then retrying.\r\n2015-03-22 11:15:19,274 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=bigaperf273,60000,1427048108439 with port=60020, startcode=1427048115174\r\n2015-03-22 11:15:19,275 WARN  [regionserver60020] regionserver.HRegionServer: error telling master we are up\r\ncom.google.protobuf.ServiceException: java.net.ConnectException: Connection refused\r\n\tat org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1678)\r\n\tat org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1719)\r\n\tat org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$BlockingStub.regionServerStartup(RegionServerStatusProtos.java:8277)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.reportForDuty(HRegionServer.java:2137)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:896)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n2015-03-22 11:15:19,276 WARN  [regionserver60020] regionserver.HRegionServer: reportForDuty failed; sleeping and then retrying.\r\n2015-03-22 11:15:22,276 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=bigaperf273,60000,1427048108439 with port=60020, startcode=1427048115174\r\n2015-03-22 11:15:22,296 DEBUG [regionserver60020] regionserver.HRegionServer: Master is not running yet\r\n2015-03-22 11:15:22,296 WARN  [regionserver60020] regionserver.HRegionServer: reportForDuty failed; sleeping and then retrying.\r\n2015-03-22 11:15:25,296 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=bigaperf273,60000,1427048108439 with port=60020, startcode=1427048115174\r\n2015-03-22 11:15:25,299 DEBUG [regionserver60020] regionserver.HRegionServer: Master is not running yet\r\n2015-03-22 11:15:25,299 WARN  [regionserver60020] regionserver.HRegionServer: reportForDuty failed; sleeping and then retrying.\r\n2015-03-22 11:15:28,299 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=bigaperf273,60000,1427048108439 with port=60020, startcode=1427048115174\r\n2015-03-22 11:15:28,302 DEBUG [regionserver60020] regionserver.HRegionServer: Master is not running yet\r\n2015-03-22 11:15:28,302 WARN  [regionserver60020] regionserver.HRegionServer: reportForDuty failed; sleeping and then retrying.\r\n{noformat}\r\n\r\nWhat happended is the region server first got master=bigaperf274,60000,1427045883965.  Before it was able to report successfully, the maser changed to bigaperf273,60000,1427048108439.\r\nWe were supposed to open a new connection to the new master. But we never did, looping and trying to old address forever.",
        "Reporting region in transition shouldn't loop forever We had an issue where a region server wasn't able to send the report region in transition request. Well after failing it just retries forever.\r\n\r\nAt some point it would have been better to just abort the region server if it can't talk to master."
    ],
    [
        "HBASE-13333",
        "HBASE-8157",
        "Renew Scanner Lease without advancing the RegionScanner We have a usecase (for Phoenix) where we want to let the server know that the client is still around. Like a client-side heartbeat.\r\n\r\nDoing a full heartbeat is complicated, but we could add the ability to make scanner call with caching set to 0. The server already does the right thing (it renews the lease, but does not advance the scanner).\r\n\r\nIt looks like the client (ScannerCallable) also does the right thing. We cannot break ResultScanner before HBase 2.0, but we can add a renewLease() method to AbstractClientScaner. Phoenix (or any other caller) can then cast to ClientScanner and call that method to ensure we renew the lease on the server.\r\n\r\nIt would be a simple and fully backwards compatible change. [~giacomotaylor]\r\n\r\nComments?",
        "Allow better control over lease timeouts on a per-scan basis Changing the global scanner lease timeout is a heavy-handed solution for long scans on a cluster that may be used by a variety of applications.  Two additions would make this easier to manage in this circumstance:\r\n\r\n1) Allow overriding hbase.regionserver.lease.period on a per-scan basis.\r\n\r\n2) Allow manual reporting of progress, similar to Hadoop's context.progress().  Example usage:\r\n\r\n{noformat}\r\nScan scan = new Scan(startRow, endRow);\r\nscan.setCaching(someVal); // based on what we expect most rows to take for processing time\r\n\r\nResultScanner scanner = table.getScanner(scan);\r\n\r\nfor (Result r : scanner) {\r\n  //\r\n  // usual processing, the time for which we accounted for in our caching and global lease timeout settings\r\n  //\r\n\r\n  if (someCondition) {\r\n    //\r\n    // More time-intensive processing necessary on this record, which is hard to account for in the caching\r\n    //\r\n\r\n    scanner.progress();\r\n  }\r\n}\r\n{noformat}\r\n"
    ],
    [
        "HBASE-13353",
        "HBASE-10728",
        "[Shell] get_counter is asking for too many parameters Trying:\r\n\r\n{code}\r\nhbase(main):036:0> get_counter 'counters', '20150101', 'daily:hits'\r\n{code}\r\n\r\nand getting \r\n\r\n{code}\r\nERROR: wrong number of arguments (3 for 4)\r\n...\r\n  hbase> get_counter 'ns1:t1', 'r1', 'c1'\r\n  hbase> get_counter 't1', 'r1', 'c1'\r\n{code}\r\n\r\nIt is asking for another, undocumented parameter. The issue is that there is a \"can be nil\" markup missing in get_counter.rb:\r\n\r\n{code}\r\n39 def command(table, row, column, value)\r\n40    get_counter(table(table), row, column, value)\r\n41 end\r\n{code} \r\n\r\nThe first line should be:\r\n\r\n{code}\r\n39 def command(table, row, column, value = nil)\r\n{code}\r\n\r\nEasy fix, but makes me wonder why this is not caught anywhere or reported.",
        "get_counter value is never used. "
    ],
    [
        "HBASE-13353",
        "HBASE-11613",
        "[Shell] get_counter is asking for too many parameters Trying:\r\n\r\n{code}\r\nhbase(main):036:0> get_counter 'counters', '20150101', 'daily:hits'\r\n{code}\r\n\r\nand getting \r\n\r\n{code}\r\nERROR: wrong number of arguments (3 for 4)\r\n...\r\n  hbase> get_counter 'ns1:t1', 'r1', 'c1'\r\n  hbase> get_counter 't1', 'r1', 'c1'\r\n{code}\r\n\r\nIt is asking for another, undocumented parameter. The issue is that there is a \"can be nil\" markup missing in get_counter.rb:\r\n\r\n{code}\r\n39 def command(table, row, column, value)\r\n40    get_counter(table(table), row, column, value)\r\n41 end\r\n{code} \r\n\r\nThe first line should be:\r\n\r\n{code}\r\n39 def command(table, row, column, value = nil)\r\n{code}\r\n\r\nEasy fix, but makes me wonder why this is not caught anywhere or reported.",
        "\"get_counter\" shell command is not displaying the result for counter columns. perform the following opertions in HBase shell prompt.\r\n1. create a table with one column family.\r\n2. insert some amount of data into the table.\r\n3. then perform increment operation on any column qualifier.\r\neg: incr 't', 'r1', 'f:c1'\r\n4. then queried the get counter query,\r\nit is throwing nocounter found message to the user.\r\n{code}\r\neg:\r\n hbase(main):010:0> get_counter 't', 'r1', 'f', 'c1'\r\n No counter found at specified coordinates\r\n{code}\r\n=====================================================\r\nand wrong message is throwing to user, while executing the get_counter query.\r\n{code}\r\nhbase(main):009:0> get_counter 't', 'r1', 'f'\r\n\r\nERROR: wrong number of arguments (3 for 4)\r\n\r\nHere is some help for this command:\r\nReturn a counter cell value at specified table/row/column coordinates.\r\nA cell cell should be managed with atomic increment function oh HBase\r\nand the data should be binary encoded. Example:\r\n\r\n  hbase> get_counter 'ns1:t1', 'r1', 'c1'\r\n  hbase> get_counter 't1', 'r1', 'c1'\r\n\r\nThe same commands also can be run on a table reference. Suppose you had a reference\r\nt to table 't1', the corresponding command would be:\r\n\r\n  hbase> t.get_counter 'r1', 'c1'\r\n{code}\r\n{code}\r\nproblem:\r\n   In example they given 3 arguments but asking 4 arguments\r\n   If run with 3 arguments it will throw error.\r\n   if run with 4 arguments \"No counter found at specified coordinates\" message is throwing even though counter is specified.\r\n{code}"
    ],
    [
        "HBASE-13570",
        "HBASE-13490",
        "hbase daemon foreground start command executes output of ulimit typo in foreground_start command.\r\n\r\n{code}\r\n    `ulimit -a` >> \"$HBASE_LOGLOG\" 2>&1\r\n{code}",
        "foreground daemon start re-executes ulimit output Linux Command execution is failing while starting HBase processes using hbase-daemon.sh file\r\n\r\nWhile starting any HBase process (HMaster or Regionserver)\r\nulimit command execution is failing.\r\n\r\n{code}\r\n echo \"`date` Starting $command on `hostname`\" >> ${HBASE_LOGLOG}\r\n    `ulimit -a` >> \"$HBASE_LOGLOG\" 2>&1\r\n{code}\r\n\r\nLog message is follows.\r\n{noformat}\r\nThu Apr 16 19:24:25 IST 2015 Starting regionserver on HOST-10\r\n/opt/hdfsdata/HA/install/hbase/regionserver/bin/hbase-daemon.sh: line 207: core: command not found\r\n{noformat}"
    ],
    [
        "HBASE-13732",
        "HBASE-13574",
        "TestHBaseFsck#testParallelWithRetriesHbck fails intermittently TestHBaseFsck#testParallelWithRetriesHbck failed intermittently (especially in Windows environment) with \"java.io.IOException: Duplicate hbck - Abort\"\r\n\r\n{noformat}\r\njava.util.concurrent.ExecutionException: java.io.IOException: Duplicate hbck - Abort\r\n\tat java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)\r\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:111)\r\n\tat org.apache.hadoop.hbase.util.TestHBaseFsck.testParallelWithRetriesHbck(TestHBaseFsck.java:644)\r\nCaused by: java.io.IOException: Duplicate hbck - Abort\r\n\tat org.apache.hadoop.hbase.util.HBaseFsck.connect(HBaseFsck.java:484)\r\n\tat org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.doFsck(HbckTestingUtil.java:53)\r\n\tat org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.doFsck(HbckTestingUtil.java:43)\r\n\tat org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.doFsck(HbckTestingUtil.java:38)\r\n\tat org.apache.hadoop.hbase.util.TestHBaseFsck$2RunHbck.call(TestHBaseFsck.java:635)\r\n\tat org.apache.hadoop.hbase.util.TestHBaseFsck$2RunHbck.call(TestHBaseFsck.java:628)\r\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:166)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\n{noformat}\r\n\r\nHBASE-13591 tried to address this issue.  It did improve the pass rate in Linux environment (after the fix, I could not repro in my machine); however, the test still failed intermittently in Windows environment during testing of 1.1 release.\r\n\r\nLooking at the code, it uses the ExponentialBackoffPolicy (starting with 200ms sleep time after first failed attempt to acquire the lock in ZK, then 400ms, then 800ms, etc.) in between retries.  Therefore, even the first hbck run completes, the second hbck run would still fail due to long sleep time.  \r\n\r\nthe proposal to fix the problem is to use ExponentialBackoffPolicyWithLimit and cap the max sleep time to some small number (eg. 5 seconds, it should be configurable).  This would make the test more robust.  ",
        "Broken TestHBaseFsck in master with hadoop 2.6.0 Got following exception and it reproducible (I can see it in recent tests runs from other patches).\r\n{noformat}\r\nRunning org.apache.hadoop.hbase.util.TestHBaseFsck\r\nTests run: 51, Failures: 0, Errors: 1, Skipped: 1, Time elapsed: 348.628 sec <<< FAILURE! - in org.apache.hadoop.hbase.util.TestHBaseFsck\r\ntestParallelWithRetriesHbck(org.apache.hadoop.hbase.util.TestHBaseFsck)  Time elapsed: 30.052 sec  <<< ERROR!\r\njava.util.concurrent.ExecutionException: java.io.IOException: Duplicate hbck - Abort\r\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:188)\r\n\tat org.apache.hadoop.hbase.util.TestHBaseFsck.testParallelWithRetriesHbck(TestHBaseFsck.java:634)\r\nCaused by: java.io.IOException: Duplicate hbck - Abort\r\n\tat org.apache.hadoop.hbase.util.HBaseFsck.connect(HBaseFsck.java:473)\r\n\tat org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.doFsck(HbckTestingUtil.java:53)\r\n\tat org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.doFsck(HbckTestingUtil.java:43)\r\n\tat org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.doFsck(HbckTestingUtil.java:38)\r\n\tat org.apache.hadoop.hbase.util.TestHBaseFsck$2RunHbck.call(TestHBaseFsck.java:625)\r\n\tat org.apache.hadoop.hbase.util.TestHBaseFsck$2RunHbck.call(TestHBaseFsck.java:621)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n{noformat}"
    ],
    [
        "HBASE-13793",
        "HBASE-13792",
        "Regionserver unable to report to master when master is restarted I was testing master branch on distributed cluster and i notice that when master is restarted  on running cluster regionservers are unable report back when master is up again. \r\nThings back to normal after i restarted regionservers. Logs showing that regionservers are correctly detecting master znode.  \r\nAfter some digging i notice that we have changed client implementation in RpcClientFactory to  AsyncRpcClient so i have tried running cluster with previous  RpcClientImpl and issue was gone. \r\nSo issue is probably caused by AsyncRpcClient which is unable reconnect to master once original connection is gone.\r\nI was able to fix issue by creating new rpcClient object inside HRegionServer#createRegionServerStatusStub() and using it for channel creation here is diff:\r\n{code}\r\ndiff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java\r\nindex fa56966..27e658c 100644\r\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java\r\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java\r\n@@ -2219,8 +2219,11 @@ public class HRegionServer extends HasThread implements\r\n           break;\r\n         }\r\n         try {\r\n+          LOG.info(\"***Creating new client connection\");\r\n+          rpcClient = RpcClientFactory.createClient(conf, clusterId, new InetSocketAddress(\r\n+            rpcServices.isa.getAddress(), 0));\r\n           BlockingRpcChannel channel =\r\n-            this.rpcClient.createBlockingRpcChannel(sn, userProvider.getCurrent(),\r\n+          rpcClient.createBlockingRpcChannel(sn, userProvider.getCurrent(),\r\n               shortOperationTimeout);\r\n           intf = RegionServerStatusService.newBlockingStub(channel);\r\n           break;\r\n{code}\r\n\r\nIf this is acceptable way for fixing this issue i will create and attach patch?",
        "Regionserver unable to report to master when master is restarted I was testing master branch on distributed cluster and i notice that when master is restarted  on running cluster regionservers are unable report back when master is up again. \r\nThings back to normal after i restarted regionservers. Logs showing that regionservers are correctly detecting master znode.  \r\nAfter some digging i notice that we have changed client implementation in RpcClientFactory to  AsyncRpcClient so i have tried running cluster with previous  RpcClientImpl and issue was gone. \r\nSo issue is probably caused by AsyncRpcClient which is unable reconnect to master once original connection is gone.\r\nI was able to fix issue by creating new rpcClient object inside HRegionServer#createRegionServerStatusStub() and using it for channel creation here is diff:\r\n{code}\r\ndiff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java\r\nindex fa56966..27e658c 100644\r\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java\r\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java\r\n@@ -2219,8 +2219,11 @@ public class HRegionServer extends HasThread implements\r\n           break;\r\n         }\r\n         try {\r\n+          LOG.info(\"***Creating new client connection\");\r\n+          rpcClient = RpcClientFactory.createClient(conf, clusterId, new InetSocketAddress(\r\n+            rpcServices.isa.getAddress(), 0));\r\n           BlockingRpcChannel channel =\r\n-            this.rpcClient.createBlockingRpcChannel(sn, userProvider.getCurrent(),\r\n+          rpcClient.createBlockingRpcChannel(sn, userProvider.getCurrent(),\r\n               shortOperationTimeout);\r\n           intf = RegionServerStatusService.newBlockingStub(channel);\r\n           break;\r\n{code}\r\n\r\nIf this is acceptable way for fixing this issue i will create and attach patch?"
    ],
    [
        "HBASE-14018",
        "HBASE-13329",
        "RegionServer is aborted when flushing memstore. + Pseudo-distributed Hadoop (2.6.0), ZK_HBASE_MANAGE = true (1 master, 1 regionserver).\r\n+ Put data to OpenTSDB, 1000 records / s, for 2000 seconds.\r\n+ RegionServer is aborted.\r\n\r\n=== RegionServer logs ===\r\n2015-07-03 16:37:37,332 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=371.27 KB, freeSize=181.41 MB, max=181.78 MB, blockCount=5, accesses=1623, hits=172, hitRatio=10.60%, , cachingAccesses=177, cachingHits=151, cachingHitsRatio=85.31%, evictions=1139, evicted=21, evictedPerRun=0.018437225371599197\r\n2015-07-03 16:37:37,898 INFO  [node1:16040Replication Statistics #0] regionserver.Replication: Normal source for cluster 1: Total replicated edits: 2744, currently replicating from: hdfs://node1.vmcluster:9000/hbase/WALs/node1.vmcluster,16040,1435897652505/node1.vmcluster%2C16040%2C1435897652505.default.1435908458590 at position: 19207814\r\n\r\n2015-07-03 16:42:37,331 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=371.27 KB, freeSize=181.41 MB, max=181.78 MB, blockCount=5, accesses=1624, hits=173, hitRatio=10.65%, , cachingAccesses=178, cachingHits=152, cachingHitsRatio=85.39%, evictions=1169, evicted=21, evictedPerRun=0.01796407252550125\r\n2015-07-03 16:42:37,899 INFO  [node1:16040Replication Statistics #0] regionserver.Replication: Normal source for cluster 1: Total replicated edits: 3049, currently replicating from: hdfs://node1.vmcluster:9000/hbase/WALs/node1.vmcluster,16040,1435897652505/node1.vmcluster%2C16040%2C1435897652505.default.1435908458590 at position: 33026416\r\n\r\n2015-07-03 16:43:27,217 INFO  [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for tsdb,,1435897759785.2d49cd81fb6513f51af58bd0394c4e0d., current region memstore size 128.05 MB\r\n2015-07-03 16:43:27,899 FATAL [MemStoreFlusher.1] regionserver.HRegionServer: ABORTING region server node1.vmcluster,16040,1435897652505: Replay of WAL required. Forcing server shutdown\r\norg.apache.hadoop.hbase.DroppedSnapshotException: region: tsdb,,1435897759785.2d49cd81fb6513f51af58bd0394c4e0d.\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2001)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1772)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1704)\r\n\tat org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:445)\r\n\tat org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:407)\r\n\tat org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$800(MemStoreFlusher.java:69)\r\n\tat org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:225)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: -32743\r\n\tat org.apache.hadoop.hbase.CellComparator.getMinimumMidpointArray(CellComparator.java:478)\r\n\tat org.apache.hadoop.hbase.CellComparator.getMidpoint(CellComparator.java:448)\r\n\tat org.apache.hadoop.hbase.io.hfile.HFileWriterV2.finishBlock(HFileWriterV2.java:165)\r\n\tat org.apache.hadoop.hbase.io.hfile.HFileWriterV2.checkBlockBoundary(HFileWriterV2.java:146)\r\n\tat org.apache.hadoop.hbase.io.hfile.HFileWriterV2.append(HFileWriterV2.java:263)\r\n\tat org.apache.hadoop.hbase.io.hfile.HFileWriterV3.append(HFileWriterV3.java:87)\r\n\tat org.apache.hadoop.hbase.regionserver.StoreFile$Writer.append(StoreFile.java:932)\r\n\tat org.apache.hadoop.hbase.regionserver.StoreFlusher.performFlush(StoreFlusher.java:121)\r\n\tat org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:71)\r\n\tat org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:879)\r\n\tat org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:2128)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1955)\r\n\t... 7 more\r\n2015-07-03 16:43:27,901 FATAL [MemStoreFlusher.1] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: [org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint]\r\n\r\n\r\n=== HMaster logs ===\r\n2015-07-03 13:29:20,671 INFO  [RegionOpenAndInitThread-tsdb-meta-1] regionserver.HRegion: creating HRegion tsdb-meta HTD == 'tsdb-meta', {NAME => 'name', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '1'} RootDir = hdfs://node1.vmcluster:9000/hbase/.tmp Table name == tsdb-meta\r\n2015-07-03 13:29:20,696 INFO  [RegionOpenAndInitThread-tsdb-meta-1] regionserver.HRegion: Closed tsdb-meta,,1435897760624.0738e3fac8ffe40d656dc91588a47aac.\r\n2015-07-03 13:29:20,703 INFO  [MASTER_TABLE_OPERATIONS-node1:16020-0] hbase.MetaTableAccessor: Added 1\r\n2015-07-03 13:29:20,704 INFO  [MASTER_TABLE_OPERATIONS-node1:16020-0] master.AssignmentManager: Assigning 1 region(s) to node1.vmcluster,16040,1435897652505\r\n2015-07-03 13:29:20,717 INFO  [MASTER_TABLE_OPERATIONS-node1:16020-0] master.RegionStates: Transition {0738e3fac8ffe40d656dc91588a47aac state=OFFLINE, ts=1435897760704, server=null} to {0738e3fac8ffe40d656dc91588a47aac state=PENDING_OPEN, ts=1435897760717, server=node1.vmcluster,16040,1435897652505}\r\n2015-07-03 13:29:20,729 WARN  [MASTER_TABLE_OPERATIONS-node1:16020-0] zookeeper.ZKTableStateManager: Moving table tsdb-meta state from ENABLING to ENABLED\r\n2015-07-03 13:29:20,734 INFO  [AM.ZK.Worker-pool2-t33] master.RegionStates: Transition {0738e3fac8ffe40d656dc91588a47aac state=PENDING_OPEN, ts=1435897760717, server=node1.vmcluster,16040,1435897652505} to {0738e3fac8ffe40d656dc91588a47aac state=OPENING, ts=1435897760734, server=node1.vmcluster,16040,1435897652505}\r\n2015-07-03 13:29:20,748 INFO  [MASTER_TABLE_OPERATIONS-node1:16020-0] handler.CreateTableHandler: failed. null\r\n2015-07-03 13:29:20,772 INFO  [AM.ZK.Worker-pool2-t35] master.RegionStates: Transition {0738e3fac8ffe40d656dc91588a47aac state=OPENING, ts=1435897760734, server=node1.vmcluster,16040,1435897652505} to {0738e3fac8ffe40d656dc91588a47aac state=OPEN, ts=1435897760772, server=node1.vmcluster,16040,1435897652505}\r\n2015-07-03 13:29:20,774 INFO  [AM.ZK.Worker-pool2-t35] master.RegionStates: Onlined 0738e3fac8ffe40d656dc91588a47aac on node1.vmcluster,16040,1435897652505\r\n2015-07-03 16:43:27,970 ERROR [B.defaultRpcServer.handler=15,queue=0,port=16020] master.MasterRpcServices: Region server node1.vmcluster,16040,1435897652505 reported a fatal error:\r\nABORTING region server node1.vmcluster,16040,1435897652505: Replay of WAL required. Forcing server shutdown\r\nCause:\r\norg.apache.hadoop.hbase.DroppedSnapshotException: region: tsdb,,1435897759785.2d49cd81fb6513f51af58bd0394c4e0d.\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2001)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1772)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1704)\r\n\tat org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:445)\r\n\tat org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:407)\r\n\tat org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$800(MemStoreFlusher.java:69)\r\n\tat org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:225)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: -32743\r\n\tat org.apache.hadoop.hbase.CellComparator.getMinimumMidpointArray(CellComparator.java:478)\r\n\tat org.apache.hadoop.hbase.CellComparator.getMidpoint(CellComparator.java:448)\r\n\tat org.apache.hadoop.hbase.io.hfile.HFileWriterV2.finishBlock(HFileWriterV2.java:165)\r\n\tat org.apache.hadoop.hbase.io.hfile.HFileWriterV2.checkBlockBoundary(HFileWriterV2.java:146)\r\n\tat org.apache.hadoop.hbase.io.hfile.HFileWriterV2.append(HFileWriterV2.java:263)\r\n\tat org.apache.hadoop.hbase.io.hfile.HFileWriterV3.append(HFileWriterV3.java:87)\r\n\tat org.apache.hadoop.hbase.regionserver.StoreFile$Writer.append(StoreFile.java:932)\r\n\tat org.apache.hadoop.hbase.regionserver.StoreFlusher.performFlush(StoreFlusher.java:121)\r\n\tat org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:71)\r\n\tat org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:879)\r\n\tat org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:2128)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1955)\r\n\t... 7 more\r\n\r\n2015-07-03 16:43:32,595 INFO  [main-EventThread] zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [node1.vmcluster,16040,1435897652505]\r\n2015-07-03 16:43:32,611 INFO  [MASTER_META_SERVER_OPERATIONS-node1:16020-0] handler.MetaServerShutdownHandler: Splitting hbase:meta logs for node1.vmcluster,16040,1435897652505\r\n2015-07-03 16:43:32,627 INFO  [MASTER_META_SERVER_OPERATIONS-node1:16020-0] master.SplitLogManager: dead splitlog workers [node1.vmcluster,16040,1435897652505]\r\n2015-07-03 16:43:32,630 INFO  [MASTER_META_SERVER_OPERATIONS-node1:16020-0] master.SplitLogManager: started splitting 1 logs in [hdfs://node1.vmcluster:9000/hbase/WALs/node1.vmcluster,16040,1435897652505-splitting] for [node1.vmcluster,16040,1435897652505]",
        "ArrayIndexOutOfBoundsException in CellComparator#getMinimumMidpointArray While trying to benchmark my opentsdb cluster, I've created a script that sends to hbase always the same value (in this case 1). After a few minutes, the whole region server crashes and the region itself becomes impossible to open again (cannot assign or unassign). After some investigation, what I saw on the logs is that when a Memstore flush is called on a large region (128mb) the process errors, killing the regionserver. On restart, replaying the edits generates the same error, making the region unavailable. Tried to manually unassign, assign or close_region. That didn't work because the code that reads/replays it crashes.\r\nFrom my investigation this seems to be an overflow issue. The logs show that the function getMinimumMidpointArray tried to access index -32743 of an array, extremely close to the minimum short value in Java. Upon investigation of the source code, it seems an index short is used, being incremented as long as the two vectors are the same, probably making it overflow on large vectors with equal data. Changing it to int should solve the problem.\r\nHere follows the hadoop logs of when the regionserver went down. Any help is appreciated. Any other information you need please do tell me:\r\n2015-03-24 18:00:56,187 INFO  [regionserver//10.2.0.73:16020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/10.2.0.73,16020,1427216382590/10.2.0.73%2C16020%2C1427216382590.default.1427220018516 with entries=143, filesize=134.70 MB; new WAL /hbase/WALs/10.2.0.73,16020,1427216382590/10.2.0.73%2C16020%2C1427216382590.default.1427220056140\r\n2015-03-24 18:00:56,188 INFO  [regionserver//10.2.0.73:16020.logRoller] wal.FSHLog: Archiving hdfs://10.2.0.74:8020/hbase/WALs/10.2.0.73,16020,1427216382590/10.2.0.73%2C16020%2C1427216382590.default.1427219987709 to hdfs://10.2.0.74:8020/hbase/oldWALs/10.2.0.73%2C16020%2C1427216382590.default.1427219987709\r\n2015-03-24 18:04:35,722 INFO  [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for tsdb,,1427133969325.52bc1994da0fea97563a4a656a58bec2., current region memstore size 128.04 MB\r\n2015-03-24 18:04:36,154 FATAL [MemStoreFlusher.0] regionserver.HRegionServer: ABORTING region server 10.2.0.73,16020,1427216382590: Replay of WAL required. Forcing server shutdown\r\norg.apache.hadoop.hbase.DroppedSnapshotException: region: tsdb,,1427133969325.52bc1994da0fea97563a4a656a58bec2.\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1999)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1770)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1702)\r\n\tat org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:445)\r\n\tat org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:407)\r\n\tat org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$800(MemStoreFlusher.java:69)\r\n\tat org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:225)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: -32743\r\n\tat org.apache.hadoop.hbase.CellComparator.getMinimumMidpointArray(CellComparator.java:478)\r\n\tat org.apache.hadoop.hbase.CellComparator.getMidpoint(CellComparator.java:448)\r\n\tat org.apache.hadoop.hbase.io.hfile.HFileWriterV2.finishBlock(HFileWriterV2.java:165)\r\n\tat org.apache.hadoop.hbase.io.hfile.HFileWriterV2.checkBlockBoundary(HFileWriterV2.java:146)\r\n\tat org.apache.hadoop.hbase.io.hfile.HFileWriterV2.append(HFileWriterV2.java:263)\r\n\tat org.apache.hadoop.hbase.io.hfile.HFileWriterV3.append(HFileWriterV3.java:87)\r\n\tat org.apache.hadoop.hbase.regionserver.StoreFile$Writer.append(StoreFile.java:932)\r\n\tat org.apache.hadoop.hbase.regionserver.StoreFlusher.performFlush(StoreFlusher.java:121)\r\n\tat org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:71)\r\n\tat org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:879)\r\n\tat org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:2128)\r\n\tat org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1953)\r\n\t... 7 more\r\n2015-03-24 18:04:36,156 FATAL [MemStoreFlusher.0] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: [org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint]"
    ],
    [
        "HBASE-14079",
        "HBASE-14078",
        "improve error message when Master fails to connect to Hadoop-auth Current error message at INFO level doesn't give any hint about what keytab and principle are in use\r\n\r\n{quote}\r\n2015-07-14 13:32:48,514 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties\r\n2015-07-14 13:32:48,640 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\r\n2015-07-14 13:32:48,640 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started\r\n2015-07-14 13:32:48,776 ERROR [main] master.HMasterCommandLine: Master exiting\r\njava.lang.RuntimeException: Failed construction of Master: class org.apache.hadoop.hbase.master.HMaster\r\n        at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:2258)\r\n        at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:234)\r\n        at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:140)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)\r\n        at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:2272)\r\nCaused by: javax.security.auth.login.LoginException: Unable to obtain password from user\r\n\r\n        at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:856)\r\n        at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:719)\r\n        at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:584)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:606)\r\n        at javax.security.auth.login.LoginContext.invoke(LoginContext.java:762)\r\n        at javax.security.auth.login.LoginContext.access$000(LoginContext.java:203)\r\n        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:690)\r\n        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:688)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:687)\r\n        at javax.security.auth.login.LoginContext.login(LoginContext.java:595)\r\n        at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:912)\r\n        at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:242)\r\n        at org.apache.hadoop.hbase.security.User$SecureHadoopUser.login(User.java:385)\r\n        at org.apache.hadoop.hbase.security.User.login(User.java:252)\r\n        at org.apache.hadoop.hbase.security.UserProvider.login(UserProvider.java:115)\r\n        at org.apache.hadoop.hbase.master.HMaster.login(HMaster.java:464)\r\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:553)\r\n        at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:351)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n        at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:2253)\r\n        ... 5 more\r\n{quote}\r\n\r\nincreasing to DEBUG also doesn't help.",
        "improve error message when HMaster can't bind to port When the master fails to start becahse hbase.master.port is already taken, the log messages could make it easier to tell.\r\n\r\n{quote}\r\n2015-07-14 13:10:02,667 INFO  [main] regionserver.RSRpcServices: master/master01.example.com/10.20.188.121:16000 server-side HConnection retries=350\r\n2015-07-14 13:10:02,879 INFO  [main] ipc.SimpleRpcScheduler: Using deadline as user call queue, count=3\r\n2015-07-14 13:10:02,895 ERROR [main] master.HMasterCommandLine: Master exiting\r\njava.lang.RuntimeException: Failed construction of Master: class org.apache.hadoop.hbase.master.HMaster\r\n        at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:2258)\r\n        at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:234)\r\n        at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:140)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)\r\n        at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:2272)\r\nCaused by: java.net.BindException: Address already in use\r\n        at sun.nio.ch.Net.bind0(Native Method)\r\n        at sun.nio.ch.Net.bind(Net.java:444)\r\n        at sun.nio.ch.Net.bind(Net.java:436)\r\n        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)\r\n        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)\r\n        at org.apache.hadoop.hbase.ipc.RpcServer.bind(RpcServer.java:2513)\r\n        at org.apache.hadoop.hbase.ipc.RpcServer$Listener.<init>(RpcServer.java:599)\r\n        at org.apache.hadoop.hbase.ipc.RpcServer.<init>(RpcServer.java:2000)\r\n        at org.apache.hadoop.hbase.regionserver.RSRpcServices.<init>(RSRpcServices.java:919)\r\n        at org.apache.hadoop.hbase.master.MasterRpcServices.<init>(MasterRpcServices.java:211)\r\n        at org.apache.hadoop.hbase.master.HMaster.createRpcServices(HMaster.java:509)\r\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:535)\r\n        at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:351)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n        at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:2253)\r\n        ... 5 more\r\n{quote}\r\n\r\nI recognize that the \"RSRpcServices\" log message shows port 16000, but I don't know why a new operator would. Additionally, it'd be nice to tell them that the port is controlled by {{hbase.master.port}}. Maybe give a hint on how to see what's using the port. Could be too os-dist specific?"
    ],
    [
        "HBASE-14339",
        "HBASE-7743",
        "HBase Bulk Load and super wide rows This may not be a huge issues but it does come up.  If the number of columns in a row are to many then KeyValueSortReducer will blow up with a out of memory exception, because it uses a TreeMap to sort the columns with in the memory of the reducer.\r\n\r\nA solution would be to add the column family and qualifier to the key so the shuffle would handle the sort.\r\n\r\nThe partitioner would only partition on the rowKey but ordering would apply to the RowKey, Column Family, and Column Qualifier.\r\n\r\nLook at the Spark Bulk load as an example.  HBASE-14150  ",
        "Replace *SortReducers with Hadoop Secondary Sort The mapreduce package provides two Reducer implementations, KeyValueSortReducer and PutSortReducer, which are used by Import, ImportTsv, and WALPlayer in conjunction with the HFileOutputFormat. Both of these implementations make use of a TreeSet to sort values matching a key. This reducer will OOM when rows are large.\r\n\r\nA better solution would be to implement secondary sort of the values. That way hadoop sorts the records, spilling to disk when necessary."
    ],
    [
        "HBASE-14653",
        "HBASE-14291",
        "Balancer is getting NullPointerExceptions {code}ERROR: java.io.IOException\r\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2165)\r\n\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:106)\r\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)\r\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster.getLeastLoadedTopServerForRegion(BaseLoadBalancer.java:847)\r\n\tat org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer$LocalityBasedCandidateGenerator.generate(StochasticLoadBalancer.java:609)\r\n\tat org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.balanceCluster(StochasticLoadBalancer.java:267)\r\n\tat org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:1263)\r\n\tat org.apache.hadoop.hbase.master.MasterRpcServices.balance(MasterRpcServices.java:435)\r\n\tat org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:53616)\r\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2120)\r\n\t... 4 more\r\n{code}",
        "NPE On StochasticLoadBalancer Balance Involving RS With No Regions When StochasticLoadBalancer attempts to balance a local RS with multiple regions with another local RS that had no regions the HBase shell call of 'balancer' gets the following NPE:\r\n{noformat}\r\nERROR: java.io.IOException\r\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2175)\r\n\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:106)\r\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)\r\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster.getLeastLoadedTopServerForRegion(BaseLoadBalancer.java:863)\r\n\tat org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer$LocalityBasedCandidateGenerator.generate(StochasticLoadBalancer.java:724)\r\n\tat org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.balanceCluster(StochasticLoadBalancer.java:325)\r\n\tat org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.balanceCluster(StochasticLoadBalancer.java:263)\r\n\tat org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:1264)\r\n\tat org.apache.hadoop.hbase.master.MasterRpcServices.balance(MasterRpcServices.java:413)\r\n\tat org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:52450)\r\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2133)\r\n\t... 4 more\r\n{noformat}\r\n\r\nIssue only occurs when one of the RSs has no regions before balancing.  Also, unsure if distributed RSs would also have same issue.  Attached 'hbase-mwarhaftig-master-Matts-MBP.log' is master's log of the error occurring.  \r\n\r\nSimpleLoadBalancer rebalances correctly when used in the same situation.  "
    ],
    [
        "HBASE-14699",
        "HBASE-6617",
        "Replication crashes regionservers when hbase.wal.provider is set to multiwal When the hbase.wal.provider is set to multiwal and replication is enabled, the regionservers start crashing with the following exception:\r\n\r\n{code}\r\n<hostname>,16020,1445495411258: Failed to write replication wal position (filename=<hostname>%2C16020%2C1445495411258.null0.1445495898373, position=1322399)\r\norg.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hbase/replication/rs/<hostname>,16020,1445495411258/1/<hostname>%2C16020%2C1445495411258.null0.1445495898373\r\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:111)\r\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\r\n\tat org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270)\r\n\tat org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.setData(RecoverableZooKeeper.java:429)\r\n\tat org.apache.hadoop.hbase.zookeeper.ZKUtil.setData(ZKUtil.java:940)\r\n\tat org.apache.hadoop.hbase.zookeeper.ZKUtil.setData(ZKUtil.java:990)\r\n\tat org.apache.hadoop.hbase.zookeeper.ZKUtil.setData(ZKUtil.java:984)\r\n\tat org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.setLogPosition(ReplicationQueuesZKImpl.java:129)\r\n\tat org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.logPositionAndCleanOldLogs(ReplicationSourceManager.java:177)\r\n\tat org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:388)\r\n{code}",
        "ReplicationSourceManager should be able to track multiple WAL paths Currently ReplicationSourceManager uses logRolled() to receive notification about new HLog and remembers it in latestPath.\r\nWhen region server has multiple WAL support, we need to keep track of multiple Path's in ReplicationSourceManager"
    ],
    [
        "HBASE-14811",
        "HBASE-14777",
        "HBaseInterClusterReplicationEndpoint retry logic is broken In HBaseInterClusterReplicationEndpoint, we do something like this:\r\n{code}\r\nentryLists.remove(f.get());\r\n{code}\r\n\r\nwhere f.get() returns an ordinal number which represents the index of the element in the entryLists that just succeeded replicating. We remove these entries because we want to retry with remaining elements in the list in case of a failure. Since entryLists is an ArrayList, the subsequent elements are shifted left in case we remove an element. This breaks the intended functionality. The fix is to reverse sort the ordinals and then perform the deletion in one go.",
        "Fix Inter Cluster Replication Future ordering issues Replication fails with IndexOutOfBoundsException \r\n{code}\r\nregionserver.ReplicationSource$ReplicationSourceWorkerThread(939): org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint threw unknown exception:java.lang.IndexOutOfBoundsException: Index: 1, Size: 1\r\n\tat java.util.ArrayList.rangeCheck(Unknown Source)\r\n\tat java.util.ArrayList.remove(Unknown Source)\r\n\tat org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.replicate(HBaseInterClusterReplicationEndpoint.java:222)\r\n{code}\r\n\r\nIts happening due to incorrect removal of entries from the replication entries list. "
    ],
    [
        "HBASE-14886",
        "HBASE-14866",
        "ReplicationAdmin does not use full peer configuration In {{listValidReplicationPeers()}}, we're creating the peer {{Configuration}} based on the source connection configuration and simply applying the peer ZK cluster key.  This causes any additional properties present in the {{ReplicationPeerConfig}} configuration to not be applied.\r\n\r\nWe should instead be using the configuration returned by {{ReplicationPeers.getPeerConf()}}, which we already call in that method.\r\n\r\n",
        "VerifyReplication should use peer configuration in peer connection VerifyReplication uses the replication peer's configuration to construct the ZooKeeper quorum address for the peer connection.  However, other configuration properties in the peer's configuration are dropped.  It should merge all configuration properties from the {{ReplicationPeerConfig}} when creating the peer connection and obtaining a credentials for the peer cluster."
    ]
]