[
    [
        "ACCUMULO-1069",
        "ACCUMULO-1398",
        "InstanceOperations get/set/remove Property methods are ambiguous InstanceOperations contains several methods for property manipulation that are ambiguous. The current API has setProperty and removeProperty methods, but two  getters: getSystemConfiguration getSiteConfguration. It is not clear which configuration the properties are being set for. I suggest deprecating and adding setSystemProperty and removeSystemProperty methods.",
        "Create command to dump running configuration Many times the configuration of Accumulo changes over time while the system is running. Keeping a CM'd copy of the configuration makes sense, but a utility does not exist to dump the current configuration.\r\n\r\nI'm proposing a new shell command to dump the running configuration to include the system and table configuration parameters. Something like:\r\n\r\ndump -a -d <directoryName>\r\ndump -t <tableName> -d <directoryName>"
    ],
    [
        "ACCUMULO-3671",
        "ACCUMULO-3630",
        "Add locality groups to data compatibility tests Our data compatibility tests for upgrade should be expanded to include flexing different locality group options\r\n\r\n* locality group in place from start of table\r\n* locality group added, full majc has happened\r\n* locality group added, full majc has not happened\r\n* locality group removed, full majc has happened\r\n* locality group removed, full majc has not happened\r\n\r\nEach should be sure to have data in the specified locality group(s) as well as in hte default locality group.",
        "Update tracing docs with new htrace syntax See the Administration chapter of the manual and distributedTracing.html."
    ],
    [
        "ACCUMULO-2838",
        "ACCUMULO-468",
        "Use resolved props in DefaultConfiguration.get() DefaultConfiguration.get() calls Property.getDefaultValue() which is slow because it does stuff w/ annotations and reflection.",
        "low-memory warning message is incorrect The tablet server allocated 6g, and never grew larger, however there were many warnings about getting low on memory."
    ],
    [
        "ACCUMULO-3270",
        "ACCUMULO-3526",
        "TabletServerBatchReader needs a better error message to accompany it's stack trace It creates cryptic stack traces that are made to help users find their dangling BatchReader, but doesn't specify that. It should.",
        "Unused scanner method ScannerBase.updateScanIteratorOption appears to be unused and untested. This is considered public API and should be tested."
    ],
    [
        "ACCUMULO-1304",
        "ACCUMULO-2916",
        "concurrent randomwalk never uses null range start or ends The Compact and Merge tests for concurrent randomwalk will always have a defined value for the range, so infinite starts/ends will never be tested.",
        "Upgrade Zookeeper to 3.4.*  We are still on 3.3.6 currently. We should upgrade to 3.4.*"
    ],
    [
        "ACCUMULO-3804",
        "ACCUMULO-3594",
        "Seal jars by default Jars are currently sealed for releases. The reason we don't always seal is that this causes problems running integration tests when we have test classes in the same packages as those in the main artifacts.\r\n\r\nWe should ensure that our tests are not in the same packages, so we can seal jars by default. This would allow us to catch problems with jar sealing like ACCUMULO-3801 earlier.",
        "Consider message replacement for Writable classes internally We have a number of writable classes, some of which are non-trivial and serialized in tables or files:\r\n\r\n* {{org.apache.accumulo.core.tabletserver.log.LogEntry}}\r\n* {{org.apache.accumulo.core.metadata.schema.DataFileValue}}\r\n* {{org.apache.accumulo.server.master.state.TServerInstance}}\r\n\r\nConsider converting this (and possibly others) to better message classes to easily add more functionality in the future."
    ],
    [
        "ACCUMULO-3141",
        "ACCUMULO-2227",
        "Many RW failures due to balance check While running RW test against 1.5.2 RC1, 10 of 17 walkers failed with a message like the following.\r\n\r\n{noformat}\r\n16 19:35:48,820 [randomwalk.Framework] ERROR: Error during random walk\r\njava.lang.Exception: Error running node Concurrent.xml\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:285)\r\n        at org.apache.accumulo.test.randomwalk.Framework.run(Framework.java:63)\r\n        at org.apache.accumulo.test.randomwalk.Framework.main(Framework.java:122)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:606)\r\n        at org.apache.accumulo.start.Main$1.run(Main.java:107)\r\n        at java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.Exception: Error running node ct.CheckBalance\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:285)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:254)\r\n        ... 8 more\r\nCaused by: java.lang.Exception: servers are unbalanced! location 2487f8db354002f count 345 too far from average 151.86666666666667\r\n        at org.apache.accumulo.test.randomwalk.concurrent.CheckBalance.visit(CheckBalance.java:86)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:254)\r\n\r\n{noformat}",
        "Concurrent randomwalk fails when namenode dies after bulk import step Running Concurrent randomwalk under HDFS HA, if the active namenode is killed:\r\n\r\n{noformat}\r\n20 12:27:51,119 [retry.RetryInvocationHandler] WARN : Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete. Not retrying because the invoked method is not idempotent, and unable to determine whether it was invoked\r\njava.io.IOException: Failed on local exception: java.io.IOException: Response is null.; Host Details : local host is: \"slave.domain.com/10.20.200.113\"; destination host is: \"namenode.domain.com\":8020;\r\n...\r\n at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1487)\r\nat org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:355)\r\nat org.apache.accumulo.server.test.randomwalk.concurrent.BulkImport.visit(BulkImport.java:140)\r\n...\r\nCaused by: java.io.IOException: Response is null.\r\nat org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:952)\r\nat org.apache.hadoop.ipc.Client$Connection.run(Client.java:847)\r\n{noformat}\r\n\r\nThis arises from an HDFS path delete call that cleans up from the bulk import. The test should be resilient here (and when the paths are made earlier in the test) so that the test can continue once failover has completed."
    ],
    [
        "ACCUMULO-2322",
        "ACCUMULO-804",
        "Tablet constructor leaks this, part 2 [~kturner] noted during review of ACCUMULO-1948 that there is a second leaking of this in the {{Tablet}} constructor at the line:\r\n\r\n{code}\r\ntabletServer.recover(this.tabletServer.getFileSystem(), this, logEntries, absPaths, new MutationReceiver() { ...\r\n{code}\r\n",
        "Hadoop 2.0 Support We should start thinking about Hadoop 2 support now that it is Cloudera's recommended distribution and many new Hadoop users will probably be adopting it.\r\n\r\nWhen I investigated this first a few months ago it seemed like the biggest barrier to this was that all the Map/Reduce related tests are implemented using pseudo-private constructors from Hadoop 1.0 that are no-longer present in Hadoop 2.0.\r\n\r\nThe main strategy to fix this should probably be to adopt the Map/Reduce cluster test object for testing the various Accumulo input formats instead of instrumenting them directly. I have used this convenience object successfully on tests utilizing MockInstance, so I think it should work fine.\r\n\r\nThere may also be some filesystem API issues but I don't think they will be too severe.\r\n\r\nThe other main issue is that we will need to actually deploy on Hadoop 1 and 2 and run the integration tests once we start supporting both, so that will be a headache for release testing that we should think through."
    ],
    [
        "ACCUMULO-2609",
        "ACCUMULO-98",
        "ShellServerIT.listscans fails {noformat}\r\njava.lang.AssertionError: Could not find any active scans over table listscans\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.junit.Assert.assertTrue(Assert.java:41)\r\n\tat org.junit.Assert.assertFalse(Assert.java:64)\r\n\tat org.apache.accumulo.test.ShellServerIT.listscans(ShellServerIT.java:1029)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:622)\r\n{noformat}\r\n",
        "Bloom filter should ignore duplicate inserts Duplicate data inserted into the bloom filter should not cause its entry count to grow.  This causes the filter to grow larger than it should."
    ],
    [
        "ACCUMULO-3763",
        "ACCUMULO-512",
        "Accumulo shell shouldn't let you into the command line if it can't connect to zookeeper Ran the MiniAccumuloRunner without specifying a properties file so that it started ZooKeeper on a random port.\r\n\r\nAttempted to run the Shell command and passed in the zookeeper instance name and zookeeper host and port (the shell ignored the port BTW...maybe another bug?) and it threw an error that it couldn't connect to ZooKeeper, but it still passed me into the shell and let me run commands.",
        "DEFAULT_MAX_LATENCY in AccumuloOutputFormat wrong units In org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat, the DEFAULT_MAX_LATENCY (line 88) is defined as:\r\n\r\nprivate static final int DEFAULT_MAX_LATENCY = 60; // 1 minute\r\n\r\nHowever, at line 233 that default value (assuming the key isn't set by the user) is provided as an argument to createMultiTableBatchWriter().  The latency parameter there is documented as milliseconds.  The DEFAULT_MAX_LATENCY should be set to 60000 if the desired default latency value is 1 minute."
    ],
    [
        "ACCUMULO-424",
        "ACCUMULO-1571",
        "data lost during merge A chopped, unassigned tablet with walogs was merged, which removed the walog entry, and the associated mutations.\r\n",
        "typo in PrintInfo: \"historgram\" The RFile PrintInfo utility's option to print a histogram of key-value sizes doesn't work properly\r\n\r\ne.g.\r\n{noformat}\r\n$> ./bin/accumulo org.apache.accumulo.core.file.rfile.PrintInfo --histogram /path/to/rfile/example.rf\r\n{noformat}\r\n\r\nWorkaround: use the misspelled argument name\r\n\r\n{noformat}\r\n$> ./bin/accumulo org.apache.accumulo.core.file.rfile.PrintInfo --historgram /path/to/rfile/example.rf\r\n{noformat}"
    ],
    [
        "ACCUMULO-570",
        "ACCUMULO-2438",
        "dirlist example README is a little inconsistent The README.dirlist sometimes shows the authorizations as \"auths\" and sometimes as exampleVis.  The new \"-e\" shell option could help explain how to give the auths to a user.\r\n",
        "deleting all table rows caused tablet load failure testTrace in ConditionalWriterIT failed:\r\n\r\n{noformat}\r\njava.lang.Exception: test timed out after 60000 milliseconds\r\n\tat java.lang.Thread.sleep(Native Method)\r\n\tat org.apache.accumulo.core.util.UtilWaitThread.sleep(UtilWaitThread.java:26)\r\n\tat org.apache.accumulo.core.client.impl.ThriftScanner.scan(ThriftScanner.java:240)\r\n\tat org.apache.accumulo.core.client.impl.ScannerIterator$Reader.run(ScannerIterator.java:84)\r\n\tat org.apache.accumulo.core.client.impl.ScannerIterator.hasNext(ScannerIterator.java:177)\r\n\tat org.apache.accumulo.core.trace.TraceDump.printTrace(TraceDump.java:133)\r\n\tat org.apache.accumulo.test.ConditionalWriterIT.testTrace(ConditionalWriterIT.java:1231)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:622)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n{noformat}\r\n\r\nExamining the logs, it seems like the client was having a hard time locating the trace table (table id 9):\r\n\r\n{noformat}\r\n2014-03-07 04:33:40,445 [impl.ThriftScanner] TRACE: Failed to locate tablet for table : 9 row : 9dd07d1bd8b9bc0d\r\n...\r\n2014-03-07 04:33:40,551 [impl.ThriftScanner] TRACE: Failed to locate tablet for table : 9 row : 9dd07d1bd8b9bc0d\r\n{noformat}\r\n\r\nExamining the master logs, I see the trace table being created, assigned, and hosted.\r\n\r\nThis test deletes everything in the trace table before tracing an operation:\r\n\r\n{noformat}\r\n    conn.tableOperations().deleteRows(\"trace\", null, null);\r\n{noformat}\r\n\r\nThe master takes the table offline and re-writes the metadata table information so that it no longer has any data.\r\n\r\nThe master then attempts to put the tablet online, but the operation fails:\r\n\r\n{noformat}\r\n2014-03-07 04:33:40,079 [master.Master] ERROR: host1:47630 reports assignment failed for tablet 9<<\r\n{noformat}\r\n\r\nThe tablet server gives the real problem:\r\n\r\n{noformat}\r\n2014-03-07 04:33:40,074 [tserver.TabletServer] WARN : exception trying to assign tablet 9<< file:////local/disk1/jenkins/workspace/accumulo16/test/target/mini-tests/org.apache.accumulo.test.functional.SimpleMacIT/1394184783304_4048/accumulo/tables/9/default_tablet\r\njava.lang.IllegalArgumentException: Time type unknown : ^@0\r\n        at org.apache.accumulo.server.tablets.TabletTime.getInstance(TabletTime.java:67)\r\n        at org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1296)\r\n        at org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1211)\r\n        at org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1067)\r\n        at org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1056)\r\n        at org.apache.accumulo.tserver.TabletServer$AssignmentHandler.run(TabletServer.java:2911)\r\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        at java.lang.Thread.run(Thread.java:701)\r\n{noformat}\r\n\r\nTablet time should always be \"L\" or \"M\".\r\n"
    ],
    [
        "ACCUMULO-4090",
        "ACCUMULO-1379",
        "BatchWriter close not cleaning up all resources I'm debugging an issue with a long-running ingestor, similar to the TraceServer.\r\n\r\nAfter realizing that BatchWriter close needs to be called when a MutationsRejectedException occurs (see ACCUMULO-4088), a close was added, and the client became more stable.\r\n\r\nHowever, after a day, or so, the client became sluggish. When inspecting a heap dump, many TabletServerBatchWriter objects were still referenced.  This server should only have two BatchWriter instances at any one time, and this server had >100.\r\n\r\nStill debugging.\r\n\r\nThe error that initiates the issue is a SessionID not found, presumably because the session timed out.  This is the cause of the MutationsRejectedException seen by the client.\r\n\r\n",
        "PermGen leak Under version 1.3.7 we are using the following code to initialize a cloudbase connection during initialization of our web app:\r\n\r\n                        ZooKeeperInstance instance = new ZooKeeperInstance(instanceName, zooKeepers);\r\n                        connector = instance.getConnector(userId, password.getBytes());\r\n\r\nThe problem is that under the hood, this call creates several threads that are not cleaned up when the app is undeployed in JBoss. This is occurring without performing any scans or interacting with cloudbase in any other way. After relatively few redeploys of the app, the PermGen Space is OOM.\r\n\r\nI can't find any reference in the cloudbase API akin to a close() method for the Connector object. This is a classloader leak effecting any webapp that is accessing cloudbase directly. The result of this leak is not simply orphaned threads, but thousands of classes not gc'd because the classloader itself can't be gc'd. This is what is filling up PermGen.\r\n"
    ],
    [
        "ACCUMULO-2752",
        "ACCUMULO-3687",
        "Missing getZooKeeperPort on AccumuloConfig interface Accidental omission that should be there to match setZooKeeperPort(int)",
        "Unwrap symlinks for ACCUMULO_HOME for standalone cluster control Noticed that RestartIT didn't actually restart the master. It was because the ACCUMULO_HOME provided was a symlink and the {{ps}} call was looking for that specific path. If we unwrap symlinks, it should be a bit more robust."
    ],
    [
        "ACCUMULO-2556",
        "ACCUMULO-3971",
        "use asynchronous calls to scale BatchReader client calls Presently, the BatchReader uses a thread-pool to make synchronous calls to tablet servers to fetch data.  That limits the number of tablet servers that can be doing queries in parallel.  On very large clusters, clients may not initiate queries to all of the servers until some queries finish.  It is not practical to create thousands of threads just to initiate queries.\r\n\r\nAlternatively, asynchronous calls can efficiently initiate queries an wait for responses, without using threads just to wait on responses.\r\n",
        "Better doc for tracing Developers who want to go beyond Accumulo's default tracing are too much on their own to figure out how to use Accumulo's tracing features.  I had to dive deep into source code to figure out how to understand the trace table format and add a ZipkinSpanReceiver.  I'd like to share what I found so that other developers do not have to to the same."
    ],
    [
        "ACCUMULO-2768",
        "ACCUMULO-114",
        "Agitator not restarting all datanodes I ran a 24 hours CI test against 1.6.0 RC5 w/ agitation.\r\n\r\nI modified the agitation settings to the following :\r\n\r\n{noformat}\r\n#time amount of time (in minutes) the agitator should sleep before killing\r\nKILL_SLEEP_TIME=3\r\n\r\n#time amount of time (in minutes) the agitator should sleep after killing before running tup \r\nTUP_SLEEP_TIME=1\r\n\r\n#the minimum and maximum server the agitator will kill at once\r\nMIN_KILL=1\r\nMAX_KILL=2\r\n\r\n{noformat}\r\n\r\nI started 3 walkers all of which died.  The walkers saw {{org.apache.accumulo.core.client.impl.AccumuloServerException}}. On the tserver the cause was {{org.apache.hadoop.hdfs.BlockMissingException}}.\r\n\r\nAfter stopping agitation scripts, I ran {{start-dfs.sh}} and saw it started 5 datanodes.  Looking at {{datanode-agitator.pl}} I think the problem is when it kills two datanodes, it only restarts one. \r\n\r\nAll of my ingest clients survived and were able to write 8 billion entries in this wacky environment.  I noticed on the monitor that there were long periods of no ingest, but it was not a complete flat line.\r\n\r\n",
        "hide passwords when logging the configuration at start-up Hide the system password from the log.  It's not that much help in debugging anyhow."
    ],
    [
        "ACCUMULO-3315",
        "ACCUMULO-1038",
        "Explicitly wait for threadpool shutdown in RandomizeVolumes Calling shutdown on the threadpool should, as I understand it, keep the process from exiting (as they would be non-daemon threads).\r\n\r\nHowever, it would be better to explicitly wait for the tasks in the threadpool to work themselves off before exiting.",
        "Code snippet for constructing iterators has wrong argument order On the \"Table Configuration\" doc page, in the section \"Setting Iterators Programatically\" (at http://accumulo.apache.org/1.4/user_manual/Table_Configuration.html#Iterators), there is the following code snippet:\r\n\r\n{code}\r\nscanner.addIterator(new IteratorSetting(\r\n    15, // priority\r\n    \"com.company.MyIterator\", // class name\r\n    \"myiter\" // name this iterator\r\n));\r\n{code}\r\n\r\nThe actual constructor signature for IteratorSetting is (priority, name, iteratorClass).  The second and third arguments should be flipped."
    ],
    [
        "ACCUMULO-701",
        "ACCUMULO-1781",
        "No longer seeing TApplicationException When an unexpected exception occurred server side, the client used to see a TApplicationException on the client side.  It appears this not happening anymore in trunk since the switch to thrift 0.8.  A lot of code depends on the previous behavior.\r\n\r\nCode that used to throw an exception to the client is now getting stuck indefinitely.  ",
        "MiniAccumuloRunner to support miniDFS in property file "
    ],
    [
        "ACCUMULO-2434",
        "ACCUMULO-3750",
        "Functional test MapReduce runner doesn't check return code of test While testing ACCUMULO-2005, I had unusually fast MR test runs.\r\n\r\nDrilling into individual task results showed the run was actually failing while reporting the map task as successful.\r\n\r\n{noformat}\r\n\r\n2014-03-05 10:52:27,755 INFO org.apache.accumulo.server.test.functional.RunTests: Running test [/usr/bin/python, test/system/auto/run.py, -m, -f, 1, -t, simple.addSplit.AddSplitTest]\r\n2014-03-05 10:52:27,788 INFO org.apache.accumulo.server.test.functional.RunTests: More: Traceback (most recent call last):\r\n2014-03-05 10:52:27,788 INFO org.apache.accumulo.server.test.functional.RunTests: More:   File \"test/system/auto/run.py\", line 29, in <module>\r\n2014-03-05 10:52:27,788 INFO org.apache.accumulo.server.test.functional.RunTests: More:     from TestUtils import ACCUMULO_HOME, ACCUMULO_DIR, COBERTURA_HOME, findCoberturaJar\r\n2014-03-05 10:52:27,788 INFO org.apache.accumulo.server.test.functional.RunTests: More: ImportError: No module named TestUtils\r\n2014-03-05 10:52:27,798 INFO org.apache.hadoop.mapred.MapTask: Starting flush of map output\r\n2014-03-05 10:52:27,811 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]\r\n2014-03-05 10:52:27,814 INFO org.apache.hadoop.mapred.Task: Task:attempt_201401282254_0002_m_000003_0 is done. And is in the process of commiting\r\n2014-03-05 10:52:27,949 INFO org.apache.hadoop.mapred.Task: Task 'attempt_201401282254_0002_m_000003_0' done.\r\n{noformat}\r\n\r\nthe current test running class ignores the return code of the test process ([ref|https://github.com/apache/accumulo/blob/1.5.1/test/src/main/java/org/apache/accumulo/test/functional/RunTests.java#L140]) \r\n\r\nInstead, we should check the status and fail the task if it returns an error.\r\n\r\nWorkaround: Job counters should show Success / Failure / Error count for tests. If none of hte counters appear, consider all tests failed.",
        "Bad instance.secret causes master to repeatedly fail fast attempting to acquire lock Accidentally restarted a small cluster with bad configuration (missing instance.secret). The tabletservers bailed out quickly, but the master sat in a tight loop trying to get the lock.\r\n\r\n{noformat}\r\n2015-04-23 11:48:12,356 [trace.DistributedTrace] INFO : SpanReceiver org.apache.accumulo.tracer.ZooTraceClient was loaded successfully.\r\n2015-04-23 11:48:12,357 [master.Master] INFO : trying to get master lock\r\n2015-04-23 11:48:12,395 [master.Master] WARN : Failed to get master lock org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /accumulo/dc25a857-19d8-4387-bec0-64b4dc17cafb/masters/lock/zlock-\r\n2015-04-23 11:48:13,043 [server.Accumulo] WARN : System swappiness setting is greater than ten (60) which can cause time-sensitive operations to be delayed.  Accumulo is time sensitive because it needs to maintain distributed lo\r\nck agreement.\r\n2015-04-23 11:48:13,410 [master.Master] WARN : Failed to get master lock org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /accumulo/dc25a857-19d8-4387-bec0-64b4dc17cafb/masters/lock/zlock-\r\n2015-04-23 11:48:14,418 [master.Master] WARN : Failed to get master lock org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /accumulo/dc25a857-19d8-4387-bec0-64b4dc17cafb/masters/lock/zlock-\r\n2015-04-23 11:48:15,426 [master.Master] WARN : Failed to get master lock org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /accumulo/dc25a857-19d8-4387-bec0-64b4dc17cafb/masters/lock/zlock-\r\n2015-04-23 11:48:16,433 [master.Master] WARN : Failed to get master lock org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /accumulo/dc25a857-19d8-4387-bec0-64b4dc17cafb/masters/lock/zlock-\r\n2015-04-23 11:48:17,440 [master.Master] WARN : Failed to get master lock org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /accumulo/dc25a857-19d8-4387-bec0-64b4dc17cafb/masters/lock/zlock-\r\n2015-04-23 11:48:18,449 [master.Master] WARN : Failed to get master lock org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /accumulo/dc25a857-19d8-4387-bec0-64b4dc17cafb/masters/lock/zlock-\r\n{noformat}\r\n\r\nLooks like the only case which exits the Master when the lock is failed to be acquired is an illegal state where the master thinks it already has the lock.\r\n\r\nIf we get a NoAuthException, we should not attempt to get the lock again."
    ],
    [
        "ACCUMULO-3927",
        "ACCUMULO-2379",
        "splits re-read metadata unnecessarily Notice that splits re-read bulk import flags, but the tablet already has them.",
        "Document config needed for running functional tests under MR Some configuration updates may be necessary to get functional tests to run under MapReduce. Most, if not all, of the changes are needed when a different user from the one set up for Accumulo is used to run the MR job. So, I expect the changes should only be in documentation."
    ],
    [
        "ACCUMULO-2354",
        "ACCUMULO-1139",
        "Master does not close TabletServerBatchReader Saw the following in the Master logs:\r\n{noformat}\r\n2014-02-11 16:55:10,820 [impl.TabletServerBatchReader] WARN : TabletServerBatchReader not shutdown; did you forget to call close()?\r\njava.lang.Throwable\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchReader.<init>(TabletServerBatchReader.java:69)\r\n        at org.apache.accumulo.core.client.impl.ConnectorImpl.createBatchScanner(ConnectorImpl.java:98)\r\n        at org.apache.accumulo.server.master.state.MetaDataTableScanner.<init>(MetaDataTableScanner.java:63)\r\n        at org.apache.accumulo.server.master.state.MetaDataTableScanner.<init>(MetaDataTableScanner.java:56)\r\n        at org.apache.accumulo.server.master.state.MetaDataStateStore.iterator(MetaDataStateStore.java:70)\r\n        at org.apache.accumulo.master.TabletGroupWatcher.run(TabletGroupWatcher.java:148)\r\n{noformat}\r\n\r\nThis happened shortly after a tablet server had died and it's tablets were in the process of being recovered elsewhere.",
        "Incorrect option for timestamp in user manual and under basic administration i see this statement:\r\n\"... You can use the \"-t\" option to scan to see the timestamp for the cell, too\"\r\n\r\nbut in the shell this is what i see:\r\n\r\n\\-st,--show timestamps                                                                    display timestamps\r\n\\-t,--tableName <table> \r\n"
    ],
    [
        "ACCUMULO-1230",
        "ACCUMULO-4113",
        "tablet server re-writes all metadata for a tablet when it is loaded While writing a new constraint for the metadata table, I saw that when the location for a tablet is set, *all* of the metadata for the tablet is written.  This is not harmful, just unexpected and a little wasteful.",
        "Fix incorrect usage of ByteBuffer While working on ACCUMULO-4098 I found one place where ByteBuffer was being used incorrectly.   Looking around the code, I have found other places that are using ByteBuffer incorrectly.  Some of the problems I found are as follows :\r\n\r\n * Calling {{ByteBuffer.array()}} without calling {{ByteBuffer.hasArray()}}.\r\n * Using {{ByteBuffer.position()}} or {{ByteBuffer.limit()}} without adding {{ByteBuffer.arrayOffset()}} when dealing with an array returned by {{ByteBuffer.array()}}.\r\n * Using {{ByteBuffer.arrayOffset()}} without adding {{ByteBuffer.position()}} when dealing with an array returned by {{ByteBuffer.array()}}.\r\n\r\n"
    ],
    [
        "ACCUMULO-1758",
        "ACCUMULO-3805",
        "Implement temporary tables It'd be useful to be able to set an Accumulo table to expire in a certain period of time or a certain period from the last activity upon it.  This feature will facilitate things like query specific searches that require building up a set of data but then not needing it past the lifetime of a session.",
        "No means to monitor/interact with FATE programmatically With FATE, we have the ability to have strong tracking of operations. However, we hide a lot of this from the client. We have admin utilities plumbed into the shell (FateCommand), but they're not actual APIs. This means there is no API available for failing/deleting/listing fate operations, but it also means there no way to be aware of the txid for a fate operation. This is critical for instances where a potentially distributed client wants to perform a FATEd operation.\r\n\r\nIn an ideal world, client would be able to pre-seed the transaction, as we do now, and then issue the command with that id. This would allow the client to be aware of the transaction before it's started so it could be shared."
    ],
    [
        "ACCUMULO-1207",
        "ACCUMULO-2611",
        "document the stats available via JSON/xml/getMasterStats Document each of the values, in particular \"scans\" and \"scanssessions\"\r\n",
        "Package a contrib directory in the tarball Since the RPMs were dropped from the the build in ACCUMULO-2606, the sysctl and limits configuration files, and the example init.d script, are no longer included in any packaging. These might be good candidates for inclusion in the binary tarball in a contrib directory. This can be done in the assemble module."
    ],
    [
        "ACCUMULO-864",
        "ACCUMULO-202",
        "Investigate speeding up network communication with relative key improvments Investigate further compression, as suggested in the ACCUMULO-790 [comment|https://issues.apache.org/jira/browse/ACCUMULO-790?focusedCommentId=13491772&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13491772].",
        "master does not rebalance after a node fails The master seems to be trying to contact a dead tablet server.  The master will not re-balance if the metrics it has for a tablet server are out-of-date.  Tablets seem to be assigned properly; just not balanced.\r\n"
    ],
    [
        "ACCUMULO-3473",
        "ACCUMULO-3053",
        "GroupBalancer should limit number of migrations The tablet group balancer added in ACCUMULO-3439 does not limit the number of migrations.  For a table with many tablet, the balancer could potentially migrate all of the tablets in a table in one balance call.  ",
        "Allow control over Monitor SSL ciphers used Jetty's SslContextFactory allows a whitelist and blacklist of SSL ciphers that can be used. Allow Accumulo configuration of these."
    ],
    [
        "ACCUMULO-2906",
        "ACCUMULO-2267",
        "configuredZookeeperPort is not cased the same way as other mentions of ZooKeeper ZooKeeper seems to be cased elsewhere in minicluster with a capital Z and K.  There are a couple of small areas where this is not the case:\r\n\r\n* configuredZookeeperPort should be cased as configuredZooKeeperPort \r\n* getConfiguredZookeeperPort should be cased as getConfiguredZooKeeperPort",
        "problems running accumulo from RPM - jps is called in the init.d scripts but it's not in the PATH it sets.  on my centos box, it's in $JAVA_HOME/bin\r\n- in bin/accumulo, accumulo-start.jar is expected to be in $ACCUMULO_HOME/lib, but actually it's in $JAVA_HOME/accumulo\r\n- in bin/accumulo, accumulo-start.jar does not have version decoration, but the installed jar does\r\n- nothing else (other accumulo jars from $JAVA_HOME/accumulo, other dependencies from $ACCUMULO_HOME/lib) seems to be getting into the classpath.  initial failure is NoClassDefFoundError: org/apache/commons/vfs2/provider/FileProvider\r\n\r\ni wonder if the standardization on RPM deployment locations is worth the departure from the standard accumulo deployment locations, at least for 1.6.0, since it seems like the impact on all the script infrastructure hasn't really been thought through yet, so close to the release."
    ],
    [
        "ACCUMULO-3895",
        "ACCUMULO-3190",
        "Accumulo init can fail halfway through I saw a situation where \"accumulo init\" exited with error code 255, the HDFS directories were successfully created, and security was not initialized.  The contents of accumulo-init.out were the following.  I realize no good can come of running init when there are no DataNodes, but it would be nice if init cleaned up after itself when it fails.\r\n{noformat}\r\n2015-06-08 23:19:17,930 [fs.VolumeManagerImpl] WARN : dfs.datanode.synconclose set to false in hdfs-site.xml: data loss is possible on hard system reset or power loss\r\n2015-06-08 23:19:17,932 [init.Initialize] INFO : Hadoop Filesystem is hdfs://c6401.ambari.apache.org:8020\r\n2015-06-08 23:19:17,933 [init.Initialize] INFO : Accumulo data dirs are [hdfs://c6401.ambari.apache.org:8020/apps/accumulo/data]\r\n2015-06-08 23:19:17,933 [init.Initialize] INFO : Zookeeper server is c6401.ambari.apache.org:2181\r\n2015-06-08 23:19:17,933 [init.Initialize] INFO : Checking if Zookeeper is available. If this hangs, then you need to make sure zookeeper is running\r\nEnter initial password for root (this may not be applicable for your security setup): ******\r\nConfirm initial password for root: ******\r\n2015-06-08 23:19:18,661 [Configuration.deprecation] INFO : dfs.replication.min is deprecated. Instead, use dfs.namenode.replication.min\r\n2015-06-08 23:19:18,944 [Configuration.deprecation] INFO : dfs.block.size is deprecated. Instead, use dfs.blocksize\r\n2015-06-08 23:19:19,154 [hdfs.DFSClient] INFO : Exception in createBlockOutputStream\r\njava.net.ConnectException: Connection refused\r\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\r\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\r\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\r\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1575)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1317)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:464)\r\n2015-06-08 23:19:19,158 [hdfs.DFSClient] INFO : Abandoning BP-431548639-192.168.64.101-1433805216294:blk_1073741834_1010\r\n2015-06-08 23:19:19,172 [hdfs.DFSClient] INFO : Excluding datanode DatanodeInfoWithStorage[192.168.64.101:50010,DS-3861ea49-69b6-4bc9-bcba-c81ed9585b51,DISK]\r\n2015-06-08 23:19:19,205 [hdfs.DFSClient] WARN : DataStreamer Exception\r\norg.apache.hadoop.ipc.RemoteException(java.io.IOException): File /apps/accumulo/data/tables/!0/table_info/0_1.rf could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1551)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3104)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3028)\r\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:723)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)\r\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2081)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2077)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2075)\r\n\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1427)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1358)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\r\n\tat com.sun.proxy.$Proxy15.addBlock(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:497)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\n\tat com.sun.proxy.$Proxy16.addBlock(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1463)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1259)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:464)\r\n2015-06-08 23:19:19,207 [init.Initialize] ERROR: FATAL Failed to initialize filesystem\r\norg.apache.hadoop.ipc.RemoteException(java.io.IOException): File /apps/accumulo/data/tables/!0/table_info/0_1.rf could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1551)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3104)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3028)\r\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:723)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)\r\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2081)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2077)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2075)\r\n\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1427)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1358)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\r\n\tat com.sun.proxy.$Proxy15.addBlock(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:497)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\n\tat com.sun.proxy.$Proxy16.addBlock(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1463)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1259)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:464)\r\n2015-06-08 23:19:19,231 [hdfs.DFSClient] ERROR: Failed to close inode 16441\r\norg.apache.hadoop.ipc.RemoteException(java.io.IOException): File /apps/accumulo/data/tables/!0/table_info/0_1.rf could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1551)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3104)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3028)\r\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:723)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)\r\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2081)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2077)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2075)\r\n\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1427)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1358)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\r\n\tat com.sun.proxy.$Proxy15.addBlock(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:497)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\n\tat com.sun.proxy.$Proxy16.addBlock(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1463)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1259)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:464)\r\n{noformat}",
        "Fix use of deprecated instance.getConfiguration() This was marked as fixed in commit {{a697516751426ec5143cea860ed6cbbaa85cf9af}} under ACCUMULO-1866, but that only fixed it for the master branch. The warning was introduced into the 1.6 branch in commit {{255ce5f25164dc1c429ee57bbeecfdb1f976cacb}} for ACCUMULO-3138."
    ],
    [
        "ACCUMULO-3868",
        "ACCUMULO-3031",
        "FileInputStream potentially unclosed in scalability Run.java {code}\r\n    try {\r\n      FileInputStream fis = new FileInputStream(sitePath);\r\n      try {\r\n        scaleProps.load(fis);\r\n      } finally {\r\n        fis.close();\r\n      }\r\n      fis = new FileInputStream(testPath);\r\n      testProps.load(fis);\r\n    } catch (Exception e) {\r\n      log.error(\"Error loading config file.\", e);\r\n    }\r\n{code}\r\n\r\nThe second instance assigned to {{fis}} might not be closed.",
        "jdk 1.7.0_65 breaks ShellSetInstanceTest Switching back to jdk1.7.0_60 fixes it.\r\n\r\nStrongly suspect [a jdk bug|https://bugs.openjdk.java.net/browse/JDK-8051012] (thanks to [~bhavanki] for finding this for me).\r\n\r\nExample output:\r\n{noformat}\r\nBad <init> method call from inside of a branch\r\nException Details:\r\n  Location:\r\n    org/apache/accumulo/shell/Shell.<init>(Ljline/console/ConsoleReader;Ljava/io/PrintWriter;)V @36: invokespecial\r\n  Reason:\r\n    Error exists in the bytecode\r\n  Bytecode:\r\n    0000000: 2a4e 1300 94b8 004d 03bd 000d 1300 95b8\r\n    0000010: 0028 b800 823a 0519 05b2 0032 a500 0e2a\r\n    0000020: 01c0 0084 b700 87a7 0009 2db7 0096 0157\r\n    0000030: 2a03 b500 9a2a 1300 9cb5 00a0 2a13 00a2\r\n    0000040: b500 a52a 0000 0000 014e 013a 0413 00a7\r\n    0000050: b800 2203 bd00 0d13 00a9 b800 28b8 002e\r\n    0000060: 3a05 1905 b200 32a5 0034 1905 c100 3499\r\n    0000070: 0022 b800 3a13 00a7 b800 2213 000d 01b6\r\n    0000080: 003e b600 4201 b600 46c0 0090 3a04 a700\r\n    0000090: 0a19 05c0 0090 3a04 a700 0cbb 0090 59b7\r\n    00000a0: 00aa 3a04 1904 b500 ae2a 0000 0000 014e\r\n    00000b0: 013a 0413 00a7 b800 2203 bd00 0d13 00a9\r\n    00000c0: b800 28b8 002e 3a05 1905 b200 32a5 0034\r\n    00000d0: 1905 c100 3499 0022 b800 3a13 00a7 b800\r\n    00000e0: 2213 000d 01b6 003e b600 4201 b600 46c0\r\n    00000f0: 0090 3a04 a700 0a19 05c0 0090 3a04 a700\r\n    0000100: 0cbb 0090 59b7 00aa 3a04 1904 b500 b12a\r\n    0000110: 0000 0000 014e 013a 0413 00b3 b800 2203\r\n    0000120: bd00 0d13 00b5 b800 28b8 002e 3a05 1905\r\n    0000130: b200 32a5 0034 1905 c100 3499 0022 b800\r\n    0000140: 3a13 00b3 b800 2213 000d 01b6 003e b600\r\n    0000150: 4201 b600 46c0 0092 3a04 a700 0a19 05c0\r\n    0000160: 0092 3a04 a700 0cbb 0092 59b7 00b6 3a04\r\n    0000170: 1904 b500 b92a 0000 0000 014e 013a 0413\r\n    0000180: 00b3 b800 2203 bd00 0d13 00b5 b800 28b8\r\n    0000190: 002e 3a05 1905 b200 32a5 0034 1905 c100\r\n    00001a0: 3499 0022 b800 3a13 00b3 b800 2213 000d\r\n    00001b0: 01b6 003e b600 4201 b600 46c0 0092 3a04\r\n    00001c0: a700 0a19 05c0 0092 3a04 a700 0cbb 0092\r\n    00001d0: 59b7 00b6 3a04 1904 b500 bc2a 03b5 00c0\r\n    00001e0: 2a03 b500 c32a 01b5 00c7 2a01 b500 cb2a\r\n    00001f0: 04b5 00ce 2a01 4e09 3704 013a 062d 01a5\r\n    0000200: 0009 2d3a 06a7 000b 1300 4ab8 004d 3a06\r\n    0000210: 1906 1300 d003 bd00 0d13 00d2 b800 2813\r\n    0000220: 00d4 b800 673a 0719 07b2 0032 a600 0bb8\r\n    0000230: 00d6 3704 a700 0d19 07c0 00d8 b600 db37\r\n    0000240: 0416 04b5 00df 2a03 b500 e22a 01b5 00e4\r\n    0000250: 2a03 b500 e72a 2bb5 00e9 2a2c b500 e4b1\r\n    0000260:                                        \r\n  Stackmap Table:\r\n    full_frame(@42,{UninitializedThis,Object[#15],Object[#25],UninitializedThis,Top,Object[#13]},{})\r\n    full_frame(@48,{Object[#2],Object[#15],Object[#25],Object[#2],Top,Object[#13]},{})\r\n    full_frame(@145,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@152,{Object[#2],Object[#15],Object[#25],Null,Object[#144],Object[#13]},{Object[#2]})\r\n    full_frame(@155,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@164,{Object[#2],Object[#15],Object[#25],Null,Object[#144],Object[#13]},{Object[#2]})\r\n    full_frame(@247,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@254,{Object[#2],Object[#15],Object[#25],Null,Object[#144],Object[#13]},{Object[#2]})\r\n    full_frame(@257,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@266,{Object[#2],Object[#15],Object[#25],Null,Object[#144],Object[#13]},{Object[#2]})\r\n    full_frame(@349,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@356,{Object[#2],Object[#15],Object[#25],Null,Object[#146],Object[#13]},{Object[#2]})\r\n    full_frame(@359,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@368,{Object[#2],Object[#15],Object[#25],Null,Object[#146],Object[#13]},{Object[#2]})\r\n    full_frame(@451,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@458,{Object[#2],Object[#15],Object[#25],Null,Object[#146],Object[#13]},{Object[#2]})\r\n    full_frame(@461,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@470,{Object[#2],Object[#15],Object[#25],Null,Object[#146],Object[#13]},{Object[#2]})\r\n    full_frame(@520,{Object[#2],Object[#15],Object[#25],Null,Long,Null},{Object[#2]})\r\n    full_frame(@528,{Object[#2],Object[#15],Object[#25],Null,Long,Object[#21]},{Object[#2]})\r\n    full_frame(@567,{Object[#2],Object[#15],Object[#25],Null,Long,Object[#21],Object[#13]},{Object[#2]})\r\n    same_locals_1_stack_item_frame(@577,Object[#2])\r\n\r\n{noformat}"
    ],
    [
        "ACCUMULO-675",
        "ACCUMULO-2232",
        "WrappingIterator's seenSeek should be protected In 1.3, the WrappingIterator was pretty much some boilerplate code. In 1.4 on, a package private boolean called seenSeek was added to help enforce the iterator contract.\r\n\r\nThis causes some issues with iterators written for 1.3 and before, because the seenSeek property can't be set by an iterator outside of the core.iterators package, which is locked down. This means that sub iterators must always delegate up to the WrappingIterator's seek() method, even if implementors want to completely override seek().\r\n\r\nI would like to provide more documentation on the WrappingIterator and make the seenSeek property protected so implementors don't need conditional logic to make the call to super.seek().",
        "Combiners can cause deleted data to come back The case-\r\n3 files with-\r\n* 1 with a key, k, with timestamp 0, value 3\r\n* 1 with a delete of k with timestamp 1\r\n* 1 with k with timestamp 2, value 2\r\n\r\nThe column of k has a summing combiner set on it. The issue here is that depending on how the major compactions play out, differing values with result. If all 3 files compact, the correct value of 2 will result. However, if 1 & 3 compact first, they will aggregate to 5. And then the delete will fall after the combined value, resulting in the result 5 to persist.\r\n\r\nFirst and foremost, this should be documented. I think to remedy this, combiners should only be used on full MajC, not not full ones. This may necessitate a special flag or a new combiner that implemented the proper semantics."
    ],
    [
        "ACCUMULO-860",
        "ACCUMULO-1751",
        "Thrift version error displays 0.8 instead of 0.9 the thrift shell script complains that it lacks 0.8 when it is in fact looking for 0.9.\r\n",
        "SimpleMacIT test for Multitable Input Format "
    ],
    [
        "ACCUMULO-2826",
        "ACCUMULO-1651",
        "IntersectingIterator should allow a single column family I know that sounds ridiculous. BUT:\r\n\r\nThe IndexedDocIterator extends the IntersectingIterator, and it should be possible to supply a single term and receive the matches in the same format the IndexedDocIterator returns boolean AND results. That is -- if one wants to search a single term, they shouldn't have to use a different iterator if they want the iterator stack to return the document in the value (as opposed to just returning the index key with no iterator at all).\r\n\r\nDoes this make sense? Should I submit the patch?",
        "GC removed WAL that master wasn't done with I have a master that's spinning trying to recover a walog that doesn't exist in hdfs.  It looks like the GC cleaned it up.  I was stopping and starting my cluster throughout this period, and there was at least a few minutes in which every service was talking SSL except the GC, so the GC couldn't receive thrift messages from other services, but [~vines] says this shouldn't affect the GC's deletion behavior.\r\n\r\n\r\nHere are some relevant logs.  Note that the master thinks its logSet includes that file straight through the time the GC removed it.\r\n\r\nGC:\r\n{code}\r\n2013-08-09 11:58:14,835 [util.MetadataTableUtil] INFO : Returning logs [!!R<< hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7 (1)] for extent !!R<<\r\n2013-08-09 11:58:14,852 [gc.GarbageCollectWriteAheadLogs] DEBUG: Removing WAL for offline server hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7\r\n2013-08-09 12:03:15,467 [util.MetadataTableUtil] INFO : Returning logs [!!R<< hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7 (1)] for extent !!R<<\r\n{code}\r\n\r\nMaster:\r\n{code}\r\n2013-08-09 11:57:45,235 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 11:57:45,238 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 11:57:45,286 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 11:57:45,324 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 11:57:45,939 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 11:57:45,942 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 11:57:45,975 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 11:57:55,612 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 11:57:55,679 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 11:57:55,739 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 11:57:55,764 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 11:57:55,784 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 11:57:56,031 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 11:57:56,046 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 11:58:56,051 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 11:59:56,057 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 12:00:56,062 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 12:01:56,066 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 12:02:56,071 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 12:08:56,103 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 12:09:56,108 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 12:10:56,113 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 12:11:56,118 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 12:13:19,883 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 12:14:19,887 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n<master was restarted here>\r\n2013-08-09 12:15:44,459 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 12:15:44,467 [recovery.RecoveryManager] DEBUG: Recovering hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7 to hdfs://localhost:54310/otherAccumuloInstance/recovery/5a383792-c89b-41ed-bc22-0802e76638f7\r\n2013-08-09 12:15:44,472 [recovery.RecoveryManager] INFO : Starting recovery of hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7 (in : 10s) created for localhost+9997, tablet !!R<< holds a reference\r\n2013-08-09 12:15:54,479 [recovery.RecoveryManager] DEBUG: Unable to initate log sort for hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7: java.io.FileNotFoundException: java.io.FileNotFoundException: File not found /otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7\r\n2013-08-09 12:16:44,487 [state.ZooTabletStateStore] DEBUG: root tablet logSet [localhost+9997/hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7]\r\n2013-08-09 12:16:44,488 [recovery.RecoveryManager] DEBUG: Recovering hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7 to hdfs://localhost:54310/otherAccumuloInstance/recovery/5a383792-c89b-41ed-bc22-0802e76638f7\r\n2013-08-09 12:16:44,490 [recovery.RecoveryManager] INFO : Starting recovery of hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7 (in : 20s) created for localhost+9997, tablet !!R<< holds a reference\r\n2013-08-09 12:17:04,494 [recovery.RecoveryManager] DEBUG: Unable to initate log sort for hdfs://localhost:54310/otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7: java.io.FileNotFoundException: java.io.FileNotFoundException: File not found /otherAccumuloInstance/wal/localhost+9997/5a383792-c89b-41ed-bc22-0802e76638f7\r\n<repeating ad infinitum>\r\n{code}"
    ],
    [
        "ACCUMULO-1800",
        "ACCUMULO-4039",
        "delete mutations not working through the Proxy Aru Sahni writes:\r\n\r\n{quote}\r\nI'm new to Accumulo and am still trying to wrap my head around its ways. To further that challenge, I'm using Pyaccumulo, which doesn't present much in terms of available reference material.\r\n\r\nRight now I'm trying to understand how Accumulo manages record (key-value pair) deletions.\r\n\r\nconn = Accumulo(host, port, user, password)\r\ntable = 'test_table'\r\nconn.create_table(table)\r\nwriter = conn.create_batch_writer(table)\r\nmut = Mutation('mut_01')\r\nmut.put(cf='item', cq='name', value='car')\r\nwriter.add_mutation(mut)\r\nwriter.close()\r\nconn.close()\r\n\r\nWill generate a record (found via a shell scan):\r\n\r\nmut_01 item:name []    car\r\n\r\nHowever the subsequent mutation...\r\n\r\nwriter = conn.create_batch_writer(table)\r\nmut = Mutation('mut_01')\r\nmut.put(cf='item', cq='name', is_delete=True)\r\nwriter.add_mutation(mut)\r\nwriter.close()\r\n\r\nResults in:\r\n\r\nmut_01 item:name []\r\n\r\nHow should one expect the deleted row to be represented? That record sticks around even after I force a compaction of the table.  I was expecting it to not show up in any iterators, or at least provide an easy way to see if the cell has been deleted.\r\n{quote}\r\n\r\n[~ecn] has confirmed the problem.\r\n",
        "try out a proactor design pattern for tserver services For large instances (i.e. lots of clients for a given tserver) we create oodles of threads on the tserver. This makes for difficulty in predicting performance, memory usage, etc. Moreover, we have operations that recurse, like a server querying itself, that we currently solve by having separate thread pools for regular table operations and metadata table operations, and we \"disallow\" things like an iterator writing to another table. One alternative option would be to switch to a Proactor pattern: https://en.wikipedia.org/wiki/Proactor_pattern\r\n\r\nThe core of this would be to switch to using a selection set rather than a thread per active connection, and then wrap everything in sessions that make progress in something like a state model, with states that account for asynchronous communications and remote work.\r\n"
    ],
    [
        "ACCUMULO-3483",
        "ACCUMULO-3820",
        "Add private modified to recently changed members in ClientOpts [~kturner] pointed out on the review for ACCUMULO-2815 that ClientOpts isn't actually a part of the client API like I thought it was. Therefore, I can go back and make more restrictive visibility modifiers on the members which I changed to encourage the use of the methods which do some extra work after the addition of the kerberos changes.",
        "Remove unused keytab variable Remove unused variable and update comment explaining why args[1] isn't used in CopyTool."
    ],
    [
        "ACCUMULO-2833",
        "ACCUMULO-1891",
        "Create and configure a shell formatter for Status messages Would be very useful to see what the progress is for each replication record by having a deserialized version of the protobuf.",
        "AccumuloSecurityException doesn't properly handle null error codes. As a part of fixing ACCUMULO-1878 I hit an error condition in examples.simple.isolation.InterferenceTest$Writer where a security exception hits a path that doesn't properly handle a null coming out of thrift.\r\n\r\n{code}\r\nCaused by: java.lang.NullPointerException\r\n        at org.apache.accumulo.core.client.AccumuloSecurityException.getDefaultErrorMessage(AccumuloSecurityException.java:30)\r\n        at org.apache.accumulo.core.client.AccumuloSecurityException.<init>(AccumuloSecurityException.java:70)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchReaderIterator.doLookup(TabletServerBatchReaderIterator.java:579)\r\n        at org.apache.accumulo.core.client.impl.MetadataLocationObtainer.lookupTablets(MetadataLocationObtainer.java:150)\r\n        at org.apache.accumulo.core.client.impl.TabletLocatorImpl.processInvalidated(TabletLocatorImpl.java:591)\r\n        at org.apache.accumulo.core.client.impl.TabletLocatorImpl.binRanges(TabletLocatorImpl.java:272)\r\n        at org.apache.accumulo.core.client.impl.TabletLocatorImpl.processInvalidated(TabletLocatorImpl.java:584)\r\n        at org.apache.accumulo.core.client.impl.TabletLocatorImpl.binMutations(TabletLocatorImpl.java:126)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.binMutations(TabletServerBatchWriter.java:560)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.addMutations(TabletServerBatchWriter.java:600)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter.startProcessing(TabletServerBatchWriter.java:180)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter.addFailedMutations(TabletServerBatchWriter.java:471)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter.access$700(TabletServerBatchWriter.java:94)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$FailedMutations.run(TabletServerBatchWriter.java:523)\r\n{code} \r\n\r\nLooking at the code for AccumuloSecurityException, everything except for getDefaultErrorMessage accounts for the error code being null."
    ],
    [
        "ACCUMULO-3700",
        "ACCUMULO-3688",
        "Table Balancer support for multi-tennacy In a multi-tenant environment I would like to be able to segregate tables to subset of tablet servers for processing isolation and maintaining SLAs.\r\n\r\nMy thinking is this could be accomplished by defining a configuration that maps namespaces to tablet servers and a custom balancer will utilize this configuration to balance tablets only on the servers associated with its namespace. \r\n",
        "Move UserCompactionStrategyIT back to MAC only The test method which tests per-table classpath won't work with the test-resources jar that contains the compaction strategy impl."
    ],
    [
        "ACCUMULO-1786",
        "ACCUMULO-1105",
        "MiniAccumuloClusterGCTest fails if GC is already running Noticed that the MiniAccumuloClusterGCTest failed locally. Found that it couldn't start because I already have a GC running in a real instance locally.",
        "random walk test is failing All random walk tests are failing:\r\n\r\n{noformat}\r\n by: ThriftSecurityException(user:root, code:BAD_CREDENTIALS)\r\n        at org.apache.accumulo.core.client.impl.thrift.ClientService$authenticateUser_result$authenticateUser_resultStandardScheme.read(ClientService.java:8039)\r\n        at org.apache.accumulo.core.client.impl.thrift.ClientService$authenticateUser_result$authenticateUser_resultStandardScheme.read(ClientService.java:8017)\r\n        at org.apache.accumulo.core.client.impl.thrift.ClientService$authenticateUser_result.read(ClientService.java:7961)\r\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)\r\n        at org.apache.accumulo.core.client.impl.thrift.ClientService$Client.recv_authenticateUser(ClientService.java:333)\r\n\r\n{noformat}\r\n"
    ],
    [
        "ACCUMULO-1236",
        "ACCUMULO-2237",
        "Remove buildnumber-maven-plugin We can, and should, remove the buildnumber-maven-plugin. It was originally added to track different SNAPSHOT versions between releases, to ease debugging. However, I don't believe that is a situation that is likely to happen, or should be supported if it does. Using SNAPSHOT builds should be \"at your own risk\" and unsupported.\r\n\r\nFurther, it is complicated by the fact that we share SVN repo revision numbers with the other ASF projects and so a wide range of build numbers may represent the same Accumulo revision.\r\n\r\nRemoving this should simplify the POM a little bit (especially the maven-jar-plugin configuration), speed up the build slightly, and make the project more portable between SCM systems.",
        "mapred-setup.sh emits error, still works Running verification step, I get this error:\r\n{noformat}\r\n...continuous/mapred-setup.sh: line 27: /conf/accumulo-env.sh: No such file or directory\r\n{noformat}\r\n\r\nEverything works, this line is just unnecessary.\r\n"
    ],
    [
        "ACCUMULO-2577",
        "ACCUMULO-2104",
        "Incorporate html and README files into the user manual The user manual should be comprehensive, with appropriate appendices. The extra html files and random README files make it difficult to navigate the documented content. They should be incorporated into the main documentation.",
        "[RW] Image failed on writing to a non-existent table On the client:\r\n\r\n{noformat}\r\n27 01:15:32,206 [randomwalk.Framework] ERROR: Error during random walk\r\njava.lang.Exception: Error running node Sequential.xml\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:285)\r\n        at org.apache.accumulo.test.randomwalk.Framework.run(Framework.java:65)\r\n        at org.apache.accumulo.test.randomwalk.Framework.main(Framework.java:125)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:606)\r\n        at org.apache.accumulo.start.Main$1.run(Main.java:137)\r\n        at java.lang.Thread.run(Thread.java:744)\r\nCaused by: org.apache.accumulo.core.client.MutationsRejectedException: # constraint violations : 0  security codes: {}  # server errors 3 # exceptions 3\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter.checkForFailures(TabletServerBatchWriter.java:537)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter.addMutation(TabletServerBatchWriter.java:249)\r\n        at org.apache.accumulo.core.client.impl.MultiTableBatchWriterImpl$TableBatchWriter.addMutation(MultiTableBatchWriterImpl.java:64)\r\n        at org.apache.accumulo.test.randomwalk.sequential.Write.visit(Write.java:45)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:203)\r\n        at org.apache.accumulo.test.randomwalk.Module.visit(Module.java:254)\r\n        ... 8 more\r\nCaused by: org.apache.accumulo.core.client.impl.AccumuloServerException: Error on server tserver2:9997\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.sendMutationsToTabletServer(TabletServerBatchWriter.java:937)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.access$1600(TabletServerBatchWriter.java:616)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter$SendTask.send(TabletServerBatchWriter.java:801)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter$SendTask.run(TabletServerBatchWriter.java:765)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        ... 1 more\r\nCaused by: org.apache.thrift.TApplicationException: Internal error processing applyUpdates\r\n        at org.apache.thrift.TApplicationException.read(TApplicationException.java:108)\r\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:71)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Client.recv_closeUpdate(TabletClientService.java:431)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Client.closeUpdate(TabletClientService.java:417)\r\n        at org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.sendMutationsToTabletServer(TabletServerBatchWriter.java:899)\r\n        ... 10 more\r\n{noformat}\r\n\r\nOn the tserver:\r\n\r\n{noformat}\r\n2013-12-27 01:15:30,334 [thrift.ProcessFunction] ERROR: Internal error processing applyUpdates\r\njava.lang.IllegalArgumentException: Table with id i9 does not exist\r\n        at org.apache.accumulo.core.client.impl.Tables.getNamespace(Tables.java:218)\r\n        at org.apache.accumulo.server.security.SecurityOperation.hasNamespacePermissionForTableId(SecurityOperation.java:330)\r\n        at org.apache.accumulo.server.security.SecurityOperation.canWrite(SecurityOperation.java:410)\r\n        at org.apache.accumulo.tserver.TabletServer$ThriftClientHandler.setUpdateTablet(TabletServer.java:1477)\r\n        at org.apache.accumulo.tserver.TabletServer$ThriftClientHandler.applyUpdates(TabletServer.java:1521)\r\n        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:606)\r\n        at org.apache.accumulo.trace.instrument.thrift.TraceWrap$1.invoke(TraceWrap.java:63)\r\n        at com.sun.proxy.$Proxy17.applyUpdates(Unknown Source)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor$applyUpdates.getResult(TabletClientService.java:2347)\r\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor$applyUpdates.getResult(TabletClientService.java:2333)\r\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\r\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\r\n        at org.apache.accumulo.server.util.TServerUtils$TimedProcessor.process(TServerUtils.java:171)\r\n        at org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:478)\r\n        at org.apache.accumulo.server.util.TServerUtils$THsHaServer$Invocation.run(TServerUtils.java:231)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        at java.lang.Thread.run(Thread.java:744)\r\n{noformat}"
    ],
    [
        "ACCUMULO-3999",
        "ACCUMULO-3148",
        "monitor status collection should be done in parallel The master collects statistics every few seconds from all the tservers so that it has up-to-date statistics to hand to the balancer.  It does this one server at a time.  On a very large cluster, the status collection can take a long time, causing the monitor to show \"blocky\" statistics in the graphs, and long \"last contact\" times in the tablet server lists.\r\n\r\nA small thread pool could be used to speed up this simple work.",
        "TabletServer didn't get Session expired in HalfDeadTServerIT Beening seeing spurious failures with HalfDeadTServerIT where it doesn't get the ZK session expiration\r\n\r\n{noformat}\r\n2014-09-15 09:39:59,201 [tserver.TabletServer] DEBUG: ScanSess tid 172.31.33.94:35957 !0 0 entries in 0.07 secs, nbTimes = [63 63 63.00 1] \r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\nsleeping\r\n2014-09-15 09:40:20,088 [tserver.TabletServer] FATAL: Lost tablet server lock (reason = LOCK_DELETED), exiting.\r\n2014-09-15 09:40:20,088 [zookeeper.ZooCache] WARN : Zookeeper error, will retry\r\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /accumulo/d0b9b8e7-9869-4b00-9ae7-317f5231f2c1/tables/1/conf/table.iterator.minc.vers.opt.maxVersions\r\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:99)\r\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\r\n\tat org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1151)\r\n\tat org.apache.accumulo.fate.zookeeper.ZooCache$2.run(ZooCache.java:261)\r\n\tat org.apache.accumulo.fate.zookeeper.ZooCache.retry(ZooCache.java:153)\r\n\tat org.apache.accumulo.fate.zookeeper.ZooCache.get(ZooCache.java:277)\r\n\tat org.apache.accumulo.fate.zookeeper.ZooCache.get(ZooCache.java:224)\r\n\tat org.apache.accumulo.server.conf.ZooCachePropertyAccessor.get(ZooCachePropertyAccessor.java:114)\r\n\tat org.apache.accumulo.server.conf.ZooCachePropertyAccessor.getProperties(ZooCachePropertyAccessor.java:144)\r\n\tat org.apache.accumulo.server.conf.TableConfiguration.getProperties(TableConfiguration.java:108)\r\n\tat org.apache.accumulo.core.conf.AccumuloConfiguration.iterator(AccumuloConfiguration.java:69)\r\n\tat org.apache.accumulo.core.conf.ConfigSanityCheck.validate(ConfigSanityCheck.java:40)\r\n\tat org.apache.accumulo.server.conf.ServerConfigurationFactory.getTableConfiguration(ServerConfigurationFactory.java:155)\r\n\tat org.apache.accumulo.server.conf.ServerConfiguration.getTableConfiguration(ServerConfiguration.java:69)\r\n\tat org.apache.accumulo.tserver.TabletServer.getTableConfiguration(TabletServer.java:3983)\r\n\tat org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1277)\r\n\tat org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1256)\r\n\tat org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1112)\r\n\tat org.apache.accumulo.tserver.Tablet.<init>(Tablet.java:1089)\r\n\tat org.apache.accumulo.tserver.TabletServer$AssignmentHandler.run(TabletServer.java:2935)\r\n\tat org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n\tat org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n\tat org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n2014-09-15 09:40:20,090 [tserver.TabletServer] WARN : Check for long GC pauses not called in a timely fashion. Expected every 5.0 seconds but was 16.3 seconds since last check\r\n2014-09-15 09:40:20,477 [datanode.DataNode] ERROR: 127.0.0.1:57185:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:42146 dst: /127.0.0.1:57185\r\njava.io.IOException: Premature EOF from inputStream\r\n\tat org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:194)\r\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)\r\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)\r\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)\r\n\tat org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:467)\r\n\tat org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:771)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:718)\r\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:126)\r\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:72)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:225)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n{noformat}\r\n\r\nIt looks like the tserver killed itself after the connection loss but before the tserver retried to connect and got the session expiration."
    ],
    [
        "ACCUMULO-309",
        "ACCUMULO-69",
        "MockBatchScanner is broken In my attempt to test a potential fix for ACCUMULO-175, I discovered the following code did not return any results-\r\n{noformat}\r\npublic class TestRunningAgainstMock {\r\n  public static void main(String[] args) throws AccumuloException, AccumuloSecurityException, TableExistsException, TableNotFoundException,\r\n      InterruptedException\r\n  {\r\n    MockInstance mi = new MockInstance(\"test\");\r\n    Connector conn = mi.getConnector(\"root\", new byte[0]);\r\n    conn.tableOperations().create(\"testTable\", true);\r\n    Mutation m = new Mutation(\"row\");\r\n    m.put(\"cf\", \"cq\", \"val\");\r\n    BatchWriter bw = conn.createBatchWriter(\"testTable\", 500, 500, 1);\r\n    bw.addMutation(m);\r\n    bw.flush();\r\n    bw.close();\r\n    Scanner bs = conn.createBatchScanner(\"testTable\", new Authorizations(), 1);\r\n    Iterator<Entry<Key,Value>> iter = bs.iterator();\r\n    if (!iter.hasNext())\r\n      System.out.println(\"No values\");\r\n    while (iter.hasNext())\r\n      System.out.println(iter.next());\r\n  }\r\n}{noformat}\r\n\r\nSwitching it to a regular scanner did work though.",
        "Document how Accumulo uses Zookeeper and HDFS Document how accumulo uses hdfs and zookeeper.  Document what data is stored where.  Also document write ahead log subsystem, as part of data flow."
    ],
    [
        "ACCUMULO-2193",
        "ACCUMULO-3220",
        "Check for concurrency issue in randomwalk State Check for any concurrency issues with the {{State}} class used for randomwalk testing, as called out by [~mdrob] in this review: https://reviews.apache.org/r/16857/.",
        "Division by zero if encryption cipher isn't a block cipher {code:title=BlockedOutputStream.java}\r\n  public BlockedOutputStream(OutputStream out, int blockSize, int bufferSize) {\r\n    if (bufferSize <= 0)\r\n      throw new IllegalArgumentException(\"bufferSize must be greater than 0.\");\r\n    if (out instanceof DataOutputStream)\r\n      this.out = (DataOutputStream) out;\r\n    else\r\n      this.out = new DataOutputStream(out);\r\n    this.blockSize = blockSize;\r\n    int remainder = bufferSize % blockSize;\r\n    if (remainder != 0)\r\n      remainder = blockSize - remainder;\r\n    // some buffer space + bytes to make the buffer evened up with the cipher block size - 4 bytes for the size int\r\n    bb = ByteBuffer.allocate(bufferSize + remainder - 4);\r\n  }\r\n{code}\r\n\r\nIf the Cipher is not a block cipher, blocksize is zero and would result in a division by zero error."
    ],
    [
        "ACCUMULO-1318",
        "ACCUMULO-1711",
        "Allow granting System.GRANT permission With the addition of pluggable authentication/authorizor/permissions handler modules (ACCUMULO-259), it seems we should rely more on these modules to set their policy for who has which permissions.\r\n\r\nAs such, I don't believe we should continue to constrain the System.GRANT permission, so that it is held only by the root user. This is an especially important consideration for ACCUMULO-1300, because in that ticket, there will always be a \"local\" root user, but there's no reason that should be the de-facto account that manages other users' permissions from.",
        "fix the documentation for start-row See parent ticket."
    ],
    [
        "ACCUMULO-3351",
        "ACCUMULO-2921",
        "Tracer can't write traces after offline and online of trace table While running tests for ACCUMULO-3167, I updated one of the tests to offline the trace table to reduce the possibility that any active logs for the trace table would exist in the metadata table.\r\n\r\nA later test went to validate that traces were found for some conditional update sessions and hung indefinitely.\r\n\r\nInspecting the tracer log, the batchwriter had two exceptions due to the trace table being offline (as expected), but never recovered when the trace table came back online.\r\n\r\n{noformat}\r\n2014-11-20 13:08:28,717 [impl.TabletServerBatchWriter] DEBUG: Table trace (in) is offline\r\norg.apache.accumulo.core.client.TableOfflineException: Table trace (in) is offline\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.binMutations(TabletServerBatchWriter.java:662)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.addMutations(TabletServerBatchWriter.java:694)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.startProcessing(TabletServerBatchWriter.java:233)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.addFailedMutations(TabletServerBatchWriter.java:551)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.access$700(TabletServerBatchWriter.java:101)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$FailedMutations.run(TabletServerBatchWriter.java:603)\r\n\tat java.util.TimerThread.mainLoop(Timer.java:555)\r\n\tat java.util.TimerThread.run(Timer.java:505)\r\n2014-11-20 13:08:28,720 [tracer.TraceServer] WARN : Problem flushing traces, resetting writer. Set log level to DEBUG to see stacktrace. cause: org.apache.accumulo.core.client.MutationsRejectedException: # constraint violations : 0  security codes: {}  # server errors 0 # exceptions 1\r\n2014-11-20 13:08:28,720 [tracer.TraceServer] DEBUG: flushing traces failed due to exception\r\norg.apache.accumulo.core.client.MutationsRejectedException: # constraint violations : 0  security codes: {}  # server errors 0 # exceptions 1\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.checkForFailures(TabletServerBatchWriter.java:537)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.flush(TabletServerBatchWriter.java:331)\r\n\tat org.apache.accumulo.core.client.impl.BatchWriterImpl.flush(BatchWriterImpl.java:59)\r\n\tat org.apache.accumulo.tracer.TraceServer.flush(TraceServer.java:245)\r\n\tat org.apache.accumulo.tracer.TraceServer.access$300(TraceServer.java:78)\r\n\tat org.apache.accumulo.tracer.TraceServer$1.run(TraceServer.java:235)\r\n\tat org.apache.accumulo.server.util.time.SimpleTimer$LoggingTimerTask.run(SimpleTimer.java:42)\r\n\tat java.util.TimerThread.mainLoop(Timer.java:555)\r\n\tat java.util.TimerThread.run(Timer.java:505)\r\nCaused by: org.apache.accumulo.core.client.TableOfflineException: Table trace (in) is offline\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.binMutations(TabletServerBatchWriter.java:662)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.addMutations(TabletServerBatchWriter.java:694)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.startProcessing(TabletServerBatchWriter.java:233)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.addFailedMutations(TabletServerBatchWriter.java:551)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.access$700(TabletServerBatchWriter.java:101)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$FailedMutations.run(TabletServerBatchWriter.java:603)\r\n\t... 2 more\r\n2014-11-20 13:08:28,722 [tracer.TraceServer] WARN : Unable to create a batch writer, will retry. Set log level to DEBUG to see stacktrace. cause: org.apache.accumulo.core.client.TableOfflineException: Table trace (in) is offline\r\n2014-11-20 13:08:28,722 [tracer.TraceServer] DEBUG: batch writer creation failed with exception.\r\norg.apache.accumulo.core.client.TableOfflineException: Table trace (in) is offline\r\n\tat org.apache.accumulo.core.client.impl.ConnectorImpl.getTableId(ConnectorImpl.java:86)\r\n\tat org.apache.accumulo.core.client.impl.ConnectorImpl.createBatchWriter(ConnectorImpl.java:128)\r\n\tat org.apache.accumulo.tracer.TraceServer.resetWriter(TraceServer.java:262)\r\n\tat org.apache.accumulo.tracer.TraceServer.flush(TraceServer.java:250)\r\n\tat org.apache.accumulo.tracer.TraceServer.access$300(TraceServer.java:78)\r\n\tat org.apache.accumulo.tracer.TraceServer$1.run(TraceServer.java:235)\r\n\tat org.apache.accumulo.server.util.time.SimpleTimer$LoggingTimerTask.run(SimpleTimer.java:42)\r\n\tat java.util.TimerThread.mainLoop(Timer.java:555)\r\n\tat java.util.TimerThread.run(Timer.java:505)\r\n2014-11-20 13:08:28,723 [tracer.TraceServer] WARN : Problem closing batch writer. Set log level to DEBUG to see stacktrace. cause: org.apache.accumulo.core.client.MutationsRejectedException: # constraint violations : 0  security codes: {}  # server errors 0 # exceptions 1\r\n2014-11-20 13:08:28,723 [tracer.TraceServer] DEBUG: batch writer close failed with exception\r\norg.apache.accumulo.core.client.MutationsRejectedException: # constraint violations : 0  security codes: {}  # server errors 0 # exceptions 1\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.checkForFailures(TabletServerBatchWriter.java:537)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.close(TabletServerBatchWriter.java:354)\r\n\tat org.apache.accumulo.core.client.impl.BatchWriterImpl.close(BatchWriterImpl.java:54)\r\n\tat org.apache.accumulo.tracer.TraceServer.resetWriter(TraceServer.java:271)\r\n\tat org.apache.accumulo.tracer.TraceServer.flush(TraceServer.java:250)\r\n\tat org.apache.accumulo.tracer.TraceServer.access$300(TraceServer.java:78)\r\n\tat org.apache.accumulo.tracer.TraceServer$1.run(TraceServer.java:235)\r\n\tat org.apache.accumulo.server.util.time.SimpleTimer$LoggingTimerTask.run(SimpleTimer.java:42)\r\n\tat java.util.TimerThread.mainLoop(Timer.java:555)\r\n\tat java.util.TimerThread.run(Timer.java:505)\r\nCaused by: org.apache.accumulo.core.client.TableOfflineException: Table trace (in) is offline\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.binMutations(TabletServerBatchWriter.java:662)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.addMutations(TabletServerBatchWriter.java:694)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.startProcessing(TabletServerBatchWriter.java:233)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.addFailedMutations(TabletServerBatchWriter.java:551)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.access$700(TabletServerBatchWriter.java:101)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$FailedMutations.run(TabletServerBatchWriter.java:603)\r\n\t... 2 more\r\n2014-11-20 13:10:12,929 [tracer.TraceServer] WARN : writer is not ready; discarding span.\r\n2014-11-20 13:10:12,930 [tracer.TraceServer] WARN : writer is not ready; discarding span.\r\n{noformat}\r\n\r\n\"writer is not ready; discarding span.\" repeats indefinitely.",
        "Manual section on Contraints should mention existing constraints The [user manual section on constraints|http://accumulo.apache.org/1.6/accumulo_user_manual.html#_constraints] claims that we don't ship with any constraint implementations.\r\n\r\nThis is no longer true, because we have [DefaultKeySizeConstraint|https://git-wip-us.apache.org/repos/asf?p=accumulo.git;a=blob;f=core/src/main/java/org/apache/accumulo/core/constraints/DefaultKeySizeConstraint.java;h=88b7eea19718c145010d915e0f97f2057dc7acf8;hb=refs/heads/1.6.1-SNAPSHOT].\r\n\r\nSome brief mention of what this constraint does, why we include it by default, and how to change that it is there would be good.\r\n\r\nAlso, we should cover [VisibilityConstraint|https://git-wip-us.apache.org/repos/asf?p=accumulo.git;a=blob;f=core/src/main/java/org/apache/accumulo/core/security/VisibilityConstraint.java;h=8613ca7e598fc1b04662a18553517b6b9342aff1;hb=refs/heads/1.6.1-SNAPSHOT] in this section, with a similar explanation of why/when/how you would configure it."
    ],
    [
        "ACCUMULO-3031",
        "ACCUMULO-631",
        "jdk 1.7.0_65 breaks ShellSetInstanceTest Switching back to jdk1.7.0_60 fixes it.\r\n\r\nStrongly suspect [a jdk bug|https://bugs.openjdk.java.net/browse/JDK-8051012] (thanks to [~bhavanki] for finding this for me).\r\n\r\nExample output:\r\n{noformat}\r\nBad <init> method call from inside of a branch\r\nException Details:\r\n  Location:\r\n    org/apache/accumulo/shell/Shell.<init>(Ljline/console/ConsoleReader;Ljava/io/PrintWriter;)V @36: invokespecial\r\n  Reason:\r\n    Error exists in the bytecode\r\n  Bytecode:\r\n    0000000: 2a4e 1300 94b8 004d 03bd 000d 1300 95b8\r\n    0000010: 0028 b800 823a 0519 05b2 0032 a500 0e2a\r\n    0000020: 01c0 0084 b700 87a7 0009 2db7 0096 0157\r\n    0000030: 2a03 b500 9a2a 1300 9cb5 00a0 2a13 00a2\r\n    0000040: b500 a52a 0000 0000 014e 013a 0413 00a7\r\n    0000050: b800 2203 bd00 0d13 00a9 b800 28b8 002e\r\n    0000060: 3a05 1905 b200 32a5 0034 1905 c100 3499\r\n    0000070: 0022 b800 3a13 00a7 b800 2213 000d 01b6\r\n    0000080: 003e b600 4201 b600 46c0 0090 3a04 a700\r\n    0000090: 0a19 05c0 0090 3a04 a700 0cbb 0090 59b7\r\n    00000a0: 00aa 3a04 1904 b500 ae2a 0000 0000 014e\r\n    00000b0: 013a 0413 00a7 b800 2203 bd00 0d13 00a9\r\n    00000c0: b800 28b8 002e 3a05 1905 b200 32a5 0034\r\n    00000d0: 1905 c100 3499 0022 b800 3a13 00a7 b800\r\n    00000e0: 2213 000d 01b6 003e b600 4201 b600 46c0\r\n    00000f0: 0090 3a04 a700 0a19 05c0 0090 3a04 a700\r\n    0000100: 0cbb 0090 59b7 00aa 3a04 1904 b500 b12a\r\n    0000110: 0000 0000 014e 013a 0413 00b3 b800 2203\r\n    0000120: bd00 0d13 00b5 b800 28b8 002e 3a05 1905\r\n    0000130: b200 32a5 0034 1905 c100 3499 0022 b800\r\n    0000140: 3a13 00b3 b800 2213 000d 01b6 003e b600\r\n    0000150: 4201 b600 46c0 0092 3a04 a700 0a19 05c0\r\n    0000160: 0092 3a04 a700 0cbb 0092 59b7 00b6 3a04\r\n    0000170: 1904 b500 b92a 0000 0000 014e 013a 0413\r\n    0000180: 00b3 b800 2203 bd00 0d13 00b5 b800 28b8\r\n    0000190: 002e 3a05 1905 b200 32a5 0034 1905 c100\r\n    00001a0: 3499 0022 b800 3a13 00b3 b800 2213 000d\r\n    00001b0: 01b6 003e b600 4201 b600 46c0 0092 3a04\r\n    00001c0: a700 0a19 05c0 0092 3a04 a700 0cbb 0092\r\n    00001d0: 59b7 00b6 3a04 1904 b500 bc2a 03b5 00c0\r\n    00001e0: 2a03 b500 c32a 01b5 00c7 2a01 b500 cb2a\r\n    00001f0: 04b5 00ce 2a01 4e09 3704 013a 062d 01a5\r\n    0000200: 0009 2d3a 06a7 000b 1300 4ab8 004d 3a06\r\n    0000210: 1906 1300 d003 bd00 0d13 00d2 b800 2813\r\n    0000220: 00d4 b800 673a 0719 07b2 0032 a600 0bb8\r\n    0000230: 00d6 3704 a700 0d19 07c0 00d8 b600 db37\r\n    0000240: 0416 04b5 00df 2a03 b500 e22a 01b5 00e4\r\n    0000250: 2a03 b500 e72a 2bb5 00e9 2a2c b500 e4b1\r\n    0000260:                                        \r\n  Stackmap Table:\r\n    full_frame(@42,{UninitializedThis,Object[#15],Object[#25],UninitializedThis,Top,Object[#13]},{})\r\n    full_frame(@48,{Object[#2],Object[#15],Object[#25],Object[#2],Top,Object[#13]},{})\r\n    full_frame(@145,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@152,{Object[#2],Object[#15],Object[#25],Null,Object[#144],Object[#13]},{Object[#2]})\r\n    full_frame(@155,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@164,{Object[#2],Object[#15],Object[#25],Null,Object[#144],Object[#13]},{Object[#2]})\r\n    full_frame(@247,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@254,{Object[#2],Object[#15],Object[#25],Null,Object[#144],Object[#13]},{Object[#2]})\r\n    full_frame(@257,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@266,{Object[#2],Object[#15],Object[#25],Null,Object[#144],Object[#13]},{Object[#2]})\r\n    full_frame(@349,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@356,{Object[#2],Object[#15],Object[#25],Null,Object[#146],Object[#13]},{Object[#2]})\r\n    full_frame(@359,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@368,{Object[#2],Object[#15],Object[#25],Null,Object[#146],Object[#13]},{Object[#2]})\r\n    full_frame(@451,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@458,{Object[#2],Object[#15],Object[#25],Null,Object[#146],Object[#13]},{Object[#2]})\r\n    full_frame(@461,{Object[#2],Object[#15],Object[#25],Null,Null,Object[#13]},{Object[#2]})\r\n    full_frame(@470,{Object[#2],Object[#15],Object[#25],Null,Object[#146],Object[#13]},{Object[#2]})\r\n    full_frame(@520,{Object[#2],Object[#15],Object[#25],Null,Long,Null},{Object[#2]})\r\n    full_frame(@528,{Object[#2],Object[#15],Object[#25],Null,Long,Object[#21]},{Object[#2]})\r\n    full_frame(@567,{Object[#2],Object[#15],Object[#25],Null,Long,Object[#21],Object[#13]},{Object[#2]})\r\n    same_locals_1_stack_item_frame(@577,Object[#2])\r\n\r\n{noformat}",
        "ZooZap Usage Message Does Not Mention -tracers The summary says it all."
    ],
    [
        "ACCUMULO-569",
        "ACCUMULO-3691",
        "enable the monitor to show the current configuration As mentioned in ACCUMULO-123, display the current configuration (minus passwords, of course) in the monitor.  Basically, display the same thing you can get in the shell.  This is helpful for remote debugging with less sophisticated users.\r\n",
        "HostAndPort is not comparable but used in TreeMap Noticied when I went to the Active Scans page on the monitor\r\n\r\n{noformat}\r\njava.lang.ClassCastException: com.google.common.net.HostAndPort cannot be cast to java.lang.Comparable\r\n\tat java.util.TreeMap.compare(TreeMap.java:1188)\r\n\tat java.util.TreeMap.put(TreeMap.java:531)\r\n\tat java.util.AbstractMap.putAll(AbstractMap.java:273)\r\n\tat java.util.TreeMap.putAll(TreeMap.java:322)\r\n\tat java.util.TreeMap.<init>(TreeMap.java:180)\r\n\tat org.apache.accumulo.monitor.Monitor.getScans(Monitor.java:542)\r\n\tat org.apache.accumulo.monitor.servlets.ScanServlet.pageBody(ScanServlet.java:47)\r\n\tat org.apache.accumulo.monitor.servlets.BasicServlet.doGet(BasicServlet.java:66)\r\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\r\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\r\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:738)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:551)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:219)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1111)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:478)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:183)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1045)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:462)\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:279)\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:232)\r\n\tat org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:534)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:607)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:536)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n{noformat}"
    ],
    [
        "ACCUMULO-1517",
        "ACCUMULO-2385",
        "Add config.html, created by mvn docs profile) to Accumulo website. The config.html page has good information. I think it should be available on the website.",
        "classload problem running functional tests {noformat}\r\n======================================================================\r\nFAIL: runTest (simple.mapreduce.MapReduceTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/user/workspace/accumulo-1.5.1/test/system/auto/simple/mapreduce.py\", line 101, in runTest\r\n    self.fail(\"Test did not finish\")\r\nAssertionError: Test did not finish\r\n{noformat}\r\n\r\nRunning the test with debug finds:\r\n\r\n{noformat}\r\nError: java.lang.ClassNotFoundException: com.google.common.cache.CacheLoader\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\r\n{noformat}\r\n"
    ],
    [
        "ACCUMULO-910",
        "ACCUMULO-3351",
        "Can't run helloworld example in Accumulo 1.4.0 I'm new to Apache Accumulo, and having problems trying to run the helloworld example.  ",
        "Tracer can't write traces after offline and online of trace table While running tests for ACCUMULO-3167, I updated one of the tests to offline the trace table to reduce the possibility that any active logs for the trace table would exist in the metadata table.\r\n\r\nA later test went to validate that traces were found for some conditional update sessions and hung indefinitely.\r\n\r\nInspecting the tracer log, the batchwriter had two exceptions due to the trace table being offline (as expected), but never recovered when the trace table came back online.\r\n\r\n{noformat}\r\n2014-11-20 13:08:28,717 [impl.TabletServerBatchWriter] DEBUG: Table trace (in) is offline\r\norg.apache.accumulo.core.client.TableOfflineException: Table trace (in) is offline\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.binMutations(TabletServerBatchWriter.java:662)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.addMutations(TabletServerBatchWriter.java:694)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.startProcessing(TabletServerBatchWriter.java:233)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.addFailedMutations(TabletServerBatchWriter.java:551)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.access$700(TabletServerBatchWriter.java:101)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$FailedMutations.run(TabletServerBatchWriter.java:603)\r\n\tat java.util.TimerThread.mainLoop(Timer.java:555)\r\n\tat java.util.TimerThread.run(Timer.java:505)\r\n2014-11-20 13:08:28,720 [tracer.TraceServer] WARN : Problem flushing traces, resetting writer. Set log level to DEBUG to see stacktrace. cause: org.apache.accumulo.core.client.MutationsRejectedException: # constraint violations : 0  security codes: {}  # server errors 0 # exceptions 1\r\n2014-11-20 13:08:28,720 [tracer.TraceServer] DEBUG: flushing traces failed due to exception\r\norg.apache.accumulo.core.client.MutationsRejectedException: # constraint violations : 0  security codes: {}  # server errors 0 # exceptions 1\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.checkForFailures(TabletServerBatchWriter.java:537)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.flush(TabletServerBatchWriter.java:331)\r\n\tat org.apache.accumulo.core.client.impl.BatchWriterImpl.flush(BatchWriterImpl.java:59)\r\n\tat org.apache.accumulo.tracer.TraceServer.flush(TraceServer.java:245)\r\n\tat org.apache.accumulo.tracer.TraceServer.access$300(TraceServer.java:78)\r\n\tat org.apache.accumulo.tracer.TraceServer$1.run(TraceServer.java:235)\r\n\tat org.apache.accumulo.server.util.time.SimpleTimer$LoggingTimerTask.run(SimpleTimer.java:42)\r\n\tat java.util.TimerThread.mainLoop(Timer.java:555)\r\n\tat java.util.TimerThread.run(Timer.java:505)\r\nCaused by: org.apache.accumulo.core.client.TableOfflineException: Table trace (in) is offline\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.binMutations(TabletServerBatchWriter.java:662)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.addMutations(TabletServerBatchWriter.java:694)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.startProcessing(TabletServerBatchWriter.java:233)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.addFailedMutations(TabletServerBatchWriter.java:551)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.access$700(TabletServerBatchWriter.java:101)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$FailedMutations.run(TabletServerBatchWriter.java:603)\r\n\t... 2 more\r\n2014-11-20 13:08:28,722 [tracer.TraceServer] WARN : Unable to create a batch writer, will retry. Set log level to DEBUG to see stacktrace. cause: org.apache.accumulo.core.client.TableOfflineException: Table trace (in) is offline\r\n2014-11-20 13:08:28,722 [tracer.TraceServer] DEBUG: batch writer creation failed with exception.\r\norg.apache.accumulo.core.client.TableOfflineException: Table trace (in) is offline\r\n\tat org.apache.accumulo.core.client.impl.ConnectorImpl.getTableId(ConnectorImpl.java:86)\r\n\tat org.apache.accumulo.core.client.impl.ConnectorImpl.createBatchWriter(ConnectorImpl.java:128)\r\n\tat org.apache.accumulo.tracer.TraceServer.resetWriter(TraceServer.java:262)\r\n\tat org.apache.accumulo.tracer.TraceServer.flush(TraceServer.java:250)\r\n\tat org.apache.accumulo.tracer.TraceServer.access$300(TraceServer.java:78)\r\n\tat org.apache.accumulo.tracer.TraceServer$1.run(TraceServer.java:235)\r\n\tat org.apache.accumulo.server.util.time.SimpleTimer$LoggingTimerTask.run(SimpleTimer.java:42)\r\n\tat java.util.TimerThread.mainLoop(Timer.java:555)\r\n\tat java.util.TimerThread.run(Timer.java:505)\r\n2014-11-20 13:08:28,723 [tracer.TraceServer] WARN : Problem closing batch writer. Set log level to DEBUG to see stacktrace. cause: org.apache.accumulo.core.client.MutationsRejectedException: # constraint violations : 0  security codes: {}  # server errors 0 # exceptions 1\r\n2014-11-20 13:08:28,723 [tracer.TraceServer] DEBUG: batch writer close failed with exception\r\norg.apache.accumulo.core.client.MutationsRejectedException: # constraint violations : 0  security codes: {}  # server errors 0 # exceptions 1\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.checkForFailures(TabletServerBatchWriter.java:537)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.close(TabletServerBatchWriter.java:354)\r\n\tat org.apache.accumulo.core.client.impl.BatchWriterImpl.close(BatchWriterImpl.java:54)\r\n\tat org.apache.accumulo.tracer.TraceServer.resetWriter(TraceServer.java:271)\r\n\tat org.apache.accumulo.tracer.TraceServer.flush(TraceServer.java:250)\r\n\tat org.apache.accumulo.tracer.TraceServer.access$300(TraceServer.java:78)\r\n\tat org.apache.accumulo.tracer.TraceServer$1.run(TraceServer.java:235)\r\n\tat org.apache.accumulo.server.util.time.SimpleTimer$LoggingTimerTask.run(SimpleTimer.java:42)\r\n\tat java.util.TimerThread.mainLoop(Timer.java:555)\r\n\tat java.util.TimerThread.run(Timer.java:505)\r\nCaused by: org.apache.accumulo.core.client.TableOfflineException: Table trace (in) is offline\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.binMutations(TabletServerBatchWriter.java:662)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$MutationWriter.addMutations(TabletServerBatchWriter.java:694)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.startProcessing(TabletServerBatchWriter.java:233)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.addFailedMutations(TabletServerBatchWriter.java:551)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter.access$700(TabletServerBatchWriter.java:101)\r\n\tat org.apache.accumulo.core.client.impl.TabletServerBatchWriter$FailedMutations.run(TabletServerBatchWriter.java:603)\r\n\t... 2 more\r\n2014-11-20 13:10:12,929 [tracer.TraceServer] WARN : writer is not ready; discarding span.\r\n2014-11-20 13:10:12,930 [tracer.TraceServer] WARN : writer is not ready; discarding span.\r\n{noformat}\r\n\r\n\"writer is not ready; discarding span.\" repeats indefinitely."
    ],
    [
        "ACCUMULO-4045",
        "ACCUMULO-2080",
        "Proposal to improve the security of Accumulo's password-based logons There are some things we can do to improve the password-based authentication to Accumulo without adding much overhead or introducing new APIs or RPCs.\r\n\r\nOf course, strong authentication using SASL or TLS is preferred, but those may not be feasible for all users, and as long as we continue to support using Accumulo without requiring these protocols, we should also consider ways to improve the simple password authentication mechanism.\r\n\r\nSo, with that in mind, I suggest:\r\n\r\n# Add a logon or \"createSession\" RPC method to effectively authenticate using a password and create a session identifier for future use.\r\n# A flag on Connector to use a session, could be added, and Connector could be made AutoCloseable, or another API could be added to clean up sessions. Sessions could also be made expirable.\r\n# When a session is created, the session information is stored in ZooKeeper, and used to authenticate a user in lieu of a password on future RPC requests. The RPC data will be able to distinguish whether it is of the session type or regular password type, for compatibility. But, mostly session data would work just like passwords do today, with a simple compare/check.\r\n# To create a session, the server (any server which can currently do authentication) receives a logon/createSession request, and responds with the user's salt, hash method, and a nonce. The client hashes the password using the salt and hash method, to reproduce the hashed version which the server already has. It then hashes again with the nonce and sends the result back to the server. The server then compares this result with the result of its own hashing of its stored value with the same nonce it sent to the client. If it matches, a session is created, stored in ZooKeeper and returned.\r\n\r\nNow, this is certainly not a perfect solution, but it could be a way we can provide some significant additional protection for password-based authentication without any of the additional security/authentication mechanisms enabled for Accumulo, and with low overhead.",
        "Active walogs deleted when using viewfs While running continuous ingest and agitation using viewfs active walogs were deleted.  \r\n\r\nThe GC asked the tserver to delete a walogs file.  The tserver checked to see if the file was in use and determined it was not, when it really was.  The problem was the path from the GC was normalized and the tserver path was not.   The problem was {{viewfs:/nn1/accumulo/wal/ip-10-1-3-15+9997/bb60f4d4-29a0-4e69-8b0f-dcd44f973cb9}} was compared to {{viewfs:///nn1/accumulo/wal/ip-10-1-3-15+9997/bb60f4d4-29a0-4e69-8b0f-dcd44f973cb9}}.\r\n\r\nThe tablet server should be comparing uuids.  This problem could occur w/ filesystems other than viewfs."
    ],
    [
        "ACCUMULO-1549",
        "ACCUMULO-2713",
        "Migrating MAC to use TestingServer for ZooKeeper Currently we start an in Java Process to run ZooKeeper for MAC. Curator has a TestingServer, which is an already managed in memory ZooKeeper. We should switch to this after the Curator migration to simplify things for MAC.",
        "Instance secret written out with other configuration items to RFiles and WALogs when encryption is turned on The encryption at rest feature records configuration information in order to encrypted RFiles and WALogs so that if the configuration changes, the files can be read back.  The code that does this recording hovers up all the \"instance.*\" entries, and does not pick out the instance.secret as a special one not to write.  Thus the instance secret goes into each file in the clear, which is non-ideal to say the least.\r\n\r\nPatch forthcoming."
    ],
    [
        "ACCUMULO-3574",
        "ACCUMULO-1722",
        "preferCachedConnections=false is not respected {code}\r\n  Pair<String,TTransport> getAnyTransport(List<ThriftTransportKey> servers, boolean preferCachedConnection) throws TTransportException {\r\n    ....\r\n    if (!preferCachedConnection) {\r\n      synchronized (this) {\r\n        List<CachedConnection> cachedConnList = getCache().get(ttk);\r\n        if (cachedConnList != null) {\r\n          for (CachedConnection cachedConnection : cachedConnList) {\r\n            if (!cachedConnection.isReserved()) {\r\n              cachedConnection.setReserved(true);\r\n              final String serverAddr = ttk.getServer().toString();\r\n              log.trace(\"Using existing connection to {} timeout {}\", serverAddr, ttk.getTimeout());\r\n              return new Pair<String,TTransport>(serverAddr, cachedConnection.transport);\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n{code}\r\n\r\nIt appears that ThriftTransportPool.getAnyTransport is returning a cache connection when the caller requested that a new connection is returned instead of a cached one.",
        "add the ability to dump user permissions to the dumpConfig admin command See parent ticket.  It would be nice to get a set of createuser commands along with all the grant commands necessary to reproduce the current user set.\r\n"
    ],
    [
        "ACCUMULO-249",
        "ACCUMULO-2600",
        "Create org.apache.accumulo.server.util.Flush program to simplify examples.  The README.bloom file has the following step:\r\n\r\nBelow the table is flushed, look at the monitor page and wait for the flush to\r\ncomplete.  \r\n\r\n    $ ./bin/accumulo shell -u username -p password\r\n    username@instance> flush -t bloom_test\r\n    Flush of table bloom_test initiated...\r\n    username@instance> exit\r\n\r\nIt would be nice to avoid the shell by using:\r\n\r\nbin/accumulo org.apache.accumulo.server.util.Flush -u username -p password -t bloom_test\r\n\r\nOf course, all flush options should be supported. If no table name was provided, could all tables be flushed? And can a comma-delimited list of table be handled?\r\n",
        "Key.clone() should return type Key {{Key.clone()}} can return {{Key}} instead of {{Object}} like it does currently. This is nicer on end-users of the API."
    ],
    [
        "ACCUMULO-886",
        "ACCUMULO-1833",
        "Create integration tests for new ClassLoader Many of the \"unit tests\" are actually testing the full functionality of the new classloader, and would be more suitable as integration tests. This has the added benefit of not taking so much time to run these tests, as they are quite time-consuming.\r\n\r\nThese tests should be moved to the integration-test phase of the build lifecycle, and run with \"mvn verify\" instead of \"mvn package\".",
        "MultiTableBatchWriterImpl.getBatchWriter() is not performant for multiple threads This issue comes from profiling our application. We have a MultiTableBatchWriter created by normal means. I am attempting to write to it with multiple threads by doing things like the following:\r\n\r\n{code}\r\nbatchWriter.getBatchWriter(table).addMutations(mutations);\r\n{code}\r\n\r\nIn my test with 4 threads writing to one table, this call is quite inefficient and results in a large performance degradation over a single BatchWriter.\r\n\r\nI believe the culprit is the fact that the call is synchronized. Also there is the possibility that the zookeeper call to Tables.getTableState on every call is negatively affecting performance:\r\n\r\n{code}\r\n  @Override\r\n  public synchronized BatchWriter getBatchWriter(String tableName) throws AccumuloException, AccumuloSecurityException, TableNotFoundException {\r\n    ArgumentChecker.notNull(tableName);\r\n    String tableId = Tables.getNameToIdMap(instance).get(tableName);\r\n    if (tableId == null)\r\n      throw new TableNotFoundException(tableId, tableName, null);\r\n    \r\n    if (Tables.getTableState(instance, tableId) == TableState.OFFLINE)\r\n      throw new TableOfflineException(instance, tableId);\r\n    \r\n    BatchWriter tbw = tableWriters.get(tableId);\r\n    if (tbw == null) {\r\n      tbw = new TableBatchWriter(tableId);\r\n      tableWriters.put(tableId, tbw);\r\n    }\r\n    return tbw;\r\n  }\r\n{code}\r\n\r\nI recommend moving the synchronized block to happen only if the batchwriter is not present, and also only checking if the table is online at that time:\r\n\r\n{code}\r\n  @Override\r\n  public BatchWriter getBatchWriter(String tableName) throws AccumuloException, AccumuloSecurityException, TableNotFoundException {\r\n    ArgumentChecker.notNull(tableName);\r\n    String tableId = Tables.getNameToIdMap(instance).get(tableName);\r\n    if (tableId == null)\r\n      throw new TableNotFoundException(tableId, tableName, null);\r\n\r\n    BatchWriter tbw = tableWriters.get(tableId);\r\n    if (tbw == null) {\r\n\r\n      if (Tables.getTableState(instance, tableId) == TableState.OFFLINE)\r\n          throw new TableOfflineException(instance, tableId);\r\n      tbw = new TableBatchWriter(tableId);\r\n      synchronized(tableWriters){\r\n          //only create a new table writer if we haven't been beaten to it.\r\n          if (tableWriters.get(tableId) == null)      \r\n              tableWriters.put(tableId, tbw);\r\n      }\r\n    }\r\n    return tbw;\r\n  }\r\n{code}"
    ],
    [
        "ACCUMULO-3765",
        "ACCUMULO-3225",
        "Allow use of pdsh instead of pssh Both randomwalk (test/system/randomwalk/bin) and continuous ingest (test/system/continuous/bin) rely on pssh to invoke commands across many nodes.\r\n\r\nPdsh seems to me to be a much more common tool. It would be nice if we could modify the scripts to support either.\r\n\r\n{{pssh -h \"hosts.txt\" cmd}} is idential to {{pdsh -w \"^hosts.txt\" cmd}}",
        "Dead code in ConfigCommand A couple of ternary statements in ConfigCommand on sysVal checking for null where sysVal is known to be non-null."
    ],
    [
        "ACCUMULO-3320",
        "ACCUMULO-3584",
        "Replication reference prematurely closed and removed. Saw the following situation in MultiInstanceReplicationIT across GC, tserver and master. 03b6dad5-56c9-4f35-8daf-a444f3252038 appeared to never get cleaned up:\r\n\r\n{noformat}\r\n2014-11-08 12:05:34,898 [tserver.Tablet] DEBUG: Logs to be destroyed: 2<< tserver:37732/file:////.../accumulo/test/target/mini-tests/org.apache.accumulo.test.replication.MultiInstanceReplicationIT_dataWasReplicatedToThePeer/accumulo/wal/tserver+37732/03b6dad5-56c9-4f35-8daf-a444f3252038\r\n2014-11-08 12:05:34,904 [tserver.TabletServer] DEBUG: ScanSess tid 172.31.47.246:52263 !0 0 entries in 0.03 secs, nbTimes = [25 25 25.00 1] \r\n2014-11-08 12:05:34,914 [tserver.Tablet] DEBUG: Recording that data has been ingested into 2<< using [file:////.../accumulo/test/target/mini-tests/org.apache.accumulo.test.replication.MultiInstanceReplicationIT_dataWasReplicatedToThePeer/accumulo/wal/tserver+37732/03b6dad5-56c9-4f35-8daf-a444f3252038, file:////.../accumulo/test/target/mini-tests/org.apache.accumulo.test.replication.MultiInstanceReplicationIT_dataWasReplicatedToThePeer/accumulo/wal/tserver+37732/8866f067-ed63-46ec-9dd1-b8d2e8381af2]\r\n2014-11-08 12:05:34,914 [util.ReplicationTableUtil] DEBUG: Updating replication status for 2<< with [file:////.../accumulo/test/target/mini-tests/org.apache.accumulo.test.replication.MultiInstanceReplicationIT_dataWasReplicatedToThePeer/accumulo/wal/tserver+37732/03b6dad5-56c9-4f35-8daf-a444f3252038, file:////.../accumulo/test/target/mini-tests/org.apache.accumulo.test.replication.MultiInstanceReplicationIT_dataWasReplicatedToThePeer/accumulo/wal/tserver+37732/8866f067-ed63-46ec-9dd1-b8d2e8381af2] using [begin: 0 end: 0 infiniteEnd: true closed: false]\r\n\r\n2014-11-08 12:05:35,008 [replication.CloseWriteAheadLogReferences] INFO : Found 1 WALs referenced in metadata in 100.3 ms\r\n2014-11-08 12:05:35,127 [replication.CloseWriteAheadLogReferences] DEBUG: Closing unreferenced WAL (~replfile:/.../accumulo/test/target/mini-tests/org.apache.accumulo.test.replication.MultiInstanceReplicationIT_dataWasReplicatedToThePeer/accumulo/wal/tserver+37732/03b6dad5-56c9-4f35-8daf-a444f3252038 stat:2 [] 5 false) in metadata table\r\n2014-11-08 12:05:35,221 [replication.CloseWriteAheadLogReferences] INFO : Closed 1 WAL replication references in replication table in 206.2 ms\r\n\r\n2014-11-08 12:05:36,505 [tserver.Tablet] DEBUG: Logs to be destroyed: !0<;~ ip-172-31-47-246:37732/file:////.../accumulo/test/target/mini-tests/org.apache.accumulo.test.replication.MultiInstanceReplicationIT_dataWasReplicatedToThePeer/accumulo/wal/tserver+37732/03b6dad5-56c9-4f35-8daf-a444f3252038\r\n\r\n2014-11-08 12:05:36,517 [replication.StatusMaker] DEBUG: Creating replication status record for file:/.../accumulo/test/target/mini-tests/org.apache.accumulo.test.replication.MultiInstanceReplicationIT_dataWasReplicatedToThePeer/accumulo/wal/tserver+37732/03b6dad5-56c9-4f35-8daf-a444f3252038 on table 2 with [begin: 0 end: 0 infiniteEnd: true closed: true createdTime: 1415448333352].\r\n\r\n2014-11-08 12:06:27,097 [util.ReplicationTableUtil] DEBUG: Updating replication status for 2<< with [file:////.../accumulo/test/target/mini-tests/org.apache.accumulo.test.replication.MultiInstanceReplicationIT_dataWasReplicatedToThePeer/accumulo/wal/tserver+37732/03b6dad5-56c9-4f35-8daf-a444f3252038] using [begin: 0 end: 0 infiniteEnd: true closed: false]\r\n{noformat}\r\n\r\nThis is problematic due to cross-process interaction. The GC happened to run just after the tserver performed a compaction and removed the log reference from the tablet. Thus, at a very small point in time, the WAL was not referenced by any tablets in the metadata table, and, as such, the GC \"closed\" the WAL replication reference.\r\n\r\nThe master saw that it was closed, cleaned up the reference and started replication. The tserver continued to use the WAL (as it does) and placed some more updates into metadata.\r\n\r\nThe other problem is that when the master saw that replication of the file was completed, it removed the references from the replication table (as expected). However, when it went to the next round of StatusMaker, it made a new record which had lost the fact that the old file had been fully replicated.",
        "remove thrift hacks thrift 0.9.1 may have fixed some of the problems we've been working around: not enough buffering, not being able to get the port number from the server socket come to mind.  Remove our hacks to get around the problems.\r\n"
    ],
    [
        "ACCUMULO-3131",
        "ACCUMULO-1545",
        "CyclicReplicationIT doesn't adhere to useSslForIT option CyclicReplicationIT isn't affected by ACCUMULO-3130 because it doesn't set up either instance with SSL. While it's nice that the test doesn't fail, it would be much better to be able to run this test with SSL turned on when requested.",
        "Use UTF-8 constant instead of String API calls that specify UTF-8 using the string form of the argument should instead use [Charset.forName()|http://docs.oracle.com/javase/6/docs/api/java/nio/charset/Charset.html#forName%28java.lang.String%29]. This constant can then be further replaced with [StandardCharsets.UTF_8|http://docs.oracle.com/javase/7/docs/api/java/nio/charset/StandardCharsets.html#UTF_8]."
    ],
    [
        "ACCUMULO-701",
        "ACCUMULO-2722",
        "No longer seeing TApplicationException When an unexpected exception occurred server side, the client used to see a TApplicationException on the client side.  It appears this not happening anymore in trunk since the switch to thrift 0.8.  A lot of code depends on the previous behavior.\r\n\r\nCode that used to throw an exception to the client is now getting stuck indefinitely.  ",
        "Classes outside of shell shouldn't be using commons-cli we still have a need for commons-cli due to ACCUMULO-1497. Besides that (and considerations for our public API), we shouldn't have commons-cli in other packages.\r\n\r\n\r\nPriority is high, because the use of commons-cli in core/**/util/Merge causes compilation to fail on master under the hadoop 1 profile. If that gets fixed otherwise, this can drop to minor."
    ],
    [
        "ACCUMULO-2955",
        "ACCUMULO-1352",
        "Master logs extra warnings about bad table name {noformat}\r\nroot@accumulo> createtable a.b\r\n2014-06-27 07:34:01,338 [shell.Shell] ERROR: org.apache.accumulo.core.client.AccumuloException: Cannot create table in non-existent namespace\r\nroot@accumulo> createtable a\r\nroot@accumulo a> renametable a a.b\r\n2014-06-27 07:34:45,313 [shell.Shell] ERROR: org.apache.accumulo.core.client.AccumuloException: Cannot move tables to a new namespace by renaming. The namespace for a does not match a.b\r\nroot@accumulo a> renametable a .a.b\r\n2014-06-27 07:40:09,238 [shell.Shell] ERROR: org.apache.accumulo.core.client.AccumuloException: Table names must only contain word characters (letters, digits, and underscores): .a.b\r\n{noformat}\r\n\r\nThe second and third ERROR also shows up on the monitor page as a WARN coming from the master. They should behave more like the first one.",
        "filter on !METADATA can prematurely delete loaded flags Comments and commits against the parent ticket (ACCUMULO-1044) address multiple causes.  This substask summarizes one of those causes.\r\n\r\nAccumulo configure a filter on the metadata table that drops inactive bulk load flags.  Inactive flags could be propagated by splitting tablets, and thats why this filter exist.  There is a bug in the way the fitler works.  It may delete active bulk load flags.  If this occurs there is a small possibility that bulk loaded files could be moved.  This would result in accumulo  complaining about missing files.\r\n\r\nA work around for 1.4.0 - 1.4.3 is to remove this filter from the metdata table.\r\n\r\n{noformat}\r\n  deleteiter -majc -n bulkLoadFilter -t !METADATA\r\n{noformat}\r\n\r\nThen periodically delete bulk load flags that are left by splits. Would need to add the filter back once this bug is fixed.\r\n\r\nAll svn commits related to this issue were done using the parent ticket."
    ],
    [
        "ACCUMULO-2585",
        "ACCUMULO-713",
        "WriteAheadLogIT.test times out {noformat}\r\njava.lang.Exception: test timed out after 600000 milliseconds\r\n\tat java.lang.Thread.sleep(Native Method)\r\n\tat org.apache.accumulo.core.util.UtilWaitThread.sleep(UtilWaitThread.java:26)\r\n\tat org.apache.accumulo.core.client.impl.ServerClient.executeRaw(ServerClient.java:113)\r\n\tat org.apache.accumulo.core.client.admin.SecurityOperationsImpl.execute(SecurityOperationsImpl.java:51)\r\n\tat org.apache.accumulo.core.client.admin.SecurityOperationsImpl.changeUserAuthorizations(SecurityOperationsImpl.java:177)\r\n\tat org.apache.accumulo.test.VerifyIngest.verifyIngest(VerifyIngest.java:86)\r\n\tat org.apache.accumulo.test.functional.WriteAheadLogIT.test(WriteAheadLogIT.java:68)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n{noformat}\r\n",
        "Improve Error Message in docs/src/user_manual/build.sh (user manual) If pdflatex is not installed, the error message can be buried in the build text. Also there is no message if the PDF file is already up-to-date. The lack of message is fine during the build process, but if the script is run manually then a feedback message is nice."
    ],
    [
        "ACCUMULO-3343",
        "ACCUMULO-889",
        "EOFException when source switching iterator fires On a high ingest/query cluster (between 10-20 nodes) I see the following -\r\n\r\n{code}2014-11-14 22:08:12,745 [tserver.InMemoryMap] ERROR: Failed to create mem dump file\r\njava.io.EOFException\r\n        at java.io.DataInputStream.readByte(DataInputStream.java:267)\r\n        at org.apache.accumulo.core.file.rfile.RelativeKey.fastSkip(RelativeKey.java:314)\r\n        at org.apache.accumulo.core.file.rfile.RFile$LocalityGroupReader._seek(RFile.java:748)\r\n        at org.apache.accumulo.core.file.rfile.RFile$LocalityGroupReader.seek(RFile.java:607)\r\n        at org.apache.accumulo.core.iterators.system.LocalityGroupIterator.seek(LocalityGroupIterator.java:142)\r\n        at org.apache.accumulo.core.file.rfile.RFile$Reader.seek(RFile.java:979)\r\n        at org.apache.accumulo.core.iterators.WrappingIterator.seek(WrappingIterator.java:101)\r\n        at org.apache.accumulo.tserver.MemKeyConversionIterator.seek(InMemoryMap.java:168)\r\n        at org.apache.accumulo.core.iterators.system.SourceSwitchingIterator._switchNow(SourceSwitchingIterator.java:171)\r\n        at org.apache.accumulo.core.iterators.system.SourceSwitchingIterator.switchNow(SourceSwitchingIterator.java:179)\r\n        at org.apache.accumulo.tserver.InMemoryMap$MemoryIterator.switchNow(InMemoryMap.java:647)\r\n        at org.apache.accumulo.tserver.InMemoryMap$MemoryIterator.access$900(InMemoryMap.java:601)\r\n        at org.apache.accumulo.tserver.InMemoryMap.delete(InMemoryMap.java:746)\r\n        at org.apache.accumulo.tserver.Tablet$TabletMemory.finalizeMinC(Tablet.java:327)\r\n        at org.apache.accumulo.tserver.Tablet.minorCompact(Tablet.java:2068)\r\n        at org.apache.accumulo.tserver.Tablet.access$4300(Tablet.java:170)\r\n        at org.apache.accumulo.tserver.Tablet$MinorCompactionTask.run(Tablet.java:2134)\r\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n        at java.lang.Thread.run(Thread.java:744){code}\r\n\r\nAfter this happens, I also see iterators failing-\r\n\r\n{code} ERROR: exception when running query\r\njava.io.EOFException\r\n        at java.io.DataInputStream.readByte(DataInputStream.java:267)\r\n        at org.apache.accumulo.core.file.rfile.RelativeKey.fastSkip(RelativeKey.java:314)\r\n        at org.apache.accumulo.core.file.rfile.RFile$LocalityGroupReader._seek(RFile.java:748)\r\n        at org.apache.accumulo.core.file.rfile.RFile$LocalityGroupReader.seek(RFile.java:607)\r\n        at org.apache.accumulo.core.iterators.system.LocalityGroupIterator.seek(LocalityGroupIterator.java:142)\r\n        at org.apache.accumulo.core.file.rfile.RFile$Reader.seek(RFile.java:979)\r\n        at org.apache.accumulo.core.iterators.WrappingIterator.seek(WrappingIterator.java:101)\r\n        at org.apache.accumulo.tserver.MemKeyConversionIterator.seek(InMemoryMap.java:168)\r\n        at org.apache.accumulo.core.iterators.system.SourceSwitchingIterator.readNext(SourceSwitchingIterator.java:116)\r\n        at org.apache.accumulo.core.iterators.system.SourceSwitchingIterator.seek(SourceSwitchingIterator.java:162)\r\n        at org.apache.accumulo.core.iterators.WrappingIterator.seek(WrappingIterator.java:101)\r\n        at org.apache.accumulo.core.iterators.SkippingIterator.seek(SkippingIterator.java:37)\r\n        at org.apache.accumulo.core.iterators.WrappingIterator.seek(WrappingIterator.java:101)\r\n        at org.apache.accumulo.core.iterators.system.MultiIterator.seek(MultiIterator.java:105)\r\n        at org.apache.accumulo.core.iterators.WrappingIterator.seek(WrappingIterator.java:101)\r\n        at org.apache.accumulo.core.iterators.system.StatsIterator.seek(StatsIterator.java:64)\r\n        at org.apache.accumulo.core.iterators.WrappingIterator.seek(WrappingIterator.java:101)\r\n        at org.apache.accumulo.core.iterators.system.DeletingIterator.seek(DeletingIterator.java:67)\r\n        at org.apache.accumulo.core.iterators.WrappingIterator.seek(WrappingIterator.java:101)\r\n        at org.apache.accumulo.core.iterators.SkippingIterator.seek(SkippingIterator.java:37)\r\n        at org.apache.accumulo.core.iterators.system.ColumnFamilySkippingIterator.seek(ColumnFamilySkippingIterator.java:123)\r\n        at org.apache.accumulo.core.iterators.WrappingIterator.seek(WrappingIterator.java:101)\r\n        at org.apache.accumulo.core.iterators.Filter.seek(Filter.java:64)\r\n        at org.apache.accumulo.core.iterators.WrappingIterator.seek(WrappingIterator.java:101)\r\n        at org.apache.accumulo.core.iterators.Filter.seek(Filter.java:64)\r\n        at org.apache.accumulo.core.iterators.system.SynchronizedIterator.seek(SynchronizedIterator.java:55)\r\n{code}\r\n\r\nStill waiting on logs related, but when the system gets into this state, all tservers are throwing errors like this (our query pattern uses a lot of batchscanners), but also we see the gc take extremely long times to run, throwing \r\n\r\n{code}[impl.ThriftScanner] DEBUG: Scan failed, thrift error org.apache.thrift.transport.TTransportException  java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected...{code} but I think that is unrelated",
        "Update jar regex in examples and scripts I tried running the example in README.mapred for 1.4.2 and got an error on the command\r\n{noformat}\r\n$ bin/tool.sh lib/examples-simple*[^c].jar ...\r\nException in thread \"main\" java.lang.ClassNotFoundException: lib.examples-simple-1.4.2-sources.jar\r\n{noformat}\r\n\r\nThe jar regex need to be updated to exclude the sources jar (currently it just excludes the javadoc jar).  We should check other jar regex to make sure they're up to date as well."
    ],
    [
        "ACCUMULO-353",
        "ACCUMULO-1807",
        "\"mvn\" should not build tgz When I restructured the pom to add the assembly module, I changed the behavior so that the assembly is always run. This is a bit over the top, and should instead be regulated to a build profile.",
        "continuous ingest verification is broken {noformat}\r\n2013-10-23 17:36:45,124 INFO  [main] mapreduce.Job (Job.java:printTaskEvents(1424)) - Task Id : attempt_1382549682839_0001_m_000015_2, Status : FAILED\r\nError: java.io.IOException: The specified table was not found for id=\r\n\tat org.apache.accumulo.core.client.mapreduce.AbstractInputFormat$AbstractRecordReader.initialize(AbstractInputFormat.java:389)\r\n\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:524)\r\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:762)\r\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:339)\r\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)\r\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)\r\n{noformat}"
    ],
    [
        "ACCUMULO-1687",
        "ACCUMULO-3827",
        "user manual refers to deprecated calls In particular, AccumuloInputFormat.setRegex.",
        "Default store types for monitor SSL are broken Not sure why I left the default types an empty string, because that doesn't actually work.  They should be \"jks\".  A workaround for 1.7.0 is to set the monitor.ssl.keyStoreType and monitor.ssl.trustStoreType properties explicitly."
    ],
    [
        "ACCUMULO-2841",
        "ACCUMULO-1658",
        "Arbitrary namespace and table metadata tags Application-level tags (tagName = tagValue) could be added to tables and namespaces, to allow applications to set application-level metadata about a namespace or table.\r\n\r\nUse cases include management for billing, administrator notes, date created, last ingest time, stats, information about the table's schema... or anything else an application might wish to use tags for.\r\n\r\nThese tags could be stored in zookeeper, but are probably best stored in the metadata table (probably in a separate reserved area of the metadata table, ~tag) because they could be arbitrarily large and do not need to be persisted in memory.\r\n\r\nThis feature would include new APIs to manipulate table / namespace metadata. Considerations should be made to ensure users have appropriate permissions to add tags to an object.\r\n\r\nThis feature could be used to implement ACCUMULO-650.",
        "System integration tests should default ACCUMULO_CONF_DIR to ACCUMULO_HOME/conf The various system integration tests need to make use of ACCUMULO_CONF_DIR and when not present, default it to ACCUMULO_HOME/conf.\r\n\r\n* Functional Tests - adds ACCUMULO_CONF_DIR to the classpath, but grabs all configuration files out of ACCUMULO_HOME/conf/\r\n* Continuous Ingest Test - start-stats.sh, mapred-setup.sh, agitator.pl, and magitator.pl all presume continuous-env.sh will define it\r\n* Random Walk Tests - reset-cluster.sh requires it, start-*.sh will only load accumulo-env.sh files that are within it.\r\n* Scalability Tests - requires it to be defined.\r\n\r\n"
    ],
    [
        "ACCUMULO-1998",
        "ACCUMULO-2868",
        "Encrypted WALogs seem to be excessively buffering The reproduction steps around this are a little bit fuzzy but basically we ran a moderate workload against a 1.6.0 server.  Encryption happened to be turned on but that doesn't seem to be germane to the problem.  After doing a moderate amount of work, Accumulo is refusing to start up, spewing this error over and over to the log:\r\n\r\n{noformat}\r\n2013-12-10 10:23:02,529 [tserver.TabletServer] WARN : exception while doing multi-scan \r\njava.lang.RuntimeException: java.io.IOException: Failed to open hdfs://10.10.1.115:9000/accumulo/tables/!0/table_info/A000042x.rf\r\n\tat org.apache.accumulo.tserver.TabletServer$ThriftClientHandler$LookupTask.run(TabletServer.java:1125)\r\n\tat org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\r\n\tat org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\r\n\tat org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\r\n\tat java.lang.Thread.run(Thread.java:662)\r\nCaused by: java.io.IOException: Failed to open hdfs://10.10.1.115:9000/accumulo/tables/!0/table_info/A000042x.rf\r\n\tat org.apache.accumulo.tserver.FileManager.reserveReaders(FileManager.java:333)\r\n\tat org.apache.accumulo.tserver.FileManager.access$500(FileManager.java:58)\r\n\tat org.apache.accumulo.tserver.FileManager$ScanFileManager.openFiles(FileManager.java:478)\r\n\tat org.apache.accumulo.tserver.FileManager$ScanFileManager.openFileRefs(FileManager.java:466)\r\n\tat org.apache.accumulo.tserver.FileManager$ScanFileManager.openFiles(FileManager.java:486)\r\n\tat org.apache.accumulo.tserver.Tablet$ScanDataSource.createIterator(Tablet.java:2027)\r\n\tat org.apache.accumulo.tserver.Tablet$ScanDataSource.iterator(Tablet.java:1989)\r\n\tat org.apache.accumulo.core.iterators.system.SourceSwitchingIterator.seek(SourceSwitchingIterator.java:163)\r\n\tat org.apache.accumulo.tserver.Tablet.lookup(Tablet.java:1565)\r\n\tat org.apache.accumulo.tserver.Tablet.lookup(Tablet.java:1672)\r\n\tat org.apache.accumulo.tserver.TabletServer$ThriftClientHandler$LookupTask.run(TabletServer.java:1114)\r\n\t... 6 more\r\nCaused by: java.io.FileNotFoundException: File does not exist: /accumulo/tables/!0/table_info/A000042x.rf\r\n\tat org.apache.hadoop.hdfs.DFSClient$DFSInputStream.fetchLocatedBlocks(DFSClient.java:2006)\r\n\tat org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1975)\r\n\tat org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init>(DFSClient.java:1967)\r\n\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:735)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:165)\r\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:436)\r\n\tat org.apache.accumulo.core.file.blockfile.impl.CachableBlockFile$Reader.getBCFile(CachableBlockFile.java:256)\r\n\tat org.apache.accumulo.core.file.blockfile.impl.CachableBlockFile$Reader.access$000(CachableBlockFile.java:143)\r\n\tat org.apache.accumulo.core.file.blockfile.impl.CachableBlockFile$Reader$MetaBlockLoader.get(CachableBlockFile.java:212)\r\n\tat org.apache.accumulo.core.file.blockfile.impl.CachableBlockFile$Reader.getBlock(CachableBlockFile.java:313)\r\n\tat org.apache.accumulo.core.file.blockfile.impl.CachableBlockFile$Reader.getMetaBlock(CachableBlockFile.java:367)\r\n\tat org.apache.accumulo.core.file.blockfile.impl.CachableBlockFile$Reader.getMetaBlock(CachableBlockFile.java:143)\r\n\tat org.apache.accumulo.core.file.rfile.RFile$Reader.<init>(RFile.java:825)\r\n\tat org.apache.accumulo.core.file.rfile.RFileOperations.openReader(RFileOperations.java:79)\r\n\tat org.apache.accumulo.core.file.DispatchingFileFactory.openReader(FileOperations.java:119)\r\n\tat org.apache.accumulo.tserver.FileManager.reserveReaders(FileManager.java:314)\r\n\t... 16 more\r\n{noformat}\r\n\r\nHere's some other pieces of context:\r\n\r\nHDFS contents:\r\n\r\n{noformat}\r\nubuntu@ip-10-10-1-115:/data0/logs/accumulo$ hadoop fs -lsr /accumulo/tables/\r\ndrwxr-xr-x   - accumulo hadoop          0 2013-12-10 00:32 /accumulo/tables/!0\r\ndrwxr-xr-x   - accumulo hadoop          0 2013-12-10 01:06 /accumulo/tables/!0/default_tablet\r\ndrwxr-xr-x   - accumulo hadoop          0 2013-12-10 10:49 /accumulo/tables/!0/table_info\r\n-rw-r--r--   5 accumulo hadoop       1698 2013-12-10 00:34 /accumulo/tables/!0/table_info/F0000000.rf\r\n-rw-r--r--   5 accumulo hadoop      43524 2013-12-10 01:53 /accumulo/tables/!0/table_info/F000062q.rf\r\ndrwxr-xr-x   - accumulo hadoop          0 2013-12-10 00:32 /accumulo/tables/+r\r\ndrwxr-xr-x   - accumulo hadoop          0 2013-12-10 10:45 /accumulo/tables/+r/root_tablet\r\n-rw-r--r--   5 accumulo hadoop       2070 2013-12-10 10:45 /accumulo/tables/+r/root_tablet/A0000738.rf\r\ndrwxr-xr-x   - accumulo hadoop          0 2013-12-10 00:33 /accumulo/tables/1\r\ndrwxr-xr-x   - accumulo hadoop          0 2013-12-10 00:33 /accumulo/tables/1/default_tablet\r\n{noformat}\r\n\r\nZooKeeper entries\r\n\r\n{noformat}\r\n[zk: localhost:2181(CONNECTED) 6] get /accumulo/371cfa3e-fe96-4a50-92e9-da7572589ffa/root_tablet/dir \r\nhdfs://10.10.1.115:9000/accumulo/tables/+r/root_tablet\r\ncZxid = 0x1b\r\nctime = Tue Dec 10 00:32:56 EST 2013\r\nmZxid = 0x1b\r\nmtime = Tue Dec 10 00:32:56 EST 2013\r\npZxid = 0x1b\r\ncversion = 0\r\ndataVersion = 0\r\naclVersion = 0\r\nephemeralOwner = 0x0\r\ndataLength = 54\r\nnumChildren = 0\r\n{noformat}\r\n\r\nI'm going to preserve the state of this machine in HDFS for a while but not forever, so if there are other pieces of context people need, let me know.",
        "Make master configurable in when it kills tablet servers On a cluster with a flaky network, the master may be unable to contact a tserver for some moderate amount of time and then direct it to terminate, even though the tserver is still up. (See {{gatherTableInformation()}} and {{StatusThread}}. It does not appear possible to configure the master to be more forgiving in these checks. Relevant constants:\r\n\r\n* {{DEFAULT_WAIT_FOR_WATCHER}} - interval between server checks\r\n* {{MAX_BAD_STATUS_COUNT}} - the maximum number of failed attempts allowed before killing the tserver\r\n\r\nMaking one or both of those configurable, or some other pertinent parameter configurable, would allow cluster admins to cope with mild network maladies. "
    ],
    [
        "ACCUMULO-3327",
        "ACCUMULO-3366",
        "tablet server re-reads the bulk loaded flags with every bulk import request On a very large cluster, which bulk loads many thousands of files every few minutes, I noticed the servers would reload the bulk imported flags with every request.  This put a lot of pressure on the accumulo.metadata table, and it just isn't necessary: the tablet should be tracking which bulk import files it has loaded, except when it first loaded.",
        "Ensure ITs can run against cluster with SSL Need to make sure that there isn't anything extra that needs to be implemented to run the ITs against an existing cluster."
    ],
    [
        "ACCUMULO-1063",
        "ACCUMULO-2347",
        "Update pom.xml to reference Commons IO v2.4 (or change Instamo to remove dependency) The MapReduceExample (for MiniAccumuloCluster) seems to depend on Commons IO v2.4 but the current compilation places Commons IO v1.2 in the lib directory.",
        "Design infrastructure for passing scan stats to compaction strategies In order to implement ACCUMULO-1266, using compaction strategies introduced in ACCUMULO-1451 seems to be the way to go.  However its not enough, the compaction strategy will need statistics from scans inorder to make decisions.   Need to come up with a design for infrastructure to collect and pass this information. "
    ],
    [
        "ACCUMULO-3904",
        "ACCUMULO-500",
        "Drop CHANGES file to make releasing easier Per [the discussion|http://mail-archives.us.apache.org/mod_mbox/accumulo-user/201506.mbox/%3CCAL5zq9b7sdSX6WOzCeiwoQV%3DW2jZ6U2Gz-kJTGDo8CHMkGGqUQ%40mail.gmail.com%3E] on the mailing lists, we should drop the CHANGES files committed to SCM until such time as somebody can both justify their existence *and* is willing to volunteer the effort to automate them (ideally such automation would involve generating a report for the binary tarball only from JIRA at the time of release, and not committing the CHANGES to SCM at all).",
        "Need mechanism for init to work with empty /accumulo directory I don't know if this should be done for 1.4.1 or 1.5.0, but this is primarily to help support ACCUMULO-404. In a secured installation, there is the assumption that the accumulo user only has rwx access in the /accumulo directory and it's children. Unfortunately, Accumulo only knows how to init with there being no /accumulo directory. While I understand we don't want to worry about clobbering the directory, we should provide a mechanism to init on top of an existing, but empty, /accumulo hdfs directory. This will allow the SA to create an accumulo directory and provide perms appropriately for the accumulo user, rather than having to grant write at the root level and then revoking that permission after initting."
    ],
    [
        "ACCUMULO-3096",
        "ACCUMULO-1529",
        "Scans stuck and seeing error message about constraint violation Just helped someone debug an issue. Their scans were getting stuck on a certain tserver (determined tserver by turning on debug in shell).  On the tserver, there was a contant stream of messages about a metadata table contstraint violate because {{Bulk load transaction no longer running}}.\r\n\r\nThe following code in {{Tablet.importMapFiles()}} \r\n\r\n{code:java}\r\n          synchronized (timeLock) {\r\n            if (bulkTime > persistedTime)\r\n              persistedTime = bulkTime;\r\n\r\n            MetadataTableUtil.updateTabletDataFile(tid, extent, paths, tabletTime.getMetadataValue(persistedTime), creds, tabletServer.getLock());\r\n          }\r\n{code}\r\n\r\nEnded up calling the following code in {{MetadataTableUtil}}.  \r\n\r\n{code:java}\r\npublic static void update(Credentials credentials, ZooLock zooLock, Mutation m, KeyExtent extent) {\r\n    Writer t = extent.isMeta() ? getRootTable(credentials) : getMetadataTable(credentials);\r\n    if (zooLock != null)\r\n      putLockID(zooLock, m);\r\n    while (true) {\r\n      try {\r\n        t.update(m);\r\n        return;\r\n      } catch (AccumuloException e) {\r\n        log.error(e, e);\r\n      } catch (AccumuloSecurityException e) {\r\n        log.error(e, e);\r\n      } catch (ConstraintViolationException e) {\r\n        log.error(e, e);\r\n      } catch (TableNotFoundException e) {\r\n        log.error(e, e);\r\n      }\r\n      UtilWaitThread.sleep(1000);\r\n    }\r\n\r\n  }\r\n{code}\r\n\r\nSo when the constraint failed, it retried forever.   It did this while holding timeLock, which in turn prevented compactions from completing, which eventually gummed up scans.",
        "FormatterCommandTest fails in Eclipse "
    ],
    [
        "ACCUMULO-1170",
        "ACCUMULO-1000",
        "Memory map in unexpected state : nextMutationCount = 8342267 mutationCount = 8336164 Found during testing 1.4.3rc1",
        "support compare and set Add support to mutation for compare and set operations.  This would allow user to specify that a row must contain certain data for a mutation to be applied."
    ],
    [
        "ACCUMULO-1751",
        "ACCUMULO-2293",
        "SimpleMacIT test for Multitable Input Format ",
        "AccumuloSecurityException might be thrown instead of TableNotFoundException on flush or clone When examining findbugs, I found that in the face of a {{ThriftSecurityException}} on {{TableOperationsImpl._flush}}, we check against the wrong {{SecurityErrorCode}} and would incorrectly throw an {{AccumuloSecurityException}} instead of a {{TableNotFoundException}} when the {{ThriftSecurityException}} contains a {{SecurityErrorCode.TABLE_DOESNT_EXIST}}."
    ],
    [
        "ACCUMULO-3984",
        "ACCUMULO-2268",
        "Update Download Links Update all pointers from www.apache.org/dyn/closer.cgi to closer.lua instead.",
        "Use conditional mutations to update metadata table For correctness Accumulo requires that only one tablet server at a time serve a tablet.   In order to enforce this constraint, Accumulo uses zookeeper locks.  It's assumed when a tablet server lock disappears that the tablet server will kill itself.  Therefore a tablet that's assigned to a dead tablet server can be safely reassigned.  However sometimes tablet servers continue to operate for a period of time after losing their locks.  Sometimes this is caused by bugs in Accumulo, sometimes it's the Java GC or swapping (and the tserver does die), sometimes it's problems with zookeeper (like the zk thread that reports lock lost dies).\r\n\r\nIn Accumulo 1.6 conditional mutations were added.  Making all tablet metadata updates use conditional mutations could make multiply-assigned tablets less able to do damage.   \r\n\r\nFor example if after a minor compaction, the metadata update mutation could require the tablet location to be the current tserver: it would prevent a zombie tserver from adding an extraneous file to the metadata table for a tablet.\r\n\r\n[~ctubbsii] has discussed refactoring all metadata code so that its more modular and works with zookeeper (for root tablet) and metadata table using same API.  This solution could depend on that.  It may also be useful to make the root tablet operate more like a regular tablet and store its list of files in zookeeper.  Then the root tablet could benefit from these changes with the right abstraction layer."
    ],
    [
        "ACCUMULO-2124",
        "ACCUMULO-3485",
        "fix minor typos and spelling in ARS There are some nice comments in the Accumulo Reservation System example.  Clean them up with trivial formatting, punctuation and spelling.",
        "accumulo client tries to connect only to the first ZK in the list, making it SPOF Accumulo client tries to connect only to the first ZK server.\r\nIf that single ZK server is down, the accumulo client does not try other ZK servers.\r\nThus the first ZK server is the single point of failure.\r\nIf it is down, accumulo client cannot work\r\n\r\n{code:java}\r\n  val instance = new ZooKeeperInstance(\"zzz\", \"no-zk-here.example.com,zk1.example.com,zk2.example.com,zk3.example.com\")\r\n  val conn = instance.getConnector(\"root\", \"pass\".getBytes)\r\n  println(\"db connected, tables=\" + conn.tableOperations.list)\r\n{code}"
    ],
    [
        "ACCUMULO-1932",
        "ACCUMULO-969",
        "Don't rely on Hadoop \"convenience\" scripts for agitator The start-dfs.sh script isn't always guaranteed to be present on a system. Using the hadoop-daemon.sh script should be a bit more portable for purposes of datanode agitation. 1.4 doesn't have datanode agitation in the current agitator.pl script",
        "commit the proxy for 1.5 "
    ],
    [
        "ACCUMULO-3761",
        "ACCUMULO-1198",
        "RowEncodingIterator should take a maximum buffer size parameter It would be nice if the RowEncodingIterator and its subclasses could specify a maximum buffer size similar to the TransformingIterator.\r\n\r\nDiscussion [here|http://www.mail-archive.com/dev%40accumulo.apache.org/msg09821.html]",
        "Accumulo trace visualization The Accumulo monitor will show the recent traces to the user, typically for minor and major compactions. This shows a basic tabbed hierarchy of each action performed and the duration of the invocation. There is often some additional data with context about what the action was performing (for example, the tablet which was compacted).\r\n\r\nIt would be nice to have a nice graph/chart which displays the same information but in a much prettier format.\r\n\r\nBasic Java and HTML/Javascript understanding required."
    ],
    [
        "ACCUMULO-158",
        "ACCUMULO-1974",
        "Add aggregator adds versioning iterator to the iterator tree. Due to a small foible, add aggregator will also add versioning iterators to the iterator tree. This has already been fixed in 1.4, but this is a note that it's a minor issue in 1.3.5 that we're going to let go. There are work arounds in place should it be an issue. If we end up going to 1.3.6, we can talk about it, but as far as I'm concerned, the issue is documented and resolved.",
        "Javadoc: core/conf Author javadoc for org.apache.accumulo.core.conf classes."
    ],
    [
        "ACCUMULO-2401",
        "ACCUMULO-2468",
        "client code cannot create an RFile without logging a warning Examining the output of the SimpleProxyIT, I noticed the test was always printing a message about not being able to access accumulo-site.xml.  This is coming from a call to AccumuloConfiguration.getSiteConfiguration() buried down in BCFile.Writer for the encryption stuff.\r\n\r\nClient-side code should not be calling getSiteConfiguration().\r\n",
        "Improve unit test coverage on o.a.a.core.data Examine and improve unit test coverage for the classes in o.a.a.core.data"
    ],
    [
        "ACCUMULO-2932",
        "ACCUMULO-1972",
        "Increased control over write durability ACCUMULO-2842 introduced the ability to configure the HDFS method that is use to sync WALs. This was mainly done as a means to investigate the performance characteristic of hflush and hsync. As such, it was was trivially done system-wide.\r\n\r\nHowever, there is also practical application to allowing this configuration, in addition to some internal concerns.\r\n\r\n# We should *always* sync WALs involved in {{accumulo.metadata}}\r\n# Data loss may be acceptable on some user tables and not on others (we may want to use hflush for some tables, hsync for other). The option must have the granularity to specify at least on the table (if not locality group).\r\n\r\nWhen multiple tablets are using a WAL with differing durability guarantees, we should choose the higher durability.\r\n\r\nIf we compare this to HBase, we could also implement it on the per Mutation level, however that's beyond the scope here.",
        "Range constructors call overridable method Several {{Range}} constructors call {{Range.beforeStartKey()}}, which is not final. This is dangerous:\r\n\r\nbq. The superclass constructor runs before the subclass constructor, so the overriding method in the subclass will get invoked before the subclass constructor has run. If the overriding method depends on any initialization performed by the subclass constructor, the method will not behave as expected.  ??Item 17, Effective Java Vol. 2, Bloch??\r\n\r\nIf {{beforeStartKey()}} cannot be made final, the code should be refactored to make the constructors safe."
    ],
    [
        "ACCUMULO-2417",
        "ACCUMULO-1306",
        "MetadataIT fails Nightly integration test run failed:\r\n\r\n{noformat}\r\njava.lang.AssertionError\r\n\tat org.junit.Assert.fail(Assert.java:86)\r\n\tat org.junit.Assert.assertTrue(Assert.java:41)\r\n\tat org.junit.Assert.assertTrue(Assert.java:52)\r\n\tat org.apache.accumulo.test.functional.MetadataIT.mergeMeta(MetadataIT.java:103)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:622)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n{noformat}\r\n\r\nLooking at the test, I'm not sure why we're checking that the delete section of the root table has entries after a merge.",
        "Mock does not implement merging Currently merge just errors if table doesn't exist. Once this is implemented, the proxy's merge test can be re-added as a test."
    ],
    [
        "ACCUMULO-3785",
        "ACCUMULO-191",
        "Syntax error in DOAP file release section DOAP files can contain details of multiple release Versions, however each must be listed in a separate release section, for example:\r\n\r\n<release>\r\n      <Version>\r\n        <name>Apache XYZ</name>\r\n        <created>2015-02-16</created>\r\n        <revision>1.6.2</revision>\r\n      </Version>\r\n</release>\r\n<release>\r\n      <Version>\r\n        <name>Apache XYZ</name>\r\n        <created>2014-09-24</created>\r\n        <revision>1.6.1</revision>\r\n      </Version>\r\n</release>\r\n\r\nPlease can the project DOAP be corrected accordingly?\r\n\r\nThanks.",
        "system scope libthrift dependency breaks transitive dependencies libthrift-0.3.jar is not available in any public maven repository, so we included it in the contrib directory and referred to it as a system scope dependency. However, building client that includes accumulo-core as a dependency results in:\r\n\r\n{noformat}\r\n[WARNING] The POM for org.apache.accumulo:accumulo-core:jar:1.3.5-incubating-SNAPSHOT is invalid, transitive dependencies (if any) will not be available, enable debug logging for more details\r\n{noformat}\r\n\r\nThe system scope dependency in the current pom.xml refers to a path that is not an absolute path when the pom.xml is referenced by external projects.\r\n\r\nOne solution would be to host libthrift-0.3 in a semi-private repository (e.g. incubator.apache.org/accumulo/mvn_repo), and change the pom to include that repository with libthrift version 0.3 with a compile scope.\r\n\r\nThis is only a 1.3 branch problem, since 1.4 and later use a thrift version that is in the apache maven repository."
    ],
    [
        "ACCUMULO-1386",
        "ACCUMULO-1180",
        "make it easy to run a single node accumulo instance It would be very useful if users could be up and running with a single node Accumulo instance in minutes w/o setting up Hadoop or Zookeeper.",
        "Makefile cannot be used to build just 32 or just 64 bit versions The way the nativemap Makefile is written, there is no way to use it to generate just one of the libraries. There is the one question of why we generate both, the other issue is that we artificially bind the two of them together with the way the Makefile is written."
    ],
    [
        "ACCUMULO-1987",
        "ACCUMULO-2139",
        "Incorrect handling of auth byte sequences in TabletServer In TabletServer.java: 667\r\n\r\nreturn security.userHasAuthorizations(credentials, Collections.<ByteBuffer> singletonList(ByteBuffer.wrap(****auth.getBackingArray()****)));\r\n\r\n(Emphasis mine obviously)\r\n\r\nThat getBackingArray() will return the whole array even when the auth object has limits set upon it.  That has the effect of passing labels to userHasAuthorization() that are incorrect.  For instance, if your label expression has & and | in it, it will pass the entire string as the label string, as opposed to just one part of it in certain parts of the parsing.\r\n\r\nThe fix is to also use the auth.offset() and auth.length() parameters when building the ByteBuffer.  Patch coming.",
        "Create list of features which need manual verification If an issue like ACCUMULO-2036 can not be tested automatically, then maybe it should be added to a list of things that must be verified before release.   Where should this list go and what else should be on it?  \r\n\r\nUpgrade seems like another candidate for the list."
    ],
    [
        "ACCUMULO-3631",
        "ACCUMULO-1437",
        "Exclude 'slf4j' artifacts from classpath in default value for general.classpaths Was testing out some Ambari integration for Accumulo that [~billie.rinaldi] and [~mwaineo] have been working on (AMBARI-5265) and found that, despite accumulo-site.xml having jars starting with slf4j excluded from the classpath, the shell would complain about duplicate slf4j-log4j12 jars on the classpath.\r\n\r\nTurns out, because access to accumulo-site.xml was restricted (and we only had client.conf to use), we fell back on the default value for general.classpaths defined in AccumuloClassLoader. A short-term fix is to update the value there to match what's in our site template.\r\n\r\nI'll add another issue for a long term fix to add classpath support to client configuration.",
        "XML/JSON output from monitor does not differentiate running and queued scans A TServer that is bogged down with compactions/scans can slow down user queries.\r\n\r\nIn trying to consume some output from the monitor to try to find when this happens, I noticed that XML/JSON output adds the running scans and queued scans together and creates one element from them.\r\n\r\nIt would be nice to have the same breakdown for scans as compactions have."
    ],
    [
        "ACCUMULO-27",
        "ACCUMULO-3892",
        "issues found during scale random-walk testing Experienced several small problems while testing accumulo on cluster using random walk testing:\r\n * zookeeper lock was not removed for a hung tablet server (see ACCUMULO-16)\r\n * spurious message about a merge not in progress\r\n * NPE during a trivial merge\r\n * balancer would not run when a tablet server was unresponsive\r\n * NPE reading permissions from a user being deleted\r\n * Time duration of zero in \"list scans\" output had html mark-up\r\n * Tables were created without the Versioning Iterator\r\n * The example policy file needs to allow accumulo classes to remove files in /tmp (memory dumps can be put in /tmp for isolation purposes)\r\n",
        "Problematic ITs There are a few tests that I've regularly seem not want to pass during nightly automated testing.\r\n\r\n{noformat}\r\nE AssertionError: test timed out after 300000 milliseconds\r\nE java.lang.Exception: test timed out after 300000 milliseconds\r\nE at java.lang.Thread.sleep(Native Method)\r\nE at org.apache.accumulo.core.util.UtilWaitThread.sleep(UtilWaitThread.java:27)\r\nE at org.apache.accumulo.core.client.impl.TableOperationsImpl.waitForTableStateTransition(TableOperationsImpl.java:1159)\r\nE at org.apache.accumulo.core.client.impl.TableOperationsImpl.online(TableOperationsImpl.java:1223)\r\nE at org.apache.accumulo.test.AssignmentThreadsIT.testConcurrentAssignmentPerformance(AssignmentThreadsIT.java:77)\r\n{noformat}\r\n\r\n{noformat}\r\nE AssertionError: test timed out after 90000 milliseconds\r\nE java.lang.Exception: test timed out after 90000 milliseconds\r\nE at sun.misc.Unsafe.park(Native Method)\r\nE at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)\r\nE at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1033)\r\nE at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1326)\r\nE at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:282)\r\nE at org.apache.accumulo.core.client.impl.TableOperationsImpl.addSplits(TableOperationsImpl.java:388)\r\nE at org.apache.accumulo.test.BalanceFasterIT.test(BalanceFasterIT.java:60)\r\n{noformat}\r\n\r\n{noformat}\r\nE AssertionError: test timed out after 360000 milliseconds\r\nE java.lang.Exception: test timed out after 360000 milliseconds\r\nE at java.lang.Object.wait(Native Method)\r\nE at java.lang.Thread.join(Thread.java:1281)\r\nE at java.lang.Thread.join(Thread.java:1355)\r\nE at org.apache.accumulo.test.InterruptibleScannersIT.test(InterruptibleScannersIT.java:98)\r\n{noformat}\r\n\r\n{noformat}\r\nE AssertionError: test timed out after 1440000 milliseconds\r\nE java.lang.Exception: test timed out after 1440000 milliseconds\r\nE at java.lang.Object.wait(Native Method)\r\nE at java.lang.Object.wait(Object.java:503)\r\nE at java.lang.UNIXProcess.waitFor(UNIXProcess.java:263)\r\nE at org.apache.accumulo.test.functional.MetadataMaxFilesIT.test(MetadataMaxFilesIT.java:84)\r\n{noformat}\r\n\r\nThis is with a quadrupled timeout (if not 8x by now). I know these tests all _can_ pass (as I ensured that doing 1.7.0 testing), but apparently something isn't good enough when running on a EC2 m1.xlarges IIRC. I believe all of these tests are stretching the bounds on what is really suitable for an integration test, IMO. Perhaps there is a better way to write the test and verify the correctness that we want expect to see, but these tests are definitely not reliably testing what we hope they are for me."
    ],
    [
        "ACCUMULO-1101",
        "ACCUMULO-648",
        "Inconsistent use of quotation marks in Accumulo documentation On the Table Configuration page, under Delete Range, all the dates have an inconsistent use of quotation marks.",
        "Test/Fix compatability with Zookeeper-3.4.x I start zookeeper with zoo-start.sh\r\n\r\nI can get to the zookeeper command line with zkCli.sh\r\n\r\nHowever, I run accumulo.sh init and then enter an instance name of kgbCloudbase\r\n\r\nthe init then hangs indefinitely.\r\n\r\nI've went into the conf folder and granted \r\n\r\npermission java.security.AllPermission;\r\n\r\n\r\nCan anybody help debug why this is hanging ?"
    ],
    [
        "ACCUMULO-1729",
        "ACCUMULO-32",
        "ThriftTransport pool does not include ssl options in cache key This ticket was created based on the following [comment|https://issues.apache.org/jira/browse/ACCUMULO-1009?focusedCommentId=13772055&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13772055] from [~mberman] in ACCUMULO-1009. I copied the comment in case the link comment stops working.  \r\n\r\n\r\n{quote}\r\nMultiple ZooKeeperInstances in the same java process with different SSL config (is this possible? It looks like you included ssl options in the Key in ThriftTransportPool?) This could happen if a single process connected to multiple Accumulo instances\r\n{quote}\r\n\r\nGenerally this should be fine.  The cached transports are keyed on (location, timeout, sslEnabled), so if you're connecting to multiple instances from the same process, they should have different locations anyway, so the different SSL settings will be segregated.  One potential area for concern is that I'm only using the sslEnabled flag, not the full set of SSL parameters, so if you have connected successfully with some cert, and then in the same process you try to connect with a different cert, you could get a cached, connected transport, even though you might not otherwise trust the remote server (or you might have an invalid client cert, if that's turned on).  It seemed to me like this risk was pretty minimal, since you're already in the same process, but if others think it's too big a risk, it would be easy to add all the SSL params to the key.",
        "Clean up bin dir The accumulo bin dir has evolved over time, and has built up some cruft.  It needs to be cleaned up.\r\n\r\nNeed to remove zoo scripts, inspect, netblocked, install.\r\n\r\nNeed to add documentation to the top of check-slaves."
    ],
    [
        "ACCUMULO-3904",
        "ACCUMULO-3037",
        "Drop CHANGES file to make releasing easier Per [the discussion|http://mail-archives.us.apache.org/mod_mbox/accumulo-user/201506.mbox/%3CCAL5zq9b7sdSX6WOzCeiwoQV%3DW2jZ6U2Gz-kJTGDo8CHMkGGqUQ%40mail.gmail.com%3E] on the mailing lists, we should drop the CHANGES files committed to SCM until such time as somebody can both justify their existence *and* is willing to volunteer the effort to automate them (ideally such automation would involve generating a report for the binary tarball only from JIRA at the time of release, and not committing the CHANGES to SCM at all).",
        "fate command not included in shell help The shell fate command is not listed in the general help.\r\n"
    ],
    [
        "ACCUMULO-1423",
        "ACCUMULO-1247",
        "Add include for **/*.jnilib in the binary-release.xml for tar.gz artifact Building a tarball on Mac with the native profile does not include the actual map lib.",
        "Make master handle tablet with multiple locations If a tablet as multiple locations in the metadata table, the the master should take no action on that tablet. I think the master may check for a tablet with future and loc, but a tablet with loc and loc.    The total number of all loc and future entries for a tablet should be zero or one.  You can have no entries, one loc or one future, corresponding to unassigned, hosted or assigned, respectively."
    ],
    [
        "ACCUMULO-1974",
        "ACCUMULO-1498",
        "Javadoc: core/conf Author javadoc for org.apache.accumulo.core.conf classes.",
        "Create document to outline step to Git transition We need to outline both how we plan to use Git (for those who are not familiar with it) and the steps necessary to get the infrastructure in place."
    ]
]